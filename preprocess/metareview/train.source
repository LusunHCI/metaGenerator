Reject. rating score: 1. rating score: 3. rating score: 3. <BRK>I found the presentation of this paper is rather bad. The structure of the paper is quite strange. The Introduction section contains a lot of stuffs that I believe should be moved to the preliminary or method sections. For example, when defining the curriculum learning problem in eq.2 and eq.3, are the f s the same? However, with the current stage of writing, I cannot recommend acceptance.<BRK>*Revision after author response*I thank the authors for the comments on my questions. Unfortunately, I do not feel that these comments addressed my main concerns. al, that was mentioned in the paper). The paper proposes a curriculum learning approach that relies on a new metric, the dynamic instance hardness (DIH). Moreover, it would be good to see an analysis of how sensitive the results are to this choice.<BRK>E..g a) the role of the function  f  that is being maximized in section 3.2 is not clear. The main contributions and pros of the paper are1. 2.An objective function for characterizing instance hardness as a dynamic subset selection problem.
Accept (Poster). rating score: 8. rating score: 8. rating score: 6. <BRK>This paper presents a new approach to representing sets of inputs and a strategy for set auto encoders. I found the work to be well presented, the experiments to be strong, and I think it will be interesting to the community. The decoder uses a similar trick to expand a latent value back to input set size, and then leverages the argsort from the input to re permute the expanded set. They point out that this helps to avoid discontinuities otherwise caused by the  responsibility problem , i.e.which feature is responsible to describe which input element[s].<BRK>This paper proposes to make permutation equivariant set autoencoders, by introducing a permutation invariant encoder (based on feature wise sorting) and at the same time an  inverse  operation (that undoes the sorting) in the decoder. I would appreciate to tone down in this paper, the "discovery" of it as a main contribution. e. The way you describe the responsibility problem (discontinuity) is very hand wavy.<BRK>The authors point out an interesting problem that the responsibility between target and input is not continuous and propose an FSPool method to alleviate this problem. Secondly, the authors make a comparison between FSPool and sum pool, average pool, and max pool, however, what about the weighted sum pool? But please respond at my concerns and I ll change the score accordingly.
Accept (Poster). rating score: 6. rating score: 6. rating score: 3. <BRK>(Wu & Dredze, 2019) and (Pires et al., 2019) identified the cross lingual success of the model and tried to understand it. Pires et al.(2019) also showed that structural similarity is crucial for cross lingual transferWhat are the main contributions of the paper? Novel findings about the effect of network architecture, input representation and learning objective on cross lingual ability of M BERTMethodology that facilitates the analysis of similarities between languages and their impact on cross lingual models by mapping English to a Fake English language, that is identical in all aspects to English but shares no word pieces with any target language. (see above for more details)Pires et al.(2019) did  study linguistics properties and similarities of target and source languages for Multilingual BERT and had similar findings as this work. QuestionsWhat kind of difference in the numbers is considered significant by the authors ?<BRK>This paper evaluates the cross lingual effectiveness of Multilingual BERT along three dimensions:  Linguistics  Architecture  Input and learning objectiveIn each of these three dimensions, the authors run experiments to test why BERT is effective at cross lingual transfer. The fine tuning is done on language, and the zero shot performance is tested on the other one. They end with an experiment that shows that the cross lingual effectiveness drops significantly when the premise and hypothesis are in different languages. This is a good motivating experiment to end the paper on. While the two language setting is easier to experiment with, I wonder how these conclusions will change if they were repeated with the 100+ language version. Please explicitly state the sentencepiece or wordpiece setup in a central piece. This should be compensated for.<BRK>Fake English: English, but with the Unicode codes of English characters all shifted by a large constant so that there is no overlap between Fake English characters and those of actual languages, but the language internal structure remains that of English. C4.Testing cross language entailment on XNLI by B BERT shows that there is a large reduction in performance when the hypothesis and premises sentences are from different languages. The task is Cross Lingual NLI (XNLI) or Cross Lingual NER. Aside from the invention of Fake English, which as far as I know is original and a clever approach to assessing the importance of token overlap in cross language transfer, the other contributions are reporting results of mechanical changes.
Reject. rating score: 1. rating score: 3. rating score: 3. <BRK>3.The results are too good to be true. It achieves surprisingly good results on benchmark datasets. To justify the experiments, the authors need to do a lot of ablation study, such as comparing with supervised learning version of this model, while in the paper there is no ablation model to explain it.<BRK>In this paper, the authors developed a graph embedding method called U2GAN based on self attention mechanism.<BRK>The submission proposes a graph neural network based on propagation with the attention mechanism. If this is the case, why not just say the output is h_v1?
Reject. rating score: 1. rating score: 1. rating score: 1. <BRK>The authors claim that the barcodes constitute a representation of target objectives that is invariant under homeomorphisms of input to the objectives. The authors present an algorithm for computing the barcodes from graph based representation of a surface, and present barcodes computed on toy examples in numerical analysis. In my opinion, the main contribution of the work i.e.creation of barcodes is based on a rather trivial idea. I therefore struggle to see how the idea can be practically significant. Can the authors further clarify why it is a significant finding for them?<BRK>Overall, the paper is very hard to read, as different concepts and terminology appear all over the place without a precise definition (see comments below). The dimensionality of the parameter space? The paper is also unclear in many parts. At least the proof in the appendix seems to be .<BRK>The main experiments are on extremely tiny neural networks, presumably owing to computational restrictions. I believe the concept of barcodes will be new to most members of the ICLR community (at least it was to me), and I appreciate the authors  effort to convey the ideas through multiple definitions in Section 2. I was also unable to fully comprehend the definitions of "birth" and "death" in this context. Overall, I think there may be some really nice ideas in this paper that could help shape our understanding of neural network loss surfaces, but the current paper does not explore those ideas fully and does not convey them in a sufficiently clear manner.
Reject. rating score: 3. rating score: 3. rating score: 3. <BRK>This paper leverage probabilistic inference techniques to maintain a posterior distribution over the parameters of a variational autoencoder (VAE). The contributions are:  A Bayesian VAE model which uses state of the art Bayesian inference techniques to estimate a posterior distribution over the decoder parameters. A description of how this model can be used to detect outliers both in input space and in the model’s latent space. Results showing that this approach outperforms state of the art outlier detection methods. The paper is well written, and the proposed ideas are well motivated. The authors should at least use one more dataset such as CIFAR10. They just use FashionMNIST vs MNIST FashionMNIST (held out).<BRK>The paper advocates to use information gain to detect whether a sample is out of distribution. To that end, a Bayesian VAE is introduced for which that quantity is tractable. The experiments show a solid improvement over previous methods. The experiments report results on the likelihood based score. I think the section needs to be removed and be revisited in future work. I think the authors should back up that this is sufficient to represent the posterior.<BRK>This paper studies the problem of out of distribution data detection, which is an important problem in machine learning. The authors propose to use Bayesian variational autoencoder which applies SGHMC to get samples of the weights of the encoder and the decoder. The proposed method is tested on two benchmarks to demonstrate effectiveness. The proposed Bayesian variational autoencoder appears to be technically sound. When applying it to OoD detection, effective sample size is used to quantify how much the posterior changes given the new data. How to determine whether a data is out of distribution or not based on ESS? Is there any reason for doing this?
Reject. rating score: 1. rating score: 1. rating score: 3. <BRK>This paper proposes a new RL based algorithm for solving the traveling salesman problem (TSP). Its main component is the combination of OR based 2 opt search and learning based k opt search. Monte Carlo tress search is employed to train the learning based k opt search. The experimental result suggests state of the art performance over existing RL based solvers. My main criticism is about the positioning of the paper in the literature. In particular, one could even assert that the proposed algorithm is an instance of the Tabu search method [1], which is based on keeping a record of actions taken and penalizing to revisit it. I would suggest the authors to avoid misleading the readers by including this fact in the paper. I also suggest providing the number of iterations it took to achieve the reported results since one could compare it to other local search methods.<BRK>In particular, MCTS is used to explore a large neighbourhood. The authors present their approach and evaluate it empirically. The presented approach is interesting; a few details could be described in moredetail and motivated better (for example how the particular functional form forestimating the potential Z of an edge was chosen). This makes itunnecessarily hard to compare results. Finally, the instances used to evaluate the approach seem relatively easy. In summary, I feel that the paper cannot be accepted in its current form.<BRK>The paper proposes an algorithm for the Travelling Salesman Problem that starts with a random tour and iteratively improves it using 2 opt local search, followed by Monte Carlo Tree Search with k opt moves to search a larger neighborhood of solutions. The rating is a weak reject for the following reasons:1. This can give the proposed algorithm a significant advantage over S2V DQN which does not have such built in knowledge. This issue is particularly important because the definition of “local moves” appropriate for a problem domain is one of the main challenges in applying local search successfully. 3.Scalability to large instances: TSP experts consider problems with hundreds of cities as practically “solved” by the current state of the art solvers like Concorde and LKH, and it is not at all clear what benefit learning can provide on such small instances.
Reject. rating score: 3. rating score: 6. rating score: 6. <BRK>TLDR: split node embeddings into medatadata and graph structure, force them to be orthogonal. The weakness of the paper stems from the proposed definition of debiasing. In fact, looking at Fig 3c information about party affiliation follows a XOR like pattern in the PCA space. This means that a linear classifier will fail (indeed the linear SVM in Table 1 fails), but a non linear one should work OK. redo Table 1 with strong non linear classifiers such a Gaussian SVM or Random Forest to show how much is not filtered out by your linear decorrelation methodFinally, contrast with the adversarial information removal [1] and  the information bottleneck [2], both of which also promise to remove non linear dependencies. It may happen that the you method works better, even though it only guarantees no linear dependencies.<BRK>Summary: The paper introduces a GNN model (MONET) for debiasing graph embeddings, by enforcing orthogonality between the embedding spaces of the graph topology & the graph metadata. They show that unsupervised learning induces bias from important graph metadata, when the metadata is correlated with the node edges. Decision: AcceptReasons for the decision: The paper is clearly written, well motivated, and well organized. The proposed algorithm and analysis seem insightful & novel, and the experimental results (showing that MONET can debias metadata from topology) are convincing. 2) In Section 3.4 [Algorithmic Complexity], it would be helpful to compare the wall clock time of MONET vs. the baselines (DeepWalk, GloVe), to give a better sense of how expensive the SVD calculation is.<BRK>The paper presents an approach to debiasing graph embeddings from known, given node attributes/metadata. The paper presents a, to my knowledge, novel approach, to avoid the leakage of meta data in the embedding, effectively debiasing it from this information (although some discussion of prior should be addressed, see below) 	The paper shows that the approach is effective compared to two baselines on two datasets (although some aspects of the experiments can be improved, see below)Weaknesses:1. The paper only evaluates on the training set. 1.3.While the paper clearly states that the approach is restricted to linear relationships, it would be interesting to look at non linear classifiers and see how well this works in practice, also in comparison to the baselines. The comparison to related work could be improved. and made more similar to Table 1, or ideally merged with it. It is also a bit unfortunate that all this additions are in supplement and not merged in the main paper.
Accept (Spotlight). rating score: 8. rating score: 6. rating score: 6. <BRK>Summary: This paper proposes a modification to standard Projected Gradient Descent to improve transferability of adversarial examples, when the source model is a ResNet like model containing skip connections. The method, Skip Gradient Method (SGM) modifies the backwards pass to scale down the gradient computed in each residual branch of the model, before these gradients are combined with the gradient from the skip connection. This thus upweights the gradients from the skip connections as opposed to residual modules. The paper demonstrates significant improvements in the single model black box transfer setting, against a variety of undefended and defended models. Strengths:  Lots of interesting empirical results here! Suggestions for improvement:I have 3 major concerns: (1) discrepancies between baselines and previously published results (2) unrealistic threat model (3) framing / conclusions drawn by paper. There s no reason in practice that an adversary would not employ an ensemble based attack if they wanted to fool an unknown model.<BRK>The paper discovers a very interesting phenomenon that adversarial examples are more transferable when the perturbations are obtained by propagating a higher ratio of gradients via shortcuts in ResNets and DenseNets. It is a good empirical paper that can inspire future research on investigating the role of shortcuts when defending adversarial examples. In real applications, γ should be selected before transferring to new black box models. The experiments in this paper are done by reporting the best results under the best γ. This is a hindsight.<BRK>The paper is about adversarial attacks and highlights a security weakness of skip connections in ResNet like CNNs, namely: skip connections make it easier to obtain adversarial examples. This observation leads to new approach to adversarial attacks, named Skip Gradient Method (SGM), which weights the residual gradient w.r.t  the skip connection gradient. The approach is validated on a variety of image classification attack scenarios (e. g. white box and transfer attacks) using two families of source models (ResNet and DenseNet). Strengths:  Simple approach that seems to be giving good results  Large number of adversarial attack scenarios tested  Good related work reviewWeaknesses:  Results are reported without variance information  There are some details missing on how the decay factor is selected   Results are reported only on one dataset (ImageNet)The paper is well written, the authors have identified a "problem" of ResNet like models and proposed an approach that can exploit the problem in adversarial attacks scenarios (SGM). 3.Section 3.3, 2nd paragraph: "Another important observation is that when there are more skip connections in a network.... the crafted attacks become more transferable....". Are these models re trained from scratch changing the random seed of model initialization, order of the dataset, or something else? 6.In some tables, the results are reported just for SGM while in other tables SGM is reported in combination with other method.
Accept (Poster). rating score: 8. rating score: 6. rating score: 6. <BRK>This work studies the identifiability of attention, i.e., to what extent does the attention weight uniquely determine the output. The paper (1) formally establishes, using a rank argument, that at least for the cases where the attention head dimensions are smaller than the sources, infinite different attention weights can yield the same attention output. Based on such an observation, (2) a principled tool to extract the `effective` attention is introduced, with which several previous observations are challenged. I vote for an acceptance. I don t have any major complaint. Below are some questions and comments. Since the paper only experimented with BERT, I think the authors mean "BERT" instead of the transformer models in general, whenever an empirical argument is drawn. Section 4.1, can the authors justify the use of MRPC for this analysis? I m not sure why that g^{MLP} is better than g^{lin} can be interpreted as "non linearities play an important role in maintaining token identifiability." Could it be the case that the non linearities make the token information more opaque, such that it can only be extracted with a more expressive model (MLP)?<BRK>The conclusion presented in this paper partially aligned with some concurrent works (Danish Pruthi et al.2019, Serrano et al.2019 and etc.) Many concrete findings in the paper make sense very much (not surprising), like 1) Input tokens retain their identity in the first a few hidden layers and then progressively become less identifiable, 2) Non linear activations are crucial in preserving token identity and 3) Strong mixing of input information in the generation of contextual embedding. The neat proof in the paper, though is based on some assumptions, formally shows that the multi head self attention is not always identifiable. The conclusion is that when the sequence length is larger than the size of attention head, there is redundant parameterization space for the attention. The authors propose “effective attention” which is orthogonal to the null space as a diagnostic tool. It seems that "redundancy" sometimes have some "benefits". I am not sure how “effective attention” would ease the training, though I agree it is a good "diagnostic tool". Another concern is about how the “non identifiability” really hurts the model? As the authors claim that contextual embeddings are strong mixing of input (which I agree), my question is "will the non identifiable attention weights, dramatically affect the mixing component of the contextual embeddings"?<BRK>Also, in the review, points are sometimes labeled by “(label)”, which is intended to label the point that immediately *follows* it in the text. While I have not examined all of them, even allowing for some possible disagreements of interpretation, I am confident there is ample relevant evidence among these papers and probably others as well. (P1) [the attention weight itself includes an irrelevant quantity [C2]] The argument provided for this in Sec 3 seems correct to me, but as stated above as a ‘minor complaint concerning exposition’, it may not make as clear as possible the obviousness of the conclusion. C(t,t) is the “[same ]token contribution”; the “contextual contributions” are the C(t,t’) for t’  ! That 18% of tokens cannot be recovered from their corresponding layer 12 encoding is the basis of the claim that equating attention from cell(t,L) to cell(t’,L) with attention from token t to token t’ is not justified in general. t. Experimental results are presented showing that after L   1, 6, 12 the median token values are about 30%, 15%, 10%. It reads to me to say that if the non linearities in BERT were removed, token identifiability would be reduced, but the experiment does not show that. The paper raises important challenges to a standard practice for interpreting the knowledge in Transformer models (especially BERT) by examining attention weights. This bit of the argument needs to be clarified. (*) The dimension D vector R is a linear combination of T vectors; if T > D, these T vectors cannot be linearly independent, so the weighting coefficients that yield R cannot be unique. The proportion of values t such that the largest contribution to Z(t,L) is from a token t’  ! The argument presented is rather overblown and may hide the obvious correctness of the result. A further advantage of the more detailed derivation is that it sets up nicely the unique decomposition of A into the component A2 in the null space and the component A1 orthogonal to the null space (of the matrix containing the T vectors being linearly combined). Given that A is underdetermined, there is a whole subspace of attention weight vectors that yield the same result R of attention. The proposed choice of A1 within that space is a natural one, but what makes it the *right* one? But what is the right criterion for picking an alternative “effective” version of A, A1?
Accept (Poster). rating score: 6. rating score: 6. rating score: 6. <BRK>The paper addresses the problem of coordination in the multi agent reinforcement learning setting. It proposes the value function factorization similar to independent Q learning conditioning on the output of the graph convolutional neural network, where the graph topology is based on the agents’ nearest neighbours. >>>  ... interplay between agents and abstract relation representationWhat is  abstract relation representation? What do you mean by that? Ideally, I would like to see an extended version of Figure 8 and 9 in the main part of the paper since they are very interesting and important for the claims the paper makes. This is a bit confusing.<BRK>This paper proposes an algorithm allowing "cooperation" between agents in multi agent reinforcement learning, modeling agents as nodes in a graph. One question for the authors: at the beginning of Section 3, it is stated that "it may be costly and less helpful to take all other agents into consideration". The paper is reasonably well motivated, grounded and written. Could they be added?<BRK>Novelty is incremental, but if the paper would otherwise be very well written, I think it could qualify for acceptance. DGN is a Deep Q Learning (DQN) agent structured as a graph neural network / graph convolutional network with multi head dot product attention as a message aggregation function. In addition to the attention based multi agent architecture, the paper introduces a regularizer on attention weights similar to the use of target networks in DQN, to stabilize training. The combination of these blocks and the considered problem setting is novel, but otherwise incremental.
Accept (Talk). rating score: 8. rating score: 8. rating score: 8. <BRK>This work addresses the task of causal discovery. The proposed contribution is to apply prior work which uses reinforcement learning for combinatorial optimization to structure learning. Overall I think this is a sensible idea, and the authors do a nice job of exposition, and empirical evaluation.<BRK>In this paper, the authors propose an RL based structure searching method for causal discovery. I have the following concerns. Overall, the idea of this paper is novel, and the experiment is comprehensive.<BRK>This seems to be incorrect. In conclusion, overall this is a sensible idea, although some of the preliminaries still remain to be polished. 2.The novel idea of applying reinforcement learning to DAG search sounds intriguing. It’s good to see that the author can successfully extend such toolbox to the field of causal structure learning.
Accept (Talk). rating score: 8. rating score: 8. rating score: 8. <BRK>The paper proposes a technique to perform reasoning on mathematical formulas in a latent space. From my experience, aligning embedding spaces is something that usually does not work very well, especially in high dimension. I don t think this is mentioned in the paper. I would imagine that even if an embedding X is a bit noisy, because not exactly equal to gamma(P) where P is the expression it represents, you could consider doing the propagation with gamma(G(X)). This could be a possibility to remove the noise you have when doing multi step operations (and potentially go way beyond 4 steps).<BRK>  SummaryEmbeddings of mathematical theorems and rewrite rules are presented. [i.e., app (emb(thm), emb(rule)) is near to emb(app(thm, rule))] This is an interesting property for the application of deep learning to automated theorem proving, though not directly a breakthrough result. I believe that s what "Pred (One Step)" expresses, but it would maybe be generally helpful to be more precise about the notation in Sect. RecommendationOverall, this is a nice, somewhat surprising result.<BRK>Novelty and significanceI really like the idea of doing math reasoning in latent space. The idea is definitely novel and interesting. Also can it work with theorems that decompose the current goal into several sub goals? I know these are not the concerns of this paper, but I would be really grateful if you could provide some intuitive answers! For example, for baseline 1, it is very hard to understand why would we want to use such an unusual baseline, and why it is called a "random baseline".
Reject. rating score: 1. rating score: 3. rating score: 3. <BRK>In this paper, the author proposed a model to address the training instability of a GAN model on the NRG task. The training process becomes a hybrid one with the original loss function in the beginning followed by the loss that pulls the response to the center of the response cluster later. The only relevance measure used in this paper is the  Rel., which the authors defined as the average of embedding distances. Such a measure is by no means an objective measure and can t really demonstrate the effectiveness of the model in terms of generating responses that are close to the ground truth.<BRK>Page 4: a limited samples? On the overall, I like the motivation and the proposed approach of this paper. Page 9: from the the? On the flip side, the technical formulation and theoretical results are not presented rigorously and important technical details are missing, as discussed below. The authors also need to improve the presentation and proof of the theoretical results; the correctness has to be checked again. If the authors like to keep them, they need to revise them based on my concerns above. It would be good to show some sample queries and corresponding "meaningful" responses produced by their proposed LocalGAN that are not considered safe responses which are produced by the other tested methods. (c) Are there multiple response clusters, that is, one for each q? The authors vaguely say that they are modeled from training data. Is it one center per response cluster? Page 3: inequation?<BRK>Contributions:The main contribution of this paper lies in the proposed LocalGAN for neural response generation. Besides the original GAN loss, the proposed LocalGAN adds an additional local distribution oriented objective, resulting in a hybrid loss for training, which claims to achieve better performance on response generation datasets. g) Generally, I think Section 3 and Section 4 are hard to follow. The final objective Eqn. (17) is also confusing. Strengths:I think the proposed model contains some good intuitions, that is, the generated responses should be modeled as a local distribution, rather than a single ground truth output during training. But at least from my point of review, this link should be anonymized. a) It is not entirely clear what the authors mean by saying "this paper has given the theoretical proof of the upper bound of the adversarial training ...". I am not sure whether Eqn. What is the value for s? This is not defined.
Reject. rating score: 1. rating score: 3. rating score: 3. <BRK>This paper presents a semi supervised approach to learn the rotation of objects in an image. The primary motivation is that for rotation estimation datasets may not always be fully labeled, so learning partially from labeled and partially for unlabeled is important. In its current state, I am unable to recommend acceptance. First, the paper solves a very interesting problem with potentially wide applications. It appears like the paper looks at the problem of 2D orientation estimation of objects in images. However, this setting is restrictive and not very practical in reality. It would have been good to see results on 3D rotations at the very least. Contribution: It is unclear to me what the primary contribution(s) of the paper is. So I wonder what is the novelty? What are the new capabilities enabled by this approach? What does 2D rotation mean for a 3D object?<BRK>This paper tackles the task of rotation estimation in a setting where both labelled and unlabelled examples are available for training. It proposes to learn a generative model of images (a VAE), where the ‘code’ is factored into a latent vector z and the object rotation r.  As in training a VAE, an image encoder that predicts the distribution over (z, r) and the generator are jointly trained, but with additional supervision on the distribution over r for the labelled examples. I think the overall idea of learning a disentangled generative model in a semi supervised setting is simple and elegant, and could in principle help leverage unlabelled data. However,  I do have some concerns regarding the specific contributions of this work, and several reservations about the experiments reported, and would overall argue for rejection. Concerns:1) The central empirical result stated is that using this approach allows one to reduce amount of labelled data by 10 20 %. First, even if valid, this is not a very convincing reduction in the amount of supervision.<BRK>Here, the label represents the geometry of the 2D rotation. z is the ordinal latent variable and r a latent representation for the rotations where the latent variable is defined in the 1 dimensional circle in R^2 so that it can naturally represent a hyperspherical latent space. The construction of the proposed CVAE is straightforward. The major reason is the lack of technical originality. The construction itself would be novel while each component (e.g., CVAE,  latent representation for the rotations, and semi supervised construction of CVAE) have been already known. Experimental results are not surprising but show that the presented method is useful to some extent. One interesting observation of this paper is that more labeled images give better results than giving greater number of renders. Expansion to 3D rotations would be a good challenge.
Accept (Poster). rating score: 8. rating score: 6. rating score: 6. <BRK>The paper describes a method to train a network with large capacity, only parts of which are used at inference time in an input dependent manner. In addition, the current work outperforms related prior work through the use of the new regularization technique (batch shaping).<BRK>The core contribution of the paper is to propose a "batch shaping" technique that regularizes the channel gating to follow a beta distribution. Combined with l_0 regularization, the proposed training technique improves the performance of channel gating: ResNet trained with this technique can achieve higher accuracy with lower theoretical MACs.<BRK>Summary: This paper studies conditional channel gated networks. This can be used to save computation. With similar inference time, the gated network can achieve better accuracy it can afford adding more layers in the network. Conclusion: The batch shaping technique introduced in this paper has significant improvement on networks that exploit conditional inference. I recommend to move Figure 7 to the main paper. What would be the qualitative differences? ResNet 50 L0 is missing in Figure 3b). There are a number of related works on adaptive spatial attention for faster inference, which can be included in the related work section.
Reject. rating score: 1. rating score: 3. rating score: 3. <BRK>Network pruning has been a field of very active research as the authors have acknowledged, however, the approach used in the paper is a very simplistic remove the ones with lowest response one, for which, there is no experiment to justify its validity w.r.t.state of the art pruning approaches. The results that this paper get to, such as few filters were pruned in the lower layers, similar classes share similar filters, are not necessarily new knowledge to the community.<BRK>This work proposes to prune filters in CNN model to interpret the correlation among different filters or classes. The proposed method is simple, by just using the averaging the value of the output of each filter as the indicator. 3.Why not use the absolute value of r_i? 2.I’m confused with the implementation of pruning.<BRK>The conclusion should re iterate the results, but it should also say why they are important. I like the broad goal of understanding CNNs. I would be surprised if activations were not in some way dependent on one another. (experiments)Experiments use AlexNet and 50 of the 1000 ImageNet challenge classes to show:1.
Reject. rating score: 1. rating score: 1. rating score: 1. <BRK>  SummaryThe submitted paper describes a system for data augmentation and program synthesis given input output examples. The system consists of (a) a discriminator able to recognise I/O examples that are likely to originate from programs humans are interested in, (b) a method to generate new example corpora using the generator from (a), and (c) a program synthesiser trained on the family of corpora generated in (b). It s substantially different from the competing idea of using a language model to generate "natural programs" from which examples are then extracted. This could be tested by holding out some of the programs from the initial set, and testing on these? 4.2 compares based on the number of examples. How many random examples can you generate in that time?<BRK>This paper presents a technique based on genetic programming to generate a suitable training corpus of programs and I/O examples using a trained discriminator network. Overall, this paper presents an interesting idea of automatically generating corpus for training neural program synthesis architectures, where most previous techniques sample synthetic programs uniformly from the space of DSL programs. Minor:References are not formatted correctly in the paper. First, the description of the overall method is too high level. Is it the case that with the discriminator based corpora, the approach is overfitting to one class of problems and not on others?<BRK># SummaryThis paper describes an approach for generating synthetic training corpora for neural program synthesis systems. # StrengthsI like the core idea of the paper: assuming some desired distribution of I/O examples, construct the training corpus ofrandom programs in such a way that its associated I/O examples approximate the desired distribution. I can see how itcould force the sampling process to discover useful program snippets for training. If evaluated properly, this workmight get accepted at a future conference.
Reject. rating score: 1. rating score: 3. rating score: 3. <BRK>However, in my opinion, its novelty and new insights are restricted, which is why I suggest to reject the paper. The reasons for this are the following. Moreover, it remains conceptually unclear, why and when the proposed model should be useful and represent a good approximation of the full rank model. I am missing the "point estimate" baseline for these models. For if the the positive impact of the Bayesian inference approach with the full rank model is small when compared to the point estimate, then, as a consequence, the impact of the low rank tying must be small when compared to the full rank model. The numbers reported in Table 3 (second group of experiments) raise some questions not addressed by the authors. It remains unclear, why the low rank model with ranks 1,2,3 gives better accuracies on CIFAR than the full rank model.<BRK>This paper showed the (diagonal) variance parameters in mean field VI for BNNs exhibit a low rank structure, and that training from scratch using such a low rank parameterization lead to comparable performance as well as increased SNR of the gradient. While the observation is somewhat interesting, currently it is only verified in a narrow range of network architectures, and it s unclear if the observation and the proposed method will still be useful on network architectures used in real world applications. As such, I believe this work would be more suitable as a workshop presentation. Another major concern is that I m not sure if the proposed low rank variational would actually save parameters in practice, since the variance parameter in MFVI could already be stored as the preconditioners in Adam like optimizers [4 5].<BRK>The paper proposes a low rank approximation to the diagonal of Gaussian mean field posterior which reduces the number of parameters to fit. Also MN has a full covariance for the Gaussian approximation, not mean field. If that s what the paper means, k tied is only compared to MN with diagonal row and column covariances. Better make this point clear. I think the trick the paper uses is a practical one but not significantly novel enough for the ICLR community. If just focusing on the diagonal covariance, it already throws away the full covariance.
Reject. rating score: 3. rating score: 3. rating score: 6. <BRK>This paper proposes a metric learning based generative model for detecting the out of distribution examples. Experimental comparison with state of the art will help to position this work. ** Update ** I read the authors comments and other reviews. This study aims is to detect out of distribution samples for better generalization.<BRK>Summary:Unlike the softmax classifier, the authors considered the generative classifier based on Gaussian discriminative analysis and showed that such deep generative classifiers can be useful for detecting out of distribution samples. Because of that, it is hard to say that contributions from proposing a training method are significant.<BRK>This paper presents an algorithm two learn both classifier and out of distribution sample detector. The proposed approach can be viewed as generalization of Gaussian discriminant analysis and one class classification. However I think there are some weaknesses of this paper* The novelty is a little thin. * Experimental result is somewhat weak. I am interested in whether the proposed approach can detect novel classes, such as training using only 9 of 10 classes on Cifar10. Combined with the good results obtained.
Reject. rating score: 3. rating score: 3. rating score: 6. <BRK>The authors propose a framework to incorporate additional semantic prior knowledge into the traditional training of deep learning models such that the additional knowledge acts as both soft and hard constraints to regularize the embedding space instead of the parameter space. In this case, the time cost should be non trivial because the distillation process requires optimizing the distance between the current embedding with the hard constraint. In general, the paper is well written and easy to follow. The authors should also compare with such models.<BRK>This paper proposes the incorporation of “prior knowledge” which enters in the form of the relations between training instances in neural network training. The authors claim that their method is a general technique but in fact, the constraints are drawn from specific tasks (VQA for example). Please clarify this. A question can be translated into a program that is composed of a set of operations. I even found a wrong reference (Section 3).<BRK>The paper argues for encoding external knowledge in the (linguistic) embedding layer of a multimodal neural network, as a set of hard constraints. The domain that the method is applied to is VQA, with various relations on the questions translated into hard constraints on the embedding space. A related comment is the use of the distillation technique. What s an operation here? I stared at this a while, and still have no idea.
Accept (Poster). rating score: 6. rating score: 6. rating score: 6. <BRK>This paper constructs a new game that requires combining visual reasoning with text understanding to win. The authors propose a new model txt2π, based on a new layer called FiLM², which combines text and visual features in a way that allows visual features to be encoded with knowledge of the text features (as in the FiLM layer from previous work), as well as text to be encoded with knowledge of the visual features. The model is trained to play the game using IMPALA. Performance is still below human performance suggesting this is a promising area for future work.<BRK>This work proposes a new environment, Read to Fight Monsters (RTFM), and correspondingly a new algorithm, txt2\pi, for solving this problem. Pros  The presentation is relatively clear. A new environment that can be impactful for the field. (2.2) Why the attention in (18) should base on the vis doc embedding instead of the goal embedding, or goal conditioned doc embedding? It would be better to include them in the paper to clarify potential misunderstandings.<BRK>This paper is about language conditioned reinforcement learning, where the agent is required to perform machine reading of documents to learn policies to solve a task in new environments. The first one is about the test suite RTFM. The second one is about txt2pi model proposed in the paper. In summary, this paper proposes an interesting test suite for language conditioned reinforcement learning, but I wish it is more realistic in the language data and more complex in the RL tasks. Thank you for clarifying some of my concerns. I am still a bit reserved because of the use of synthetic corpus.
Accept (Poster). rating score: 8. rating score: 8. rating score: 3. <BRK>First the authors propose and implement and new scalable RL training procedure they call  Decentralized Distributed Proximal Policy Optimization  (DD PPO). Code is also provided. The algorithmic contribution is useful and the results are state of the art. The paper is written very clearly, and it was a pleasure to read. I recommend this paper for acceptance.<BRK>Proposed technique was validated for pointgoal visual navigation task on recently introduced Habitat challenge and sim. *** I haven`t changed my mind after the rebuttal: the paper is good and should be accepted.<BRK>Summary: This paper proposes a Decentralized Distributed architecture for PPO. The idea was demonstrated on Habitat Sim, the Habitat Autonomous Navigation Challenge 2019. Overall, the paper is well written and easy to follow. The experimental results look promising but lack extensive evaluations given such a technical contribution. I have some following major comments. 1.As it is a technical paper, it would be nice if the authors could describe more on the implementation of challenges and the hacks. 2.The experiments could also be compared to one or two other distributed RL frameworks.
Accept (Poster). rating score: 8. rating score: 8. rating score: 6. <BRK>3.Theorem 2.2 shows that the average loss converges. Classification task with shallow Relu and logistic loss. This paper should be accepted as an oral presentation in ICLR. If the authors can address some of the questions in the comments, I will be happy to increase the score. In general, if we replace logistic loss with square loss, will this make it harder to train neural networks? Also discuss the stochastic GD/generalization error? b.On page 1, “… and standard Rademacher tools but exploiting how little the….”, “but” seems to be a typo. c.On page 2, “also suffices via a smoothness based generalization bound”, “suffices” should be “suffice”. If K_1 has constant spectral norm(which is the case if all the data points are orthogonal to each other), then \gamma_1 will depend on 1/n.<BRK>In particular, the authors were able to show a width dependence that is polylogarithmic on the number of samples, probability or failure, error tolerance, and a margin parameter. While this concern warrants a careful discussion (below), I believe the paper still offers a nice analysis of shallow networks. Overall, I would recommend accept for this paper. Discussion of ContributionsAs the title may suggest, it is a bit surprising that we can show that polylog width is sufficient. The connection drawn between the margin assumption and neural tangent kernel (Proposition 5.1) is also interesting on its own. The authors intended this result to serve as a justification of the margin assumption (2.1). Perhaps I m missing some obvious ideas here, but I would still like the authors respond with some more details. The paper is presented in a very transparent way, and the authors were being honest in chapter 5 about the worst case dependence on the number of samples.<BRK>Overall, the paper is well written and easy to follow. However I still have some questions about this paper. One of my major concerns is that there might be an important error in the proof of the main theorem. However, Lemma 2.5 only shows that $|f(x_i,W_0,a)|$ is small, and the reason $\hat R^{(t)}(\bar W)$ can also be small is not explained in this paper at all. Based on Lemma 2.5, I can roughly get that $\hat R^{(0)}(\bar W) $ can be small, but the reason why $\hat R^{(0)}(\bar W) $ is small is unclear to me, especially when the network width m is only polylogarithmic in n and \varepsilon. Moreover, this paper does not provide sufficient comparison with existing work. Finally, the authors’ claim in the title that the width of the network is poly logarithmic with the sample size n might be misleading.
Accept (Poster). rating score: 6. rating score: 6. rating score: 6. <BRK>TLDR: The function these deep set networks can approximate is too limited to call these networks universal equivariant set networks. Also, try using the consistent dimension for x throughout the paper, it confuses the reader. It is trivial to show that this function is permutation equivariant. Then, can the function family the authors used in the paper approximate this function? The paper is not very self contained. Spare few lines of equations to define what are DeepSets and PointNetSeg in the paper and point out the difference since these networks are used throughout the paper extensively without proper mathematical definition.<BRK>The paper presents proof that the DeepSets and a variant of PointNet are universal approximators for permutation equivariant functions. The second issue I would like to raise is related to discussions around the non universality of the vanilla PointNet model. The results of this paper are important. This second part is indeed what is used in the proof of the universality of the permutation invariant version of DeepSets, making the connection more visible.<BRK>Perhaps at the beginning it would be more useful to spend more time on a roadmap of the results presented in the paper and to explain the exact significance of why the reader should want to continue reading. *Paper summary*The authors design a set architecture, which is equivariant to permutations on the input. I think the structure of the paper is fine for this sort of work. Nicely this architecture relies on a correction to PointNet, called PointNetST, which they show is not equivariant universal.
Accept (Poster). rating score: 6. rating score: 6. rating score: 3. <BRK>Contributions:The main contribution of this paper lies in the proposed unlikelihood training objective for open ended text generation. The key idea is to enforce the unlikely generations to be assigned lower probability by the model. Strengths:(1) Writing & Clarity: The proposed model is very well motivated, the paper is well written, and clearly presented. I find the gradient analysis in Section 5.1 is especially interesting. (3) Experiments: The authors did a careful job in experiments design, and conducting the experiments. I feel the experiments are solid and convincing. Or, are there any better designs? Or, this is not needed? Or, do the authors plan to also apply the proposed method to this application?<BRK>This paper proposes training losses, unlikelihood objective, for mitigating the repetition problem of the text generated by recent neural language models. Specifically, the paper argues that the main cause of the degenerated output is the maximum likelihood objective commonly used to train language models. The prior objective is used along with the MLE, while the later and more expensive is used for fine tuning. Overall, this paper tackles a relevant problem and could propose a novel method. This could be a sign of the generalization problem of the proposed method. Additional results on different model architectures would be helpful. What is the increase in training time?<BRK>This paper targets on solving the dull and repetitive outputs in MLE training for neural text generation. The authors propose a new unlikelyhood training to avoid assigning much probability to frequent and repetitive words. The authors combine the proposed algorithms and beam search and state that the results improved over beam blocking and neculus decoding. The unlikelyhood training is to provide a set of negative candidates and minimize the probability of these tokens. A better discussion should be made on this to explain why it performance or if ppl has some problem. This comparison is not really fair.
Reject. rating score: 1. rating score: 6. rating score: 6. <BRK>Section 2 3: Hope the authors could clarify / strengthen these points in revision: 	Since the discussion in Section 3 is based on the optimization problem in Equation (7), this problem should be well motivated. Currently it is presented as a problem that has been explored previously by Zhang et al and Aggarwal et al.However, after taking a look at those papers, I don’t understand where this regularization term comes from. Since this term is key to the paper, it should be well explained here. End of section 3: The authors conclude “our cost formulation using (17) with (18) and (19) is more general compared to the standard CycleGAN, since a general form of measurement data generator Hx can be used”. I don’t see the connection between the theory and this claim. I might be missing something, but I’m not sure that this approach is different enough from CycleGAN.<BRK>The paper presents an interesting connection between cycleGAN, penalized Least Squares (PLS) and optimal transport (OT). As a consequence, we can see the overall training procedure when H is fixed (as it is the case in both experiments), as learning for a generator in a WGAN way, that also enforces a consistency constraint (that can be seen as a regularization of the OT problem). The formulation is more generic than the classical cycleGAN formulation. Why enforcing this minimal cost equation ?).<BRK>This paper frames and contextualizes CycleGAN as a stochastic generalization of penalized least squares for inverse problems, providing several unifying theorems, rederiving some modern CycleGAN architectures within the optimal transport framework, and also demonstrating the practical use of modified architectures derived using this framework for accelerated MRI and microscopy. The primary thing the authors could do in order to raise my score, would be to take an additional pass at grammatical clarity for the paper.
Reject. rating score: 1. rating score: 3. rating score: 6. <BRK>This paper proposes an approach for the certification of speech classification neural networks against adversarial perturbations. I feel that this paper is very difficult to follow because of the lack of background technique explanations based on neural network certification, and the lack of technical surveys of speech recognition and related areas. Comments  The authors only list CTC related techniques as state of the art ASR, but state of the art ASR is still based on the HMM/DNN hybrid system or attention based encoder decoder/RNN transducer. Also several technical terminologies are not common in the speech recognition are (e.g., automated speech recognition  > automatic speech recognition)  "Additionally, audio systems typically use recurrent architectures (Chiu et al., 2017)": There are a lot of state of the art ASR systems including TDNN (Kaldi), CNN, and transformer. Again the paper does not have enough surveys. The community was already moved from MFCC to log mel filterbank.<BRK>In this work, the authors study the task of building neural network classifiers for audio tasks which can be certified as being resistant to an adversarial attack. One of the contributions of this work is the development of abstract transformers which can be used for the data processing frontend used in typical audio applications. Overall, this work is interesting and I think it would be a great addition to the conference. The paper is generally well written in the initial sections, and the main ideas are very clearly presented. Also, why are the approximation volumes not comparable between the two systems. Minor comment: It is true that most works in audio classification and speech recognition use processed frontend features such as MFCCs. However, there is also a significant body of work which operates directly on the time domain signal. Learning the speech front end with raw waveform CLDNNs.<BRK>This paper presents an end to end neural network verifier that is specially designed for audio signal processing to certify the robustness of a system when facing noise perturbation. The approach is based on abstract transformers to deal with non linearity in the audio signal processing pipeline and LSTM acoustic model. This is an interesting paper globally but I have some concerns. 1.There should be a more thorough introduction on how to verify the robustness of a neural network classifier for noise perturbation.
Reject. rating score: 3. rating score: 3. rating score: 3. <BRK>The basic idea is to rotate the features using a random rotation matrix. Experimental results are shown on standard classification datasets, object detection, and speech recognition. While the text briefly mentions this and acknowledges that the resulting operator is not a rotation matrix anymore, this is not further mentioned in the text. The proposed approach effectively applies a signed and uniformly scaled permutation matrix to the features and adds the results back onto the features. 3) Section 3.2 is completely disconnected from the rest of the paper. Similar to the other reviewer, I still believe that the improvement even beyond CIFAR is too small.<BRK>This paper proposes a new regularization method to mitigate the overfitting issue of deep neural networks. The experiments have shown some improvement over existing methods. I have some concerns about the proposed method as follows:1. However, it is well known that the rotation matrix is an orthogonal matrix, which cannot decorrelate two random variables. For a large D, such as 224x224 in the imagenet, 1/D is very small.<BRK>v. For the results on COCO, is the same regularization method used even for ImageNet pre training? High level comments: I find the proposed method interesting, and I think the paper is well written. There are a number of other proxies for mutual information in the community (such as CCA from arXiv:1806.05759) that the authors should also evaluate. iv.For several of the experiments, the authors report that they use RotationOut only for a few residual blocks.
Reject. rating score: 3. rating score: 3. rating score: 3. <BRK>The paper studies a phenomenon of unusual memorisation in deep overparametrized neural networks. Authors observe that, if an auto encoder overfits to machine precision on a number of images, they can be reliably decoded from random noise and that it is even possible to memorise this way a sequence of images. Essentially, images from such a training set become attractors for the mapping defined by the auto encoder. I would be also interested to at least an interesting discussion, if not an answer, to the question of why and how exactly trained images become attractors.<BRK>This paper empirically demonstrates that DNNs can be trained to be identity mappings for small quantities of samples. I was unable to replicate exactly your results but they were similar enough. "Since overparameterized autoencoders interpolate the training data", again this is a fairly important assumption and it needs to be defined very clearly, because it could mean many things. It also demonstrates that for many parameterizations, these identity DNNs also have a small number of attractors, iterative fixed points, and can also learn short circular sequences of examples.<BRK>A more thorough discussion of related work would be helpful. The authors show really interesting results where they are able to retrieve a small subset of encoded images (mnist) by giving the autoencoder random noise. The overall problem is a really interesting one which is to try to develop associative memory, retrieval models. Although the work is interesting, the only related work the authors cover is hopfield networks.
Accept (Spotlight). rating score: 6. rating score: 6. rating score: 3. <BRK>This paper introduces a model to predict the duration of stay of goods in a warehouse, and releases an extensive dataset of warehousing records used in the experiments. I found the paper very interesting to read, and I believe it could serve as an inspiration to researches more interested in applied ML, since is shows that relatively standard architectures can make a big impact in solving real world problems. My guess is for example that the brand name is a key information to have when predicting DoS. To me, the released dataset is one of the key contributions of this work.<BRK>The paper introduces the duration of stay estimation problem in the warehouse storage application and describes a way to formulate the problem, prepare datasets, design loss functions and train models. However, I do think the problem is interesting since it can directly lead to significant real world improvements by improving the machine learning task. It is also nice to see the authors will publish the datasets to enable future research along this direction. The paper is well written. I also like the detailed description of the problem formulation, datasets, and the loss function design. For example, given that the input features are limited, can a GBDT do the job as well?<BRK>The authors report significant positive real life results in two warehouses and releases their dataset. The release of a large realistic dataset is a major contribution to this field. Despite this I will not recommend this paper for publishing at ICLR, simply because I think it falls outside the scope of the conference. It seems that the general problem would be to minimize the expected cost (distance moved) over the lifetime of the warehouse. Why?I would expect the right thing to do is to maximize the probability of the observed duration of stay under the model, i.e max_\theta p(DoS|x, \theta), i.e.min_\theta \sum_i  log(p(DoS_i|x_i, \theta). If this is log normally distributed, then a log normal distribution is probably a good distribution.
Accept (Poster). rating score: 8. rating score: 6. rating score: 6. <BRK>The paper proposes an unsupervised framework to address the problem of noisy pseudo labels in clustering based unsupervised domain adaptation (UDA) for person re identification. In addition to that, ablation studies conducted to evaluate each component in the proposed MMT framework. Since the conventional triplet loss cannot properly work with soft labels, a softmax triplet loss is proposed to enable training with soft triplet labels for mitigating the pseudo label noise.<BRK>This paper proposes an unsupervised domain adaptation method for person re identification. Also, soft softmax triplet loss is proposed to handle soft labels for triplet loss. The handling label noises in unsupervised domain adaptation on person re identification are new. However, I would like to see more insights into the proposed model for the contribution of the general deep learning conference. First, this paper lacks a survey of works on handling label noises.<BRK>This makes me suspect that the proposed method benefits from ground truth information of the target domain, which makes the comparison unfair. After reading the reviews and the comments, I choose to stand with the other reviewers. This paper uses mean teacher to ease the noisy pseudo label of clustering methods for domain adaptive Person re identification task. The authors also propose a variant of triplet loss for soft labels.
Reject. rating score: 3. rating score: 6. rating score: 8. <BRK>2) The solution proposed by the authors might have a big practical advantage for cross domain imitation learning settings. iii) Is it possible to run experiments on environments presented in Invariant Features paper so that the importance of CDIL can be assessed better. WeaknessThe biggest weakness of the paper is in the evaluation section. ii) Why are the other methods not able to align well when there is no domain shift? Questions1) How were the alignment videos generated? For example, the authors present the task R2W: reacher2 tp that has a "third person" state space with a 180 camera angle offset. ii) It is unclear what the R2W task entails or how difficult it is.<BRK>This paper proposes Generative Adversarial MDP Alignment (GAMA) for imitation learning. For a new test MDP pair (x,y) where expert demonstrations are available for y, GAMA can use f to map a state of x to a state of y, mimic the expert behavior, then use g to map the expert action back to an action in x. Pros  The writing is great and easy to follow  The method is theoretically motivated  Experiments prove effectiveCons  The proposed method may not work well for complicated environments(1) In the discussion after Def.4, given an alignment task set D_{x,y}, how do we know whether a common (w.r.t.all MDP pairs) reduction exists? The domain dynamics can be difficult to learn for complicated environments, which may jeopardize the learning of f and g as a result because they depend on the accuracy of the learned dynamics. How about the rest?<BRK>The paper proposes a learning approach for zero shot imitation learning in an RL setting across domains with different embodiments and viewpoint mismatch. The proposed approach involves two steps, alignment and adaptation. In contrast to previous work, the alignment between domains, represented as MDPs, is learned from unpaired, unaligned samples from both domains. The paper presents a theoretical formulation of the cross domain imitation learning problem, and presents an algorithm for training alignment and adaptation from data. I think the paper is well written and theoretically well founded. However, the text refers to Alignment complexity on the Left and Adaptation complexity in the Middle.
Accept (Poster). rating score: 6. rating score: 6. rating score: 6. <BRK>This paper presents STOVE, an object centric structured model for predicting the dynamics of interacting objects. The paper is well written and clearly motivated but comes across as an incremental improvement on top of prior work. These results can add further strength to the paper.<BRK>Using a Graph Neural Network for modeling latent physics is reasonable and has been shown to work on related problems before (see referenced work above and related work mentioned in the paper). Overall, the paper is well structured, nicely written and addresses an interesting and challenging problem.<BRK>There is a repeated claim made in the paper that the system presents output that is “convincing” and “realistic” over hundreds of time steps. In this paper the authors present a graph neural network for modeling the dynamics of objects in simple environments from video. The primary technical contributions of the work appear to be the graph network, the experiments, and their results. However, I’m not convinced of many of the technical details in the paper.
Reject. rating score: 3. rating score: 3. rating score: 3. <BRK>This paper proposed a deep amortized clustering framework which learns to cluster data efficiently  based on the combination of set transformer and amortized clustering. Without answering this basic concerns, the proposed method may be hard to be accepted. Also, how the set transformer extracts  information useful for clustering is very unclear and needs more elaborations.<BRK>The proposed method is essentially based on the idea behind set input neural networks [1], which consists of modeling the interaction between instances within a given dataset. Compared with the previous work [1], the main difference is that DAC does not need to specify the number of clusters, as in the case of Bayesian nonparametrics, making it more flexible for clustering complex datasets. [1].I would like the authors to clarify a bit more the novelty of the paper. As also mentioned in the discussion on page 8, it would be important to consider uncertainties in cluster assignments, as already done in Ref. ArXiv:1901.00409, 2019.<BRK>[Overview]In this paper, the authors proposed a new clustering method called deep amortized clustering (DAC). Inspired by Lee et al 2019, the authors exploited a transformer to gather the contextual information across different dataset points and then predict the cluster label for each data point. The authors should definitely describe the process more clearly.
Reject. rating score: 1. rating score: 6. rating score: 6. <BRK>presented in the above papers. The authors then compare the efficacy of a human and computer to maintain accuracy in the presence of a reduced entropy representation of an image. Please justify the use of entropy to quantify the distortion. Although the authors offer some intriguing methods, I found the results to not be compelling nor improve our understanding of the relative differences between human and machine perception.<BRK>This paper proposes a method to understand and compare the performance of DNNs classifier, which is different from the precise prediction in the notion of correct/wrong. By comparing the results with human’s and DNNs’, the author claims that it will have more challenges for DNNs in this laconic image classification task than human will have. For the theoretical demonstration, in this paper, the author uses approximating minimal entropy to quantify the minimal content of an image DNNs or humans need to give correct category. The intuition of this method is suitable. However, the experiments do not provide convincing evidence to existing approaches. At last, the comparison between different models should give a more visualized figure to illuminate the difference.<BRK>The paper proposes and studies a task where the goal is to classify an image that has been intentionally degraded to reduce information content   hence the name "laconic" image classification. Overall I find the comparison of human and machine performance interesting and hence recommend accepting the paper.
Reject. rating score: 1. rating score: 1. rating score: 3. <BRK>Summary:This paper proposes a simple procedure to display the salient areas that determine classification decisions from deep networks. In practice, these area are computed by shadowing the image using a Gaussian and measuring the network contribution at every location of the image. There would also be a lot of work for improving the writing of the paper. The paper is badly written.<BRK>* For a general reader, metrics such as IOU should be explained more clearly. The introduced method GLAS, scans over an image and lights/shadows each part of the image to assign an importance score to different regions of the image based on the change in model s prediction. The motivation of this work is to increase the inherently low speed of (some) methods in this family and to give better explanations by I vote for rejecting this paper as the contributions to what already exists in the literature are not clear and the provided experimental results are not convincing. All in all, the true contribution of this work to other existing methods in this family is not enough for this venue.<BRK>This paper introduces a simple and effective method for pinpointing salient features contributing to discriminating different classes in classification. It is based on masking images using Gaussian Gaussian light and shadow (GLAS) and estimating its impact on output. 3) Experimental comparisonThe comparisons are not convincing.
Reject. rating score: 3. rating score: 3. rating score: 3. <BRK>I understand that the proposed bound is derived based on a few assumptions, which do not necessarily hold true in real cases, and thus it is only an approximate bound, meaning that the bound is not theoretically guaranteed to be a ‘superset’ of the true bound. Therefore, it is fair to say that the practical value of the proposed approach depends on its performance on real tasks. My first impression is that it is hard to see the expected accuracy robustness trade offs from them and the results appear rather random. That said, there are several cases for which the proposed method show better accuracy and robustness than IBP, as is claimed in the paper. However, these results shown in Figs 6 and 7 appear to be somewhat inconsistent with those reported in the paper of IBP [Gowal et al., 2018)].<BRK>This work proposes some expected bounds in order to improve the robustness accuracy trade off on standard CNNs. The bounds numerically improve (Gowal et al), thus this method works numerically well. My main concern is about the fact that several assumptions seem relatively constraining or are not quantitatively justified. In expectation for random NNs, the proposed bounds are tighter. Cons:  Some assumptions seem pretty unrealistic to me. For instance, the fact that the neural network should have Gaussian i.i.d weights: it is thus surprising that this technique works in real life settings. Did the authors check that their CNNs had Gaussian weights? A major difference with (Gowal et al) is that: here, the bounds are in expectation whereas the bound in (Gowal et al) are deterministic. In this paper, there are some approximation assumptions (e.g., large input, large number of hidden layers) without non asymptotic arguments. This could be improved. given a,a  and b,b  there always exists m such that a> b m and a < b +m, like m> max(|a b|,|a  b |). I think I do not understand this assumption...  How simple is it to extend those theoretical results for NNs to the case of CNNs?<BRK>* While reviewing the paper, I also spotted some strong similarities between the methods proposed (particularly subsection 3.3) and the Fastlin method. I m also re raising some concerns that I made in a previous review that the authors didn t address. The only difference is in the way the coefficients of the diagonal matrix are computed. The authors of this paper propose a different way of propagating the bounds, which is not a rigorous bound computation method, but for which they show that, with some strong assumptions on the distribution of the weights, the expectation of the generated results are valid bounds that are tighter than the ones generated by IBP. The paper explains clearly the "how" of how these bounds are achieved. So the relation to the true bounds is only given by the Assumption? * The reporting of Figure 5 and 6 is weird because according to the text, each datapoint seems to be the average robustness of networks trained with different hyperparameters, so it s hard to interpret. How is the real network trained? Is 99% the nominal accuracy?
Reject. rating score: 1. rating score: 3. rating score: 6. <BRK>R1.Note that in almost all classical optimization routines, the learning rate has a (very intuitive) scaling on the problem parameters   for e.g.in gradient descent, the learning rate looks like 1/smoothness. This term is misleading in what it represents. By using such a power of the smoothness of the problem, the other component of the learning rate (alpha_t) is no longer a scale free quantity. It has to depend on other problem dependent parameters for the overall learning rate to be scaled appropriately based on the problem characteristics. I can accept the claim of authors if I see more experiments tuning the lambda parameter for YOGI as well. This seems to have been added in subsequently just to boost the performance. The paper is well written and elaborates on (i) issues faced by adaptive gradient methods in contrast to standard SGD + momentum, (ii) presents experimental results on training standard conv net based architectures on image classification benchmarks and on training LSTMs on PTB and (iii) presenting theoretical analysis relating convergence of the method to a first order critical point for smooth stochastic non convex optimization. If we think of such full matrix adaptation methods, is this paper implying the use of other matrix powers (other than a square root) as used by adagrad (or other adaptation approaches)? This appears very unnatural to me. In a sense, if one has an `"adaptive" optimization method, it’d be unnatural to have to use some form of step decay of the learning rates (alpha_t s) in conjunction with these methods. One would typically just use SGD+momentum with some form of such a step decay of the learning rates.<BRK>This paper proposes a new variation on adaptive learning rate algorithm that unifies SGD with momentum and Adam/Amsgrad. (2) The theory doesn t justify the practice. (3) The practical usefulness of the algorithm isn t clear. Here are my detailed comments:(1) The paper provides an observation which they call "small learning rate dilema": One often uses a smaller base learning rate for adaptive gradient methods than SGD with momentum. Based on this observation, they propose to penalize the adaptiveness by adjusting the value p in their algorithm. However, the proposed adjustment seems like a trivial one, without giving too much insights into why learning rate decay is not compatible to adaptiveness. (b) Why does learning rate decay gives a boost to performance? (2) I have two criticisms to the theoretical analysis carried out in the paper. So it is not better than the baseline. The second criticism is related to the novelty of the theorems. But based on my crude assessment,  theorems are mostly mechanical applications of prior work to the current extended version.<BRK>Motivated by this observation the authors suggest Padam, a modification of Adam/Amsgrad. The authors also suggest an explanation for why the generalization gapof Adam happens:They claim that it is due to the "small learning rate dilemma" that happensas follows. The authors suggest that Padam with p < 1/2 can use larger learning ratesbecause it does not have as large effective steps if second moments are small. The authors also prove convergence rates for making gradients small with Padam. The experiments seem to be fairly performed (in terms of hyperparam search). The rigorous convergence analysis is laudable although perhaps not as relevantas the practical usefulness of the suggested approach. 1.The "small learning rate dilemma" phenomenon needs to be more clearly defined	and explained. I would rather the paper not make some nonrigorous claims if there is no	proof. Page 4: "It is very likely that Adam/Amsgrad is "over adaptive"	This seems to me a strong claim and the explanation that follows it to me	is not rigorous enough.
Reject. rating score: 3. rating score: 3. rating score: 6. <BRK>The paper is also mostly self contained and friendly to non specialists who may not work on feature selection primarily. All these limit the novelty of the idea.<BRK>This paper presents a method to provide some level of interpretation on the influence of input features on the response of a machine level model all the way down to the instance level. The paper proposes “proper test statistics” for model agnostic feature selection. I have found the paper interesting. The topic is relevant and the approach is interesting.<BRK>In the study of (Candes et al., 2018), the choice of the test statistic as well as how one estimates conditional distributions were kept open. Overall, I think the paper is well written and the idea is stated clearly. The proposed algorithms look simple and easy to implement.
Accept (Poster). rating score: 8. rating score: 6. rating score: 6. <BRK>[Summary]This paper proposes to learn fluid dynamics by combining the position based fluids (PBF) framework and continuous convolution. [Major comments]For now, I slightly lean towards acceptance, as I like the idea of combining PBF and continuous convolution for fluid simulation, and the method seems to have a much better performance than the baselines.<BRK>This paper proposes a novel technique to perform fluid simulations. The authors also develop a specific type of continuous convolution that yield better and faster inference than the benchmark algorithms. * I found the explanation of Lambda in Figure 1 too short to be understandable.<BRK>This paper applies 3D convolutions to the problem of Lagrangian fluid simulation. This paper contributes a novel method for performing 3D convolutions on unordered particle sets, and it shows that the learned fluid dynamics generalize to novel situations.
Accept (Poster). rating score: 8. rating score: 6. rating score: 3. <BRK>The paper studies influence of different hyperparameters of neural networks: architecture, width, depth, initialization, optimizer, etc. I agree with R2 that paper is relying too much on the appendix, but hope that authors could fix this somehow. To do this, paper propose a clever trick: train a model to mimic identity function, so that output should be exactly the same, as input. This allows rich visualization and easy evaluation via correlation or MSE.<BRK>This paper studies the inductive bias in deep neural networks. The paper is clearly written, present a broad set of experiments, and provides interesting insights, that are somewhat surprising. As someone from outside the area, my main concern with the paper is that I somehow missed the bigger picture. The authors present multiple different pieces of evidence that demonstrate the different conditions on which different model variants learn the different functions (memorization and generalization), but do not provide high level intuitions about what can be done with this information, and what are potential takeaways for the community. Aside from that, I think the paper puts too much of its content in the appendix (about 3 times as much content as in the main paper).<BRK>This paper studies the inductive bias of neural nets by considering the toy example of learning an identity map through a single data point (and hence the NNs are always overparametrized). The authors also present results under various different settings such as changing the filter size or the number of hidden channels of CNNs. Overall, this is a well written paper with an interesting set of experiments. However, I do have several concerns regarding the generality of the observed phenomenon in this paper.
Reject. rating score: 1. rating score: 6. rating score: 8. <BRK>In other words, the target graph is not sparse. In this case,  the complexity of the proposed algorithm can be intractable, especially when k is large and m is close to n^2. In general, the paper is easy to follow and well organized.<BRK>Since the authors use the Gumbel softmax trick the method is end to end differentiable and the output consists of a node classifier and a graph generative model that can be used to sample sparsified graphs. The existence of this previous work also reduces the novelty of the proposed approach.<BRK>The paper proposes a trainable graph sparsification mechanism that can be used in conjunction with GNNs. The paper is well written and easy to follow. I think that overall the method is sound and well executed.
Reject. rating score: 3. rating score: 3. rating score: 6. <BRK>This paper considers the task of instruction following where an agent navigates/interacts with a 3D environment conditioned on goals provided in natural language. Their main result is the demonstration of transfer from agents trained using synthetic instructions to environments with more variation (e.g.synonyms) or natural instructions provided by humans on two tasks involving object manipulation. Not much technical novelty2. Does the performance depend on this? 2.The typo noise (TN) seems to be a key driver of performance. —————Post rebuttal update:Thanks to the authors for their response and for updating the paper! It seems like the main claimed novelty of the paper is the proposed CMSA method. However, the empirical results are not convincing/rigorous enough to provide the reader information on 1) whether CMSA is a useful method (since it is used only with BERT and does not seem to affect results on its own compared to MP, SA, TN, etc.) and 2) when should one use/not use BERT and CMSA (BERT+CMSA actually does quite poorly acc.to table 5). Further, the other reviewers also pointed out concerns regarding the difficulty of the task and complexity of language used.<BRK>The authors present a method of transferring template based instruction following agents to natural language instructions by using language encoders trained on large text corpora. Strengths:  The paper is written well, it is easy to understand and follow. The experimental results indicate that it is possible to transfer an agent trained on template based instructions to natural language instructions using language models trained on large text corpora. It is not necessary to train the agent on natural language instructions. It seems like there is no ‘place’ action, how does the agent place the object? Weaknesses:  The paper lacks significant technical novelty. A major concern is that the natural language instructions considered in the paper do not have much diversity with respect to language. There are 80 objects in the lifting task and I suspect there are very few referring expressions for these objects and they mostly involve a synonym. The authors argue that the number of objects in the paper is much more than prior work. Since the focus of this paper is to tackle natural language, I believe the number and diversity of objects and tasks need to be much higher than 80 objects and 2 tasks used in the paper. It seems like the most diversity is coming from synonyms which can probably be handled with a dictionary or wordnet rather than requiring a language model. I would have liked to see many more tasks and a multi task learning model which is also able to distinguish between the task based on natural language instructions in addition to understanding object word synonyms and referring expressions. It is true that this paper handles object interaction and natural language instructions in a partially observable setting, however, previous work has tackled other challenges which this work does not tackle. I wouldn’t call moving objects using high level symbolic actions as ‘manipulation’.<BRK>This work proposes applying natural language encoders pre trained on a large text corpora (e.g.BERT) to training agents to follow natural language instructions in a simulated environment. I only have minor questions & feedback, see below. Illuminating analysis where using BERT indeed performs better on capturing phrasal and sentence level equivalence in natural language instructions. To phrase this in a different way, why do you think BERT+MP doesn t perform well on this? Are the results in Figure 2 computed from a BERT+MP model or a BERT+CMSA model? Updates:Having read other reviewers  comments and also the authors  response, I would also like to call into question the difficulty of the experiments in the paper   for the lifting task, the model is always presented with a command "Lift X", and essentially only needs to identify the correct object out of two at test time. Also, for the putting task, the model only needs to disambiguate between 6 possible combinations (3 movable and 2 fixed objects). Especially with a sophisticated model like BERT, I would have liked to see tasks where human instructions are more complex than this simple task.
Accept (Spotlight). rating score: 8. rating score: 8. rating score: 6. <BRK>I would particularly like to see a discussion of limitations already in the main part of the paper. 2014.p1: I agree with the argument that programs might be favored over natural language to specify goals as they are unambiguous.<BRK>This paper provides a method for instructing an agent using programs as input instructions, so that the agent should learn to contextually execute this program in a specified environment, learning to generalise as needed from perception, and to satisfy concerns that in the language of planning would be called monitoring and execution. I would note that the authors are revisiting concerns well studied in the planning literature.<BRK>A big issue I have with the evaluation in the paper is that I do not see the benefit of having the experiments with natural language at all. The focal point of the paper are agents able to execute tasks in the form of programs. This paper presents a reinforcement learning agent that learns to execute tasks specified in a form of programs with an architecture consisting of three modules.
Reject. rating score: 3. rating score: 8. rating score: 8. <BRK>The authors claim that their evaluation method is able to circumvent the problem with removing individual pixels (which is the removed information of one pixel is mitigated by the spatial correlations in the image and therefore will not result in a proportional loss of prediction power) by removing  features  instead. Their definition of a feature, though, are segments generated by simple segmentation methods. This method does not remove "the interdependency of inputs" for the saliency evalatuion metric. A few suggestions:* The authors talk about a  true explanation . It is also important to prove that the introduced evaluation metric of IROF would assign perfect score for a given true explanation. * There are many many grammatical and spelling errors in the paper. * Many of the introduced heuristics are not backed by evidence or arguments.<BRK>The paper presents a study on explanation methods, proposing an interesting way to aggregate their results and providing empirical evidence that aggregation can improve the quality of the explanations. ", but I cannot find where this decrease is reported. On page 5, in the parentheses it is reported that a given explanation method is no better than random choice. The paper also presents a score for evaluating explanation methods, which shows good results.<BRK>This paper, inspired by the established technique of model ensembling, proposes two methods (AGG Mean and AGG Var) for aggregating different model explanations into a single unified explanation. The only obvious downside of AGG Mean and AGG Var is that one would have to implement and run all constituent evaluation methods, which is expensive. Just as an idea for future work: given N explanation methods, one could ablate away one method at a time, thus getting an idea of whether any of the N explanations are redundant in the presence of others. I support paper acceptance.
Reject. rating score: 1. rating score: 3. rating score: 3. <BRK>Quick Summary: Based on semi quantitative analysis the paper first proposes two rules for quantization of DNNs, then extends previous methods based on these rules to propose specific technique for quantizing activations and weights. However the contributions were not really very novel in my opinion.<BRK>This paper presents network quantization techniques for weight quantization (SAT) and activation quantization (CG PACT). The authors present the effectiveness of their algorithm in MobileNet V1, V2, and PreResNet 50. I believe that incorporating all these would not introduce much computational overhead. In particular, I have some doubt in assumptions which is listed in ‘Other comments’.<BRK>This paper proposes two rules for efficient training of quantized networks by investigating the scale of the logit values and gradient flow. One of my main concerns is that the analysis of the rules for weight and activation quantization are separated. 4.Keep the same number of decimal places in the tables.
Accept (Poster). rating score: 8. rating score: 8. rating score: 6. <BRK>Interpreting the policies of RL agents is an important consideration if we would like to actually deploy them and understand their behaviours. Prior works have applied saliency methods to DRL agents, but the authors note that there are two properties   specificity and relevance   that these methods do not take into account, and therefore result in misleading saliency maps. While the improvements on Atari are hard to quantify, the results on chess and go are more interpretable and hence more convincing. Finally, the chess saliency dataset is an exciting contribution that can actually be used to quantify saliency methods for DRL agents. The proposed method is relatively simple, but is well motivated and demonstratively (quantitatively, which is great work for general interpretability methods) better than prior methods, and in addition the authors introduce a new dataset + quantitative measure for saliency methods, so I would give this paper an accept. Although the authors motivate their choice of perturbation based saliency methods as opposed to gradient based methods, they should expand their review of the latter.<BRK>This paper proposes an algorithm for explaining the move of the agents trained by reinforcement learning (RL) by generating a saliency map. The experimental results demonstrated that the proposed saliency map successfully focused only on important parts while the other method tend to highlight some irrelevant parts also. I think the paper is well written, and the basic idea look reasonable and promising.<BRK>iv) Apply non relevant perturbations to the salient pieces. The paper shows illustrative examples of the proposed approach and two previously proposed alternatives on Chess, Go and three Atari games. See whether the method still produces good results for other chess agents (ideally trained without human heuristics or data). Main idea of relevance is to “normalize” by taking into account change in Q value for all other actions as well. To this end: two experiments with human chess players/experts (puzzle solving, and expert designed saliency maps). The proposed measure seems to do reasonably well on board game domains, in particular chess. Ideally derive a saliency measure based on these formal definitions.
Reject. rating score: 1. rating score: 3. rating score: 3. <BRK>Verdict: Due to lack of clarity in describing the main methods, and missing comparison to any HRL/option baselines, I recommend rejection. More Detail:The paper is lacking clarity in its current form. This sentence runs on: "Given a set of past experiences...". By my reading of the paper, this claim is not studied. "HarfCheetah"::"HalfCheetah"References:Vezhnevets, Alexander Sasha, et al."Feudal networks for hierarchical reinforcement learning."<BRK>The authors propose a Hierarchical Reinforcement Learning (HRL) framework based on learning latent representations of action sequences. They use a Recurrent Variational Autoencoder (RVAE) to encode action sequences from previous experience or expert demonstration. The approach is developed both intuitively and theoretically. How does the work of the authors go beyond this original work on learning temporal abstractions for HRL? We ask the authors to clarify this in the paper. In the latter case, the comparison might be improper. Overall, we believe this is a promising paper, but we are not sure if it is ripe for publication at ICLR in its current state.<BRK>Summary: This paper studies the hierarchical reinforcement learning (HRL) problem. It proposes a framework TAIC that learns temporal abstraction from past experience or expert demons without task specific knowledge. I would think this paper is slightly below the borderline. It is an interesting method of encoding the option sequence by a continuous variable. This can only happen in the deterministic environment. For instance, from (s1,a1)  > (s2, a2), where s2 is generated from a random distribution, and a2 is based on s2. * the paper is overlength
Reject. rating score: 3. rating score: 3. rating score: 6. <BRK>This paper proposes to improve performance of NLP tasks by focusing on negative examples that are similar to positive examples (e.g.hard negatives). Since those labels are not provided in the original data, examples are classified using heuristics (e.g.negative examples that contain a lot of features predictive of a positive class will be considered as hard negative examples), which are used to provide distant supervision. This could free up space for more experiments. 3) About the experimental setting:3.1) The performance reported is well below the use of recent work on these datasets and recent models such as BERT.<BRK>This paper is aimed at tackling a general issue in NLP: Hard negative training data (negative but very similar to positive) can easily confuse standard NLP model. It would be important to see if the proposed method is also beneficial with the state of the art neural networks on the two applications. Strenghts:+ the paper proposes a reasonable way to try to improve accuracy by identifying hard negative examples+ the paper is well written, but it would benefit from another round of proofreading for grammar and clarityWeaknesses:  performance of the proposed method highly depends on labels of hard negative examples. The experiments are not making a convincing case that similar improvements could be obtained on a larger class of problems.<BRK>The authors propose a novel approach to leverage Distant Supervision for discriminating between positive examples and "negative examples that share salient features with the positive ones." The main feedback for the authors is to describe "early & in detail" the distant supervision techniques used in the experiments.
Reject. rating score: 3. rating score: 3. rating score: 3. <BRK>The paper introduces a new encoding for cell structures to improve efficiency of neural architecture search methods. As a second contribution the paper proposes Bayesian optimization with an ensemble of neural networks as probabilistic model for the objective function. Both contributions combined show superior performance on the Nasbench101 benchmark as well as competitive performance on the DARTS search space. While the paper identifies an important problem   encoding of architectures   I do not think the paper is ready for acceptance. The results would be more convincing if these methods are included in the comparison. After some discussion with the authors, they agree that existing BO methods based on neural networks could also be applied to this setting and even say that they may perform well with the path encoding. However, they are not include them in the comparison and only promise to add them for the final evaluation.<BRK>The paper considers the neural architecture search using Bayesian optimisation. The paper first propose a path based encoding scheme to featurise the neural architectures. A path encoding of a cell is created by enumerating all possible paths from the input node to the output node. Then, they propose to train 5 neural networks and ensemble these networks to get the prediction (including the predictive value and uncertainty). While the writing is readable and the experiments seem promising, the reviewer thinks that the novelty and contribution of the paper are limited. Particularly, using 5 neural networks for estimating the uncertainty is less convincing. This is because it would have been better if we can train a Bayesian neural network to provide the uncertainty quantification directly.<BRK>This paper develop a path based encoding scheme to featurize the neural architectures that are used to train the neural network model, and design a NAS algorithm that performs Bayesian optimization using a neural network model. The experiments show the priority of the proposed method. The author did not give a clear explanation of why does this method work. The paper introduced a path based encoding scheme, which seems have nothing different from enumerating all possible paths. Any additional operations should be clarified in the paper. Eq.(1) trained several different networks to predict the accuracy. However, when using early stop stragegy, the intermediate accuracy is not convincing, and the new architecture selected based on UCB may not perform well when training with full epochs.
Reject. rating score: 1. rating score: 3. rating score: 6. <BRK>This paper proposes a method to learn word embedding by incorporating additional sentiment information. I recommend rejecting this paper because (1) the writing is unclear and hard to follow, and (2) the experiment results are not convincing. Thus, it is hard to draw a supportive conclusion. 3.Plots in the experiment results are illegible. Finally, I d recommend the authors have a professional writer (English) review the paper before submission.<BRK>The paper proposed a word embedding model to incorporate the sentiment information. Improved experiment results on word similarity and low frequency embeddings are presented. Overall, the paper incorporates the sentiment information in a neat way. Detail comments are as following, 1. Would it possible that the improved performance on low frequency example is just a side effect of the biased introduced by the prior?<BRK>The paper aims at extending GloVe word embedding model so that the resulting embeddings should capture sentiments (e.g."good" is positive while "bad" is negative). I would accept this paper because:   This paper is well written, with thoughtful maths details. The proposed models, although are extensions of GloVe, gives interesting (and rigorous) points of how to add sentiment information. The experiments do support what the paper claims. I would reject it because of the experiments.
Reject. rating score: 1. rating score: 3. rating score: 3. <BRK>This paper proposes to learn representations of protein and molecules for the prediction of protein ligand binding prediction. The technical novelty is very limited. It may be more suitable for a domain journal instead of ICLR which focuses on method innovation.<BRK>The authors present a model with state of the art performance for predicting protein ligand affinity and provide a thorough set of benchmarks to illustrate the superiority of combining learned low dimensional embedding representations  of both ligands and proteins. I originally suggested to accept the paper, but agree with the other reviewers that the novelty of the work in this paper likely doesn t meet the bar for acceptance given that the most significant contributions of this paper are around combining good ideas from other papers without much additional novelty.<BRK>This paper tries to solve the protein legend binding prediction problem in the computational biology field. The novelty of the paper is limited. 3.The experiments are only performed on one dataset. For example, the authors may try to explain the result: why the model used embedding from unsupervised learning is better than the hand crafted features. 3.What s the detailed performance of the model on proteins belonging to different families? This can be more interesting. Usually, it is a very important piece of information for the biology people.
Reject. rating score: 3. rating score: 6. rating score: 8. <BRK>The authors use both PDE generated simulated data and some real world set to evaluate the proposed metric. The authors claim using PDEs to generate data can have some special control of the data de similarity, on the other hand, the performance on the simulated data does not indicate how practical the designed “metric” is for real world data which is possibly the main interest of most of the audience. For example we may design experiment comparing inter/intra class metric comparisons. And how that metric can be used to improve the sota results on the data sets? I am worried how those two terms balance. Also it is a batch dependent loss given the \bar{c} and \bar{d}.<BRK>This paper is very well written and easy to understand. They focus no domains of data where there exists some controllable parameter(s) for data generation, using this parameter in a way that resembles self supervised learning losses. The main contribution I think is in the use of correlations of changes in the scoring space (d) with changes in the generative parameters. The main weakness of the paper though is the novelty / proper connection to self supervised learning work.<BRK>This is a well written paper which looks into options for learning similarity metrics on data derived from PDE models common in the sciences. Quite a bit of space is devoted to ensuring that the learned metric actually satisfies pseudo metric axioms. The empirical testing is also thorough, and I particularly appreciate the use of the random weight networks as a baseline — I think it is good to note that these are actually fairly competitive on many of the test data sets (in fact, I believe it should be in bold for "TID" in table 1). It would be nice to see a discussion of how this could be useful for parameter inference in PDE models. If there are other important applications of a distance learned in this way, I think the paper could benefit *greatly* by pointing them out.
Accept (Poster). rating score: 8. rating score: 6. rating score: 6. <BRK>The authors proposed a meta learning based black box adversarial attack method to reduce the number of queries. The proposed method could attain comparable l2 norm and attack success rate with much fewer queries. Last, I would recommend acceptance for this paper.<BRK>Are the images from the training set or test set. The overall idea of this paper is interesting to improve the transferability of adversarial examples through meta learning. 2.What if you try to learn the adversarial perturbation directly instead of learning the gradient of images? What if you take multi step ZOO to give a much stronger prior to finetuning?<BRK>This paper proposed a meta attack approach that is capable of attacking a targeted model with fewer queries. It effectively utilizes meta learning approaches to abstract generalizable prior from previous observed attack patterns.
Accept (Poster). rating score: 8. rating score: 8. rating score: 6. rating score: 3. <BRK>This paper presents DynE, a self supervised approach for learning dynamics aware state and action representations. The paper is very well written and the approach looks quite promising.<BRK>This paper presents an approach to learning state and action representations through self supervision, such that these representation can be used for downstream reinforcement learning. Overall the paper is well motivated and clearly written.<BRK>An assumption of the work is that the pixel observations are Markovian. If not, this should be explained in Section 3.2. The method description and overall writing is very clear.<BRK>Summary: The paper proposes training dynamics aware embeddings of the state and k action sequences to aid the sample efficiency of reinforcement learning algorithms. Can the authors expand on how this is done? How does this trade off with the benefits one could potentially get in the RL phase? This trade off need not be optimal for $k 1$.
Reject. rating score: 6. rating score: 6. rating score: 6. <BRK>This paper studies approximation of the potential energy of molecules by a message passing architecture. 2) A proposed modification to the MPNN architecture proposed in [Gilmer et al., 2017], in order to account for physical properties in the message passing procedure. The paper is overall well written and clear. No details are given about the training of the models. 2.Even if it builds upon previous work, the (VI)MPNN model may be further explained. The explanation on the considered modifications of MPNN may be clearer (maybe with the introduction of a more mathematical notation).<BRK>This paper presents a number of new / extended datasets for the evaluation of ML based prediction of energies of unstable systems, as well as a network (VIMPNN) that includes a new and better way of including bond type information. In 4.2 it is explained how different ways of incorporating bond information were evaluated, and it is stated that “best results were obtained in the case a.ii). I would suggest systematically evaluating the different options and including the results in an appendix. It would be good to include experimental results to motivate this. The choice for method a.ii is made based on empirical results. This is not a problem in itself, but I would suggest that the authors change the wording to not over promise on the physics inspiredness. I assume this speedup is relative to DFT. It would be good to be explicit about that, and also discuss the speed relative to the MPNN baseline (I suppose MPNN and VIMPNN are similar).<BRK>The paper tackles the problem of estimating the electronic structure s ground state energy of a given atomic system by means of supervised machine learning, as a fast alternative to conventional explicit methods (DFT). However, try to be more explicit on what is shared and what isn t, in the architecture you finally pick.
Reject. rating score: 3. rating score: 6. rating score: 6. <BRK>The paper proposes an auxiliary module for GNNs to boost the representation power. The new module consists of virtual supernode, attention unit, and gating unit, each of which is demonstrated useful in the experiments. This largely reduces the novelty of this paper and make it incremental, because using virtual supernodes is not this paper’s original idea. The paper is generally well written.<BRK>The authors propose an auxiliary module that can be attached to a GNN that can boost the representation power of GNNs. The auxiliary module has three building blocks: 1. a supernode, 2. a transmitter unit and 3. a warp gate unit. Experiments are well designed. While the authors claim that to be one of the shortcomings of existing GNNs, it is not clear whether the proposed method solves that problem. Overall, I think this is a good paper that the community will benefit from.<BRK>Experiments show that this improves results of a number of common GNN architectures on four datasets. Strong/Weak Points+ Simple but useful extension of the existing super node idea+ Experiments on a number of datasets and baseline GNN architectures, providing ample experimental evidence of the usefulness of the method. Writing is overcomplicated and uses a lot of jargon ("transmitter unit", "warp gate", "intermodule hyperspace"). RecommendationThis is a nice contribution of minor novelty, with empirical evidence of its usefulness. I believe the paper should be accepted to a large conference such as ICLR. This idea seems related (in that it alternates local and global information exchange).
Reject. rating score: 3. rating score: 3. rating score: 3. <BRK>But even just running on MR would be better than introducing new tasks without published results from prominent exploration methods like RND. 2) No comparisons to published RND baselines. Indeed, the lack of a Montezuma s Revenge run is particularly glaring. But the authors also claim that in these situations it won t do worse than RND, and without a Montezuma s Revenge run, this claim isn t well founded empirically. * While the Fetch results are impressive, they should be contextualized by the results obtained in the HER paper.<BRK>The authors show that this reward signal has interesting exploration properties. Besides, they perform additional ablation studies to better investigate the properties of their approach. To me, these remarks should be removed from the paper and kept for another paper about the biological significance of the model. In my opinion, the authors should remove it for now and move Appendices E and F to the main paper instead. It would be more informative to use 0 everywhere and 1 when successful. typos:Eq (1) and (3) should finish with a dot as it is the end of a sentence.<BRK>The proposed algorithm is interesting and has an intuitive appeal, the magnitude of the TD error as an "auxiliary reward" seems like a natural choice to guide exploration. There are also some certain parts that are unclear in the experimental results. Overall, because of these issues, I cannot recommend acceptance although I would be willing to increase my score if my concerns are addressed. Other points:1) Some discussion of possible pitfalls for the algorithm could be added. Could the authors clarify how TD error based exploration avoids the exploitation versus explorationtradeoff? If this is a key choice, then it should be mentioned in the paper.
Reject. rating score: 1. rating score: 3. rating score: 6. <BRK>In this article, the authors propose a single image super resolution network that can generate high resolution images from the corresponding C JPG images. There are still some issues as follows:1.<BRK>This paper addresses the task of single image super resolution (SISR) on compressed JPG images. The authors proposed a two stage network which recovers the lossed information of compressed JPG (C JPG) on the first stage and handles standard SR on the second stage. However, my main concerns on this work are: 1. this works is not technically sound w.r.t.its novelty; 2. the efficacy of each loss function is not well supported by ablation studies and 3. the comparison experiments with other methods are not clearly stated. **Main arguments:**1.<BRK>This paper focuses on the super resolution (SR) task: to get better super resolution images from compressed JPG inputs. (3) The paper is well organized.
Reject. rating score: 3. rating score: 3. <BRK>The reviewer views the major contribution of this paper as formulating and solving the glucose management problem as an RL problem. Based on the understanding of the reviewer, such an action is a proxy to the meal announcement and is not considered as an input from the user for the deployed policy. Overall, the reviewer finds that the authors provide a reasonable approach to model the blood glucose management problem for type 1 diabetes. Therefore, the authors can consider further justifying why meal announcement is an important bottleneck to alleviate in blood glucose management, which is currently not well explained in the paper.<BRK>To push this review over the edge, the authors should address these papers in the related work, and discuss how this paper s method compares. Additionally, the novelty of the actual RL methods is not entirely clear. The importance of their application is self evident. DecisionShould their claim to novelty hold up, then the authors have provided evidence that RL can be useful for this important application of glucose control.
Accept (Poster). rating score: 8. rating score: 8. rating score: 8. rating score: 6. <BRK>Overall, while I think the computational cost of the proposed method is high, rendering it less practical at this point, I believe the approach has potential and the result obtained so far is already significant. The idea is to first learn discrete representation (vector quantization is done by Gumbel softmax or k means) from audio samples with contrastive prediction coding type objective, and then perform BERT style pre training (borrowed from NLP). This paper presents a method for unsupervised representation learning of speech. One observation from the submission is that the token set may need to very large (from tens of thousands to millions) for the system to work well, making the BERT training computationally expensive (I noticed that the BERT model is trained on 128 GPUs).<BRK>The paper proposes a way to pre train quantized representations for speech. I think this paper presents a useful contribution as far as improving speech / phoneme recognition using self supervised learning goes, and also has useful engineering aspects in terms of combining CPC and BERT. this makes a lot of sense especially given that CPC / wav2vec recovers phonemes and quantizing the phonemes will recover a language like version of the raw audio. After pre training, the authors use the learned representations for speech recognition.<BRK>Strengths:The core strength of this paper is in the results that are achieved on standard speech recognition benchmarks. I think the main technical novelty is in combining discritization with future time step prediction (but see the weakness below). [4] actually compares VQ VAE and the Gumbel Softmax approach. This paper is different in that it incorporates future time step prediction. Overall assessment:I think the results as well as the new combination of existing approaches in the paper warrants publication. Rather than motivating the work in this way, it might be helpful to focus the contribution as a combination of future time step prediction and discretization (both of which have been considered in previous work, but not in combination). Section 2.2: Similarly to the above question, would there be a way to incorporate the BERT principles directly into an end to end model, e.g.by randomly masking some of the continuous input speech?<BRK>Though rather dense in its exposition, this paper is an interesting contribution to the area of self supervised learning  based on discrete representations. The authors take it as a given that discrete is good because it allows us to leverage work in NLP. That makes sense   but at what cost? The state of the art on LibriSpeech is not Mohamed at al.
Reject. rating score: 3. rating score: 3. rating score: 3. <BRK>Original review:This paper presents a method for active learning on graphs, including a novel setting of transferring an active learning policy to unseen graphs. The problems tackled here are important and the method is shown to improve over previous work in some cases.<BRK>In this paper, the authors proposed a new method for active learning on node classification with GCN. Experiments on benchmark dataset show the effectiveness of proposed method compared to several baselines. Typo in first sentence.<BRK>The paper studies a universal policy for labeling nodes on graphs with multiple training graphs which can be transferred to new unseen graphs. The model is based on active learning and transfer learning. However, there is no method for solving this challenge. However, explanation about the active learning is confusing. Compared to the results of the model before selecting, how significant is the improvement after selecting all the node in the budget?
Reject. rating score: 1. rating score: 1. rating score: 3. <BRK>The paper aims to improve exploration in DRL through the use of planning. At the same time, no theoretical argument is attempted that would make up for the very narrow nature of the experiments. Could the authors taxonomize the landscape of planning and provide a specific argument for focusing on RRT?<BRK>The paper is mostly easy to read and I enjoyed reading it. The authors address an important issue of exploration in reinforcement learning and the used of a model based planner is certainly a promising direction. Perhaps switching from RRT to RRT* helps but the authors chose not to do it.<BRK>This paper suggested that conventional deep reinforcement learning (D RL) methods struggle to find global optima in toy problem when two local optima exist. However, the authors failed to provide sufficient analyis and theoretical support for the proposed method, plus it did not address the weakness of the RRT method the problem of planning time.
Reject. rating score: 3. rating score: 3. rating score: 3. <BRK>The paper addresses the problem of high cost transfer between server and user for machine learning applications. These learned masks are then applied to the image before sending it to a server with where inference takes place to reduce file transfer costs. In the experimental study, the paper shows that on computer vision tasks inputs can be reduced with relatively little drop in accuracy and analyses how hyperparameters of the model affect its performance.<BRK>The authors argue that data transfer costs for ML inference may be significant in some scenarios, such as for remote sensing. The experiments cover multiple datasets and multiple types of mask. I believe the problem is well motivated, but not very much explored yet. The paper proposes a reasonable approach to reduce data transfer costs. As the objective function is modified during training by adjusting \lambda, the performance/size trade off is only loosely specified. The channel selection task (4.1) is potentially interesting, but lacks a baseline. It is unclear whether this is necessary, and there are no related experiments.<BRK>The paper presents an approach to discrete input selection for NNs, using the Gumbel Softmax trick at its core. It motivates this problem in the context of communicating data over a network with limited bandwidth budget. It proposes constructing different kinds of masks that can be applied over channels or pixels in the input, grounding the discussion in the image domain. The only incremental contribution in this work is the specific mask types and mask specific losses.
Reject. rating score: 3. rating score: 3. rating score: 3. <BRK>I tend to vote for rejection for this paper mostly because, while it seems to me to be a very efficient and practical engineering project, but relatively lack the novelty in terms of the algorithm. Cons:  The proposed algorithm lack novelty.<BRK>It would be clearer to present this part in the main text, not Appendix. Although the method seems reasonable and the evaluation shows good results, I think that the paper can be improved for the following three reasons.<BRK>5.Some of the references in the text do not have year of publication, e.g., Kulkarni et al. However, the paper states in A2 that GE and LR learn from inferior samples. Can the authors please clarify this?
Reject. rating score: 3. rating score: 3. rating score: 6. <BRK>In this paper, how the length of the trajectory in the input space is amplified by a ReLU neural network is analyzed. Specifically, the paper studied the case when the weights and biases are sparse random matrices. Though the results are interesting, I am slightly lean to the rejection side. The main reason is that the motivation is not strong enough and it makes the entire work somehow incremental. However, I am not so excited about these results because I cannot find a practical value from them. From the technical side, it is nice the proof is written in line by line.<BRK>Summary: The authors examine trajectory growth of deep ReLU neural networks whose weights come from a (random) “sparse Gaussian”, “sparse uniform”, and “sparse discrete” distribution. They do this by extending the proof of Raghu (2017) so that it can handle more general distributions than the standard Gaussian. They also provide some numerical experiments verifying their theories. Strengths: The paper is well written and the proofs are clearly explained. I’m grateful that the authors specifically mentioned where their proof deviates from the original and they clearly delineate how their proof method extends Raghu (2017)Weaknesses: This is an interesting direction, but I do not believe there are enough results to constitute an accept. If the authors are following Raghu (2017), then I would have also liked to see analysis on trained networks as done in Raghu (2017) for example. I also think the title of the paper is too general for the specific results contained in the paper, namely sparsity should at least be mentioned in the title.<BRK>This submission proposes an alternative way to lower bound the trajectory growth through random networks. It generalizes to a variety of weights distributions. I give an initial rating of weak accept because (1) the paper is well written and well organized. (2) the numerical simulation results support the claims and proofs. (3) the investigation on sparsely connected networks seems timely. However, I m not an expert in this area. It also seems that most derivation and insights are from previous literature Raghu 2017, which makes the contribution of this submission limited. I have a question which may be invalid. For Figure 3, the observed expectation matches perfectly with the the lower bound for all three distributions.
Reject. rating score: 3. rating score: 8. rating score: 8. <BRK>This paper proposes a method called Self Taught Associative Memory (STAM) for Unsupervised Progressive Learning (UPL) , i.e., learning salient representation from streams of mostly unlabeled data with occasional class labels, where the number of class increases over time. The motivation of this paper is quite interesting in that the authors try to mimic how animals learn. The model shed light on the problem of catastrophic forgetting by introducing dual memory organization. To be specific, Short Term Memory contains a set of centroids associated with the unlabeled data, whereas Long Term Memory stores the prototypical centroids, which are frequently seen patterns. In addition, the model utilizes novelty detection technique to introduce new centroids to each layer of the model, and it prepares the newly created centroids to be associated with new classes. Each step of the architecture is well formulated mathematically and the necessity of the step in the model is explained clearly. The problem proposed, Unsupervised Progressive Learning (UPL) problem, is novel. This fact would hinder the model from understanding complex representation. This paper is a good start in tackling the Unsupervised Progressive Learning problem, but some weaknesses are present in the nature of the model architecture as mentioned above. However, the approach taken appears to be ad hoc. This does not mimic animal learning at all.<BRK>The paper presents a new problem formulation in the broader area of lifelong learning. Specifically, the Unsupervised Progressive Learning (UPL) problem, requires a learner to consume a stream of data, where each data point is associated with a class, but only very few labels are provided. Strength: 	I think the problem formulation is very realistic and interesting and I am not aware of it being explored previously 	The newly proposed architecture STAM is also interesting. The paper evaluates STAM on UPL on three datasets, MNIST, EMNIST, SVHN 	The paper ablates several aspects of the model evaluates the effect of hyper parameters. The paper misses to include baselines, both, w.r.t.the proposed method, as well as for the learning problem. (For single pass continual learning, see e.g.[B], although that work is supervised)1.2. [A] seems to study a similar setup and proposes an approach which should also be applicable in this work. While it remains unclear if STAM can generalize to realistic images, the idea of UPL is very interesting and speaks for accepting the paper, although there are several weaknesses the authors should address.<BRK>This paper sets up a new problem based on a continuous stream of potentially partially labelled data, which the authors call the Unsupervised Progressive Learning problem. The STAM architecture is tested on example problems from the UPL problem. I find this paper to approach a problem that is not well studied in the literature, and it is well written and easy to read. It would be good if the authors made the benchmark (/how they are generated) public. But I have one main question about this work: what is the memory cost of the STAM architecture? Is this memory cost greater than the cost of just storing all the labelled inputs that have been seen? I would imagine that just replaying these stored labelled inputs (or just training a NN on these stored inputs) would provide extremely high accuracies for the datasets considered in this paper. I would also argue that the brain *does* replay memory in some manner in order to learn. But that is a debate for another time! By fixing the LTM centroids that are learnt on previous data, the method is essentially freezing previous knowledge and then learning new knowledge by increasing model capacity. I would also be interested to see, as future work, the uncertainty estimates of this new architecture and its robustness to adversarial examples. I wonder if that is still the case if \theta is also increased along with \Delta?
Reject. rating score: 1. rating score: 3. rating score: 3. <BRK>This article proposes a method for object counting which can be trained with weak supervision. Which makes the method not a weakly supervised one. This article is an extension of density based object counting methods for weakly supervision.<BRK>This paper presents a method to train a network for counting/localizing objects in a weakly supervised framework. This paper is leaning toward rejection due to the following two reasons. (2) They are derived in an ad hoc way based on less theoretical background and thus lack novelty. This paper lacks detailed analysis and discussion about the regularization in this counting framework. Show the mathematical definition of q_\Theta(D|I) before that.<BRK>The main contribution of the paper is the extension of techniques for weakly supervised localization, i.e.given ground truth counts of objects in a given image, one can do training to generate hidden layer density maps that allow for feature detection and localization of objects. My rating is based on the fact that the results are preliminary and further quantitative experimentation is needed.
Accept (Talk). rating score: 8. rating score: 6. rating score: 6. <BRK>Posterior sampling for multi agent reinforcement learning: solving extensive games with imperfect information This paper investigates the use of Thompson sampling in multi agent reinforcement learning. There are several things to like about this paper:  This paper is definitely "groundbreaking" in that it makes a true extension to the existing literature: PSRL has been relatively well studied in single agent RL but never (to my knowledge) in the multi agent setting. The general structure of the paper and presentation is good.<BRK>PSRL This work considers the task of finding a Nash equilibrium in a two player zero sum imperfect information game, where some aspects of the game are not known to the agents (specifically, the chance node probabilities, and the reward function). The current work deosn t provide any evidence of what benefits the Bayesian approach provides over model free regret minimization. [1] Lanctot, Marc, et al."Monte Carlo sampling for regret minimization in extensive games." There *are* other regret minimizers that do work in the model free setting ([1], [2], [3]), none of which are discussed by the authors.<BRK>The paper proposes a sample efficient way to compute a Nash equilibrium of an extensive form game. The algorithm works by maintaining a probability distribution over the chance player / reward pair (i.e.an environment model). 1.The paper is very densely written. It would benefit the manuscript greatly to provide a figure which shows how the algorithm works for a small toy game. I ask the authors to add a figure and address the issues above. I am not an expert in this sub field so I may have missed aspects of the paper.
Accept (Poster). rating score: 6. rating score: 6. <BRK>This paper focuses on the problem of reasoning about uncertain poses and orientations.<BRK>The paper proposes a Brigham loss (based on the Brigham distribution) to model the uncertainty of orientations (an important factor for pose estimation and other tasks). This distribution has the necessary characteristics required to represent orientation uncertainty using quaternions (one way to represent object orientation in 3D) such as antipodal symmetry. +Uncertainty quantification of neural networks is an important problem that I believe should gain more attention so I am happy to see papers such as this one.
Reject. rating score: 3. rating score: 6. rating score: 6. <BRK>This paper proposes to pre train policies on some goal reaching tasks, and then leverage the associated successor features to improve the learning of a new task. Is the number of features n also hand defined? Furthermore, it seems that their method doesn’t really learn anything new in most of the tasks, it just stays at the same performance that is started with after the whole pre training steps.<BRK>The paper also needs to be more clear in terms of contributions; it seems that there is a significant overlap between this work and (Barreto et al., 2017, 2018); some clarification would be helpful here. Also the results for DIAYN are a bit surprising to me since in all the experiments the performance of the method is underwhelming; this is especially surprising because in the original DIAYN paper the method performed well in reasonably complex tasks.<BRK>The authors propose a framework for discovering a set of policies without external supervision which can then be used to produce reasonable performance on extrinsic tasks. The work exhibits originality in that it shows that disentangled representations, learned by intrinsic rewards,  can lead to learn behaviours that are transferable to novel situations. Although the problem talked here is of high relevance and the approach proposed is original and supported by theoretical results, I am leaning to reject for the following reasons:  Missing connection to some existing works in the literature. If not, it should be made clearer that this framework works on 2d domains, where the tasks are navigation tasks. (I am specifically referring to the representation learning phase).
Reject. rating score: 3. rating score: 3. rating score: 6. <BRK>This is a different problem setting and the distinction is really not clear from the first (several) pages. Once this trajectory is found, it can be used to form an initialization for a policy gradient method. Particularly finding methods that are compatible with state of the art policy gradient approaches. The quality of \hat{a}, \hat{b} seems like it should be very important... but I don t get much insight to that spelled out in the paper.<BRK>This paper proposes a method, R3L, for exploration in reinforcement learning. In R3L exploration, it considers the task as a planning problem, and applies RRT to find feasible solutions (trajectories). The paper provides theoretical guarantees of R3L exploration finding feasible solutions. Due to this reason I think this paper should be rejected. But once a generative model of the environment is given, the exploration problem will be very different with the exploration considered in RL.<BRK>Firstly, is this algorithm applicable to all RL problems? The assumptions seem quite strong, and furthermore it assumes the existence of a  goal  state, which is not always the case. In some sense it seems similar to this paper: https://arxiv.org/abs/1606.04460 which should be discussed.
Accept (Poster). rating score: 6. rating score: 6. rating score: 6. <BRK>This paper proposes a method for training Bayesian neural nets on a stream of non stationary data. I found this paper to be well written, the contribution is clear and the background material is well explained. The task is also important.<BRK>Overall assessment:Pros: I like the ideas in the paper and they are presented well. This  is a sensible heuristic because we want to keep datapoints in the memory that contribute the most to the posterior approximation. (2) The second contribution of the paper is the use of Bayesian forgetting and OU to use to deal with the data distribution shift. The paper is well written and easy to understand. What does it mean for p(dtk|w) to be well approximated by  r(w;dtk)?<BRK>My concern regarding novelty remains unchanged but I still suggest acceptance since the contributions are of practical interests and the paper is well written. 1.Summary:This proposes considers neural networks training with non stationary streaming data.
Accept (Poster). rating score: 6. rating score: 6. rating score: 6. <BRK>Overall take: this paper proposes a thorough treatment of learning with noisy labels in a semi supervised manner, designing the algorithm and testing it empirically, which is an interesting and important contribution. Most existing label noise works focus on certain noise models, but this paper is more general and proposes an interesting method to learn with noisy labels in a semi supervised manner, and the experimental results are promising.<BRK>This paper proposes an algorithm that learns with noisy labels that achieves state of the art results. It borrows the idea from both semi supervised learning and learning with label noise. It would be good to include them in the related work as well.<BRK>This paper proposed a method named DivideMix for learning with noisy labels, on top of the recent semi supervised learning method MixMatch from Google. As a consequence, it is not surprising that the latest MixMatch can work as well, since MixMatch comes from mixup, virtual adversarial training and entropy minimization. However, the significance may still be high according to the reported experimental results, and thus we may accept it to let more deep learning practitioners see the promising results.
Reject. rating score: 1. rating score: 3. rating score: 3. <BRK>Actually similar method has been proposed in Hiroshi Inoue (2018) as a data augmentation method. What surprised me the most is that after trying to connect UMixUp with adversarial training in the whole paper, there is no evaluation of adversarial robustness in the experiments? The main character of adversarial training is an improvement in robustness and degeneration on clean accuracy, which is different from the performance of UMixup or Mixup.<BRK>This paper proposes a novel data augmentation method, untied MixUp (UMixUp), which is a general case of both MixUp and Directional Adversarial Traning (DAT). Actually, both MixUp and UMixUp are shown to converge to DAT when the number of training samples tends to infinity. What s more important is that both papers propose that the mixing ratio of two samples is not linearly but depending on the strength of their signals.<BRK>This paper introduces directional adversarial training (DAT) and UMixUP, which are extension methods of MixUp. DAT and UMixUp use the same method of MixUp for generating samples but use different label mixing ratios where DAT retains the sample s original label.
Reject. rating score: 1. rating score: 3. rating score: 3. <BRK>Summary:In this paper, the authors propose a new method to apply continual learning on sequential data. The model is constructed by combining an Autoencoder and LSTM/LMN for each task. The experiments on several datasets show the proposed model outperforms basic LSTM/LMN. Thus, It is interesting to see that continual learning is used in sequential data. + The motivation of the proposed model is clear. However, most continual learning methods can still be applied in this scenario, at least regularization based methods [1,2] can be simply applied in this scenario. The authors may need to compare the proposed method with them in the future version. For example, in the natural language processing field, we can regard sentiment analysis on one language as one task.<BRK>The paper is written as if the benchmark is the main contribution and the architecture improvement is just a delta on top of this, but it gets confusing when the methods section starts off with just directly stating the new architecture. This doesn’t quite parse to me. The description of the tasks is very informal and hard to follow. The results in the experiments section are very hard to parse. The captions need much more detail for eg Table 2. Could we also possibly have more baselines from continual learning? I also think that the task descriptions should be much earlier in the paper and desribed in much more rigorous detail.<BRK>The paper proposed an interesting continual learning approach for sequential data processing with recurrent neural network architecture. The authors provide a general application on sequential data for continual learning, and show their proposed model outperforms baseline. Then, I decide to give a lower score that even the authors suggest that the main contribution is a definition of problem setting.
Accept (Talk). rating score: 8. rating score: 8. rating score: 6. <BRK>This paper presents a scalable reinforcement learning training architecture which combines a number of modern engineering advances to address the inefficiencies of prior methods. It effectively addresses the problems with prior architectures and the accompanying source code is clear and well structured. It is also extensively tested on several RL benchmarks. 2.The proposed framework is especially suited for training large models as the model parameters are not transferred between actors and learners.<BRK>The paper presents SEED RL, which is a scalable reinforcement learning agent. The approach restructure the interface / division of functionality between the actors (environments) and the learner as compared to the distributed approach in IMPALA (a state of the art distributed RL framework). The results are very good, shows good scalability, and significantly reduced training times. The paper is well written, easy to read, and I enjoyed it.<BRK>The paper proposes a new reinforcement learning agent architecture which is significantly faster and way less costly than previously distributed architectures. The paper reads very well and the experimental results indeed demonstrate improvement. Nevertheless, even though working in deep learning for years and have also some experience with Reinforcement learning I am not in the position to provide an expert judgment on the novelty of the work.
Reject. rating score: 1. rating score: 3. rating score: 8. <BRK>For example, “Note that for given (feature)”, “…make the representation of nodes in different (class) more separable.”, “Noted that there are some other (variant) of GNN filters that (does) not fall into…” in page 4. The paper proposes an assessment method called the Graph Filter Discriminant Score based on the Fisher Score. It measures how well the graph convolutional filter discriminate node representations of different classes in the graph by comparing the Fisher score before and after the filter. Based on the GFD scores of different normalization strategy and different order of the graph filter in the experiments on synthetic data, the authors answer the first two questions: (1) There is no optimal normalization for all graphs. For the third question, the authors propose a learnable linear combination of a limited family of graph convolutional filters as the layer of model AFGNN, which can learn the optimal arguments of the combination based on the FGD score. The presentation of the last paragraph of “graph filter discriminant score” in page 4 can be improved. It could be interesting to see the comparison of time between the proposed method and the GAT. For the graph filter discriminate analysis, is it fair to compare the learned layer with the other base filter using the GFD score? Maybe one or two sentences on this will be helpful.<BRK>This paper introduces an assessment framework for an in depth analysis of the effect of graph convolutional filters and proposes a novel graph neural network with adaptable filters based on the analysis. The assessment framework builts on the Fisher discriminant score of features and can also be used as an additional (regularization) term for choosing optimal filters in training. The proposed analysis using the Fisher score is reasonable and interesting, giving us an insight into the role of graph filters. Even though the analysis is limited (using simple graph models and filter family) and the result is not surprising (given no free lunch theorem, there is very likely to be no single silver bullet fo graph filters), I appreciate the analysis and the result. But, I have some concerns as follows. 1) The proposed GNN and the optimization processThe proposed method is to extend CNN to a simple linear combination of different filter bases with learnable weights, which I don t think is very novel. Adding the GFD score as an additional constraint term is interesting, but the way of optimizing the whole objective function is unclear. I hope this is also clarified in more detail. I recommend the authors use for evaluation more realistic datasets that can be found in related research.<BRK>This is a very interesting study about GNN. Authors proposed to extend LDA as a discrimination evaluator for graph filleters. Also authors proposed Adaptive Filter Graph Neural Network to find the optimal filter within a limited family of graph convolutional filters. There are some questions authors should clarify. Eq (3) defines GFD for a pair of classes i and j. For a graph with more than two classes, the GFD will be the average of all pairs? Will class imbalance will have any impact on this GFD measure? Errors:•	we studies the roles of this two components •	there exist a best choice •	we only consider to to find
Accept (Poster). rating score: 6. rating score: 6. rating score: 6. <BRK>This paper looks at how deep convolutional neural networks for image denoising can generalize across various noise levels. The authors show that most of the energy of the clean image falls into the signal subspace and the effective dimensionality of this subspace is inversely proportional to the noise level. Even though it is theoretically not too well motivated in the paper why the bias term degrades generalization performance, the experimental results seem to clearly demonstrate the merit of bias free denoisers. Therefore, I would recommend accepting this paper, if the authors provide a theoretical discussion on why the bias term might degrade generalization performance. The definition mentions dependence on noise variance, but the formula does not have it.<BRK>This paper studies the generalization properties of convolutional neural networks for image denoising. This paper shows that removing constant terms from CNN models provides strong generalization across noise levels. Also, this paper provides the interpretability of the denoising model based on a linear algebra method. Q1.Is there the possibility that the model without the constant term shows strong generalization in other image processing tasks such as image deblurring and dehazing?<BRK>This paper proposed to remove all bias terms in denoising networks to avoid overfitting when different noise levels exist. With analysis, the paper concludes that the dimensions of subspaces of image features are adaptively changing according to the noise level. An interesting result is that the MSE is proportional to sigma instead of sigma^2 when using bias free networks, which provides some theoretical evidence of advantage of using BF CNN. One main practical concern is that only Gaussian noise is considered in this paper which provides good theoretical analysis.
Accept (Poster). rating score: 8. rating score: 6. rating score: 3. <BRK>3.I ll be interested to see what the other reviewers say, but I found Figure 2 hard to follow. What is meant by "sufficiently random" here? While the authors suggest that self ensembling prevents samples from oscillating in and out of training set, is this a guarantee or an empirical observation?<BRK>Note that this is an academic/scientific paper, not an industrial product, so you don t need to combine all things that might work. The proposed method significantly outperformed all baseline methods. This is the major contribution of the paper.<BRK>The motivation of the paper is very clear.
Accept (Poster). rating score: 8. rating score: 6. rating score: 6. <BRK>The work describes an application of a spatial point process for solving problems with missing data. The method incorporates VAE framework to effectively handle missing points via smooth intensity estimation and enjoys amortized inference for efficient computations and quick prediction generation. Using a sequence of mild assumptions, the authors show connection to a popular VAE based collaborative filtering model, which turns out to be a special case of their approach. Clearly stating the region of applicability of the proposed approach would be enough. I haven’t carefully verified all the derivations, though.<BRK>ICLR reviewIn this paper, the authors propose to tackle the multivariate spatial point process model with a variational inference approach. In experiments, the results show that the proposed approach outperforms VAE based collaborative filtering on Gowalla datasets and MovieLens datasets. You are reporting NDCG@100, which is a different measure. Therefore I guess the main contribution of this paper is Eq.(9).If so, I m expecting more detailed comparison with VAE CF in the paper (not only quantitive evaluation).<BRK>In this paper, the authors propose a VAE model for spatial point processes. The model generalizes the kernel density based intensity and applies variational inference. I found the probabilistic modeling interesting. The authors might also want to provide the definitions of NDCG@k and Recall@k, at least in the appendix. I appreciate the author s detailed response and updated paper.
Reject. rating score: 1. rating score: 1. <BRK>After rebuttal:Thank you to the authors for responding to my review. I believe this evaluation is defensible, but of course the final evaluation is up to the chairs. Therefore, it is difficult to ascertain the research contribution. Before rebuttal:The submission presents a hierarchical Bayesian optimization (HiBO) approach to solving a postural control task expressed as a proportional derivative (PD) controller. Weaknesses:  The paper does not make use of representation learning (no, e.g., neural networks are employed) and is therefore out of place at ICLR.<BRK>How to quickly learn control policies with minimized number of environment interactions have long been an important problem. To tackle this problem, this paper proposed a "hierarchical Bayesian optimization (HIBO)" algorithm to optimize the "feature parameter \phi" (which I don t know what that is) and the "policy parameter \theta" hierarchically. The paper only conducts one experiment on the Humanoid control balancing. A Humanoid is expected to learn how to balance as quick as possible to reduce the interactions with the environments, which suits well with Bayesian optimization.
Accept (Poster). rating score: 8. rating score: 6. rating score: 6. <BRK>This paper first introduces a method for quantifying to what extent a dataset split exhibits compound (or, alternatively, atom) divergence, where in particular atoms refer to basic structures used by examples in the datasets, and compounds result from compositional rule application to these atoms. The paper then proposes to evaluate learners on datasets with maximal compound divergence (but minimal atom divergence) between the train and test portions, as a way of testing whether a model exhibits compositional generalization, and suggests a greedy algorithm for forming datasets with this property. In particular, the authors introduce a large automatically generated semantic parsing dataset, which allows for the construction of datasets with these train/test split divergence properties. Finally, the authors evaluate three sequence to sequence style semantic parsers on the constructed datasets, and they find that they all generalize very poorly on datasets with maximal compound divergence, and that furthermore the compound divergence appears to be anticorrelated with accuracy. This is an interesting and ambitious paper tackling an important problem. It is worth noting that the claim that it is the compound divergence that controls the difficulty of generalization (rather than something else, like length) is a substantive one, and the authors do provide evidence of this.<BRK>This paper introduces a method for generating training/test data for measuring the model s ability of "compositional generalization" in complex compositional tasks/domains such as natural language understanding, visual understanding and other domains. The experimental results verify that using their method they can obtain train test data sets with uniform atom distributions with large divergence in compound distributions, and they find that there is a surprisingly large negative correlation between the accuracy of existing state of the art learning methods and the compound divergence. and it would seem that the generation method/system and the generated data would be useful as benchmark data for the community. The paper lacks technical novelty other than the training and test data generation approach, but having one available to the community with these apparently desirable characteristics as benchmark data for measuring complex, compositional generalization capabilities, and that could be invaluable to the research community.<BRK>This topic studied in this paper is interesting and is helping to promote the following developing of algorithms with compositional generalization ability. How to control the trade off between the atom and compositional divergence? It s interesting to show how differentcompositional divergence can affect the performance of different models. 2.Many previous works are proposed for improving the generalization ability of the seq2seq models[1]. More experiments need tobe conducted using these previous methods. [1]Compositional generalization in a deep seq2seq model by separating syntax and semantics
Reject. rating score: 3. rating score: 3. rating score: 6. <BRK>The authors propose a new training regime for multi resolution slimmable networks. Pros:+ The authors correctly identify input resolution as one of the aspects of lightweight network design that is often overlooked+ They propose a practically viable training scheme that can be used to train & select networks given resource constraints+ The paper is well written and includes many insightful experimental findingsCon:The authors specify the mutual learning from width and resolution as their main contribution.<BRK>This paper proposes a multi resolution training scheme for a slimmable network. ( ) Looks like there exists missing in details of the experiments. (+) The paper is well written and looks justified well. After that, the final rating might be changed.<BRK>More in detail, the paper proposes a method of joint training of multiple resolutions networks, leveraging student/teacher/distillation from scratch. The paper is well written and presented with extensive results, comparing computational complexity/accuracy curves to existing state of the art architectures, as well as results on transfer learning to show that the feature learned do indeed generalize and don t necessarily overfit to imagenet.
Reject. rating score: 1. rating score: 6. rating score: 6. <BRK>The paper studies last iterate convergence of simultaneous gradient descent and related algorithms in (convex concave) GANs. The experiments are very weak. However, when it comes to experiments, they only compare against vanilla Adam (!) I would summarize the experiments as *suggesting* that anchoring doesn’t hurt on MNIST or CIFAR 10. The motivation for the paper is GANs, but GANs are not convex concave. The analysis is therefore not directly relevant. From the experiments, it is not clear *at all* whether anchored Adam is *actually* an improvement in practice over any of the alternative algorithms that the authors discuss or cite.<BRK>*Summary*This paper provides the analysis of three algorithms in the context of minmax convex concave games: Simultaneous stochastic subgradient method, Simultaneous gradient with optimism and Simultaneous gradient with anchoring. This subsection is basically the proof of the convergence of the continuous version of  GD O. Stating the result before the proof would help the reader to understand where the authors want to go. A FID above 40 for MNIST is very far from standard results (that are below 1). Is the condition $\epsilon$ only necessary for the proof ?<BRK>This paper analyzes the dynamics of stochastic gradient descent when applied to convex concave games (motivated by the game used to train GANs, which is typically not convex concave), as well as the previously proposed GD with optimism and a new anchored GD algorithm that provably converges under weaker assumptions than SGD or SGD with optimism. The samples in Figure 3 are pretty far from the state of the art, and in any case Figure 3 doesn t even say which training method generated them.
Accept (Poster). rating score: 6. rating score: 6. rating score: 6. <BRK>In this work, authors show that the bad performance of Adam is from the large variance of adaptive learning rate at the beginning of the training.<BRK>In the first part of the paper, the authors analyzed the variance issue exists in the existing ADAM algorithm, such that with limited samples in the early stage of training, the variance of the adaptive learning rate becomes rather large and it induces high variance to the gradient update to ADAM.<BRK>Authors propose a way to rectify the variance of the adaptive learning rate (RAdam) and apply the optimizer to applications in image classification, language modeling and neural machine translation.
Reject. rating score: 3. rating score: 3. rating score: 6. <BRK>The paper introduces a method to self supervised train a model for object detection/segmentation. Here are some high level concerns. 1.As mentioned in the "Implementation details",  naive end to end training is difficult... we use ImageNet trained weights for initialization . To justify the claims and effectiveness of the method, it should include a comparison with [R1], which demonstrates the possibility of doing detection with a pretrained model. Other work along this line should be also good reference. This is also self supervised learning. At least such a self supervised trained model can act as an initialization. Considering the above points, the paper does not appear compelling, due to lack of either careful claims or justification.<BRK>This paper provides a new self supervised proposal based approach for object detection and segmentation. The author introduces a Monte Carlo based optimization to solve the inefficiency problem in the discrete proposal based forward process defined in (Crawford and Pineau 2019). A little bit of a philosophical question is whether this a problem worth pursuing as well.<BRK>This submission proposes a self supervised segmentation method, that learns from single object videos by finding the region where it can segment an object, remove the entire bounding box around it, inpaint it, then finally put the object back. Croitoru et al.also relies on video to extract the object features, and this requirement is not as explicit in this work. Method:The method is interesting and clever. Ski PTZ Dataset should then offer a better comparison, but here the method struggles to compete with Croitoru et al.2019, which has a 11 point higher F measure.
Reject. rating score: 3. rating score: 6. rating score: 6. <BRK>Summary: This work leverages knowledge distillation both in pre training and fine tuning stages to learn a more compact student model that approximates the performance of a teacher model. Extensive experiments with different knowledge distillation loss functions are conducted on a number of representative language representation models including BERT, GPT 2 and MASS to show the benefit of the method. Weaknesses:My main concern is about the limited novelty of approach.<BRK>I tend to think that the paper should get accepted though since the empirical work is really exhaustive and impressive. The authors propose a method for distilling language models such as BERT into much smaller versions with similar accuracy. It can be very useful in practice.<BRK>As opposed to previous works, where distillation is performed in the fine tuning stage, the authors of LightPAFF propose a two stage distillation procedure instead that performs distillation at the pre training phase and fine tunes this distilled model for use in a downstream task using a big fine tuned teacher model and the dataset of the task.
Accept (Poster). rating score: 8. rating score: 6. rating score: 3. <BRK>Training for 5,000 sentences * 10 epochs may just not be enough for the RL fine tuning to make a big difference. In fact, some of the performance gains of using REINFORCE/CMRT can be attributed to making the model s output probability distribution more peaked, and not necessarily from making the target tokens more probable as commonly assumed. I have summarised the key strengths of the paper below, along with several suggestions and questions that I hope will be addressed by the authors. If this is the case, this should be made clearer.<BRK>Similar BLEU scores are obtained with either type of rewards, which is an interesting and perplexing result (in my opinion). However, the analysis may still be beneficial to the community. I would assume the peakiness effect to be mostly neutral with normalized rewards, or diminish with negative rewards (on average). Does this distinction matter? In appendix A.1, some values are wrong. Do these affect the final result? Update to Review 2I appreciate that the authors corrected appendix A.1. As a nitpick, for the gradient of the reward of {a, c}, there is a x that should be replaced by \theta. While the scope of the paper is somewhat limited, the theoretical contributions about MRT are valuable.<BRK>This work carefully studies RL for neural machine translation and draws several conclusions:1. One of the most common RL methods for MT does not optimize the expected reward, as well as show that other methods take an infeasibly long time to converge. 3.Observed gains may be due to effects unrelated to the training signal, concretely, changes in the shape of the distribution curve. Previously I had concerns about the convergence of REINFORCE with deep NNs as its policy. After checking the references provided by the authors, the local convergence can indeed be guaranteed. 2."reducing a constant baseline from r, so as to make the expected reward zero, disallows learning." This conflicts with my intuition. Where is the experiment supporting this claim?
Accept (Poster). rating score: 6. rating score: 6. rating score: 3. <BRK>The paper proposes a using pixel adaptive convolutions to leverage semantic labels in self supervised monocular depth estimation.<BRK>This work proposes to leverage a pre trained semantic segmentation network to learn semantically adaptive filters for self supervised monocular depth estimation. Post rebuttal update  The authors have addressed many of my initial concerns and provided valuable additional experimental evaluations. The proposed technical approach is to use the pixel adaptive convolutions by Su et.<BRK>Similarly, Fig 4 shows there is advantage in the use of the pre trained semantic network, it is not clear if this difference is significant. The paper is well written and easy to follow. What does a RMSE difference of 2.3 mean in the context of depth estimation?
Accept (Poster). rating score: 6. rating score: 6. rating score: 3. <BRK>Summary: This paper proposed a new adversarial attack method based on model based RL. Unlike existing adversarial attack methods on deep RL, the authors first approximate the dynamics models and then generate the adversarial samples by minimizing the total distance of each state to the pre defined target state (i.e.planning). Using Cartpole, Fish, Walker, and Humanoid, the authors showed that the proposed method can pool the agents more effectively. It would be interesting if the authors can consider this case.<BRK>This paper presents an adversarial attack for perturbingthe actions or observations of an agent acting near optimallyin an MDP so that the policy performs poorly. This paper presents an empirical step in the directionof showing that such attacks are possible, but in the contextof the other adversarial attacks that are possible, this isnot surprising alone and would be much stronger withother contributions.<BRK>  *Synopsis*:  This paper looks at a new framework for adversarial attacks on deep reinforcement learning agents under continuous action spaces. They propose a model based approach which adds noise to either the observation or actions of the agent to push the agent to predefined target states. *Review*  The paper is well written, and has some interesting discussion/insight into attacking deep RL agents in continuous actions spaces. I think these details can be safely placed in the appendix, but should appear somewhere in the final version.
Reject. rating score: 3. rating score: 3. rating score: 6. <BRK>Summary: This work proposes an unsupervised hierarchical graph representation learning method, named BayesPool. The method learns a coarsening sequence of graphs together with the corresponding node representations. The coarsening sequence is learned using the method in Loukas (2019). The node representations are learned using an encoder decoder structure, where the encoder encodes a graph to coarsened node representations, and the decoder decodes the node representations to a coarsened graph. However, they only provide very limited experimental results, which is not very convincing. Moreover, the authors also do not explain clearly on when the node representation of the coarsening sequence is needed.<BRK>The paper proposes an unsupervised approach to learn a representation of graphs. The idea comes from an encoder decoder architecture, which is common in related literature. The paper uses a variational Bayes approach in the learning process. However, I vote for rejecting this submission for the following concerns. (1) I did not find too many significant differences between this paper and [Kingma & Welling, 2014] in the design of encoder decoder architecture as well as the learning procedure (I am not an expert in this area so please correct me if I am wrong). I think it would be helpful to demonstrate the representation power of the learned representation of the graph in tackling other tasks.<BRK>The authors propose in this paper a new unsupervised graph representation learning method. The method leverages recent advances in graph coarsening, mainly Loukas  method. The experimental evaluation is quite thorough and shows that the method performs quite well, especially considering it is unsupervised but is compared to supervised representation methods. A missing part would be to explore the relevance of the learned representation for other tasks (i.e.to use a multi task data set). Of course as the representation is learned in an unsupervised way, one can argue that the current evaluation is already providing an answer. Overall, I find the paper clear, but the variational bayes part could be much clearer.
Reject. rating score: 3. rating score: 3. rating score: 6. <BRK>However, as stated by other reviewers as well, the claims of the paper are quite ambiguous. I do not understand equations 2 to 5. This is not clear to me. I would not object to accepting the paper but I find it difficult to recommend accept for this paper. Perhaps the authors can be more clear in their claims. For figure 3 again what datasets were used? The paper, in turn, proposes an early stopping criterion for handling label noise. The learning stage where the highest accuracy on the test set is achieved. A comparison of this sort would have been useful. The y label says accuracy but it seems that the plot is about loss. The plot shows that the DNN achieved a 100% accuracy in 5 epochs. The paper says that these stages are persistent across multiple architectures and datasets and as proof the paper says ‘we verified that’.<BRK>This paper proposes a new loss function for dealing with label noise, claiming that the loss function is helpful in preventing overfitting caused by noisy labels. (3 The assumptions \hat{p}+\hat{k}+\hat{l} 1 is very strong to me.The events should be dependent. This makes all the theoretical analyses pseudo and not convincing at all. The authors may spend more effort to make the part clear, reasonable, and convincing. For example, "the key point is that it always holds on some degree", on which degree and why always holds? Therefore,"Overall, I cannot understand why the proposed loss function works and cannot recommend acceptance for the current version.<BRK>Update after rebuttal:The good:The rebuttal and updated paper address many of my concerns. Most importantly, the updated paper demonstrates the three stage phenomenon on Open Images and adds experiments on IMDB showing that the Gambler s loss with AES helps a lot. This needs to be clarified with the way Figure 12 turned out. It opens with the observation of three distinct stages when training in the presence of label noise. Importantly, there is a ‘gap’ stage during which the network has not begun memorizing noisy labels and early stopping is ideal. Specifically, the improvements on CIFAR 10 only appear for large corruption rates (0.7+), and performance is lower than the baselines for other corruption rates. However, seeing as this is a distinct approach from the baselines and that it demonstrates some promise, I recommend borderline accept. Other points of concern that I have are listed below. The idealized gap assumption is behind the AES criterion, but Figure 5 suggests that the AES criterion works well on CIFAR 10, so what do the authors mean when they say the assumption doesn’t work as well on CIFAR 10?
Reject. rating score: 6. rating score: 6. rating score: 6. <BRK>I think the method is straightforward and reasonable with only a few questions:1. The paper introduce two kinds of method to regularize the entropy.<BRK>Summary:The authors propose a method for training easy to quantize models that are quantized after training (post training quantization). Weaknesses of the paper:  The authors could have applied CAT to other tasks such as Image Detection, while proving inference times on CPUs.<BRK>In this article, the authors propose a compression aware method to achieve efficient compression of feature maps during the inference procedure.
Reject. rating score: 1. rating score: 3. rating score: 6. <BRK>The paper proposes an end to end joint model for named entity recognition (NER) and relation extraction (RE), using pre trained language models. Although the paper is well written and shows good results, I would reject the paper because:   the idea is trivial and simple. I don t think there s significant novelty here: all the components are existing and combining them seems very trivial to me. the good performance seems to be from BERT rather than the model s structure (table 2 suggests that). I thus think the contribution of the paper is pretty not significant. I think the paper does not fit this conference. It is better to be presented in a Demonstration section at a *ACL conference.<BRK>The paper proposes a new joint learning algorithm that works for two tasks, NER and RE. The model is based on a pre trained BERT model, which provides the word vectors of the input word sequence. The second branch uses entity labels predicted by the first branch, so joint learning may benefit both tasks. The design of the architecture is novel, but it is also not groundbreaking. The writing is generally clear. 2.I d like to see another ablation study of whether RE helps NER. Writing: 3. how are predicted labels embedded?<BRK>The paper presents an end to end methods for jointly training named entity recognition (NER) and relation extraction (RE). The model leverage pre trained BERT language models, making it very fast to train. Pros:  the paper is well written and very clear  the proposed model has two main advantages: (1) it is very fast to train due to the use of pre trained BERT representations and (2) it does not depends on any external NLP tool (such as dependency parser)Cons:   I think the main source of improvement comes from the BERT representations used as input. As proposed in the comments, this should be assessed in the paper by replacing BERT representations by non contextual representations such as GloVE. Without this ablation study, the contributions of this paper are to show that using BERT representations as input (1) leads to better performances for NER+RE  and (2) makes the model faster to train. This is not really surprising...
Reject. rating score: 3. rating score: 3. rating score: 6. <BRK>The claim is supported with experiments on CIFAR10, ImageNet and COCO. The overall results are modest improvements, but the point the authors make is that reducing collapse is correlated with improved performance. I selected to reject the paper because in its current form a general readership will struggle to understand it. An expert familiar with the details of the field (not just the general area) can probably disentangle the text, but otherwise it s too convoluted.<BRK>This paper studies the channel collapsed problem in CNNs using  BN+ReLU  . Experiments on ImageNet and COCO demonstrate that the proposed CE block can achieve higher performance than the conventional CNNs by introducing little computational complexity. The authors give a explicit explanation as well as prove of the proposed scheme.<BRK>I like the experiments with both heavy and light network architectures to show the effectiveness of the proposed method. (+) The authors provided plausible and sufficient backups for the claim. If the authors could address all the concerns above and refine the paper with better readability, then I could increase the score. I recommend the authors define them for clarity. Please specify the way of measuring it. (a) and 3.(b). Please clarify this by comparing with the proposed method. Please clarify the way of measuring the sparsity ratio in Figure 3d.
Accept (Poster). rating score: 8. rating score: 8. rating score: 6. <BRK>  SummaryA method to predict likely type of program variables in TypeScript is presented. Experiments show that the method substantially outperforms sound typing in the TypeScript compiler, as well as a recent method based on deep neural networks. The hyperparameter selection regime (and the experiments used to find them) is not described  RecommendationThis is an application driven paper with nice practical results.<BRK>To test this, an ablation where LambdaNet does *not* split the identifiers, would provide a better comparison among the sequential representation of DeepTyper and the type constraint graph of LambdaNet. This paper presents a GNN based method for predicting type annotations in JavaScript and TypeScript code. It would be useful to know how LambdaNet compares to JSNice too.<BRK>This paper proposed to use Graph Neural Networks (GNN) to do type inference for dynamically typed languages. Overall this paper tackles a nice application of GNN, which is the type prediction problem that utilizes structural information of the code. 2) The construction of the dependency graph is heuristic.
Reject. rating score: 3. rating score: 6. rating score: 6. <BRK>It is unclear if the proposed method is competitive with the methods above. Experimental results for dialog response generation, summarization, and image generation (as a grammar model on images) demonstrate that the proposed method is faster than the original Scheduled Sampling. The second paper proposes MIXER, which is a method based on reinforcement learning. Now I lean to reject this paper because of the concerns above.<BRK>This paper proposes a technique for scheduled sampling that can be parallelized. The proposed method generally obtains performance on par with scheduled sampling, but is much faster. The paper is well written and the method clearly explained.<BRK>The explanation of the proposed scheduled sampling can be much simplified. There are other factors that can affect performance or make parallelization effective. I wonder what are the performance gains of parallel scheduled sampling on normal scheduled sampling with respect to avg. One of the outcomes of this method, that might not been stressed enough in the paper, is a neat way of doing scheduled sampling for the transformers architecture which wasn t straight forward before.
Reject. rating score: 1. rating score: 3. rating score: 8. <BRK>The paper proposes a method inspired by the categorical cross entropy (CCE). S NCA also plays with the temperature but does not learn it. On the other hand, learning the temperature in a similar way has been proposed in the deep metric learning literature (e.g.TADAM).On the other hand, the paper has some nice contributions:  The main "novelty" of ICE seems to be the analysis in Section 3.4 of the partial derivatives and their impact on the sample reweighting. I vote for reject for the following reasons:  The paper is too similar to NCA and S NCA (introduced in Section 2.2).<BRK>Overview  This paper proposes a new objective function called Instance Cross Entropy (ICE) for metric learning. I found the paper quite difficult to follow. However, I think the technical presentation needs to be improved significantly in order for the paper to be accepted for publication. I would highly recommend the paper be reformatted and the authors use brackets around references.<BRK>The paper proposes a method to measure the difference between anestimated instance level matching distribution and its ground truthone, based on instance cross entropy (ICE). The authors should explain in more detail how is the non lineartransformation achieved. The paperwill benefit if time results are reported for different approaches. The authors performed several experiments with convincingresults.
Reject. rating score: 3. rating score: 3. rating score: 3. <BRK>The paper addresses the problem of survival analysis (predicting time until, e.g., death) using topic modeling. The paper presents another method based on archetypal analysis, which adds a supervised term to the loss.<BRK>Their contributions are: (1) integrating two different topic modeling approaches with survival models for joint learning and (2) showing results on two medical datasets with some brief analysis of what topics are recovered. This is an interesting task to be using with topic modeling.<BRK>The paper considers the problem of interpreting the predictions for survival analysis using topic models. novelty is limited since most pieces were already present in previous work.
Reject. rating score: 1. rating score: 1. rating score: 3. <BRK>This paper proposes an extension to gradient episodic memory (GEM) to improve its performance and backwards transfer. However, I have a number of concerns which I think, unfortunately, preclude publication at this point. Primarily, while I believe the ideas are intuitive and novel, the experimental evaluation does not appear to support the claim that the proposed method significantly improves GEM (only a minor improvement is shown at best). Some claims in the paper also need to be tempered:  The abstract suggests performance improves "remarkably", but experiments do not support this.<BRK>This paper presents three improvements to the previous GEM algorithm: choosing support examples better (the GEM paper used a random set), incorporating soft gradient constraints, and specifying the magnitude of the dot product in the gradient optimisation problem (in order to increase positive backward transfer). I recommend to reject this paper. I will now elaborate on these points. Secondly, this paper introduces a significant number of hyperparameters over the original GEM algorithm. The method of choosing the support set also makes sense to me, however there are many hyperparameters in this idea. This indicates to me that improvements need to still be made to the idea.<BRK>Paper proposes three improvements upon the gradient episodic memory (GEM) method [Lopez Paz,2017]. The update is improved by promoting positive backward transfer, not only limiting the gradient to the constraints imposed by exemplars of previous tasks, but aiming to improve also for previous tasks by preferring gradients which have high cosine similarity with the gradients on exemplars. Results on CIFAR 100 and MNIST permuted are presented. Conclusion: The proposed improvements seem sensible. However, the paper should have built upon the more recent A GEM paper. I, therefore, recommend a weak reject. Their analysis should compare with this paper. Are they also averaging over the unseen classes? The results of the ablation do not convincingly show the merits of two of the three proposals. (error in the legend the support+soft should be green).
Accept (Poster). rating score: 8. rating score: 6. rating score: 3. <BRK>However the stated task is certainly a non trivial one, and the qualitative results and experiments give compelling evidence that the authors are proposing a powerful framework for conditional image generation. Summary   In this work the authors propose a two stage procedure for training a GAN which generates plausible faces conditioned on the raw waveform of a speech signal.<BRK>4) The format of references should be consistent. Experimental results show that the proposed network can not only match the relationship between the human face and speech, but can also generate the high quality human face sample conditioned on its speech. My concerns are as below. This paper presents a multi modal learning framework that links the inference stage and generation stage for seeking the possibility of generating the human face from voice solely based on the audio visual data without any human labeled annotations.<BRK>The whole framework is built on top of cGAN based projector discriminator framework where the input condition vector is also used in the discriminator stage. The authors compared with one recent work and demonstrated improvements on face retrieval experiment. 2	Based on the listed generated examples in Fig.2, most faces are frontal, especially along Z axis, not sure if the variation of Z can determine the head orientation.
Accept (Spotlight). rating score: 8. rating score: 8. rating score: 6. <BRK>The author question an important aspect which is very often taken for granted in the RL community, that a good representation could lead to data efficient RL. They show negative results, providing pessimistic lower bounds for both value based and policy based learning. Additionally, it offers some hope for sample efficient RL, by discussing the exponential separation between policy based RL and imitation learning, reminding the community that sample efficient RL can still be achieved by IL even if it can’t be achieved through good but not perfect representation.<BRK>The derived theorems show that there exists MDPs which require an exponential number of samples to learn a near optimal policy even if a good but not perfect representation is given to the agent for both value based and policy based learning. Overall, I think this paper presents a solid contribution and recommend acceptance.<BRK>This paper s contribution is a sample complexity lower bound for linear value based learning and policy based learning methods. The bound being exponential in the planning horizon is bad news, and has some implications with respect to further analysing sample complexity in RL. The gist of this paper is that one can craft a hard MDP which requires visiting every state at least once, and that since this MDP s state space is exponential in the MDP s horizon, then there exists a set of MDPs which require an exponential (in the horizon) number of trajectories to be solved. The writing of the paper is good, I was able to understand everything (I think).
Accept (Spotlight). rating score: 8. rating score: 6. rating score: 6. <BRK>The paper first clarifies some points of confusion regarding RL as inference, namely the fact that RL was originally an inference problem all along. “... RL as inference has inspired many interesting and novel techniques, as well as delivered algorithms with good performance on problems where exploration is not the bottleneck (Gregor et al., 2016)”.<BRK>The authors develop a criticism of the "RL as inference" standard approximations and propose a simple modification that solves its main issues while keeping hold of its advantage. I hope this will be completed in the final version.<BRK>The paper always talks of shortcomings (plural) of RL as probabilistic inference, but only provides one argument (suboptimal exploration) with respect to this. The changes made to the paper overall address my concerns. While I did not read all of the appendix, Section 5.1 is much more readable and understandable in the new version.
Reject. rating score: 3. rating score: 3. rating score: 6. rating score: 8. <BRK>The paper proposes to use the mirror descent algorithm for the binary network. The paper is easy to read and follow, and the main contributions are clearly stated. Q2.It is not clear to me, why mirror descent is better than proximal gradient descent, i.e., proxQuant, in this application. The paper will be more convincing with these methods.<BRK>This paper proposes a neural network (NN) quantization based on Mirror Descent (MD) framework. The proposed method is a natural extension of ProxQuant, which adopted the proximal gradient descent to quantize NN (a.k.a $\ell_2$ norm in MD). However, the authors do not analyze the convergence of the MD with nonconvex objective function in NN quantization neither how to choose the projection for mirror mapping construction.<BRK>This paper proposes a Mirror Descent (MD) framework for the quantization of neural networks, which, different with previous quantization methods, enables us to derive valid mirror maps and the respective MD updates. Although the novelty of this work is somehow limited, i.e.appling MD from convex optimization to NN quantization, the authors provides sufficient effort to explore how to success to adopted it the literature.<BRK>A good paper that uses the Mirror Descent paradigm for learning quantized networks. Though Mirror Descent is not their original idea, but using it in the context of learning quantized network is novel and interesting. Empirically, they showed better results than existing method, with comparisons with reasonable baselines including using relaxed projected gradient descent. Also you mention MD can be used for more than quantization, but compression in general, it’d be better to add that discussion, or remove this sentence.
Reject. rating score: 3. rating score: 3. rating score: 6. <BRK>The paper proposed a new adversarial text generation framework based on tree structured LSTM. Compared with two existing methods, the proposed method gives better successfully attacking rates. The tree structured LSTM model is an existing work but applying it to generate adversarial text is new. The difficulty of generating good adversarial text lies 1) high success rate and 2) the generated texts are reasonable (e.g.syntactically correct) and are not contradictory to the original texts. The paper achieves good success rate based on its experimental results but doesn t convince me that 2) is also guaranteed. Is this method not effective to attack QA task? More examples can be added to reduce "noise" mentioned in the paper. For example, is the autoencoder trained by the authors or is from the existing model?<BRK>Motivated by recent development of attack/defense methods addressing the vulnerability of deep CNN classifiers for images, this paper proposes an attack framework for adversarial text generation, in which an autoencoder is employed to map discrete text to a high dimensional continuous latent space, standard iterative optimization based attack method is performed in the continuous latent space to generate adversarial latent embeddings, and a decoder generates adversarial text from the adversarial embeddings. Pros: This paper is well written overall. All the techniques are standard or known. 3) It is unclear why tree structured LSTM instead of a standard LSTM/GRU should be chosen in this framework for adversarial text generation. Therefore, it is valid to argue that whether the proposed framework can generate legitimate adversarial text to human readers or not. 6) It’s better to include many examples of generated adversarial text in the appendix. are also missing. However, some technical details are questionable, and the produced results without rigorous metrics seem to be unconvincing.<BRK>This paper proposes a new attack framework AdvCodec for adversarial text generation. The main idea is to use a tree based autoencoder to embed text data into the continuous vector space and then optimize to find the adversarial perturbation in the vector space. Experimental results on sentiment analysis and question answering, together with human evaluation on the generated adversarial text, are provided. On the other hand, it is not clear to me why the proposed method would not change the ground truth answer for QA. Currently the authors claim to achieve this by carefully choosing the initial sentence as the initial point of optimization, which seems a bit heuristic.
Reject. rating score: 1. rating score: 3. rating score: 3. <BRK>This paper proposes to introduce adversarial perturbations into intermediate layers of a neural network, to achieve more efficient adversarial training. The best results are often obtained by only training on adversarial examples.<BRK>This paper proposes perturbation biases as a counter measure against adversarial perturbations. The perturbation biases are additional bias terms that are trained by a variant of gradient ascent.<BRK>The more powerful projected gradient descent should be used for a better test against adversarial examples. There are serious clarity issues with the writing of this paper. The authors should start with how the new bias terms affect inference and predictions, before going to their updates. There are also limitations in the experiments.
Reject. rating score: 3. rating score: 3. rating score: 6. <BRK>The convergence and time complexity are analyzed. Actually, the proposed framework is learning an extra graph adjacency matrix from nodal features, and further train GNN jointly on those two graphs. Therefore, the analysis of \lambda is necessary. However, this part is missing in the paper. Besides, LDS only uses the optimized graph structure to train GNN, while the proposed framework use both learning structure and the original one (or the kNN result).<BRK>Strengths:1）the paper proposes a learnable similarity metric function and a graph regularization for learning an optimal graph structure for prediction. 2）Besides raw node features, the paper attempts to optimize graph structures via learned node embeddings in an iterative manner. Weaknesses:1）Compared with LDS [1], this work seems to overlook the bi level optimization problem for learning model parameters based on the optimal graph structure. The reason behind this method is expected. 5) Although this method is claimed efficient, it is indeed slower than the classic GNNs due to the iterative operation. Those three datasets are popularly used in GNNs as testbed. Overall, this proposed method is well motivated, but the technical novelty is limited.<BRK>The paper introduces an iterative method called IDGL for learning both the graph structure (more precisely adjacency matrix) and parameters of the graph neural network. 4, 5, 6 (which should be considered to be the most important in the paper) are exactly similar to those in [1] (see Eq.12 in [1]). This reduces the novelty of the paper. About the experiments, I have several concerns. [1] How to learn a graph from smooth signals, Kalofolias et.
Reject. rating score: 1. rating score: 3. rating score: 6. <BRK>For the example that the authors provided, $\sin x$, why being able to represent that kind of function is an advantage for the graph classification problem? I could not find a strong contribution of this paper. This paper pointed out the stability problem (which is also discussed in [2,3]), but do not provide a solution in the domain of continuous deep models. Then it becomes a standard invertible DL model with discrete layers, where the invertible building blocks have a specific *restricted form*. about the invertible building blockThe proposed invertible building block replaces  sum  in [5] by a function \psi. This is not novel enough to serve as a contribution. A stronger result is needed to demonstrate the advantages. 2018.[2] Chang, Bo, et al."Reversible architectures for arbitrarily deep residual neural networks." International Conference on Machine Learning. From my point of view, their difference is that ODE model is more smooth. However, the benefit of using a smoother model is still unclear.<BRK>This paper proposed the Neural ODE on a graph, termed with a graph ODE, to tackle this problem. Finally, regarding the empirical performance of graph ODEs, the performance improvement from existing GNNs is within the standard deviations. On the other hand, if the authors are interested in the extension of neural ODEs to graphs, I expect a more detailed relationship between the neural ODEs framework and underlying GNNs. Since Theorem 1 is applicable not only graph ODEs but also Neural ODEs, it implies that ordinal Neural ODEs are also vulnerable to the instability. "Deeper insights into graph convolutional networks for semi supervised learning."<BRK>Summary:  This work extends Neural ODE to graph networks and compares the continuous adjoint method with propagation through the ODE solver. The paper addresses an interesting and important problem and it is well written in general. To determine the significance of this work, I have two questions:Question: 1. What is the major difference between the original Neural ODE and the Graph Neural ODE? 2.It seems in Mechanical engineering, various adjoint methods such as Discrete adjoint (e.g.[1])  has been studied. How about the average? How is the runtime comparing normal NN, adjoint, and direct propagation. Runtime has been a major disadvantage for Neural ODE.
Reject. rating score: 1. rating score: 3. rating score: 8. <BRK>However, the following concerns are important to clarify for the authors:1) Compared to existing work, the proposed method can preserve high data utility due to the fact that the proposed method doesn t ensure differential privacy for the discriminator. How about the discriminator is attacked? I agree for some applications, we can have this assumption about discriminator, but it is very important to better understand the limitation and risk of this assumption. The real world applications are not simply defined by us. It would be convincing to report experiments on more datasets such as CIFAR 10 and others. 4) For experiment comparison and analysis, the authors adopt quite large epsilon, i.e., \epsilon   1 and \epsilon   10. Can the authors report experiment comparison with such meaningful epsilon?<BRK>This paper studies the problem of differentially private data generator. Inspired by the general GAN framework and the PATE mechanism, the authors propose a new differentially private training algorithm for data generator. The problem of training data generator with privacy guarantee considered in this paper is very interesting, and the proposed algorithm looks novel. In addition, what is data dependent Renyi differential privacy? In Theorem 7, there are some constraints on different parameters, will them be satisfied by your algorithm? 6.How will the number of teacher models affect the privacy guarantee? 2.Algorithm 2 should be moved to main context.<BRK>This paper presents a framework for Differentially private data generation that enables more accurate training of downstream learners without compromising on the privacy guarantees. The key insight is to introduce an ensemble of teacher discriminators in the GAN formulation instead of a single discriminator. A gradient aggregator is also introduced for transmitting loss signal to the student without losing privacy by using adversarial perturbations and random projections. I have very little knowledge of this field, but the main idea idea seemed quite novel and insightful. The experimental results seemed quite good, with improvements across the board.
Reject. rating score: 3. rating score: 6. rating score: 8. <BRK>Detailed comments:  MelNet is not a "fully end to end generative model of audio". MelNet can model the long range structure for unconditional generation of speech, but its audio fidelity is not as good as autoregressive or non autoregressive models on raw waveforms. It would be more interesting if MelNet jointly models the magnitude and phase information. My major concern is about the usefulness of the model:1) The unconditional speech generation is an uncommon & less useful task in general. If the task is purposely constructed, the learned representation is more useful than the generation itself (e.g., van den Oord et al.2017).However, the authors have not demonstrated the usefulness of the learned representation for any downstream task.<BRK>In this paper the authors present a new generative model for audio in the frequency domain to capture better the global structure of the signal. For this, they use an autoregressive  procedure combined with a multiscale generative model for two dimensional time frequency visual representation (STFT spectrogram). Cons: (1)The use of  STFT is not justified why not wavelet spectrogram to capture both scale and time?<BRK>The authors introduce MelNet, an autoregressive model of Mel frequency scaled spectrograms. The challenge the authors are attempting to address is modeling of audio structure on both long and short timescales. The continuations of primed examples in both domains are particularly impressive qualitatively, as they maintain much of the character of the priming sample. * The paper is a bit thin on metrics. Human listening studies compare long term structure, but not short scale fidelity.
Reject. rating score: 3. rating score: 3. rating score: 3. <BRK>SummaryThis papers proposes a recursive parameterization of gates in a recurrent model. I believe for a fair comparison they should be the same. The approach shows slight improvements over baselines on a range of tasks. Questions to Authors<BRK>The gating function is applied recursively for N number of steps and depth of recursion is learned softly in data driven function. Would be open to discussions and raising scores if authors convince me otherwise. I would suggest the authors to remove word non autoregressive and just stick with word parallel. I believe that not mentioning these papers and not comparing to them empirically  this is a major drawback of this paper.<BRK>This paper proposes a neural sequence modelling unit called METAGROSS. In principle, the aim of this unit is to introduce recursive parametrization of  gating functions, building on the gated RNN paradigm. However, this figure is not referenced in the text and not explained further. The authors also show that integrating a non autoregressive variant of the proposed meta controller into the Transformer architecture can also be beneficial.
Reject. rating score: 1. rating score: 3. rating score: 3. <BRK>This paper proposes a defense against black box adversarial attack. The authors train an ensemble of deep networks, and output a null label when the ensemble disagree. The idea is itself new, but very similar ideas are well known in the literature, and it is difficult to conclude that the proposed approach is superior.<BRK>First of all, I think the authors do not do enough literature research on the topic of adversarial examples:1. In Sec 2., the authors only mention the FGSM in White box Attacks. Only claiming effectiveness under black box attacks is not informative or convincing. The experiment results are also weird. This clean performance is not acceptable, no matter how robust is the model.<BRK>Summary: This paper proposes the concept of buffer zones and suggests to use unanimous voting as a way to induce such buffer zones. I think the conjecture that buffer zones are widened when the models are diverse deserve to be empirically tested. Why did the authors settle on Ax + b in particular? Why not xA + b? I just want to know that the paper is doing due diligence regarding related work in this setting.
Reject. rating score: 1. rating score: 3. rating score: 6. <BRK>This paper describes the "OmniNet" architecture, which is essentially a transformer to convert any 2 dimensional (time and spatial) input into a sequence of output tokens. The idea is that a single model (the "Central Neural Processor") would learn to perform multiple tasks on multiple inputs at the same time. There is other, published work on multi task training for CTC models w.g. Is there any indication of this happening in the proposed work?<BRK>The authors propose an extended and unifying learning architecture – OmniNet  based on transformer, which tackles tasks with various modalities such as images, text and videos. The proposed model has multiple peripheral networks each majoring on one unique modality of data. Overall, this paper is well written, and technically sounds, with comprehensive experimental results. However, I still have two concerns below that prevent me from giving a direct acceptance. 1.However, considering the proposed model attempts to solve the multi task learning problem, there seems no multi task learning methods compared as baselines, making it hard to justify the performance.<BRK>and discuss on the differences of your model w.r.t.previous work (this is completely missing in the paper). I am pointing to some references below. However, I would say the setup is well engineered. Arguments:1) There are many works in multi task learning after Luong et al., 2016, please refer them in the related work section (this section is very short!)
Accept (Talk). rating score: 8. rating score: 8. rating score: 6. <BRK>The paper proposes learning a branching heuristic to be used inside a branch and bound algorithm used for solving integer programming problems corresponding to neural network verification. As a baseline it would be good to include the results for branch and bound using strong branching. I’m surprised that the reduction in the number of branches closely follows the reduction in the running time. I enjoyed reading it.<BRK>Summary:This paper deals with complete formal verification of Neural Network, based on the Branch and Bound framework. The authors focus on branching strategies, which have been shown to be a critical design decision in order to obtain good performance. Comments:* "This allows us to harness both the effectiveness of strong branching strategies and the efficiency of GPU computing power". * The description of the Nodes indicates that all hidden activation have a representative node in the GNN.<BRK>This paper proposes to use graph neural networks (GNNs) to replace thesplitting heuristic in branch and bound (BaB) based neural network verificationalgorithms. 5.There have been a few strong baselines in this field that the authors do notdiscuss and compare against, including [4][5][6]. The authors should make sure to include the ablation study results, anda detailed discussion on training data generation time and training time in thefinal version of the paper. Questions and suggestions for improvements:1.
Reject. rating score: 1. rating score: 6. rating score: 6. <BRK>This paper performs an empirical comparison of a number of model based RL (MBRL) algorithms over 18 benchmarking environments. The authors propose a set of common challenges for MBRL algorithms. I appreciate the effort put into this evaluation and I do think it helps the community gain a better understanding of these types of algorithms. My main issue with the paper is that I don t find the evaluation thorough enough (for instance, no tabular environments are evaluated) and the writing still needs quite a bit of work. I encourage the authors to continue this line of work and improve on what they have for a future submission! Model based are also mostly run on simulations, so sample efficiency is not necessarily the cause model free are only run on simulations. Preliminaries: "In stochastic settings, it is common to represent the dynamics with a Gaussian distribution", this is only for continuous states. Sec 4.1: "we modify the reward funciton so that the gradient... exists..." which environments were modified and how did they have to be modified? Sec 4.1: You discuss early termination but have not defined what exactly you mean by it. Fig 1: 12 curves is still a lot and really hard to make much sense of. Sec 4.1: "it takes an impractically long time to train for 1 million time steps for some of the MBRL algorithms" why? There are also so many acronyms on the LHS it s difficult to keep track. Table 2: What about memory usage? Sec 4.5: "This points out that when learning models more data does not result in better performance." This seems like it s closely correlated with the particular form chosen for the model parameterization more than anything. Sec 4.7: "Early termination...is a standard technique... to prevent the agent from visiting unpromising states or damaging states for real robots." I ve never seen this used as a justification for this. Minor comments to improve writing:  When using citations as nouns, use \citep so you get "success in areas including robotics (Lillicrap et al., 2015)" as opposed to "success in areas including robotics Lillicrap et al.(2015)" (the latter is what you have all throughout the paper).<BRK>This paper presents a systematic empirical evaluation of model based RL algorithms on (mostly) continuous control environments from OpenAI Gym, with comparison to popular model free algorithms. It identifies three challenges for model based RL, learning the dynamics, selecting the planning horizon, and applying early termination to guide learning. A systematic comparison of model based RL algorithms is missing from the literature, and I believe that this paper does a fairly thorough job of providing such a comparison. A wide range of algorithms are selected, and the environments are representative of those commonly used in the literature. The first two challenges identified have been recognized in the literature. For example, Vemula et al.(2019) [1] discuss the planning horizon in random search RL algorithms. However, I would like to see some results on the policy search algorithms such as PILCO in Section 4.5, even if they are on the simpler environments. Currently they are not represented in Table 4. There are several instances where the writing should be clarified, e.g.acronyms are not explained before they are used. 2.Table 1 is a bit difficult to parse. Maybe it could be split up, or some algorithms/environments deferred to the appendix.<BRK>The performance in each case is averaged across 4 different seeds. Computation time is also reported. Furthermore, the authors analyze the performance hit incurred from adding noise to the observations and to the actionsFinally, the authors propose to characterize what hinders the performance of model based methods, ie., what they call the dynamics bottlenecks, the planning horizon dilemma, and the early termination dilemma. As a conclusion, it turns out no clear winner stands out, which motivates further development of model based approaches. Strong and weak pointsThis is a very interesting empirical study, especially since  it includes a comparison with model free algorithms,  it considers computational aspects and indicates what algorithms can be run in real time,  the authors use open source software (PyBullet) as simulators, which makes the study more reproducible (although the code has not been shared yet)But,  4 seeds averaged across is clearly low, given the well known variance of RL algorithms. In fact, the std values in the tables prove this point. Not as a criticism but rather a suggestion, it would have been useful to summarize the table 2 by comparing the algorithms using normalized values (mean   0, std   1) averaged across the environments. One the (strongest) weak points for me remains the assumption of the given differentiable reward function, as learning the reward function might be challenging for the model, for instance when it is sparse. I would have made the same remark for (Learning Latent Dynamics for Planning from Pixels,  Hafner et al.) but the paper does not seem to be not peer reviewed. Edit: the paper is indeed published at ICML 2019. Question:  could the author elaborate on the early termination? it is not precisely defined anywhere and yet, seems to be an important point.
Reject. rating score: 1. rating score: 1. rating score: 3. <BRK>I like the idea and hope the authors improve the paper and submit to a future conference. SummaryThis paper combines hierarchical RL with meta learning. The idea is that high level plans transfer across settings (e.g.picking up a mug), while low level execution may differ across tasks (e.g.different robot morphologies). To this end, the approach meta learns a two level hierarchical policy. The lower level policy is trained via HER to reach these goals (it may need to be completely re trained at test time). I do not think that these tasks require hierarchy to solve, as the exact same tasks (with the same simulated robot) were solved in Hindsight Experience Replay, Andrychowicz al. 2017.Thus HER (preferably implemented with SAC rather than DDPG for fair comparison) is a vital baseline that is missing from Figure 3. In conclusion, my current impression is that while the idea is interesting, the results achieve the same performance as a non hierarchical method, which is not included as a baseline.<BRK>This paper studies the problem of leveraging past experience to quickly solve new control tasks. To that end, the paper introduces an meta RL algorithm that, given a new task, attempts to solve it by adapting a high level, goal setting module, and learn a new, low level policy to reach each commanded goal. * "In this paper, We have"   "We" should not be capitalized. UPDATE AFTER AUTHOR RESPONSE  I thank the authors for at least reading the reviews. The proposed method outperforms the baselines on each task. While much emphasis is put on the hierarchical aspect of the algorithm, I don t think that the tasks used in the experiments require hierarchy to solve (see [Plappert 18]).<BRK>In this algorithm, a two layer hierarchical policy is used where the high level policy generate goals for the low level goal reaching policy to reach. The authors evaluated the proposed method on simulated robotic manipulation tasks and compare to PEARL as baseline. However I do find a number of shortcomings that need to be addressed. The structure of the paper is well organized and  the authors include informative illustration to explain the architecture of the hierarchy of policies. Con:1.The experiments presented in this paper do not include appropriate comparisons to baseline methods. Therefore, directly comparing the proposed method to any general meta RL algorithm is unfair. The idea in the paper is well presented and carefully investigated.
Accept (Poster). rating score: 8. rating score: 8. rating score: 6. rating score: 6. rating score: 3. <BRK>This paper presents a non asymptotic analysis of Variance Reduced TD (VRTD), proposed by Korda and La (2015), to apply variance reduction ideas to temporal difference learning, specifically TD(0) with linear function approximation. 6) The result for Markovian case requires the bias term to not dominate. I wonder how vanilla TD would perform with $O(mM)$ samples coupled with a simple strategy of reducing the step size by half when the value function estimates stop changing. Essentially, the rates showed in this paper require $O(mM)$ samples. I like the clarifications in Section 3.22) Thanks for clarifying that all the comparisons are done in terms of a fixed number of total gradient computations.<BRK>This work is brought as reanalysis of the centered TD algorithm from Korda and La (2015), which is known to contain several errors in its analysis and statements. Despite the advantages above, I am disturbed by the lack of comparison of the computational burden introduced by the M inner loops to vanilla TD. It is not clear from the results whether a practitioner would prefer paying those extra computations to reduce the bias and variance by the M dependent factors in the convergence rate. Second, the novelty of this work is not highly significant. The following comments/questions are in order:1. Is it always \alpha?<BRK>This paper provides theoretical guarantees for variance reduced temporal difference algorithms. Convergence guarantees and convergence rates are provided for both sampling schemes. The paper is well written. Considering the importance of TD, I think this analysis will provide some insights for future directions of TD. I am wondering how VRTD works in practice comparing to naive TD. What if it s replaced by wall clock time?<BRK>Summary:In this paper, the authors study the variance reduced TD (VRTD) algorithm, by Korda and Prashanth (2015)  (KP15), for policy evaluation in RL. The new analysis is based on a new technique to bound the bias of the VRTD gradient estimator, and shows the advantage of VRTD over vanilla TD (the analyses by Bhandari et al.2018 and Srikant and Ying 2019), both in terms of variance and bias, that are reduced by increasing the batch size. Is it \Phi?<BRK>The paper is on temporal difference learning, specifically variance reduction of it. As per the claims of the paper, a previous method from (Korda and La, 2015) had technical errors, which the paper corrects and provides a better analysis of variance reduction. In the end, the paper focuses on the variance of the gradient estimator in temporal difference learning, and on analysis of the bias error. Strengths:+ The paper seems to conduct some serious analysis of the variance in temporal difference learning.
Reject. rating score: 1. rating score: 3. rating score: 6. <BRK>This paper tackles zero shot and generalised zero shot learning by using the per image semantic information. Due to the lack of such expensive annotations, this paper can be only evaluated on CUB and Flowers datasets,  results on other popular zero shot learning datasets e.g., AWA, SUN, ImageNet, are essentially missing. (2) novelty is limited. Using per image semantic information is not new in zero shot learning at all. The loss function of this paper is also similar to  Reed et al.16, which used the max margin loss to align image and text pairs, v.s. (3) experiments are insufficient to support the contribution. I would expect all the approach will benefit from per image side information. Post rebuttal comments:  In the author responses, the authors have written long stories to fight against my reviews. But unfortunately,  a large part of the responses is not addressing my concerns. The authors repeatedly argue that their main contribution is to show that image level supervision is an effective way to tackle (generalized) zero shot learning. Extending this idea to generalized zero shot learning is not a sufficient contribution for ICLR.<BRK>The four losses consist of a classification loss given text descriptions, a classification loss given images, two contrastive losses given pairs of text and images. The final performance on the CUB and FLOWERS data set is impressive. See also (Karpathy et al., 2014; Wang et al., 2016) and in particular (Xian et al., 2016). The subtle differences between all these losses are whether there is a margin, whether the loss is smooth (i.e., a softmax as opposed a max), and how the negative samples are selected. I am surprised that the loss function being used by many are considered a contribution in the paper. Overall, the novelty is limited. Below are some minor points and questions for the paper. ... anchor embeddings learned in one modality as prototypes ... > the word "prototype" is used extensively in the rest of the paper. i know this is a common term in the zero shot learning community, but it might be good to give a formal definition early or at least give an informal definition. ... the probability of image v_i and text t_j to belong to the same object distance > isn t this the constrastive loss? can this be related to how negative samples are selected?<BRK>This paper presents two main contributions: first, a simple retrieval based objective for learning joint text and image representations, and second, a metric scaling term that improves performance on unseen classes in the generalized zero shot (GZSL) setting. They evaluate on two datasets, CUB and FLOWERS, consisting of images paired with text descriptions, and show that their relatively simple technique outperforms complex GAN and VAE based models. Weak accept, see considerations below. However, there are some gaps in the analysis, and the paper would benefit from a more careful comparison to the literature. However, it’s not clear how novel the retrieval model is; as implemented, it’s similar to a sampled softmax or noise contrastive estimation   the paper would benefit from a more careful comparison of their approach to established alternatives, and a discussion of why this approach should learn better representations than a VAE or GAN. It’s also not clear how novel the metric scaling technique is. The observation (Section 4.5) that the retrieval only model performs well without any class labels is interesting.
Accept (Poster). rating score: 8. rating score: 6. rating score: 6. <BRK>The paper presents an improvement to the task of transfer learning by being deliberate about which channels from the base model are most relevant to the new task at hand. It does this by apply attentive feature selection (AFS) to select channels or features that align well with the down stream task and attentive feature distillation (AFD) to pass on these features to the student network. Their major argument is that plain transfer learning is redundant and wasteful and careful attention applied to selection of the features and channels to be transfered can lead to smaller faster models which in several cases presented in the paper provide superior performance. Paper is clear and concise and experimentally sound showing a real contribution to the body of knowledge in transfer learning and pruning.<BRK>Also, the results of the empirical test it would be useful to understand the challenges to train the network. The results compare the new methodologies with different databases which increase the credibility of the results. However, there is a couple of additional question that is important to manage:  1) The paper presents three different contributions. 2) The comparison of the results are very focused on AFDS, Did you compare the results with different transfer learning approach?<BRK> This paper proposes a method called attentive feature distillation and selection (AFDS) to improve the performance of transfer learning for CNNs. The authors argue that the regularization should constrain the proximity of feature maps, instead of pre trained model weights. Overall, this is a good work in terms of theory and experimentation, thus I would recommend to accept it. The approach is well motivated, and the literature is complete and relevant. To improve this paper, the authors are suggested to address the following issues:1.
Reject. rating score: 3. rating score: 3. rating score: 3. <BRK>Specifically, the authors have found out that examples that are affected the most by pruning are more difficult to classify even for the non pruned network, due to low image quality, mislabeling, or being atypical from the class prototype, and performed a further human study to analyze the source of difficulty. The effect of pruning could largely differ from one method to another, but the authors do not experimentally compare the effects of different pruning methods. Also, it is highly likely that the findings discussed in the paper may be only true for input independent pruning approaches, and may not generalize to input dependent pruning method.<BRK>The paper claims that neural network pruning methods have different impact on accuracy in class wise and sample wise. In overall, the paper addresses an important problem of investigating the effects of pruning. The experiments performed here seems fairly extensive. Also, the accuracy differences in Table 1 do not appear to be that large in my opinion. What if the whole training and pruning is re performed after excluding the classes which the accuracy is decreased more by pruning? What would happens if we exclude the PIEs in training set, and performs training & pruning from scratch?<BRK>This paper study how the pruning impacts the images/classes and observes which images/classes do not generalize in the pruned networks under ResNet 50 with ImageNet dataset. The authors also verify the properties of PIE and conclude the paper. First, it is well known that hard (noisy or corrupted) images are harder to be correctly classified in networks of smaller model power (Hendrycks & Dietterich, 2019).
Reject. rating score: 3. rating score: 6. rating score: 6. <BRK>The paper proposes a framework for evaluating the sensitivity of a QA model to perturbations in the input. The core of the idea is that one can replace content words (i.e.named entities and nouns) in questions in such a way that makes QA models more confident of their original answer (despite, presumably, the question now being unanswerable). Impact: the method is essentially a data augmentation approach over a fixed list of words.<BRK>This paper studies undersensitivity of the neural models for reading comprehension. Then, they add perturbations to the input to turn an answerable question into an unanswerable question, using two methods, POS tag based and named entity based. Finally, this paper shows data augmentation and adversarial training for this perturbation help the model to be more robust, especially in a biased data scenario. The weakness of this paper is:1) the observations are somewhat obvious: it is hard to expect the model to always assign lower probabilities to the original answer when, for example, the named entity in the question is replaced to entities with the same type. 2) Table 2 shows that the perturbation does not always work; especially with POS based method, only half of cases work.<BRK>I enjoyed reading the overall paper, especially the experimental results, which provides lots of insights about the techniques. The proposed techniques are simple but they are well executed in the experiment with reasonable justification. One major concern of the proposed approach is the sub optimality by the pre trained RC model. I wonder how the authors tackle this issue in the experiment. ExperimentAdversarial attacks should show how an existing system is fragile to be attacked, but at the same time augmenting or adversarially training with them needs to improve its generalization power of the system against the attacks. In this work, authors showed a result of adversarial training/augmentation but its generalization power on original task (i.e., HasAns case) was not that powerful. It would be more convincing to see how this generalization from adversarial attacks can take benefits from bit different tasks such as open end reading comprehension as a perspective of data augmentation. I see no comparison with other attacking/defending methods in Tables 3 and 4.
Accept (Poster). rating score: 6. rating score: 6. rating score: 6. <BRK>This paper provides extensive experimental results to investigate the influence of hyper parameters on fine tuning and challenges several commonly held beliefs. The authors discover that the optimal momentum value is closely related to domain similarity. Similar to training from scratch, the actual effect at play is the effective learning rate and ‘effective’weight decay. However, for the five datasets provided, the similarity of them are really close, making this claim less convincing. The conclusion is reasonable, but the authors may need a more reliable method to compare the similarity between datasets.<BRK>This paper studies the role of different hyperparameters in finetuning image recognition models on new target tasks. They also show important correlations between momentum, learning rate, and weight decay. Overall, despite some issues detailed below, the paper is clearly written, presents a coherent story, and its conclusions will be useful to the community. My main concern about this paper relates to the importance of momentum. The authors argue that this hyperparameter is "critical for fine tuning performance".<BRK>This submission studies the problem of transfer learning and fine tuning. This submission proposes four insights: Momentum hyperparameters are essential for fine tuning; When the hyperparameters satisfy some certain relationships, the results of fine tuning are optimal; The similarity between source and target datasets influences the optimal choice of the hyperparameters; Existing regularization methods for DNN is not effective when the datasets are dissimilar. Pros:+  This submission provides interesting facts that are omitted in previous research works. Cons: 	All experiments results are based on same backbone, which makes all discoveries much less reliable. More experiments on other backbones are necessary. Furthermore, this submission claims that the regularization methods such as L2 SP may not work on networks with Batch Normalization module. Providing a complete hyperparameter selecting strategy for fine tuning could be an important contribution of this submission. But this submission does not propose a proper method for measure the similarity or provide detailed experiments on previous measurements. This submission omits that Kornblith et al.(2018) also referred to the fact that the momentum parameter of BN is essential for fine tuning and provided a strategy in section A.5. This submission gives important discoveries about the hyperparameter choice in the fine tuning setting. I vote for rejecting this submission now but I expect authors to improve the submission in the future version.
Reject. rating score: 3. rating score: 3. rating score: 3. <BRK>If f(x_j)   1 (for all j(?)) The fact that the voxelization of the point cloud is sparse is only mentioned very late in the paper.<BRK>The paper starts out highly technical. I recommend introduction of notation before use, especially as Section 1 is titled "Introduction". Only mean values for IoU and accuracies are reported, but no estimates of variance/spread.<BRK>This paper proposes Narrow Band Parallel Transport Convolution (NPTC) for point cloud data. The general idea is to use gradients of some distance function to define the vector field.
Reject. rating score: 3. rating score: 3. rating score: 6. <BRK>They train and test CAEs with bottlenecks consisting of different ratios of spatial resolution versus number of channels, as well as different total number of neurons. Their first main finding is that the spatial resolution of the bottleneck is a stronger influencer of generalization to the test set than the number of channels and the total number of neurons in the bottleneck. However, I find the experimental findings and discussion of borderline quality for a full conference paper. This is on the low side of things, especially when quite extensive claims are made. Another example is on page 7 under bullet point 4, where the authors discuss the obtained evidence against copying behaviour when the bottleneck is of the same size as the input. It has potential, but I wouldn’t go much further than that. Additional feedback to improve the paper (not part of decision assessment)  Section 2.3.1: what was the original resolution of the pokemon dataset?<BRK>This paper investigates convolutional autoencoder (CAE) bottleneck. In particular, two observations are made: (1) By measuring the performance of the latent codes in downstream transfer learning tasks, the authors show that  increased height/width of the bottleneck drastically improves generalization; The number of channels in the bottleneck is secondary in importance. (2) CAEs do not learn to copy their input, even when the bottleneck has the same number of neurons as there are pixels in the input. For example, A.<BRK>On the other hand, further experiments to provide initial insights into the further questions raised by the authors would improve the  novelty  aspect of the paper. The authors evaluate convolutional autoencoders (CAE) by varying the size (width & height) and depth of the bottleneck layer on three datasets and compare test and training performance. The authors also investigate the belief that a bottleneck layer of size equal to the input image will copy the image. I am not an expert in the field of (C)AEs.
Accept (Poster). rating score: 8. rating score: 6. rating score: 6. <BRK>This paper analyzed the principles for a successful transfer in the hard parameter sharing multitask learning model. I think it will potentially inspire the community to have more thoughts about the transfer learning. Overall I think it is a good work with interesting discoverings for the multi task learning.<BRK>Overall, the paper contributes to the conversation around multitask learning but would benefit from comparing again external work on multitask learning (e.g.see under minor) and from bridging between theory and experiments (e.g.experiments with the models described in the theory section   linear/ReLU). The benefits of a bottleneck in multitask learning are well known (based empirical results). While the model with non linear activation is mentioned at places, nearly all theorems rely on the linear model instead such that it might make sense to either work towards generalising the theorems or emphasising that most only apply to linear models.<BRK>This paper studies how to improve the multi task learning from both theoretical and experimental viewpoints. Consequently, they propose an algorithm which is basically applying a covariance alignment method to the input. The paper is well written, and easy to follow. Pros:A new theoretical analysis for multi task learning, which can give insight of how to improve it through data selection. They empirically show that their algorithm improves the multi task learning on average by 2.35%. Cons:There is not much of novelty in the algorithm and architecture. They have not provided any insight of how much restrictive this assumption is.
Reject. rating score: 1. rating score: 3. rating score: 3. <BRK>The method is intuitive and easy to implement. Although this paper shows the problem of FID for capturing the conditional consistency sprightly with the toy dataset, however, this problem does not obviously show up on real data.<BRK>Pros:  FJD is an intuitive extension of FID for conditional generative models. The paper is easy to read and experimental details are clearly stated. 3.The advantage over the prior work is not clear.<BRK>This paper proposes a variant of the use of Frechet Inception Distance (FID) for the evaluation and benchmarking of conditional GAN models. The experimental results of this paper seem to confirm this.
Reject. rating score: 3. rating score: 3. rating score: 6. <BRK>Pros:Their idea to utilize a decision tree for domain adaptation sounds novel. Cons:This paper is not well written and has many unclear parts. 3, In open set domain adaptation, simply minimizing domain distance can harm the performance. It was also unclear. This setting is also unclear. From the cons written above, this paper has too many unclear parts in the experiments and method section. I cannot say the result is reproducible given the content of the paper and the result is a reliable one. They need to present more carefully designed experiments.<BRK>This paper proposes a new target objects for training random forests that has better generalizability across domains. The authors demonstrated that the proposed method outperforms existing adversarial learning based domain adaptation methods. StrengthThe paper is clearly written. It would be great if the authors could clarify the setup of the baseline methods(e.g.Whether the baseline methods also take benefit of imagenet dataset, and is trained end to end). Overall I think it is a borderline paper that might be interesting to some audiences in the conference.<BRK>This paper proposes an approach to building random forests that arebalanced in such a way as to facilitate domain adaptation. Experimental results are given on a range of standardand open set domain adaptation datasets. The paper has a number of issues:1. These problems are not terribly distracting, but the   manuscript could use more polish. How many trees   are used? What is the max depth? These parameters should be   discussed and included in the ablations in order to appreciate the   complexity/performance tradeoffs.
Reject. rating score: 1. rating score: 3. rating score: 3. <BRK>I like the area of research the authors are looking into and I think it s an important application.<BRK>The paper designs three sets of metrics to measure the quality of layout prediction based on the literature and the domain specifics of user interface interaction. The presentation is nice.<BRK>Summary: This paper introduces the task of using deep learning for auto completion in UI design. Preliminary experiments indicate that the recursive model performs best and that the task is reasonable difficulty.
Reject. rating score: 3. rating score: 6. <BRK>The authors consider active deep learning. They propose decomposing predictive entropy into a) vacuity (lack of evidence) and b) dissonance (contradictory evidence). In practice this is achieved by having the NN output the parameters of a Dirichlet, which allows an additional degree of freedom describing variance/vacuity. It wasn t clear to me how this "subjective logic" theory gets you to the specific definitions of vacuity/dissonance, or whether these were just proposed by the authors. Equation 6 seems to come out of nowhere (whereas the rest of the derivations using the Dirichlet are very intuitive). The idea of encouraging the network to be uncertain far from data is also reasonable. While some Bayesian models such as Gaussian process regression with a RBF kernel give you this for free, it is certainly true that DL methods do not have this characteristic in general. DNNs can operate on very high dimensional, structured inputs. How sensitive is the method to the vacuity/dissonance weighting? For many applications that s more important. Overall I thought this paper had some promising ideas but they need to be more thoroughly tested empirically to give some sense of how robust and generalizable the approach is.<BRK>This paper propose an active deep learning model. By leveraging subjective Logic, they propose to decompose the entropy of a predicted class distribution into vacuity (lack of evidence) and dissonance (conflict of strong evidence). Instead of using the predicted class distribution, they estimate the supporting evidence for each class. They show better performance than the baselines on both synthetic and real datasets. First of all, for the readers who are not familiar with Subjective Logic or probabilistic logic in general, it is a bit hard to follow the reasoning behind the  equations in Sec 3 and  4. In the experiments on MNIST, it would be best to visualize the image samples selected by the active learning model in the early and later stages, so that we can have a more intuitive understanding of vacuity vs. dissonance.
Accept (Spotlight). rating score: 8. rating score: 6. rating score: 6. <BRK>The paper addresses the task of learning temporally stable features for point clouds with an application to upsampling point clouds. Thus, the very topic of research is significantly novel and promising.<BRK>Overall, this paper has some significant points on point cloud super resolution, with the caveat for some clarifications on the theory and experiments.<BRK>Summary:This paper proposed a deep network for point cloud sequence super resolution/upsampling. Strengths:Interesting problem and novel idea. The proposed method is technically sound. The scale of the evaluation demonstration is not convincing enough for the readers that this work could be generalized to more complicated testing scenarios. Quantitative results are also limited.
Accept (Poster). rating score: 6. rating score: 6. rating score: 6. <BRK>A closed form algorithm for pruning is presented, and the method is evaluated on several datasets.<BRK>I have never worked on RNNs or pruning. > In the prequel, we postulated tThe prequel? This question is not rhetorical   I know very little about this topic.<BRK>I m borderline on this paper. The motivation for why the gradients are normalized like this is still confusing.
Reject. rating score: 1. rating score: 1. rating score: 1. <BRK>In order to re use the policy which was trained on the old observation function, they propose to learn a mapping function to map the new observations to the old ones. I believe the work is interesting as generalization and reducing the sample complexity of learning policies, for example through re use of old policies, is of high current interest. However, I believe this paper requires more work to show the feasibility of the proposed approach. In particular:  The proposed method has the problem that matching is done  locally  and without any guarantee that the mapping function will converge to the correct mapping. This is not a problem of the method itself but of the challenging problem setup. The authors discuss this and propose two approaches to alleviate this. In summary: I believe this interesting work, but requires more experiments in different environments and additional ablation studies to show the feasibility of the proposed method.<BRK>The method works by learning a translation model that translates new state representations to old state representations. The authors evaluate the method on the MountainCar environment and show that the adaptation model is more efficient than training a new policy from scratch. I believe this is a more fair comparison in terms of sample efficiency than training from scratch. A comparison that is missing from the paper is to fine tune the existing model.<BRK>Summarize what the paper claims to do/contribute. * This paper claims to propose the first method to translate an environment representation to a different representation when that changes. Reject.* The results were not adequate. I suggest exploring more environments and more complex ones than MountainCar. * I do not believe this is the first attempt in translating an environment represention to a different one. Other techniques in domain adaptation have been working on this for quite some time.
Accept (Poster). rating score: 6. rating score: 6. rating score: 6. <BRK>The sample complexity of the problem is rather prohibitive. This seems like a much simpler setup compared to the general setting considered in the paper.<BRK>The paper has two main contributions. While the name suggests that the “execution tree” is, indeed, a tree, is this guaranteed to be the case? The references are somewhat inconsistently formatted.<BRK>Comments:In general, I think this is a good quality paper. after reading the response  I d like to thank the authors for giving more explanations.
Reject. rating score: 1. rating score: 3. rating score: 3. <BRK>The paper proposes a new faster algorithm to add inequality constraints to neural layers. The paper focuses on a novel constraining approach with seemingly superior scalability, and this is potentially a significant contribution. However the paper does not motivate the constraining at all. I am baffled by this, since one would assume at least some benefits from all of this work could be presented. The only mentions are binarization of the predictions (which softmax already does), and monotonicity/convexity of neurons, with no proposed benefits. I had hard time following the method, and I its not clear how the neural network is modified and how backpropagation is performed with the contraints. Apparently additional neural layers are added that map z s to r s. The backpropagation in the constrained case is undefined. The method is interesting, novel and seemingly efficient; but it is insufficiently defined, the method is not motivated and experiments are quite weak with little comparisons and no experiments with practical value.<BRK>The paper presents a method for imposing linear inequality in neural networks. Although the contribution of the paper is potentially significant, some details are not clearly described. The neural network is trained so as to satisfy the constraint represented by rays. The empirical results show that the variational autoencoder trained with the proposed method can generate images that satisfy linear constraints. However, the evaluation is limited to a checkerboad constraint, and other examples of practical linear constraints are not clear. I would like authors to address the following points in the rebuttal:  I do not understand the procedure of the proposed method. Please describe how the satisfaction of the constraints are guaranteed. I recommend authors to put a pseudo code of the proposed algorithm for clarity. The checker board constraint on MNIST images is interesting, but it would be better to show more examples of linear constraints. If possible, please give some more examples of linear constraints and their results.<BRK>This paper proposes a method to impose linear inequality constraints on neural network activations. The contributions claimed are:* Novel technique to impose inequality constraints on neural network activations. Overall, the approach is well motivated, and well placed in the literature. However, the experimental analysis does not support all the claims made by the authors as it focuses on a single dataset (i.e., MNIST) and single constraint (i.e., checkerboard pattern). 1) The softmax used to satisfy the constraints is preceded by a batch normalization layer. Could the authors provide experiments that justify the use of the batch normalization layer? 2) An important factor that is unexplored in this manuscript is the softmax temperature. The loss appears to reach some plateau (9% from optimal) and, thus, there appears to be some trade off between reconstruction and projection. Overall, it would helpful to add more setups and different types of constraints (other than a last layer projection; e.g., monotonicity).
Accept (Poster). rating score: 6. rating score: 6. rating score: 6. <BRK>This work proposes a set of efficient algorithms for learning and prediction in Bayesian quantized networks, which allows for differentiable learning without the need to sampling.<BRK>This paper proposed Bayesian Quantized Neural Networks (BQN), which is a rough Bayesian treatment for quantized neural networks (QNN). Minor:P2: bf Bayesian  > BayesianP2: theta that predicted  > theta that predictP6: Lemma 4.2  > Theorem 4.2P6: by with  > withAssuming most of my concerns can be addressed during the feedback period, I tend to give a ‘weak accept’ to this submission. The topic of Bayesian formulation and inference in the context of QNN is interesting.<BRK>(1) Contributions:The paper proposes an algorithm for training quantized Bayesian neural networks (BQN). The idea of BQNs and propagation as tensor contractions is interesting and novel. This leads to a fast and efficient algorithm for training. Overall, I believe that the number of equations and theorems hinders the readability of the paper. I do not think this paper should be 9.5 pages.
Accept (Poster). rating score: 8. rating score: 6. rating score: 6. <BRK>The authors propose a learning rule based on regression discontinuity design (RDD) and show that this leads to stronger alignment of weights (especially in earlier layers) compared to previous methods. Overall, the paper is very well written and addresses an important problem.<BRK>commentsOverall I find this paper to be well written and _accessible_ to someone who is not familiar with the biologically plausible learning algorithms. I had one minor comment on the arrangement of the writing of the paper.<BRK>Overall, this is a decent piece of work with some potential. Post rebuttal: My only major concern was the lack of sufficient empirical evidence to support the idea. The comparison is performed on only two data sets and each algorithm is better on one.
Accept (Poster). rating score: 8. rating score: 6. rating score: 6. rating score: 6. <BRK>This work proposes a neural network architecture for joint depth and camera motion estimation on video sequences. + Principled approach that marries the best aspects of deep learning with classic principles from multi view geometry.<BRK>This paper pushes forward the research of deep learning based video 3D reconstruction by decomposing the problem in two stages:1. The full system is highly engineered and complicated. So this paper focuses more on the camera pose estimation than the depth, which is a good start point to achieve better multi view capabilities in a CNN framework. Depth estimation from multi view stereo2.<BRK>This paper proposes a framework for training machine learning models that simultaneously estimates depth of objects and poses of a single camera in a sequence of images from a single camera, in others words, a video. The motion module estimates motional information of a camera assuming the depth of each object is given. The authors formulate the aforementioned two modules as neural networks, so that they can be trained end to end, and proposes various way of initializing the two modules.<BRK>The authors proposed to estimate depth from a video sequence. In the model, the pipeline iteratively estimates motion and depth by separate modules, which can be trained in an end to end fashion. Overall, I think this is a useful work and may be considered for publishing. The introduction of the related works is well written.
Accept (Poster). rating score: 6. rating score: 6. rating score: 6. <BRK>Main contribution of the paper  The paper proposes a new pruning method that dynamically updates the sparse mask and the network weight. Concerns  It seems that the paper omits the existing work (You.et.al   https://arxiv.org/pdf/1909.08174.pdf), which seems to share some contribution. The reviewer wants the author to clarify the differences and the strongpoints compared to the work. Does the author think that the proposed method can enhance the latency? Conclusion  The author proposes a simple but effective dynamic pruning method.<BRK>But it seems that there is a typo in the definition of strong convexity on Page 4:  \Delta f(w)  should be  \Delta f(v) . The authors also showed the convergence rate and the fundamental limit of the proposed algorithm with two theorems. This paper is well written and very pleasant to read. I would like to accept this paper.<BRK>This work proposes a simple pruning method that dynamically sparsifies the network during training. While similar methods have been explored before, this work proposes a slight twist; instead of updating the weights of the model by following the gradient of the parameters of the dense model, they update the parameters of the dense model according to the gradients of the sparse model. This work is in general well written and conveys the main idea in an effective manner. It is also a timely contribution as sparse models / compression are important topics for the deep learning community. The proposed gradient estimator seems to be an instance of the STE [1] estimator, that, as the authors mention, has been using at the Binary Connect algorithm.
Reject. rating score: 1. rating score: 3. rating score: 3. <BRK>Be positive and generous. Reject.I don t think the paper is contributing something new to the literature.<BRK>* In a similar vein, I wonder how this compares to the Snapshot Ensembles paper, which has a similar guarantee of doing 1 training run and giving M models for an ensemble. The validation accuracy for a single task will be largest at a specific epoch.<BRK>It shows that the optimal epoch varies by label. LimitationsThe paper is very preliminary in nature. The proposed approach is not very interesting, brutal force and clustering.
Accept (Poster). rating score: 6. rating score: 6. rating score: 6. <BRK>This paper tries to analyse the Transformer, widely applied building block of a neural network component, to improve understanding of the internals of the model. By introducing positional embeddings, the paper relaxes the restriction on permutation equivalence and proves that the Transformer is a universal approximator of any sequence to sequence function. Overall, the paper presents an interesting analysis of the Transformer providing some practical implications, with a caveat for some clarifications on the experiments. The structure of the manuscript could be improved. One of the natural question after reading this paper is whether the claims made in section 5 are what actually happens inside of the Transformer because we often observe the attention layers of the first few stacks of Transformer blocks do something. Here are a few minor comments on the structure of the manuscript. Also, the title of section 5 seems not very informative (proof sketch of proposition 4).<BRK>‘Accept’ because the results seem important. It relies crucially on the notion of C3, the utility of which for understanding how real Transformers work is questionable. (The modified attention layers replace softmax with argmax and replace ReLU with a (varying) piecewise linear activation functions “with at most 3 pieces, at least one of which is constant” [Step 2, p. 4.])The paper should focus on steps (3) and (5), the only parts of the proof that pertain to what is special about the Transformer: attention. These should be explained fully and clearly in the main text. It is clear that the notion of contextual mapping plays an important role in their proof of the universal approximation theorem, but does it play any role in the operation of Transformers in practice? The subtitle of Sec.5, “Demystifying Transformers”, is not clearly justified.<BRK>This paper discusses the universal approximation capability of the Transformer, under certain assumptions, analyze the role of different components of the Transformer (e.g., self attention layer for contextual mapping), and propose the use of some other layers that can also provide contextual mapping. The transformer has been used extensively in many applications today, however, deep theoretical understanding of it is not sufficient. Universal approximation capability is a very important theoretical property of deep learning, and advances on the universal approximation of the Transformer is important for the deep learning community. Therefore, I think people will be willing to see the results in this paper. Consequently, things are not clear regarding”a. 4)	The experimental study in the paper is not very comprehensive.
Reject. rating score: 1. rating score: 3. rating score: 6. <BRK>This paper proposed to train a network with training curves and corresponding parameters, and use policy search to find optimal parameter to replace hundreds or thousands of training in real case scenario, and it is clearly much faster using the trained network to infer parameters, instead of tuning the network manually. The first point would be: what s the meaning of synthetically generating training curves other than proving that transformer achieves good performance in modeling discrete distribution? The cited paper  Learning an adaptive learning rate schedule  does not appear online.<BRK>This work focuses on learning a good policy for hyperparameters schedulers, for example learning rate or weight decay, using reinforcement learning. The main novelties are two folds. On the methodology side, using predicted learning curves instead of real ones can speed up training significantly. Human baseline does not need any training! On the writing part, it would be nice to provide more context for both transformer, Proximal Policy Optimization and Simulated Policy Learning to make the paper more self complete. I like the directions using surrogate to speed up HPO in general but I feel the learning curve prediction part can be improved. How does the transformer based method comparing to others?<BRK>Currently I lean towards accepting this paper for publication, despite a few issues. I understand the experiments are a proof of concept, but it would be nice to get a feeling for what some of the other methods do. Most importantly, I would like to see a measure of variance/uncertainty like confidence intervals included in the results; otherwise it s impossible to assess whether the results are likely to be significant or not. The proposed model based method is compared against a human and a model free baseline training a Wide ResNet on CIFAR 10.
Reject. rating score: 1. rating score: 3. rating score: 3. <BRK>This paper proposes a GAN oriented framework for training robust to noise neural link predictors.<BRK>This paper presented a jointly learning framework based on GAN for tackling both knowledge graph completion and noise detection simultaneously. Are the results easy to reproduce? The paper is well motivated.<BRK>This paper proposes to provide a novel noise aware knowledge graph embedding (NoiGAN) by combining KG completion and noise detection through the GANs framework.
Reject. rating score: 1. rating score: 3. <BRK>Such an experiment would make this point more compelling. This prior work is not acknowledged in the current paper. The current scope of the experiments is too limited to conclusively show these points.<BRK>In experiments, the performance of Q MTL is not so good when compared with ensemble learning. This paper proposes a quasi multitask learning (Q MTL) for supervised learning. The rationale behind Q MTL is unclear to me. However, I did not see any analysis on this aspect.
Reject. rating score: 3. rating score: 3. rating score: 6. <BRK>The proposed generative model utilizes both shared and private per population latent variables. This regularizer forces private population representations to: (a) maximize mutual information with input samples from their population, and (b) minimize mutual information with input samples from other populations. In other words, private representations are forced to be "meaningful" on the corresponding population alone. Quality:The paper is well written. I find the proposed method to be quite interesting. Originality:Even though, as authors point out, there is a substantial amount of work in this field, I believe that their approach is novel and has its own merits. If so, I think these points should be mentioned in the text. Using very similar latent variables for two similar populations would be penalized by the regularizer (not too significantly though). Can it come from a parametrized function family with parameters being optimized during training?<BRK>The paper proposes to model multiple datasets from differing distributions with shared latent structure and private latent factors. The main techniques include architecture design which encourages the isolation of shared and private latent factors and a mutual information based regularizer. I enjoyed reading it. However, I found that the paper has some weaknesses:1. The novelty is not enough. The authors also mentioned several previous works in Section 3, e.g.Multi level  VAEs, oi VAEs. 2.More importantly, I did not see any baselines in the experiments except vanilla VAE.<BRK>The main contributions of this paper are to explicitly learn the commonly shared and private latent factors for different data populations in a unified VAE framework, and propose a mutual information regularized inference in order to avoid the “leaking” induced by the shared representations across different populations. The isolation of the commonly shared and population specific latent representations learned by the proposed are empirically demonstrated on several applications. However, I have some concerns regarding this paper as follows. It might be not efficient for optimization. Thus, it will be helpful if the authors provide the model efficiency analysis compared with other baseline methods.
Accept (Poster). rating score: 8. rating score: 6. rating score: 6. <BRK>In this paper authors propose a novel idea (called Filter Summary, FS)  how to compress convolutional neural networks with 2D convolutions (kernels are 3D tensors, it is also applicable to the 1D convolutions). Via experiments it is demonstrated that proposed approach provides compression of the model while having close to the baseline quality for image classification and object detection tasks and for small and large datasets. It is experimentally showed that architecture search works for proposed convolution.<BRK>This paper proposes a method, termed as Filter Summary (FS), for weight sharing across filters of each convolution layer. Moreover, a fast convolution algorithm is designed for the convolution layer with the FS. Some promising results demonstrate the effectiveness in CNN compression and the acceleration on the tasks of image classification, object detection and neural architecture search.<BRK>The paper presents a novel compact parameterization of convolution filters. A fast algorithm is presented for convolving using filters parameterized in this way. The results show the potential of the method to reduce the number of parameters at a modest drop in accuracy, though it is not clear how the method stacks up against state of the art, or what the improvement in wall clock runtime is. One minor weakness of the experiments on classification (3.1) is that the training procedure for FSNet and the baselines are different, with FSNet using cyclic learning rates and a larger number of epochs, making the results somewhat difficult to interpret.
Reject. rating score: 3. rating score: 3. rating score: 3. <BRK>This paper proposes a novel rule for scaling the learning rate, called the gain ratio, for when the effective batch size (induced by synchronous distributed SGD) is increased. Is it possible for $r_{t   1} \gamma > 1$? How large of a batch size can one use with AdaScale before the algorithm breaks down (if at all)?<BRK>The paper is well written and generally easy to read, although I didn t check all theory in the paper. The paper is good and I like it, although I think the novelty and contribution is slightly too low for ICLR. However, from a scientific perspective it provides no significant contribution.<BRK>The biggest issue I have with the paper is that I can t tell if it s better of worse than linear learning rate scaling from their experiment section.
Accept (Spotlight). rating score: 8. rating score: 6. rating score: 6. <BRK>The authors do a good job of identifying the sliver in which they make their contribution and motivate the same appropriately. Experimentally, the authors find that the attribution scores assigned by the proposed approach are more correlated with human annotations compared to prior approaches and additionally, the generated explanations turn out to be more trustworthy when humans evaluate their quality.<BRK>This paper proposes a hierarchical decomposition method to encode the natural language as mathematical formulation such that the properties of the words and phrases can encoded properly and their importance be preserved independent of the context. The proposed method is a modification of contextual decomposition algorithm by adding a sampling step.<BRK>How do you deal with these? The authors conducted experiments on SST and Yelp. Results show that their proposed context independent attribution correlates better with a trained linear model s coefficient, achieves higher human trust. The performance of CD in Table 1 seems very different to the original CD paper (which is 0.758 for SST and 0.520 for Yelp).
Reject. rating score: 1. rating score: 3. rating score: 3. <BRK>Looking at the results they don t look all that great and the proposed method is hard to identify as the best. The idea doesn t strike me as particular innovative, feeling like a natural extension of the prior work listed.<BRK>The paper s technical novelty is limited to this simple loop extension and generator sharing.<BRK>3）This article also does not give the training computational complexity and testing time cost of the proposed method. The indicators provided in this article are not objective enough.
Reject. rating score: 1. rating score: 3. rating score: 3. <BRK>Modivation:I had a hard time understanding the motivation of this work   specifically the connection to the universal learning framework which to be fair I am unfamiliar with. I am not considering this as part of my review but I would encourage the authors to look at [1]. Writing:The writing was clear and typo free. Overall the results were not convincing to me. The main increase in performance is in the PGD. Finally, having some comparisons to other defense strategies would improve this paper.<BRK>In this paper, the authors proposed the Adversarial predictive normalize maximum likelihood (pNML) scheme to achieve adversarial defense and detection. However, the experimental results indicate that the proposed method is more suitable for the models trained under PGD based attacks. According to the analysis shown in the paper, the proposed method works best when the adversary finds a local maximum of the error function, which makes it more robust to strong attacks.<BRK>The experimental results in Table 1 seems to be very good. The authors provided some explanation on why the adversarial pNML should work. The section 6 adaptive adversary part is not clear.
Accept (Poster). rating score: 8. rating score: 6. rating score: 6. <BRK>In this work, a multi agent imitation learning algorithm with opponent modeling is proposed, where each agent considers other agents’ expected actions in advance and uses them to generate their own actions. The authors suppose fully observable Markov Games in the paper, but it makes me confused when I consider the experiments in the submission. Since authors argue CoDAIL is a decentralized algorithm, I think agents are not allowed to use others’ observation for opponent modeling, but it seems that agents fully utilize others’ observations. For each iteration of CoDAIL, (1) each agent trains opponent models (other agents’ policies) by minimizing either MSE loss (continuous actions) or CE loss (discrete actions), (2) samples actions from those opponent models, (3) updates individual rewards (discriminators) and critics and (4) updates policies with multi agent extention of ACKTR (which is used in MA GAIL and MA AIRL as well).<BRK>This paper proposes to model interactions in a multi agent system by considering correlated policies. In order to do so, the work modifies the GAIL framework to derive a learning objective. The paper is a natural extension of GAIL/MA GAIL. I have two major points that need to be addressed. 1.The exposition and significance of some of the theoretical results is unclear. They proposed a likelihood free method to estimate importance weights, which seems might be necessary for this task as well (re: qs.on how are importance weights estimated?).<BRK>The authors propose a decentralized adversarial imitation learning algorithm with correlated policies, which recovers each agent’s policy through approximating opponents action using opponent modeling. Extensive experimental results showed that the proposed framework, CoDAIL, better fits scenarios with correlated multi agent policies. With the new assumption, the paper re defines the occupancy measure and introduces an approach to approximate the unobservable opponents’ policies, in order to access opponents’ actions. Below, I have a few concerns to the current status of the paper.
Accept (Poster). rating score: 8. rating score: 6. rating score: 3. <BRK>This paper proves that, modulo permutation and scaling, ReLU networks with non increasing widths are uniquely characterized by the function they induce (excepting some degenerate cases). This result is not apriori obvious and is of interest. However, it is not clear which elements of the proof technique are inapplicable to leaky ReLUs. It would be helpful to include a brief discussion on this. I recommend acceptance, since I didn t find any proof errors and the contribution is clear.<BRK>The paper shows that for ReLU networks satisfying certain conditions, the weights and biases leading to the exact functional form are the ones obtained by neuron permutations and rescalings, and that no other reparametrizations preserving the function exist. I find the topic very interesting and the authors’ approach reasonable. I appreciate that they clearly qualify the assumptions used. It would be interesting to study the regime where the functions are not exactly the same. In particular, this is very relevant because we only care about answers on a discrete set of points (train set / test set), and on top of that we only care about the argmax of the logits, rather than the actual detailed answer. I believe such a result would be significantly stronger and more relevant to the practical applications of DNNs, however, I understand that it might be more difficult to obtain. Overall, I enjoyed this paper and I think it deals with an important problem.<BRK>The proof technique is novel, and provides some insights in the geometry space of the loss surface. I think the proof technique could have its general implications to some other research, however, the direct value of this paper is not very clear. The redundancy in over parametrized networks and the redundancy in ReLU networks are different concepts and the connection between them is not well established. However, many widely used networks are not of this kind. It is strange that the authors did not cite it. ** I read the author rebuttal. I still think the authors should think harder about the implications of their work, either theoretically or practically.
Reject. rating score: 3. rating score: 3. rating score: 6. <BRK>This work apply the wait k decoding policy on the 2D CNN based architecture and transformer. In the transformer based model the author proposed to recalculate the decoder hidden states when a new source token arrives. The training with multiple k provides similar gain as training with one k larger than the value used at the inference time. Overall the contributions are limited. There is quite some room for this paper to improve its clarify, especially in terms of annotations and explaining the proposed ideas.<BRK>The authors did some interesting experiments between caching and updating decoder. For the second bullet, I think the original wait k also did the same thing(they mentioned this in the paper clearly). So there is nothing new about bullet 2. 3) updating the hidden state of the decoder introduces more complexity during the inference time. 4) it is also interesting to show more comparison between different models  training time with the original STACL.<BRK>It s all with low confidence. The paper is about an improved method of training latency limited (wait k) decoders for transformer based machine translation, in which the right context is limited to various numbers. On the plus side the paper says it sets a new state of the art for latency limited decoding for a German English MT task, and it involves transformers, which are quite hot right now so the  attendees might find it interesting because of that connection.
Reject. rating score: 3. rating score: 3. rating score: 3. <BRK>This work investigates a new problem setting that combines few shot image classification and out of distribution detection. The main procedure for the task still follows a standard few shot classification task, but in each episode, the data from a different distribution may be presented together with the query images. The paper evaluates with three different scoring functions on four few shot classification datasets and nine out of distribution datasets. For example, the text mentions “All results are evaluated using 1000 test episodes” without information of how the in distribution data and out of distribution data (especially the OOS data) is chosen in each episode. The last is the writing style which has a noticeable fraction of content not directly related to the proposed problem setting. What’s the impact of doing out of distribution under the few shot setting? How does the “N way X shot” setting affect the difficulty of the problem?<BRK>The distinction is that OOE samples are from same dataset but come from classes not represented by the support set. The paper proposes two new confidence scores,  MinDist and LCBO. StrengthsThe paper proposes benchmark datasets for out of distribution detection of few shot classification. The paper presents baseline results for two popular few shot classifiers — Prototypical Networks, and MAML. The paper shows that a simple distance metric based approach improves the performance on both tasks. What is the motivation to detect OOS and OOE in the few shot setting given the accuracy is already low? The contribution is mainly the metrics. Overall, the paper does not have enough interesting results for acceptance.<BRK>This paper presents a method for out of distribution (OOD) sample detection for cases that only a few samples are available from positive classes. The authors bridge the gap between novelty detection and few shot classification models. The proposed model has access to the negative samples (OOD samples) during training. First, this assumption is not realistic, as in all such applications, the irregular (OOD) samples are either poorly sampled or not sampled at all. This makes them not directly comparable to the proposed methods that its settings are more or less advantaged.
Reject. rating score: 1. rating score: 3. rating score: 6. <BRK>The paper proposed an unsupervised anomaly detection method for the scenarios where the training data not only includes normal data but also a lot of anomaly data. How to determine which data samples are anomalous is a key to the success of the model, but the proposed method based on the variance assumption is too intuitive and not convincing. In addition, the experimental results on the very simple MNIST task is very poor, putting the effectiveness of the proposed model in doubt. The extension seems to be very straightforward.<BRK>For this purpose, the authors iteratively use: 1) autoencoders to learn the representation of the data; 2) applying in the latent space clustering to get a new training set and retrain autoencoders. The experimental results show that the author’s method performed better results than such a baseline model as one class SVM and one class NN. As the proposed approach is a heuristic, the experiments should be done more persuasively, including more metrics used and more alternative algorithms considered. Is the DAGMM method SOTA in anomaly detection with deep autoencoder? There are many other methods with similar ideas.<BRK>In this paper the authors propose a framework for anomaly detection. The method is based on autoencoders and reconstruction error, but instead of training the autoencoder using all the data points, the method iteratively uses some form of clustering to determine the points which presumably belong to the normal set, and uses them for training the autoencoder. This helps make the method robust when the portion of anomalous data points is high. Given clarifications in the author response, I would be willing to increase the score.
Reject. rating score: 3. rating score: 6. rating score: 8. <BRK>This paper proposes an SCM model based on masked neural networks to capture arbitrary conditional relationships combined with meta learning style adaptation to reflect the effects of various unknown interventions. The authors should describe what are the underlying interventions in each dataset a bit more. Given the lack of theoretical / conceptual guarantees that the methodology will work, our faith in the proposed methodology rests entirely on the empirical experiments.<BRK>This paper proposes a MAML objective to learn causal graphs from data. The novelty of the paper seems to be in the application of the MAML framework to causal discovery which is interesting to me. Overall, the experiments look reasonable and the method itself seems interesting although further work is needed to show it is useful.<BRK>While the paper adopts the core design choices from recent prior art (Bengio et al., 2019), the proposed methodology (especially ii)) is sufficiently novel to be published as a main track conference paper. It is understandable that given the premature stage of the causal inference research might not grant standardized data sets at a larger scale, but at least lack of this quantitative scalability test could be acknowledged and the related claims could be a little bit softened. The intervention prediction heuristic is splendid.
Accept (Poster). rating score: 6. rating score: 6. rating score: 6. <BRK>They demonstrate that RL agents struggle with a lot of the tasks in JBW. The majority of the paper describes the technical details of JBW, and show that RL agents can struggle to solve continually changing tasks in JBW. 2.Decision (accept or reject) with one or two key reasons for this choice. I m borderline. It is valuable to have environments that support continual learning, although the experimental investigation into different forms of non stationarity would be more informative.<BRK>I think it’s important to study scenarios such as the ones described here and this provides a tractable way to start. I’m referring to never ending learning as NEL throughout. Can we either remove it or stick to the later formalism? This sentence is a bit confusing. Why is this actually representative of the real world? DesignDoes the user have control over all the agents? Could those be described a bit further? Overall, I like the paper and the introduced environment.<BRK>SummaryThis paper introduces a new environment for testing lifelong or never ending learning. The contributions in this paper extend upon previous work by building an easily controllable environment generator with key necessary features for lifelong learning including: non stationarity, multiple task specification, and multiple sets of observable features. ReviewThe paper highlights many key characteristics of an environment that are challenging to current RL models. I find the proposed environment to be incredibly intriguing and would find it valuable to the field of lifelong learning (or continual learning or never ending learning, etc.). I think the size and scope of the environment generator is impressive, showing a considerable amount of engineering effort has gone into its design. I am unclear if the environment is trivially solvable by using more computation resources (e.g.bigger networks).
Reject. rating score: 1. rating score: 6. rating score: 6. <BRK>1.Strange new results compared with its previous versionThe paper has a previous arxiv version. While the method does not change, the performance in this current submission has dramatic change compared with previous version: the proposed model seems much improved, while some important baselines (that outperform the proposed model) only have 50% of it previous performance in this submission. However in previous version for the same setting and experiment (in Table 2 and 3 of its arxiv version), the performance are much lower especially the Structural alert prediction results. The author needs to justify this dramatic change. 3, the evaluation of interpretability is not convincing.<BRK>SummaryThe authors propose a new pooling layer, LaPool, for hierarchical graph representation learning (Ying et al., 2019) by clustering nodes around centroids that are selected based on "signal intensity variation". For instance, is it the case that Graph U Net is not interpretable because there s no explicit clustering? Weaknesses:   The paper has issues with clarity.<BRK>The paper introduces a new pooling approach "Laplacian pooling" for graph neural networks, which the authors claim is able to better preserve information about the local structure, and to provide interpretability.
Reject. rating score: 1. rating score: 3. rating score: 6. <BRK>The authors should reference and discuss the relation of their work in particular to [1].<BRK>Recommendation:Due to the limits of experiments design and setting, this is a weak reject. The logic chain of this paper is complete. Also, the paper should contain detailed settings about attacks.<BRK>The strategy amounts to sampling several random direction vectors in a ball of constant radius centered at a training example and averaging their predictions.
Reject. rating score: 1. rating score: 3. rating score: 6. <BRK>This paper proposes an algorithm for imitation of expert demonstrations, in situations where the imitator is acting under a different environment (different dynamics, for instance) than the one used to collect expert demonstrations. The algorithm builds on GAIL with the following modifications – the discriminator is made dynamics invariant by adding a domain adversarial loss, and the policy is made to condition on a dynamics context. I have the following concerns about the paper:1. How is this approach different?<BRK>Summary:The submission considers the problem of imitation learning when the dynamics of the expert are not known to the agent and the dynamics of the agent may change frequently. The ablations with respect to the adaptability and GRL are crucial. This network is used for the test case, where the true dynamics of the agent are not known. In order to address this problem, an additional head is added to the discriminator that outputs a prediction of the dynamic parameters. Presentation/Clarity:The presentation of the work is arguably the main weakness of the paper.<BRK>The paper describes an approach that combines domain adversarial neural network with generative adversarial imitation learning. Also in this paper the source domain only contains demos from one env, which might highlight the importance of the gradient reversal layer.
Reject. rating score: 3. rating score: 3. rating score: 3. <BRK>I propose to reject the paper in its current form, and consider the following negative points for further improvement:1. The posed problem is not really few shot learning, in (now classical) few shot learning, such as few shot classification on benchmarks such as miniImageNet, CUB, tieredImageNet, CIFAR FS, FC100, etc.<BRK>Why was this not considered for the image completion experiments? 1) Although, the paper is well written and easy to follow the technical contributions of the paper are limited.<BRK>I do not feel that it would be    easy to reproduce the results reported in this paper without    significant guesswork.
Reject. rating score: 1. rating score: 1. rating score: 1. <BRK>Clearly, this is supposed to be Tables 6 and 7. The paper has several significant problems with the “\cite”s and “\ref”s in the paper.<BRK>Hence, the experimental results could be more convincing if the paper include more 4. This paper is below the bar of acceptance for the following reasons:1.<BRK>However, it is not clear to me how the ranking was decided in equation 6 by the softmax function. There are other confusions that need to be addressed for better understanding.
Reject. rating score: 3. rating score: 3. rating score: 3. <BRK>You could reinstate connections from the features of subsequent tasks to the earlier task and train, but presumably these would initially interfere with the original subnetwork.<BRK>Generally, there are two group of methods. And, the proposed method called CLNP leverages the merits from both of them.<BRK>General:The paper proposed neural pruning method to overcome the catastrophic forgetting. 2.Good results of outperforming several SOTA algorithms.
Reject. rating score: 3. rating score: 6. rating score: 6. <BRK>The problem tackled in the paper is the compression of generators in adversarially trained models. The attempt to remove filters was presented in the last figure, and it does not work as good as all other results presented in the paper. The comparison in Figure 1 is arguably misleading as well. For example, one of the methods that were mentioned (LIT) does achieve a factor of 1.8 model compression, yet the comparison was not carried out directly with that method, but a modification proposed by the authors of this paper. Weights pruning is simply one of the approaches for model compression, so you cannot ignore the alternatives. Also, section 4 probably has to be rewritten, since some unorthodox notation is used.<BRK>In this paper, the authors tackle the task of compressing a network. The task is well motivated and situated in the related literature. On one side, the results demonstrated in the evaluation arecompelling, on the other side, the compression factor is only 50%, which is muchlower than seen in related work. It is clearwhat "discriminative loss" is as it is the one used in every GAN. The distribution it outputs is thereforecompletely unknown and potentially non overlapping with either of the true orthe generator ones. In that case it is hard to predict what the discriminatorwould do on completely out of distribution samples. Could you provide an explanation of why it is not a problem in practice? Do you have to try multiple initializations? It should be accepted if someclarifications are made in section 3.<BRK>This paper proposes a method to compress GANs. The authors present intuitive reasons for why this is the case. Their “self supervised” method works by using the pre trained discriminator network, while compressing only the generator. The paper is clear and well written. It presents a way of pruning GAN generator network and although of limited novelty, it might be an interesting read as it provides extensive and convincing experiments in a clear manner. It does have several parts though which require additional clarification. The idea of using the pre trained discriminator network seems reasonable, but I am missing what the compression method for the generator network actually is (Section 4). The authors claim that the “self supervised” method generalizes well to new tasks and models. A more appropriate way of putting it might be ‘can be applied to other tasks and models. In Section 4 the authors write: “Our main insight is found,” but then they describe the GAN method. The qualitative results in Figure 1 suggest that their “self supervised” method is better than the other baselines. The analysis in Section 6 seems out of context with the rest of the paper. General remarks:In the first read of Section 3 it is not clear what [a], [b], [c] are. It would be good to first refer to Table 1. Table 1: why is there a “?” only on the “Fixed” column?
Reject. rating score: 1. rating score: 3. rating score: 3. rating score: 3. <BRK>The paper studies the norm of hidden activation of each layer and the norm of weight gradient of each layer for deep ReLU neural network. The results are correct and the paper is easy to follow. However, the result has been given in previous work. I do not recommend the acceptance.<BRK>The paper shows that under He initialization and for sufficiently wide network, (1) the norm of the activations of a L layered ReLU is preserved w.r.t the input across layers and (2) the norm of a weight matrix gradient at different layer is only dependent on the norm of the top layer error and the input, because the norm of back propagated gradient is approximately preserved. The paper is clearly written and easy to read and the proofs are quite straightforward. That being said, the results are not surprising and from my point of view, the overall novelty of this paper is a bit marginal for top tier conference like ICLR.<BRK>The concentration bounds also suggest lower bounds on the width of the ReLU layers. Since gradient descent based techniques seem to prefer solutions that are close to initialization, the analysis in this paper might be a useful starting point in understanding generalization. To summarize, I do not see how the authors claims about explaining overparameterization (even at initialization) can be made.<BRK>This work considers random parameter initialization in neural networks (In particular the initialization presented in He et al.) The authors show that the norms of the outputs and gradients (for gradients, under a different assumption on the dimension of the matrix) remain constant through the different layers. 1.Very similar in nature to previous results, for example the results presented in [1] Theorem 5.4 give very similar concentration results and use very similar mechanisms.
Reject. rating score: 3. rating score: 3. rating score: 6. <BRK>The main contribution of the paper lies in formulating discrete choice models into an MDP, and showing that the value function is concave with respect to the policy (represented by conditional choice probability). So policy gradient algorithm can provably converge to the global optimal. How restrictive the assumptions are in Definition 1.4? In particular, R_min is defined from Assumption 2.2 as “the immediate reward … is bounded between [R_min, R_max]”. Is this really true? At least, does the experiment satisfy Definition 1.4? 2.The experiment is on a relatively small problem. For example, a variant of policy iteration where policy evaluation is not solved exactly, but instead approximated by applying a small number of Bellman iterations.<BRK>These models are popular in econometrics, and aim at modelling the complex behavioural patterns of individuals or firms. First it identifies a subclass of discrete choice models (essentially MDP with stochastic rewards) where the value function is globally concave in the policy. The consequence of this observation is that a direct method, such as the policy gradient that circumvents explicitly estimating the value function, can (at least in principle) converge to the optimal policy without calculating a fixed point. The paper deals with Discrete choice models with unobserved heterogeneity as a special class of MDP’s and is relevant to ICLR. These assumptions seem to be important in subsequent developments for showing the concavity but they seem to be coming from out of the blue. Unfortunately the authors do not provide any intuition/discussion   an example problem with these properties would make these assumptions more concrete. I was hoping to find such an example in the empirical application however this section does not make the necessary connections with the theoretical development.<BRK>The paper is consider dynamic discrete choice models. It shows in an important class of discrete choice models the value function is globally concave in the policy, implying that for example policy gradients are globally convergent and are likely to converge faster in practice compared to fix point approaches. However, as an informed outsider, I am also a little bit confused. Then they provide an algorithm that is kind of close (at least in spirit) to policy gradient. While this is of course fine, the authors should clarify what is already known in the AI and ML literature (including the work described above). Nevertheless, the proof that there are convergent policy gradients for some dynamic discrete choice models appears interesting, at least to an informed outsider. However, this results heavily hinges on e.g.(Pirotta 2015). Here is where they make use of their assumption. So, the only point, in my opinion, that should be clarified is the usefulness of the considered class. For an informed outsider, this is not easy to see.
Reject. rating score: 1. rating score: 1. rating score: 3. rating score: 6. <BRK>This paper proposes a novel approach to deal with the computational problems of self attention without introducing independence assumptions. However, evaluation for this paper is severely lacking. The following should be added:Evaluation on a variety of different tasks, such as image segmentation, temporally consistent object detection, object tracking, etc. Runtime (in inference) comparisons for each of the datasets and for each of the baselines.<BRK>Is it because the proposed method cannot be adopted on other popular CV tasks, such as detection, segmentation, and classification? This paper attempts to solve this problem and proposed the Axial Attention method. The proposed method looks novel to me, but some of the related works are missing and the experiment session is insufficient. [a] CCNet: Criss Cross Attention for Semantic Segmentation[b] A^2 Nets: Double Attention Networks2) self attention has shown its effectiveness on a broad range of computer vision tasks, including image generation, detection, segmentation, and classification.<BRK>This paper claims to propose a new approach to solve the computational problems of self attention. However, the paper mainly focuses on adapting Transformer for image generation, which has far less applications. The whole paper needs to be rewritten to make their target and contribution clearer. 1.The authors overclaim that they provide a new approach for accelerating self attention. 2.For a paper claim to improve self attention, they should show its effectiveness on a broad range of tasks, with comprehensive experimental evaluation. Overall, the authors need to rewrite the paper. They should either show more applications with the proposed self attention approach or treat it as a new approach for image generation.<BRK>This paper proposes axial attention as an alternative of self attention for data arranged as large multidimensional tensors, which costs too much computational resource since the complexity of traditional self attention is quadratic in order to capture long range dependencies for full receptive fields. The axial attention is applied within each axis of the data separately while keeping information along other axes independent. The proposed axial attention can be used within standard Transformer layers in a straightforward manner to produce Axial Transformer layers, without changing the basic building blocks of traditional Transformer architecture.
Reject. rating score: 3. rating score: 6. rating score: 6. <BRK>In this paper,  the authors pay attention on the bottleneck in the NAS of its large architecture space which cause low efficiency. They introduce the multi agent reinforcement learning method to take the neural architecture search as a multi agent reinforcement learning problem. (2)Purpose two lightweight implementation. (3) Presenting 3 new datasets for NAS evaluation to minimize algorithmic over fitting. It seems like that it is the first work to combine multi agent reinforcement learning with NAS, and you have make complete proof about the algorithm s efficiency both mathematically and empirically. But from the view of multi agent reinforcement learning, there are also some points which make me confused. The main problem is coordination, and I understand it as the agents in your work aim to get a joint action and the training process of them are independent, but we all know that in multi agent problems, the changing of agent s policy will cause change of the environment, so it will bring the instability, so I want to know that how you deal with the instability or whether the instability influence a lot in your work? Another problem may be not a theoretically problem that I want to know that have you made the guarantee of the consistency of agents  policies when using parallel training (May be the framework in coding process guarantee it ?) or the consistency is unnecessary to talk because it doesn t influence the result?<BRK>In this paper, the authors proposed MANAS, which is based on DARTS, by approximating the problem space by factorizing them into smaller spaces, which will be solved by multiple agents. While the overall idea seems simple but the coordinating among agents can be difficult, where the authors proposed credit assignment techniques to address the issue. The final algorithm is evaluated on CV datasets as well as 3 new datasets. Overall, I found the motivation and proposed solution by the authors convincing. However, in a search space where random searcher is competitive, it is important for us to have an in depth understanding on the proposed techniques. Especially when the experimental results is not fully comparable (it is difficult to control #params to evaluate the Test Error, and Search Cost being a one time cost), I think the magnitude of the improvement showed in the experimental results itself might not be enough to justify this new approach. I am curious about the how does the number of agents affects the experimental results. It seems that it is not mentioned in the experiment section (or I might missed it?). And do we need to search for the best number of agents, which will add to the search cost? I am also curious on if one can combine other search algorithms with the similar idea on dividing the search space, e.g., using random search on a subspace in a coordinate descend fashion. It will be great if the authors can provide more in depth analysis on different component of the proposed algorithm so that we can fully understand the source of improvement.<BRK>This work built on top of DARTS. In their setting, each edge on the DAG (same as the one proposed in DARTS) has one agent associated with it and every agent maintains weights to propose operations. The author introduced two ways to update these weights: 1) solving a least squares assuming the validation loss decomposes linearly on the operations (MANAS LS); 2) only update the weights for the activated operations (MANAS). Because the distributed nature of the agents, this work is memory efficient and it allows searching directly on large datasets. The empirical results showed competitive performance in less GPU days comparing to  DARTS and recent variants. The paper is well written. Apart from the theoretical contributions, the empirical evaluations are well done: the author used 3 more datasets instead of the usual CIFAR 10 and IMAGENET. Also, the random search are brought into picture which I think every NAS paper should include. It s surprising to see MANAS LS sometimes outperform MANAS. For me, MANAS is a more principle way. Do the authors have more explanations? It s nice that the authors apply bandit framework to derive theoretical guarantees, but how close are these guarantees to the practice (for example on the benchmarks used in the work)? Is there some study for that? As there are not so many NAS works with theories, I think it would be nice if the authors could also comment on that.
Reject. rating score: 3. rating score: 3. rating score: 6. <BRK>This paper shows that RNN (of infinite horizon) can be universal approximators  for any stochastic dynamics system. The paper is well written and easy to follow. However, I have concerns about its novelty. Overall the paper seems to be a straightforward application of universal approximation theorem of deep neural network. Also with the same technique the authors also reach the same conclusion for nonlinear dynamics system (Sec.4.2).From the proof, there is no much difference between linear case (Kalman Filter) and nonlinear case, since apparently DNN can fit everything. It is not clear what can we learn from this analysis and its impact is likely to be limited.<BRK>It considers a general state space model and uses feedforward neural nets (FNN) to learn the filtering and forecast distributions. Based on the well known universal approximation property of FNNs, the paper shows that their RNN based filter can approximate arbitrarily well the optimal filter. The paper targets an important problem in statistical learning and offers some interesting insights. The property of the RNN based filter is interesting, but using it is, I believe, very difficult from a practical point of view. Could the authors please give some comments/discussion about this issue?<BRK>The paper shows that recurrent neural networks are universal approximators of optimal finite dimensional filters. More specifically, the paper provides a theoretical analysis on the approximation error for any stochastic dynamic system with noisy sequential observations. I find the work interesting. While I understand that this a purely theoretical work, it would be instructive to have practical demonstrations, showing what s happening when learning is actually done.
Accept (Talk). rating score: 8. rating score: 8. rating score: 6. rating score: 6. <BRK>The idea of target embedding autoencoders is extremely relevant for problems where the dimension of the label space is as large (or larger) than the dimension of the input features. The experiments are thorough, the theoretical guarantees are extremely well thought of and derived. I vote for a strong accept for this paper. I would like to see some references to the extreme multi label classification problems (http://manikvarma.org/downloads/XC/XMLRepository.html) and some of the other probabilistic approaches attempted in this domain (please see https://papers.nips.cc/paper/5770 large scale bayesian multi label learning via topic based label embeddings and the references and citations).<BRK>This work introduces the idea of target embedding autoencoders for supervised prediction, designed to learn intermediate latent representations jointly optimized to be both predictable from features and predictive of targets. More examples of this would be useful to provide in the intro. Overall the idea seems reasonable   if the targets have some common set of factors, just predict those instead of predicting the full target value which might be harder to get right. Does this mean this algorithm has been proposed before or is it that it can ALSO work on non static classification tasks? I wonder if the authors can comment on how often Assumption 1 and 2 are actually satisified? Or is there more to it? Is the main points of contribution the theoretical analysis and the extended experiments to sequence data rather than static classification? The results do seem to show a signficant benefit as compared to FEA or base models. It also seems like this is applicable across multiple disease datasets. Generally seems like a well grounded and meaningful contribution with many improvements.<BRK>1.Summary: In this paper, the authors proposed a Target Embedding Autoendocer (TEA) model for supervised representation learning. Hypothetically, this model should be especially useful when the target vector has a much higher dimension than the feature vector. 2.Overall assessment: The motivation of this paper is well justified. In my view, it s necessary to test on more different types of datasets to prove the usefulness of a model, especially if it is a general framework like TEA. Demonstrate the performance of TEA on more advanced models and more difficult tasks can deliver more insights to the community. 3.3 No state of the art models are used in experiments. It s very likely that some existing work has already adopted the idea of target embedding. This part is one of the most important parts of this paper.<BRK>This paper examines target embedding autoencoders (TEAs) in theory and practice. The forward pass of the decoder (for the output space) is shared by the input to output computation. The paper s presentation is confusing on this matter, at it claims to be the first to "motivate and formalize" TEAs; I do not believe it is appropriate to claim such a contribution in light of prior work. [B] operates in the same supervised representation learning setting proposed here. The real applications explored by [A] and [B] are perhaps more challenging than the datasets used in experiments here. This paper s theoretical analysis does appear to set it apart from prior work. I have updated my overall rating.
Reject. rating score: 1. rating score: 3. rating score: 3. <BRK>The theorems and their proofs are fine (similar to Meng et al.(2018)). is that the authors apply G SGD to RNNs instead of MLPs/CNNs.<BRK>It is hard to understand what the boxed steps are saying. Second, in Section 3 the definition of the reduction graph and how it is obtained from the directed graph is not clear, which is problematic since it is one of the key ideas in the paper.<BRK>The intuition is to leverage the reduction graph of RNN to removes the influence of time steps.
Accept (Talk). rating score: 8. rating score: 8. rating score: 6. <BRK>The models and datasets covered in the experiments are adequate to demonstrate that the presented technique is worth exploring, but probably not for someone considering applying it in the context of a deployed federated learning application. You ve comprehensively addressed my questions and I think this paper should be accepted.<BRK>I hope the authors continue to improve the readability of Sec.2.1Review Summary Overall I think this is almost above the bar to be accepted, and I could be persuaded with a strong rebuttal. The strengths here are the extensive experiments and the easy to implement method. Paper Summary This paper addresses the problem of federated learning, where J separate "clients" with disjoint datasets each train a neural network model for a supervised problem, and then try to aggregate all J individual client models into one "global model" in a coherent way. However, the extension to convolutional layers or recurrent layers has yet to be solved, which is the focus of this paper. Then, the global model weights for that layer is the average of the aligned client weights.<BRK>This paper offers a beautiful and simple method for federated learning.
Reject. rating score: 3. rating score: 3. <BRK>The authors propose to use numerical differentiation (using random perturbation) to approximate the Jacobian of a particular update (essentially equations 5~7) which plays an important role in the estimation of HMMs. I have found the paper poorly presented. this seems to be a storage problem and cannot be a complexity issue.<BRK>The paper is about a method for estimation of parameters of a collection of HMMs and the main contribution is the  combination of classical EM with a neural net. I am not sure what the benefit of the “e commerce” application is to the community. If the authors can comment about the last few points above (especially about open dataset, reproducibility) then I will reconsider raising the rating. (In their practical example that isn’t applied to more general recommender systems, this doesn’t really seem to be the case.So it’s unclear )+ The technical contribution of the gradient estimation seems sound.
Reject. rating score: 3. rating score: 3. rating score: 6. <BRK>+ This is an understudied area of research in summarization research, only few previous works have been done; for example, (Shang et al.2018).The proposed approach is end to end  trainable, whereas (Shang et al.2018)’s method tackles the problem in two separate stages. The proposed model (in Fig.2) consists of the encoder, the decode and hierarchical attention module. The encoder and the decoders are standard ones based on (Nallapati et al.2017) and (Li et al.2015), respectively. This paper argues that the proposed hierarchical attention is novel with no clear ground.<BRK>This paper proposes a technique for generating summaries of interleaved texts. Unlike previous work that first perform unsupervised clustering to extract ordered threads, the authors instead propose a hierarchical model that directly process interleaved threads. In particular, the authors propose using a hierarchical encoder that encodes words  > post  > threads as well as a hierarchical decoder that decodes threads  > words. On synthetic interleaved datasets composed from PubMed and StackExchange, the proposed method outperform seq2seq with attention as well as a pipeline system that applies unsupervised clustering followed by seq2seq.The authors present results for well chosen ablations as well as baselines. My concerns stem from the synthetic nature of these tasks. due to the nature of these two tasks, is disentanglement even helpful for summarization? The algorithm in the appendix for interleaving is not very helpful.<BRK>This work proposes an architecture to generate summaries for multi participant postings as in chat conversations. The provided baselines are quite weak compared to the SOTA summarization methods at the moment, although none of them is directly modelled for interleaved text summarization through multi summaries. The presented work proposes a hierarchical attention model to solve this task in an end to end fashion. overall:Although this paper is well written and well motivated.
Reject. rating score: 3. rating score: 3. rating score: 3. <BRK>The paper consider important and interesting problem: how to generate a sequence from minority class if we want to do oversampling with synthetic data in a way similar to SMOTE. "Effective data generation for imbalanced learning using conditional generative adversarial networks." Experiments are not convincing, as the authors don t compare to the state of the art approaches. F1 score is often not the best metric for imbalanced problems.<BRK>The paper is well written. A quick search on google, I found this paper:"Multi Task Generative Adversarial Network for Handling Imbalanced Clinical Data" by Mina Rezaei et al., arXiv:1811.10419v1Moreover, the paper doesn t seem to be comparing their results with other state of the art imbalanced sequence classification methods. For these two reasons, I do not recommend this paper for publication at this point.<BRK>ADASYN: Adaptive synthetic sampling approach for imbalanced learning. 2.In table 2 of the 5%  data imbalance, the proposed method is not as good as the baseline. 2.The authors created their own baseline, and compared against it. 3.Figure 2 is hard to understand.
Reject. rating score: 3. rating score: 3. rating score: 6. <BRK>1.Decision (accept or reject) with one or two key reasons for this choice. The comparison between different topologies is nice, but implies that the structure of the graph has to be fixed manually. This seems to be a severe and unscalable constraint. What happens if the agents are decentralized and the (shared weight) pairwise functions are separate? Authors only evaluate on a predator prey problem. Make it clear that these points are here to help, and not necessarily part of your decision assessment.<BRK># SummaryThis paper proposes a pairwise communication between agents using a shared neural network. # Clarity  The paper is well written, and the figures are very clear. # OriginalityThe main novelty seems to be coming from the idea of parameter sharing between pairwise payoffs, but the overall architecture seems to be the same as [Castellini et al.].<BRK>The presentation of the “Method” section is not clear enough to evaluate the contribution of this paper. I think the experiments are too simple and not convincing. Idea 3 is not implemented in this work. [3].Son, Kyunghwan, et al."QTRAN: Learning to Factorize with Transformation for Cooperative Multi Agent Reinforcement Learning."
Reject. rating score: 1. rating score: 1. rating score: 1. <BRK>This paper proposes to use an extra feature (grammatical number) for context aware coreference resolution and an attention based weighting mechanism. The approach proposed is built on top of a recent well performing model by Lee et al.The improvement is rather minor in my view: 72.64 to 72.84 in the test set. There is not much in the paper to review. I don t think the one extra feature warrants a paper at a top conference. The weighting mechanism over the features is also unclear to me why it benefits from attention.<BRK>This paper extends the neural coreference resolution model in Lee et al.(2018) by 1) introducing an additional mention level feature (grammatical numbers), and 2) letting the mention/pair scoring functions attend over multiple mention level features. The proposed model achieves marginal improvement (0.2 avg.F1 points) over Lee et al., 2018, on the CoNLL 2012 English test set. I recommend rejection for this paper due to the following reasons:  The technical contribution is very incremental (introducing one more features, and adding an attention layer over the feature vectors). The experiment results aren t strong enough. And the experiments are done on only one dataset. I am not convinced that adding the grammatical numbers features and the attention mechanism makes the model more context aware.<BRK>This paper unfortunately violates the blind review policy: its acknowledgement exposes the authors. I thus support desk rejection.
Reject. rating score: 3. rating score: 3. rating score: 6. <BRK>This paper proposes a GAN based approach for unsupervised video inpainting. The main critique I have is that the work seems incomplete and does not show great empirical results, which makes this work a much weaker contribution. Obviously, the authors should complete the experimental results on baseline method 2. Another limitation of this work is that a known mask distribution is assumed, while this might be a reasonable assumption in some cases, it can also be easily violated in real world problems. Overall I think this work attempts to solve an interesting problem with an incremental but reasonable approach, and more empirical evaluation is needed to make the paper meet the bar for acceptance.<BRK>Their approach proposed a method that uses GANs for denoising images to now handle full sequences of images by building on top work that handles inpainting in single images. 4) The paper is well written. Weakness1) From the code and the description in paper, it seems that a different corruption (https://github.com/anon ustdi/ustdi/blob/7a81db4972ef9d4eabbd8fe354a8984a7771ae5d/src/datasets/corrupted.py) is applied for each step. If this is the case, I am afraid the method cannot be called unsupervised as across many steps the model would have seen different corruptions of the same video and across many such corruptions the model can learn what an uncorrupted video looks like. If that is the case, the authors need to make comparison with other supervised inpainting methods as well [1,2]2) Is there a dataset for which these corruptions exist naturally? DecisionWhile the presented approach is good, further experiments are required to further validate the effectiveness of their approach in an unsupervised setting.<BRK>The paper addresses the problem of reconstructing a video sequence that contains occlusions. The authors show good results in reconstructing these video sequences in an unsupervised manner. The paper uses a GAN based network to accomplish inpainting in the occluded regions. The authors claim that the method is very flexible in terms of the data that needs to come in, and test this by deploying the method to solve quite different missing data problems (Using different type of occlusion and in different contexts). I think we need more data for this to be a reasonable constraint. The paper is well written, and the methods and experiments are convincing.
Reject. rating score: 3. rating score: 6. rating score: 6. <BRK>I am not familiar with the quantum algorithm literature, hence I cannot judge the novelty of the proposed algorithm. Basically, this is a quantum algorithm for computing the smallest negative eigenvalue (and the associated eigenvector) of square matrices. The potential issues of such dependencies should be discussed. The paper focuses on an individual step of a single iteration (i.e.finding the negative curvature), but it doesn t mention how would the iterative process look like.<BRK>This paper proposes a quantum algorithm aiming to solve the eigenvalue decomposition for the Hessian matrix in second order optimization. The authors propose some plug in algorithms in the context of quantum computing. Moreover, this paper does not evaluate the proposed algorithm with any comparative experiments.<BRK>I could not find definition of T_H anywhere. CommentsI appreciate the clarification of the notation at section 2.1, however many exotic notation for machine learning researchers are presented before this section without any reference to 2.1.
Reject. rating score: 1. rating score: 3. rating score: 6. <BRK>Authors should explain more clearly that the proposed contribution is a set of rules used to learn a distance between features in order to associate object instances. However, the connection with tracking is presented only in related works (at the end of the paper) and no comparison with other tracking approach is presented. Additionally the experimental evaluation is very weak in several points (see below). Contribution: this approach proposes a set of rules to train a network to be able to learn the correct association  between two set of features.<BRK>This paper proposes AlignNet, a bipartite graph network that learns to match to sets of objects. I do not think the paper meets the acceptance threshold, and recommend for weak rejection. While the paper proposes an interesting architecture to address the alignment problem, it has noticeable flaws in its experimental designs.<BRK>This is important to an ICLR submission. With the assumption of object persistence and inspired by the sticky indices, this paper proposed a novel object alignment method for matching arbitrary entities in different sets. I like this idea and have one concern about the experiment.
Accept (Poster). rating score: 6. rating score: 6. rating score: 3. <BRK>This paper proposes an extension of NeuralLP that is able to learn a very restricted (in terms of expressiveness) set of logic rules involving numeric properties. To this end, authors make this process tractable by using dynamic programming and by defining such a matrix as a dynamic computation graph by means of the cumsum operator. Authors evaluate on several datasets   two real world and two synthetic   often showing more accurate results than the considered baselines. Another concern I have is that the expressiveness of the learned rules can be somehow limited, but this paper seems like a good star towards learning interpretable rules involving multiple modalities. Missing references   authors may want to consider citing https://arxiv.org/abs/1906.06187 as well in Sec.2   it seems very related to this work.<BRK>This paper proposed several extensions to the Neural LP work. Experiments on benchmark datasets show significant improvements over previous methods, especially in the case where numerical variables are required. I think overall the paper is written clearly, with good summarization of existing works. Also I like the simple but effective tricks for saving the computation and memory. One main concern is, how general this approach would be? 3) I would suggest a different name other than Neural LP N, as it is somewhat underselling this work. Also it makes Table 2 not that easy to read.<BRK>This paper proposes an interesting extension to the Neural LP framework for learning numerical rules in knowledge graphs. The authors demonstrate its effectiveness on both synthetic knowledge graphs and the parts of existing knowledge graphs which consider numerical values. I recommend the paper to be rejected in its current form for the following 3 reasons:(1) Although the idea of making numerical rules differentiable is interesting, the current proposed method can only deal with one form of numerical predicate, which is numerical comparison. The limitation to such a special case makes the paper somewhat incremental. (2) The paper does not do a great job of convincing the reader that the problem it is trying to solve is an important matter, or the proposed method is indeed effective in some applications. The authors should try to find a real world domain which can really demonstrate the effectiveness of the method. (3) The experiment section lacks more detailed analysis which can intuitively explain how well the proposed method performs on the benchmarks. The experiment section needs significant improvement, especially when there is space left. The authors can consider improving the paper based on the above drawbacks.
Accept (Poster). rating score: 8. rating score: 6. rating score: 6. <BRK>The authors introduce the idea of distributed backdoor attacks in the FL framework, in which the dishonest participants in FL add local triggers to their training data to influence the global model to classify triggered images in a desired way. The paper contributes a novel model for conducting backdoor attacks in the FL setup, and shows that this model is more successful at attacking when training using robust FL algorithms than the standard centralized backdoor attack model.<BRK>This paper proposes a distributed backdoor attack strategy, framed differently from the previous two main approches (1) the centralised backdoor approach and (2) the (less discussed in the paper) distributed fault tolerance approach (often named "Byzantine"). The authors also compare two aggregation rules for federated learning schemes, (Fung et al 2018 & Pillutla et al 2019), suggesting that both rules are bypassed by the proposed distributed backdoor attack.<BRK>This paper studies backdoor attacks under federated learning setting. Post rebuttal commentsI appreciate the authors  great effort to address my concerns!
Reject. rating score: 1. rating score: 3. rating score: 6. <BRK>Many algorithms use function approximation, off policy learning and bootstrapping together This is an unstable combination of techniques. In this paper, the authors propose a graph perspective on the replay memory which allows to analyze the structure of deep RL. The motivation of the paper is meaningful. The paper in the current form needs to be polished again. It would be better if this paper can provide a clear illustration for the proposed method as well as the experiments section.<BRK>The paper proposes Qgraph, an algorithm that  addresses the problem of extrapolation error that appear in RL tasks with continuous action spaces. The authors describe a method to construct a graph from transitions generated by some policy. Then this representation is simplified and  used to compute Q values using methods for tabular MDPs. The related work section is missing several methods that attempt to address the same problem. The clarity of the paper can be improved. 3) What are the assumption regarding the initial state distribution? Although it is not stated clearly in the paper, it seems also requires to have a finite set of initial states.<BRK>This paper is trying to tackle the soft divergence issue in deep RL when algorithms combine function approximation, off policy learning and bootstrapping, which is also called deadly triad by Sutton & Barto (2018). The paper proposes a way to represent the transitions in the replay memory as a data graph, then construct a simple MDP from it. Much more accurate Q values could be computed from the simple MDP and it provides a lower bound for the Q values in the original problem. In this way, the method becomes less prone to soft divergence. In typical continuous state spaces, the same state might not appear twice in the sampled transitions. Maybe I m missing something, it s not very clear to me how the proposed method could be applied in the common case in deep RL where there s seldom a loop and the states are rarely visited twice.
Accept (Poster). rating score: 8. rating score: 8. rating score: 6. <BRK>This paper is a neural architecture search paper. In particular, it applies this to finding better neural architectures for video understanding, emphasizing exploring the video temporal resolutions needed and how to combine intermediate representations capturing appearance and motion. The results reported are very strong.<BRK>This submission proposes a way to do multi stream neural architecture search for video classification. I give an initial rating of accept because (1) there are not many work on video architecture search yet (2) the paper is well written (3) experiments are complete and results are strong.<BRK>Overall, this paper presents a concrete application of neural architecture search for video CNN with interesting results. Otherwise, the improvement shown in the paper is not that surprising.
Reject. rating score: 1. rating score: 1. rating score: 8. rating score: 8. rating score: 8. <BRK>[Additional review]This paper proposes a technique to incorporate document level topic model information into language models. While the underlying idea is interesting, my biggest issue is with the misleading assertions at the very beginning of the paper. In the second paragraph of Section 1, the paper claims that RNN based LMs often make independence assumptions between sentences, hence why they develop a topic modelling approach to model document level information. Some issues with this claim:1. a.Evidence 1: Khandelwal et al.(2018) showed that LSTMs memorise word orders from the past ~50 tokens, and retain semantic information from the past ~200 tokens; both of which extend far beyond the length of an average sentence, suggesting that information from the previous sentences is used in the predictions of the current sentence. on PTB test set, while LSTMs that condition on multiple sentences get a much better ~50 something ppl. Crucially, these prior works defeat the paper’s motivation of why it claims to need topic models in the first place (i.e.to model cross sentential context), while just concatenating multiple sentences as context would do, and in fact has been done many times. 4.The perplexity results (Table 1) are not done on very standard datasets (no PTB evaluation for instance). In the paper s defense, it seems that they were following the experimental setup of Wang et al.(2019), but the paper should elaborate more on the choice of evaluation datasets. I am not very familiar with the topic modelling literature, but it would be nice if the induced hierarchy can be evaluated quantitatively.<BRK>For example, (1) in the last paragraph of page 2, they claimed that the language component is used in their model to capture syntactic information, which I do not feel comfortable to accept; (2) in the first paragraph of page 3, it says "we define d_j as the BoW vector summarizing only the preceding sentences", without further information, I have no idea what a BoW vector looks like or how it is constructed; (3) in the last paragraph of page 3, it says using Dirichlet priors to make "the latent representation more identifiable and interpretable, but also facilitates inference", which I really don t know what it means. There are a few more examples like these. To be specific, in their definition, d_j refers to a summary of all the sentences other than s_j. In other words, there is a huge overlap between any two d_j and d_{j }. Therefore, I am not sure the decomposition on the right hand side of equation 5 (particularly, the decomposition of p(d_j | ...) ) is valid. Although they have some interesting results and the lowest PPLx comparing to other models, I do not think this paper is ready to be accepted.<BRK>This paper presents rGBN RNN, a model that integrates a hierarchical recurrent topic model with an RNN based language model in order to incorporate global semantic information and improve capturing of inter sentence relations. The proposed model improves in perplexity across the three tested datasets over state of the art models of comparable type, and follow up analyses show strong performance in sentence and paragraph generation, as well as learning of sensible hierarchical topics. Some areas for improvement:It seems strange not to mention all of the recent high profile work on LM based pre training, since my impression is that these models operate effectively with large multi sentence contexts. I would like to see more discussion of how this work fits with that. I don t know that it makes sense to highlight as the contribution of this model that it can "simultaneously capture syntax and semantics". It s not clear to me that other language models fail to capture semantics (keeping in mind that semantics applies within a sentence and not just at a global level)   rather, it seems that the strength of this model is in capturing semantic relations above the sentence level. The claim is that "the color of the hidden states of the stacked RNN based language model at layer 1 changes quickly ... because lower layers are in charge of learning short term dependencies", but looking at the higher layers I m not seeing clear evidence of capturing of long distance dependencies, or even clear capturing of syntactic constituents. The takeaways from that figure should be made clearer and should be sure to correspond to what we can actually confidently conclude from that analysis.<BRK>The proposed model can capture the dependence across the sentences in language generation though the recurrent latent topics. The parameters of both the hierarchical recurrent topic model and language model are learnt using a hybrid inference algorithm  combining  variational inference to estimate language model and inference network parameters and MCMC to infer rGBN parameters. The effectiveness of the proposed model on the language modeling task is demonstrated on 3 datasets using Perplexity and BLEU score. The paper also provides a visual representation of the topics and their temporal trajectories. Though the novelty of the model is limited, learning and inference with the proposed model is non trivial. Further, the paper show an improvement in performance on language modeling using the proposed approach over SOTA approaches, demonstrating the significance of the proposed approach. Though the paper is relatively well written, it would have been good to explain some points on architecture and inference. It would have been better to provide the rationale behind some architectural decisions like associating \theta^1 and g^1 as against g^3. Related to this, Figure 1 has a typo where \theta^2 is associated with g^3. An explanation on combining all the  latent representation in the RNN model used for language modeling will be helpful, though this is motivated by previous approaches. A proper explanation the TLASGR MCMC approach for sampling from the posterior of  rGBN parameters is missing in the main paper. It would be good to provide some details of this in the main paper. Experimental section compares the proposed approach against many SOTA approaches for the language modeling task. It would have been good to provide a quantitive evaluation of the topic modeling task also  in addition to demonstrating them qualitatively.<BRK>This paper presents a method for natural language generation, using a language model, informed by a topic model. The topic model is a hierarchical recurrent topic model that attempts to extract document level word concurrence patterns and topic weight vectors for sentences. The proposed method is a combination of two existing methods, i.e.gamma belief networks  and stacked RNN, where the stacked RNN is improved with the information from recurrent gamma belief network. Overall, this is a well written paper, clearly presented, with certain novelties. The method is well formulated mathematically and evaluated experimentally. One suggestion is that the authors didn t include computational analysis about the complexity and loads of the proposed method as compared with the baseline methods.
Reject. rating score: 1. rating score: 3. rating score: 6. <BRK>The paper proposes a new way of stabilizing Wasserstein GANs by using Sinkhorn distance to upper bound the objective of WGAN s critic s loss during the training. 2.It is unclear how costly the method is in terms of resources and training time. 4.In two out of three experiments, WBGAN GP does not beat WGAN GP. At the beginning of p.13 authors take D_theta(x)   sign(P_r(x)   P_g(x)), which is not a Lipschitz function. It is possible that P_g !<BRK>+ The overall writing is clear and well motivated. It is not clear from the paper whether the  []_+ op back propagates gradient or not, which will make a big difference here. A more detailed review can be found below to help the author(s) improve their work.<BRK>This paper proposes bounding the Wasserstein term in WGAN with an aim to stabilize training. The proposal is demonstrated to empirically improve stability over the baselines in the mid resolution experiments presented in Section 4.2. On the other hand, in the high resolution experiments in Section 4.3, the authors suggest that WBGAN with Sinkhorn does not scale well, and show some results with the bound determined on the basis of a separate run to investigate the converged value of the Wasserstein term, which is impractical.
Reject. rating score: 3. rating score: 3. rating score: 3. <BRK>This paper studies the problem of learning from corrupted labels via picking up clean instances from training dataset. The sample selection mainly based on function R(t), which controls how many instances are kept. Experimental results on both synthetic data and real world data demonstrate the effectiveness of the proposed method. The paper is very verbose and hard to follow. 2.A key part of the paper is the curvature of R(t), which is based on intuition. Does this mean this paper is contradicting its self? The curvature of defined R(t) is not needed?<BRK>This paper develops a method for sample selection that exploits the memorization effect. The paper is difficult to understand, and much of this difficulty stems from poor writing / presentation.<BRK>The experimental results using synthetic noise corruption are indicative of improved performance compared to the baseline techniques. I think the paper would benefit from a good proofread not just from the grammar/spelling perspective (which there are multiple instances which could be improved) but also from the overall presentation and legibility perspective.
Accept (Poster). rating score: 8. rating score: 6. rating score: 3. <BRK>This paper proposes a combined architecture for image text modeling. However the authors should release the code however they see fit, this is more of a personal preference on the part of this reviewer. The authors have done a commendable job adding detail, further analysis, and experiments in the appendix of the paper. Combined with the included code release, this paper should be of interest to many.<BRK>Summary: The authors design a new model for bidirectional joint image text modeling using a variational hetero encoder(VHE) randomized generative adversarial network (GAN) that integrates a probabilistic text decoder, probabilistic image encoder, and GAN into an end to end multimodal model. Their proposed VHE GAN model encodes an image to decode its associated text and feeds the variational posterior as the source of randomness into the GAN image generator.<BRK>This paper proposed VHE GAN for the text to image generation task. The motivation for the paper is not clear. Therefore, the authors should claim how the proposed VHE variational posterior can help the task. However, the paper also fails to tell the intuition of the deep topic model and PGBN text decoder. The comparison only included old baselines. In addition, the paper does not provide an ablation study to analyze the effect of each component proposed (e.g., Poisson gamma belief network, a deep topic mode).
Accept (Spotlight). rating score: 8. rating score: 6. rating score: 6. <BRK>This paper presents a semantic parser that operates over passages of text instead of a structured data source. This is the first time anyone has demonstrated such a semantic parser (Siva Reddy and several others have essentially used unstructured text as an information source for a semantic parser, similar to OpenIE methods, but this is qualitatively different). The key insight is to let the semantic parser point to locations in the text that can be used in further symbolic operations. This is excellent work, and it should definitely be accepted. I have a ton of questions about this method, but they are good questions.<BRK>I think you should pull at least some commentary about the constant used in Table 3 from Appendix B and include it in the main paper (or at least mention Appendix B is the place to look). I would pair your experiment description with the results rather than grouping all experiment descriptions and then grouping all results, especially when the order of the experiment descriptions does not match the order of the results presented. Overall it seems like a solid work; good empirical results showing improvements of each purported contribution. The model itself is a relatively simple construction of basic component, but the combination with the DSL is intuitive and makes sense. I don t think the novelty in model here is the main selling point anyways; the training variants and the demonstration of how well a DSL approach can do combined with previously introduced methods. I find the model description to be slightly unclear. In Fig 1 for example there is an arrow that connects passage to compositional programs. I think you should elaborate on how the attention over the encoded text interacts with the attention over previously generated tokens. Equations would make this far more explicit as is I am left with a lot of questions on how to implement your model.<BRK>This paper discusses an extended DSL language for answering complex questions from text and adding data augmentation as well as weak supervision for training an encoder/decoder model where the encoder is a language model and decoder a program synthesis machine generating instructions using the DSL. Do LSTMs output the indices or are you selecting from a preselection spans as part of preprocessing? Overall, I like the paper and I think it contains simple extensions to previous methods referenced in the paper enabling them to work well on these datasets. A deep dive on this would be a great insight (in addition to performance improvement statements on page 7).
Reject. rating score: 6. rating score: 6. rating score: 6. <BRK>*Summary* This paper study the convergence of Hamiltonian gradient descent (HGD) on minmax games. The paper show that under some assumption on the cost function of the min max that are (in some sense) weaker than strong convex concavity. *Decision*I think that is work is clearly very interesting. And this paper brings nice tools to analyse HGD. Making this algorithm not very practical. The authors should address that in their revision.<BRK>Summary:The paper, considers methods for solving smooth unconstrained min max optimization problems. I understand that the main contribution of the work is the theoretical analysis of the proposed method but would like to see some numerical evaluation in the main paper. **********after rebuttal********I would like to thank the authors for their reply and for the further clarification. The authors need to elaborate more on this.<BRK>I found this to be an interesting approach. This provides some further theoretical justification of the success of CO on large scale GAN problems. The paper is presented in a clear manner, with the objectives and analysis techniques delineated in the main paper.
Reject. rating score: 1. rating score: 1. rating score: 1. <BRK>The presentation of this paper is, in general, all over the place. Are the authors referring to the entire distribution of natural images, of which the training set is assumed to be a subset? This was not clear to me.<BRK>The paper is well written and was easy to read. A mathematical proof seems out of reach. While constrained not to be too unlikely, I wonder whether the likelihood increases or decreases with the dimensionality of the latent space and this would make an interesting plot.<BRK>Regarding the quality of writing: * There are various sloppy sentences in crucial parts of the paper. to invertibility of random like neural nets.
Reject. rating score: 3. rating score: 3. rating score: 6. <BRK>I thank the authors for the clarifications and the modifications to the paper. However, I still lean towards rejection. Would it make more sense to normalize by the initial norm of the weights? While I believe there is interesting content, with the current presentation style it is really difficult to digest. The paper studies transfer of representations learned by deep networks across datasets and tasks. Moreover, the plot seems to mix two factors: magnitue of the gradient and the smoothness of the loss landscape. Namely, the paper analzes the standard setup with pre training on one dataset and further fine tuning on a different  dataset. On the theory side, an analysis of transfer in two layer fully connected networks, based on the ICML 2019 work of Arora et al., is proposed. 1b) Experimental methodology is not presented in the main paper and not referred to. This is partially affected by the aforementioned issues with presentation.<BRK>There are of course other ways of carrying out transfer learning, but this paper focusses on these methods.They attempt to assess the viability of such a process in its improvements to generalisation and improvements to the loss landscape. The majority of the paper focus on experimental results, while the final 2 pages present some theoretical work that explains those results. As the paper suggests, there have been several advances in the use of transfer learning that showcase its benefits. This paper tries to fill that gap. This is why I recommend a weak reject, and a summary of my reasons for this are as follows:1) Section 3: In this section, the authors try to show that transferred networks tend to have better generalisation when the dataset being transferred to is similar to that the network was pre trained on (ImageNet dataset). As such, while the goal of the experiment is interesting, it is not clear how interpretable the results are, nor the validity of the conclusions raised. This would have been an interesting control to see whether the pre training does indeed improve generalisation.<BRK>The paper gives an extensive empirical and somewhat limited theoretical analysis for the transferability in DNNs. It is shown that transferred models tend to have flatter minima with improved Lipschitzness in the loss function when good choices of pretraining are made. Analyzing the Frobenius norm of the deviation between fine tuned network and fixed network is reasonable. The theoretical analysis seems like a good start, but it is not sufficient in general. However, analyzing transferability is an important topic that needs to be evaluated more. This paper presents interesting new steps towards that goal.
Reject. rating score: 1. rating score: 3. rating score: 3. <BRK>Pros:1.Besides the cons, the paper is written clearly and is easy to follow. To verify their claims, the authors considered autoencoding task for ModelNet40 dataset.<BRK>The paper is about decoder networks for 3D point cloud data, i.e., given a latent vector that encodes shape information, the decoder network outputs a 3D point cloud. Relative to what?<BRK>Major comments:One major shortcoming of the paper is its very limited experimental evaluation, that does not really allow to judge whether the proposed decoder architecture brings the real utility for most problems.
Reject. rating score: 3. rating score: 3. rating score: 6. <BRK>However, I have some major concerns regarding this paper, which I would like the authors to clarify. However, after reading the authors rebuttal, I still think that there is more work to do in terms of comparison to prior work. 1) the authors chose to model the problem from the raw wave. The authors provide results on the TIMIT dataset and compare the proposed model to several baselines.<BRK>This paper should be rejected for the following reasons:  The novelty is very limited: this work applied a well known architecture (CNN) to a common problem, phoneme recognition. Given that the rebuttal and the new version of the paper didn t address my major concerns, I am keeping my original rating. Some references are missing (see below).<BRK>To address this issue, an extrapolation detection problem is formulated to predict the fricative phoneme a few ms in advance. After a careful preprocessing of the data, long segments of raw audio are given as input to the convolutional net.
Reject. rating score: 1. rating score: 3. rating score: 3. <BRK>4) Similarly, the authors claim that “Unlike parameter regularization methods or iCARL or FSR, our approach further reduces the memory storage by replacing logits of each data or network parameters with one prototype of each class in the episodic memory”. You might want to split the introduction into two chapters. The sentence after Eqn. Is this right?<BRK>Is in practice M<<D ? The idea of using prototypes for continual learning is interesting, as the authors point out, this does not require adding new neurons to the network for new tasks.<BRK>What are we supposed to tak away from the TSNE plots? What s the evidence for this? The latter two sections also have some information that should go in the related work (aka introduction).
Reject. rating score: 3. rating score: 6. <BRK>Two optimisers are proposed for finding the most important features according to this measure, in order to explain why a classifier is making a certain prediction. The experiments are both qualitative (showing pixel wise importance maps for image problems) and quantitative (showing how well the various feature importance scoring algorithms do albeit on the measure explicitly optimised by the new proposed algorithms). I felt that Reg Greedy is a really nice idea but the paper did not do it justice. Surely, the evaluation metric should not match the (novel) objective of the proposed method?<BRK>The manuscript proposes a method for model explanation and two metrics for the evaluation of methods for model explanation based on robustness analysis. More specifically, two complementary, yet very related, criteria are proposed: i) robustness to perturbations on irrelevant features and ii) robustness to perturbations in relevant features. The manuscript has a good flow, and its content is easy to follow. The formal presentation of the proposed method is good. In my opinion this trend shouldn t be surprising given the fact that the proposed method is specifically optimized on such criteria as it is clearly stated by the title of Sec.3.
Accept (Poster). rating score: 6. rating score: 6. rating score: 6. <BRK>In this manuscript, authors propose an OFA NAS framework. They train a supernet first and then finetune the elastic version of the large network. After training, the sub networks derived from the supernet can be applied for different scenarios directly without retraining. The motivation is clear and interesting. My concerns are as follows.<BRK>If a clear, and TIMELY code (for the SEARCH phase, not only for the Eval phase) release could be done, then at least from the perspective of application, the impact of this paper could be further enhanced. It seems natural to train a large network, and from it to train sub structures, since overparameterization helps NN training. However, it is hard to imagine that training from large to small could eliminate the “interfering” of subnetworks, let alone “while maintaining the same accuracy as independently trained networks”. The experiments show that the proposed method is promising.<BRK>In this papers, the authors learn a Once for all net. This starts as a big neural network which is trained normally (albeit with input images of different resolutions). This results in a network from which one can extract sub networks for various resource constraints (latency, memory etc.) The method as described *is not reproducible*. The scheduling of sampling subnetworks is alluded to on page 4, and that s it. This defeats the narrative of the paper (once for all plus some fine tuning isn t exactly once for all). I would be curious as to why this is.
Accept (Poster). rating score: 8. rating score: 6. rating score: 6. <BRK>The resulting framework is applied to an ImageNet GAN for a variety of transformation, producing results which qualitatively and quantitatively indicate that the method works for the shown transformations, along with some analysis of the behavior of the model.<BRK>The goal of being able to manipulate continuous factors of variation within generative models is useful for controllable image synthesis, and the proposed method clearly achieves the desired result. Minor things to improve the paper that did not impact the score:2) In the abstract: "Our method is weakly supervised...".<BRK>The paper proposes an algorithm to find linear trajectories in the latent space of a generative model that correspond to a user specified transformation T in image space.
Reject. rating score: 1. rating score: 3. rating score: 3. <BRK>The novelty here would be that the authors propose incorporating an Adam like optimizer and periodically resetting the ADAM parameters. I would not consider using Adam to be sufficiently novel for publication in this venue, and the results from using parameter resetting are not so spectacular or convincing that they qualify, either.<BRK>I will stand by my score. The approach is evaluated on both a synthetic problem and on Atari games. Another downside of this work is that the convergence analysis is done on AMSGrad instead of Adam which the experimental results are based on, why were experiments not done with AltQ AMSGrad? I still find it odd that no experiments are done with AltQ AMSGrad since the convergence analysis was done for this algorithm.<BRK>It does the same for a variation of this algorithm where the momentum like term is reset every now and then. Would it be fair to say that AltQ is _online_ Q learning? The empirical results are not necessarily that convincing and there are important details missing. * The reason I ask is that DQN, without a target network, does work in approximately half of the Atari games. Which Atari version was used?
Accept (Poster). rating score: 6. rating score: 6. rating score: 3. <BRK>This paper proposed a pre trainable generic representation for visual linguistic tasks call VL BERT. Overall, I think this paper is well written and has solid experiment results. One of the nice features I found on this work is it s joint train with text only corpus and faster RCNN weight. 2: I notice there is a change in the textual input which take visual feature embeddings.<BRK>### Summary:This paper propose a new model for learning generic feature representations for visual linguistic tasks by pretraining on large scale vision and language datasets like Conceptual Captions and language only datasets like BookCorpus and English Wikipedia. I liked some of the design choices made in the paper. ### Strengths:  The paper explores an interesting direction of learning generic feature representations for visual linguistic tasks for down streaming tasks. They distinguish the proposed method from existing work and also compare the performance of the proposed approach with concurrent work on downstream tasks showing performance on par or better than existing methods.<BRK>Experiments show state of the art results on different downstream tasks. # 3.Novelty and MotivationThe novelty of the paper is quite limited. * Once textual embeddings are masked by [MASK], the related visual embedding (whole image) is also masked?
Accept (Poster). rating score: 6. rating score: 6. rating score: 3. <BRK>The Jacobian, reinterpreted as living in the input image space (as the authors do), is a map of which input pixels have the strongest effect on the output of the network. This paper proposes a novel regularization strategy for improving the robustness of networks to adversarial noise. A separate discriminator network is training to distinguish real input images from these adapter processed input output Jacobians. This is an amusing original idea, and I think this paper probably should be accepted to ICLR   though I don t hold that position very strongly.<BRK>The authors attempt to train for interpretable jacobian in order to improve the robustness of the model. This is done by employing a GAN like procedure where a discriminator attempts to distinguish between the transformed jacobian matrix (fake images, equivalent to generator) and real images. Experiments indicates that this improves robustness compared to unprotected models and approximately similarly to models trained with adversarial training. It however doesn t make the the case for why it is a good idea. I think that these aspects should be improved before the paper is ready for publication.<BRK>Previous research like Etmann et al 2019 and Tsipras et al 2018 showed that robustness leads to saliency. I think this is a very interesting work. The training method proposed in this paper is still kind of preliminary, though. It would be great if the authors can provide the training time comparison between JARN and some state of the art robust training methods.
Reject. rating score: 3. rating score: 3. rating score: 6. <BRK>The authors show, under mild assumption, the linear convergence (with high probability) of the training loss in the over parameterized setting. *Minor comments: (do not need to be addressed in the rebuttal)*  In the proof of Theorems B.1 and B.2 the statement “clearly T_0>0” should justified. Thus, the analysis of the whole dynamics is reduced to a perturbation analysis of the dynamics around the initialization. This statement is true but a continuity argument that should be stated. The contributions of this paper are the following: in the setting introduced in [15] (the ref numbers correspond to the ones in the paper) the authors prove the linear convergence of the training loss for a large enough output layer of a 2 layer Relu network with Weight Normalization. Regarding to [15] the difference is that the authors study Weight Normalization. They are able to get better bounds on the over parametrization, and provide an analysis that highlights two training regimes: a fast one (V dominated) and a stable one (G dominated) that could explain the benefit of weight normalization. However, the dependence in t could be easily avoided using the proof technique from [12] (Lemma 3.2) where the result is actually stated for any weight close enough to the initialization. The main issue to me is that this work builds up on the work by [15] without addressing the issues risen by [12]. One important takeaway from [12] that the ‘lazy training’ regime studied might not be a desired behavior and might not happen in practice. Thus, I think that extending the work form [15] to WeightNormalization would be interesting if the phenomenons described by the theory are backed up by some practical evidence. The magnitude of the weights v_k is treated as it was alpha the coefficient used to parametrized their initialization. Finally, regarding the proof of Theorem 4.1 and 4.2, I found it way more clear in [12]. I think it would improve the quality of the work but did not have a major impact on my grade. Moreover, the omission that Theorem 4.1 and 4.2 are only true with high probability makes even that fact even more unclear. *Questions:*“we believe that G dominated convergence is common”, can you provides evidence that G dominated and V dominated  stages occurs the way you describe it in the discussion ? Same for the claim “The direction of the weights changes rapidly at the earlier stages of training when α is small, and G dominated convergence ensues as α grows, leading to improved stability.” Particularly the link between your theory, the norm of v_k and alpha.<BRK>Global convergence of NNs is an important research direction in deep learning. There have been significant progresses in this direction since last year. This is the first paper (to my best knowledge) to prove such convergence result for weight normalization. The paper is very well written and the presentation is very clear and I really enjoy reading it! The proof builds on (and very similar to) [2] and other recent works concerning the global convergence of NNs in the kernel regime. In particular, the authors show some improvements over  previous works: e.g.the width required for the proof here is n^4 (n   dataset size) rather than n^6 without normalization ([3]). There are also some other interesting results concerning weight normalization, e.g.the NTK could be decoupled into the sume of the `directional`   NTK and the `length`  NTK. Overall, this is a good paper but the contribution might not be sufficient for acceptance for the reasons below. 1. the overall framework is very similar to previous works and the dynamics of NNs still lives in the kernel regime, which might not be the most interesting regime for deep learning. 2.In this kernel regime, global convergence (or GD dynamics) might not be the most interesting object to study since we already have many existing works in this direction. This requires a lower bound for the width that NN cannot converge if the width is below the threshold. Neural tangent kernel: Convergence and generalization inneural networks.<BRK>This paper presents a general proof of the convergence of two layer ReLU networks with weight normalization trained with gradient descent. Depending on the lengths of kernels the training process can be divided into two regimes, corresponding to updates of lengths and directions, respectively. And there are transitions from one regime to the other when the lengths gradually change during the training process. The proofs are very solid and the results look strong to me. My only concern is that this paper is obviously squeezed to fit into 8 pages. I am not sure whether this is acceptable. Personally I think there is no need to do so and it s okay to just use 10 pages based on the loaded contents of this paper. I d like to keep my original score for this paper after reading the authors  response.
Reject. rating score: 3. rating score: 3. rating score: 6. <BRK>Summary:This work studies the instability problem of DDPG in the setting of a deterministicenvironment and sparse reward. This work designed a toy environment to showcasethe potential issues of leading to the instability of DDPG. It is essential to analyze and understand the intrinsic properties of the classic algorithms, and it would benefit the research community a lot if the empirical study is appropriately designed and conducted. Detailed comments:Methodology:The experiments conducted cannot support the conclusions in this paper. In my opinion, this is an observation,instead of giving any useful conclusion. Several potential solutions are discussed while no empirical evidence or theoreticaljustification is provided, even in the designed 1D Toy example. It would be more convincing that the conclusions can be validated on more challengingtasks such as regular continuous action benchmarks (mujoco, etc.) Writing:The presentation of this work is not ready for publication, given its current form. What is the definition of reward function? The formula as shown in Eq 4e is not clear.<BRK>The paper investigates why DDPG can sometimes fail in environments with sparse rewards. It presents a simple environment that helps the reader build intuition and supports the paper s empirical investigation. The paper then augments DDPG with epsilon greedy style exploration to see if the cause of these failures is simply inadequate exploration. Surprisingly, in 1% of cases DDPG still fails. Finally, the paper gives a brief overview of some existing potential solutions. Currently, I recommend rejecting this paper; while it is very well written and rigorously investigates a problem with a popular algorithm, the paper does not actually present a novel solution method. It does a great job of clearly defining and investigating a problem and shows how others have attempted to solve it in the past, but stops there. It doesn t propose a new method and/or compare the existing methods empirically, nor does it recommend a specific solution method. This lowers its potential impact a little. I would ve liked the related work section to elaborate more on the relationships between the insights in this paper and those of Fujimoto et al.(2018a), since the paper says the insights are related. Section 3 gave a very clear overview of DDPG. The simple environment described in section 4 was intuitive and explained well, and the empirical studies were convincing. Submitting it to a workshop could be a good opportunity for discussion, feedback, and possible collaboration, all of which might help inspire a solution method.<BRK>Overview: This paper describes a shortfall with the DDPG algorithm on a continuous state action space with sparse rewards. To first prove the existence of this shortfall, the authors demonstrate its theoretical possibility by reviewing the behavior of DDPG actor critic equations and the “two regimes” proofs in the appendices. In this demonstration, they use a very simple environment they created, “1D Toy”. The 1D Toy environment is a one dimensional, discrete time, continuous state and action problem. Moving to the left at all in 1D Toy results in a reward and episode end. Episode length was set at 50, as the agent could move to the right forever and never stop the episode. If not, the actor would drift to a state were it no longer updates, and the critic would similarly no longer update either, resulting in a deadlock and suboptimal policy. The paper is well written, the theory is sound, and the experiment is sufficient to describe the stated deadlock situation that DDPG can contain during training. I have a few comments/questions for the authors which I have written below. The other two involve replacing two of the update functions. I’m curious to see in which directions you see this work being extended. You briefly mention in the conclusion that there would be more formal studies: how do you imagine these being?
Reject. rating score: 3. rating score: 3. rating score: 6. <BRK>This paper experimentally investigates how fast the generalization error decreases when some specific kernel functions are used in real datasets. This decreasing rate is theoretically analyzed by using the approximation theory of RKHS in the teacher student setting. It is shown that the rate is determined with the smoothness and effective dimensionality of input. I think the setting where the teacher is not included in the student RKHS is also analyzed, for example, in the following papers (there are also several related papers):F.J. Narcowich, J.D. The pros and cons of the paper are summarized as follows. Interpolation of spatial data – A stochastic or a deterministic problem? Therefore, I still feel that the paper requires more expositions about the relation to the literature. However, this paper misses several related work in the literature. It is mentioned that this paper investigates the "generalization error."<BRK>In order to rationalize the existence of non trivial exponents that can be independent of the specific kernelused, this paper introduces the Teacher Student framework for kernels. In this scheme, a Teacher generates data according to a Gaussian random field, and a Student learns them via kernel regression. The paper is well written, tghe major issue of this paper is the lack of comparison with other previous methods. Therefore, the efficacy of the proposed model can not be well demontrated.<BRK>This paper studies, empirically and theoretically, the learning rates of (shift invariant) kernel learners in a misspecified setting. Neither seems to match the experimental rate on MNIST and CIFAR 10; this paper proposes a theoretical model that can more or less match the experimental rate with essentially reasonable assumptions. A related comment on your main theorem: your target function evaluated at every conceivable point (not just on a grid) is a sample from a Gaussian process. Empirically, your investigations are nice, but it would be good to consider some other shift invariant kernels as well: inverse multiquadric, Matérn, or spline RBF kernels would be prominent options. Also, honestly, I m not sure ICLR is the best venue for it (if I had written this paper around this time, I probably would have submitted it to AISTATS; it s certainly not *off* topic for ICLR, but fairly distant from most work at it). Under (25): "where where."
Reject. rating score: 1. rating score: 1. rating score: 3. <BRK>The authors present a model for time series which are represented as discrete events in continuous time and describe methods for doing parameter inference, future event prediction and entropy rate estimation for such processes.<BRK>The paper focuses on the problem of modeling, predicting and estimating entropy information over continuous time discrete event processes. The authors also use inferred model with previously developed techniques for estimating entropy rate. The authors describe the methods and provide the evidence of the effectiveness of their method with experiments on a synthetic dataset. We encourage the authors to look at  the relevant literature and position their work in comparison to these existing approaches. Figure 4 is not correctly referenced in the paragraph right above Section 4.3.<BRK>This paper proposed a model for continuous time, discrete events prediction and entropy rate estimation by combining unifilar hidden semi Markov model and neural networks where the dwell time distribution is represented by a shallow neural network. Comments:The literature review on previous work for continuous time, discrete events prediction is not thorough enough. The experiments are rather simple both in terms of the model used to generate the data and the number of different data sets being used.
Accept (Poster). rating score: 8. rating score: 6. rating score: 6. <BRK>This paper proposed a novel learning to learn framework based on zeroth order optimization. An ablation study is also conducted to study the effect of each component in the proposed framework. Overall, this paper is pleasant to read and well motivated. The applications are of practical importance.<BRK>This paper proposed a learning to learn (L2L) framework for zeroth order (ZO) optimization, and demonstrated its effectiveness on black box adversarial example generation. However, I have some concerns about the current version. 4) The computation and query complexity are unclear during training and testing. It seems that training a L2L network is not easy. Please try to make these points clearer in the revised version.<BRK>The paper proposes a zeroth order optimization framework that employs an RNN to modulate the sampling used to estimate gradients and a second RNN that models the parameter update. The paper evaluates the proposed framework on MNIST and CFAR tasks, as well as a synthetic binary classification task. At various points, the paper comments that other methods scale poorly with the dimensionality of the query space, which is true of the proposed method unless the operations are parallelized. It would be helpful to make these claims more formal, particularly in light of the fact that the mean does not change. How important is this to performance? How sensitive is convergence to the frequency with which standard Gaussian sampling is used? 1 is unclear.
Reject. rating score: 1. rating score: 1. rating score: 3. <BRK>At a first glance, this paper proposed an interesting refinement of interval bound propagation (IBP). However, it has a major flaw in empirical evaluation, and the proposed "theory" and "bounds" are also questionable and have many issues. Conversely, attack based methods like PGD give an upper bound, as there can be stronger attacks that further decrease accuracy. This is quite disappointing. Although this tampered "s" bounds may empirically help to improve robustness, it is not theoretically sound; training a sound bound helps to obtain better certified accuracy.<BRK>Summary: This paper studies the problem of certified robustness to adversarial examples and proposes a new training method to obtain better certified robustness. This new method is based on using a double margin regularizer.<BRK>The authors first demonstrate that many existing approaches are special cases of regularized objectives, and then provide a theoretical analysis on the relationship between the local minima of the original loss and the corresponding regularized loss. This is a super general and vague motivation, and is not specialized to the DoubleMargin regularizer. The argument can actually be used for justifying arbitrary regularizers... The second part of this paper proposes the DoubleMargin regularizer.
Accept (Poster). rating score: 6. rating score: 6. rating score: 6. <BRK>This paper propose to estimate the confidence sets for deep neural networks with PAC guarantees by combining calibrated prediction and generalization bounds from learning theory.<BRK>The paper proposes an algorithm combining calibrated prediction and generalization to construct confidence sets for deep neural networks with PAC guarantees. Concerning experiments that are not about Reinforcement learning, the paper proposes different strategies to learn a ResNet architecture for ImageNet. The paper is very solid theoretically and experimentally.<BRK>Summary: This paper presents an approach for generating confidence set predictions from deep networks. * In general, the proof is quite hard to follow. * Notation, in general, is a bit of an issue in this paper. This should be corroborated. What is meant by better?
Accept (Poster). rating score: 8. rating score: 6. rating score: 3. <BRK>This is the most important section which can be of interest to the community to understand VAEs  limitations, a good contribution on its own. I am not suggesting that you should remove it. It is not going to be change the review.<BRK>This paper studies the vulnerability of representations learned by variational auto encoders (VAE). For simplicity, it could be just named as latent space adversarial training.<BRK>This paper analyzes the shortcoming of VAE objective, and propose a regularization method based on a selection mechanism that creates a fictive data point by explicitly perturbing an observed true data point. It is lead to Wasserstein distance between representations.
Accept (Talk). rating score: 8. rating score: 6. rating score: 6. <BRK>The paper uses the continuous Game of Life as a testing ground for algorithms that discover diverse behaviors. The problem is interesting, under explored, and rich. The  combines a variety of interesting ideas including compositional pattern producing networks (CPPNs) to learn structured primitives. The hypothesis on p34, sec E.4.2 that the VAE’s 8 dim bottleneck helps focus on animals rather than non animals (which are differentiated more in terms of textures and details) is important and should be checked. It’s not clear why initializations are an parameter to vary when considering diversity of solutions. The experiments along the x axis vary according to initialization, but also according to the nature of the goal space and other features. It seems a bit incoherent. Overall I think this is a good paper. However, the paper is extremely long, and it feels as though the authors have to some extent lost control of the material. I could add more comments but TL;DR it needs a lot of editing and pruning.<BRK>The paper describes an algorithm to find diverse patterns in Lenia (a continuous CA system) by using a CPPN to generate initial states, and a stochastic exploration algorithm to mutate parameters of the CPPN + CA parameters. Initial inspection reveals that handdesigned goal states produce the most interesting non animal patterns. With regard to animal forms, it appears to me that Online goal learning harms the diversity of animal forms considerably compared to PGL and perhaps HGS. High frequency spatial structure seems to be lost there. I would like to see a further analysis of maybe 10000s of such images generated, and an understanding of exactly why RGS produces the same kind of red linear patterns, and why HGS produces the distribution of pattern types in Figure 29, and why non animal types differ in PGL vs HGS, and why high frequency spatial structure is lost in OGL.<BRK>The focus of the presented paper is on formulating the automated discovery of self organized patterns in high dimensional dynamic systems. Overall, I have the impression this is an interesting paper that could be accepted to ICLR. The idea of applying IMGEPs to explore parameters of a dynamic system is novel and interesting, which could also simulate further research in this field. Furthermore, the paper well written, technically sound, and the results are interesting. Moreover, given that this paper has more than 35 pages appendix material, it seems this work would better be suited for a journal as for a conference. How is the distribution chosen exactly? Section 3.2 appears a bit repetitive and could be more concise. I don t think it is necessary here to contrast manual vs learned features of the goal space.
Accept (Poster). rating score: 6. rating score: 6. rating score: 6. <BRK>This paper continues the recent direction (e.g.Amos & Kolter 2017) of differentiating through optimal control solutions, allowing for the combination of optimal control methods and learning systems. The paper has some nice contributions and I find this research direction to be very exciting, which is why I think it merits acceptance, however I find the experiments (Section 4) could be greatly improved. The main contribution of the paper are the analytical derivative of the solution to the DARE. The pre stabilising controller reformulation is a neat trick. The main issue I have with this paper is that the experiments are performed only on a toy 2D problem. Of course it is important to start with a toy problem, but once positive results have been shown, it would be much more convincing if the paper showed some more complicated system, possibly an iteratively linearised non linear system.<BRK>This paper shows how to make infinite horizon MPC differentiableby differentiating through the terminal cost function and controller. Recent work in non convex finite horizon continuous control [1,2,3] facea huge issue in selecting the controller s horizon length andbetter understanding differentiable infinite horizoncontrol has potentially strong applications in these domains. The imitation learning experiments on a small spring dynamicalsystem are a necessary sanity check for further work, butmany other more complex systems could be empirically studiedand would have made this paper stronger. One point that would be useful to clarify: the DARE solution in (7,8) isderived to optimally control a LTI system *without* control/state bounds butis then used to control the LTI system *with* control/state bounds in (4). Does this lead to suboptimal solutions to the true infinite horizon problem?<BRK>The paper shows how to use the Discrete time Algebraic Riccati Equation (DARE) to provide infinite horizon stability & optimality to differentiable MPC learning. The paper also shows how to use DARE to derive a pre stabilizing (linear state feedback) controller. The paper provides a theoretical characterization of the problem setting, which shows that prior work on differentiable MPC learning may lead to unstable controllers without the proposed augmentations using DARE. Can the authors compare & contrast with this paper? The idea of pre stabilization is interesting, and seems related to this paper: https://arxiv.org/abs/1905.05380**** After Author Response ****Thanks for the response, I am raising my score to weak accept.
Accept (Poster). rating score: 8. rating score: 6. rating score: 1. <BRK>This paper provides a unified theoretical framework for regularizing GAN losses. So far this was only observed experimentally but without any theoretical insight. The result goes beyond that as the criterion could be applied to general convex cost functional. The main general theorem is Theorem 1 which states 3 conditions on the optimal critic and 2 others on the generator. The paper is mainly concerned by the conditions on the optimal critic and show that the first 2 conditions can be achieved by the Spectral normalization, while the last one can be achieved by some gradient penalty. The paper is clearly written, well structured and pleasant to read. Limitations: The paper considers only the setting where the optimal critic is reached and therefore it is still unclear if the analysis carries on to the training procedures used in practice (non optimal critic). The authors recognize this limitation and leave it for future work. Overall, I feel that the paper provides good insights on what regularization is important for training gans and why. For that reason, I think this paper should be accepted. I do agree with reviewer 1 that a better discussion of the connection to [2] should be included since that paper was interested in  ensuring weak continuity of the loss, which can be thought of as  a first requirement to get more regularity of the cost functional. On gradient regularizers for MMD GANs.<BRK>The work studies the relationship between the stability and the smoothness of GANs based on the proposition which was proposed by Bertsekas . Meanwhile, it develops regularization techniques that enforce the smoothness conditions, which can lead to stability of the GAN. 2.The proofs of the theorems and the propositions in this paper are gorgeous and beautiful. Cons1.As the paper concludes, in practice, it is impossible to let the generator be trained after the discriminator attain theoretical optimal. As a paper which topic is about the training process of GAN, it is better to account for real situation. The main theorem only gives the sufficiency of those conditions. I think it’s necessary to give an example which can imply that anyone condition is essential. The condition (D3) doesn t necessarily still hold if only adding the gradient penalty term to the objective function.<BRK>The paper provides new theoretical view on GAN regularisation. However, it lacks proper empirical evaluation and makes an impression of a work in progress. Furthermore, the conclusions lead mostly to common techniques that have already been studied. Pros:  Theorem 1 provides sufficient conditions for convergence of generator gradients to zero (under assumption of optimal discriminators). Cons:  No evaluation with respect to any reasonable GAN setting. The main insights of sections 4 and 5 are trivial, like enforcing Lipshitzness of optimal discriminator by optimization of only Lipshitz discriminators. Theorem 1 is not  iff , so Lipshitz constraint *may be* not enough. It is unclear, however, why discriminator function would belong to such space. This setting fails, as stated earlier in the Introduction. (4) It seems there is conceptual misundersting of what MMD GANs are in Appendix B.
Reject. rating score: 1. rating score: 1. rating score: 1. <BRK>In this paper, the authors proposed a fairness aware learning method. I suggest swapping Figure 2 and Figure 3. Why not use DI as the x axis? Because of the issues in experiments, it is hard to evaluate the improvements of the proposed method.<BRK>The domain of X and Y in equation (2) is not defined anywhere, neither is the distance. Measures such as the quantiles or probability laws need to be formally defined before they are used in definitions.<BRK>This paper proposes a variant of adversarial learning to achieve some of the popular group fairness definitions. Finally, the paper is quite poorly written. Authors essentially describe demographic parity as fairness, while it is simply one of the several definitions of group fairness.
Reject. rating score: 1. rating score: 3. rating score: 6. <BRK>However,  this method appears to not consider that information. (3) The experiments are very limited in nature and fails to demonstrate the efficacy of the proposed approach effectively. Do the authors have insight on whythis is the case? al.[2] On improving deep reinforcement learning for POMDPs, Zhu et.<BRK>This will make the paper more self contained. In addition, the fact that PPO+RNN shows much better performance than the proposed method on Obstacle Tower is also worrying.<BRK>The authors study the problem of RL under partially observed settings. While most current (D)RL approaches use RNNs to tackle this problem, RNNS are trickier to optimise than FFNNs   in practice RNN based DRL agents can perform well on partially observed problems, may require more effort to optimise, and may underperform FFNNs on domains with no/less partial observability. There are several things to be done to improve the paper however.
Reject. rating score: 3. rating score: 3. rating score: 6. <BRK>This paper proposes a representation learning algorithm for RL based on the Information Bottleneck (IB) principle. This results in a loss that is difficult to optimize directly in the general case: the authors thus propose to approximate it through a variational bound, using Stein variational gradient descent (SVGD) for optimization, which is based on sampling multiple Z_i’s for a given state X, so as to compute an approximate gradient for the parameters of the function mapping X to Z. Experiments show that when augmenting the A2C algorithm with this technique, (1) the mutual information I(X, Z) decreases more quickly (better « compression » of the information), and (2) better sample efficiency is observed on 5 Atari games (with also encouraging results with PPO on 3 Atari games). The key negative point is definitely the weak empirical evaluation.<BRK>The authors discussed the information bottleneck in the setting of RL thoroughly, and proposed a novel target Y (\log P) in the scenario of policy optimization. I think the author should motivate the usage of implicit models (the embedding function $\phi(x, \epsilon)$) and then show that we should optimize the implicit encoders using amortized SVGD. The motivation of using amortized SVGD is unclear. Baselines including other representation learning methods should be compared. The authors only compared with naive A2C or PPO, while there are many other methods which can be directly utilized to learn the representation during the policy optimization process. Overall I think the paper studied an interesting problem while the motivation and the advantage of the proposed method are still unclear, which requires more discussion and comparison with other methods which can be utilized for representation learning. "Learning to draw samples with amortized stein variational gradient descent.” UAI 2017.<BRK>This paper investigates the information bottleneck approach in learning representations in reinforcement learning. The authors propose 1) combining actor critic objective and IB loss; 2) deriving optimal target distribution of the composite objective; 3) optimizing a lower bound of target function and using SVGD for optimization; 4) empirically show that the proposed method achieves promising results; 5) empirically show that there exists E C process in RL; 6) discuss the relationship with MINE. The experimental results are promising. 2.If not use the lower bound of the target distribution (use P rather than U), is there any method to solve this?
Accept (Poster). rating score: 6. rating score: 6. rating score: 6. <BRK>This paper presents an approach for navigating and exploring in environments with dynamic and environmental hazards that combines geometric and semantic affordance information in a map used for path planning. Results in a VizDoom testbed show favorable performance compared to both frontier and RL baselines, and the author s approach is more sample efﬁcient and generalizable than RL based approaches. I wouldn t consider any particular aspect of this paper to be that novel, but it is a nice combination of leveraging active self supervised learning to generate spatial affordance information for fusion with a geometric planner.<BRK>The paper proposes to learn affordance maps: a method to judge whether a certain location is accessible. This is done by distilling a series of "trial and error" runs and the relation of a pixel in the image/depth plane to a corrdinate into a model. I like the idea and think the paper should be accepted. The idea to use trial and error (something I prefer to self supervision, which is used differently in many contexts, I believe) to obtain a data set for learning a model is nice and very practical. Please correct this. What he means is that a part of the data is used to predict another part of the data. The "trial and error" method is clearly not viable for robotics setups, as hazards are costly.<BRK>This generates a collection of partially densely labelled images, on which a segmentation network can be learned that learns which part of the RGBD input are navigable and which should be avoided. For navigation, navigability of the current frame is predicted, and that prediction is down projected into an "affordance map" that is used for navigation. How can this approach work for moving obstacles? ###### Post rebuttal updateI am happy with the author s response to my concerns, and they have included corresponding discussions in their paper. Thus, I am improving my rating to recommend acceptance of this paper to ICRL2020.
Reject. rating score: 1. rating score: 3. rating score: 6. <BRK>Weakness:  The novelty of the proposed approach is very marginal  The experiments are very weak. This paper studied unsupervised graph representation learning. The paper could be better fit to a more applied conference. However, the novelty of the proposed method is very marginal.<BRK>The paper presents an unsupervised method for graph embedding. The approach is rather a mix of previous works and hence not novel. Also, the Figure 1. is taken from the original paper of WL kernel.<BRK>This paper proposes a framework for learning distributional representations of graphs in the following way: First, each graph is represented as a collection of subtree patterns. Overall, the idea of formulating graph representation learning as a language model is interesting. The neural language models rely on the concept of context in documents.
Reject. rating score: 1. rating score: 6. rating score: 8. <BRK>I have several concerns about the clarity of the paper, its applicability to the motivating problem, and the completeness of the results that motivate my "reject" recommendation. The motivation for the paper is also unclear.<BRK>The authors introduce ReSWAT, a method for transformation resilient watermarking of images via adversarial training. W can be any transformation (in this paper, an l infty bounded perturbation) that imputes an imperceptible distortion to a given input, while D is a detector that distinguishes watermarked from non watermarked images. In particular, it can be viewed as creating an adversarial defense to the Expectations over transformations attack of Athalye et al.Experimental results on a bunch of different datasets confirm that the method works,The paper is well written and the contributions are clear.<BRK>This paper is about a novel method to add watermarks to images and audio that is highly robust to several transformations that is closely related to gan methods. The proposed method is novel and goes in the interesting direction of learning watermarks using adversarial training techniques. Experiments shows that the method has good performance but it is only compared to Broken Arrows (i.e.a zero bit watermarking) which is the state of the art of this type of watermarking.
Accept (Poster). rating score: 6. rating score: 6. rating score: 6. <BRK>In this paper, the authors developed graph scattering transforms (GST) with a pruning algorithm, with the aim to reduce the running time and space cost, improve robustness to perturbations on input graph signal, and encourage flexibility for domain adaption. First, it is good to see sufficient theoretical analysis on the optimization algorithm and algorithmic stability. However, one concern here is on the time complexity analysis.<BRK>One of their drawbacks is that, due to having to consider all scattering paths (all possible combination of wavelets coming from each layer) their space complexity scales exponentially with the depth of the network. The paper proposes a simple scheme for pruning the contributing wavelets so as to retain only those that are best aligned with the input signal of the given layer. Depending on what downstream algorithm the scattering network s output is fed into, sometimes small coefficients could be important too. It might be unfair to only use an SVM in the experiments, since for SVMs larger magnitude features are important more likely to be relevant. Some stability results are presented, but they are not terribly deep, and do not address the fact that the optimization is problem is highly nonlinear and therefore even after relatively small perturbations drastically different sets of wavelets may be selected. Nonetheless, this is an interesting contribution to the field of graph scattering networks.<BRK>This is simple, and numerically efficient. I find this idea elegant because it is simple, and it seems to work well(and even improve standard GST). Figures and Tables are too small...post rebuttal:I found the arguments of the authors convincing, they addressed my concern and revised the paper accordingly. Cons:  The bound in lemma 1 sounds pretty un optimal. In the current setting, discriminative scattering path could be discarded simply because the wavelet combined with a modulus did a good job at demodulating the signal to the low frequency domain. minor:   "Under certain conditions on graph filter banks, GSTs are endowed with energy conservation properties, as well as stability meaning robustness to graph topology deformations (Gama et al., 2019a)."
Reject. rating score: 1. rating score: 1. rating score: 3. <BRK>This paper introduces a compositional generative model of images, where the image is described by a variable number of latent variables. The appearance vectors are then stochastically sampled without replacement according to a location distribution. Unlike AIR, this approach does not model the object scale. I recommend REJECTing this paper.<BRK>After reading reviews and comments I have decided to confirm the initial rating. The work presents an approach to encode latent representations of objects such that there are separate and disentangled representations for location and appearance of objects in a scene. We leave this for future work.”.<BRK>This paper presents a probabilistic generative model for identifying the location, type and colour of images placed within a grid. The main concerns with the paper are as follows:1) The implementation details for the work are relegated to an appendix.
Accept (Poster). rating score: 8. rating score: 8. rating score: 3. <BRK>*** Increased to Accept from Weak Accept after author rebuttal and changes to the paper ***This paper proposes a method, SNOW, for improving the speed of training and inference for transfer and lifelong learning. * Since the source model is fixed, the applicability of the approach to lifelong learning is heavily dependent on the usefulness of the source model to subsequent tasks. Furthermore, there can be no transfer between the tasks trained in the delta models.<BRK>After rebuttal:Authors have addressed all my doubts. I recommend accepting this paper. Before rebuttal:Summary:This paper proposes a new way to do transfer learning. 1.This is an interesting model to do transfer or lifelong learning but only for ConvNet architectures with image data. This needs to be elaborated and highlighted in the paper. I guess it is training throughput.<BRK>This paper attempts to tackle transfer learning and lifelong learning problem by subscribing to knowledge via channel pooling. Experiments show effectiveness of the proposed method. Pros:Overall, this paper is well written and easy to follow. The technique is sound and the problem studied in this paper is significant. The main reason is that lifelong learning basically requires only one model that will continue to learn from new tasks.
Accept (Poster). rating score: 6. rating score: 6. rating score: 6. <BRK>The paper proposes a technique to handle a certain type of constraints involved in Markov Decision Processes (MDP). The authors compare with the competing methods that they think are most appropriate. The numerical experiments seem to show superiority of their method in most of the cases. For now, I will take the authors  word, and rely on other reviewers. Overall, this seems like a nice contribution based on the importance of the problem and the good experimental results, hence I lean toward accepting. I do have some concerns that I mention below (chiefly that the theory presented is a bit of a red herring), but it may be that the overall novelty/contribution outweight these concerns:(1) Concern 1: the theorems (Thm 3.1, 3.2) apply to the intractable version, and so are not relevant to the actual tractable version of the algorithm. (2) Concern 2: Fig 3(e), "Grid" data, your algo with KL projection does worse in Reward than TRPO, which is not unexpected since TRPO ignores constraints. For the PCPO update, the theorem needs to mention that H is positive definite. It wasn t obvious to me when the constraint set is closed and convex. You also don t need to exclude theta from the set C, since if it is in the set C, then the quadratic term will be zero, hence less than/equal to zero.<BRK>This paper proposes a new algorithm   Projection based Constrained Policy Optimization, that is able to learn policies with constraints, i.e., for CMDPs. The algorithm consists of two stages: first an unconstrained update for maximizing reward, and the second step for projecting the policy back to the constraint set. The authors provide analysis in terms of bounds for reward improvement and constraint violation. This is an interesting work with impressive results. Given these clarifications in an author s response, I would be willing to increase the score. 1) Incremental workThe work extends the CPO [1] with a different update rule. Instead of having the update rule of CPO that does reward maximization and constraint satisfaction in the same step, the proposed update does that in two steps. The theory and the algorithm stem directly from the original CPO work, including appendix A C. The authors claim that another benefit of PCPO is that it requires no hyper tuning, but same is true for CPO (in the sense that they both don’t need Lagrange multiplier) . I’m assuming it is the same $\delta$ that is used in Lemma A.1. The underlying algorithm is still based on monotonic policy improvement theory in general, and more specifically on TRPO, so it should still have line search as part of the optimization procedure.<BRK>The proposed constrained policy optimization algorithm is shown to be useful on a range of control tasks, satisfying constraints or avoiding constraint violation significantly better than the compared baselines. Existing TRPO method already proposes a constrained optimization method (equation 2 as discussed), where the constraint is within the policy changes. This paper further introduces additional constraints (in the form of expected cost or safety measures) where the intermediary policy from TRPO is further projected into a constraint set and the overall policy improvement is based on the constraint satisfied between the intermediary policy and the improved policy. In other words, there are two levels of constraint satisfaction that is required now for the overall PCPO update. I am not sure of the significance of theorem 3.2 in comparison of theorem 3.1? The fact it requires two approximations, mainly for the projection step seems to add further complexity to the propoed approach? Two baselines are mainly used for comparison of results, mainly CPO and PDO, both of which are constrained policy optimization approaches. The experimental results section requires more clarity and ease of presentation, as it is a bit difficult to follow what the results are trying to show exactly. Overall, I think the paper has useful merits   although it seems to be a computational ly challenging approach requiring second order approximations for both the KL terms (reward improvement and project step). I think it is useful to introduce such policy optimization methods satisfying constraints   and the authors in this work propose a simple approach based on projecting the policies into the constraint set, and solving the overall problem with convex optimization tools.
Reject. rating score: 3. rating score: 3. rating score: 3. <BRK>This paper proposes a new method for learning diverse policies in RL environments, with the ultimate goal of increasing reward. The paper develops a novel method, called interior policy differentiation (IPD), that constrains trained policy to be sufficiently different from one another. Smaller concerns and questions:  There are a couple of instances where I found the claims of the paper with respect to related work to be over stated. I like the idea of enforcing a constraint on the policy diversity via manipulating the transitions that the agents can learn from. There is an initial engineering overhead, but it’s possible to generate environments programmatically with different properties, resulting in different agent behaviours. I think the paper should scrap the ‘social influence’ angle and instead frame it as ‘increasing policy diversity’. But is this also true for policy diversity?<BRK>This paper proposes a new way to incentivize diverse policy learning in RL agents: the key idea is that each agent receives an implicit negative reward (in the form of an early episode termination signal) from previous agents when an episode begins to resemble prior agents too much (as measured by the total variational distance measure between the two policy outputs). The results in Fig 2 are difficult to assess for diversity, and this is also true for the video in the authors  comment. Given that this notion of diversity is itself being claimed as a contribution, I would expect to see comparisons against prior methods, such as in DIAYN. The experimental setting is also not fully clear to me: throughout experiments, are the diversity methods being evaluated for the average performance over all the policies learned in sequence to be different from prior policies?<BRK>The paper presents a new algorithm for maximizing the diversity of different policies learned for a given task. The diversity is quantified using a metric, where in this case the total variation is used. A policy is different from a set of other policy if its minimum distance to all the other policies is high. Why does the total variation needs to be different at *every* time step?
Accept (Spotlight). rating score: 8. rating score: 8. rating score: 6. <BRK>The experimental results are convincing. However, it is worth noting that the reviewer is not an expert in the field, and it is hard for him to compare the proposed approach with previous work. Even though K matrices are aimed at structured matrices, it would be curious either to empirically compare K matrices to linear transformations in fully connected networks (i.e.dense matrices) or to provide some theoretical analysis. The paper is generally easy to follow.<BRK>This paper introduces a structured drop in replacement for linear layers in a neural network, referred to as Kaleidoscope matrices. The matrices are constrained so that an actual permutation matrix is always sampled, and the permutation is (had to be?) There seems to be some blurring between the meaning of structure (used to motivate K matrices in the introduction) and sparsity (used to analyze K matrices).<BRK>The authors propose learnable "kaleidoscope matrices" (K matrices) in place of manually engineered structured and sparse matrices. The balance of the paper empirically tests the claims of learnable structure and efficiency.
Reject. rating score: 1. rating score: 6. <BRK>This paper proposes Deep Learning Alternating Minimization (DLAM) algorithm. First, deep learning optimization problems are formulated as multi convex Problem 2, by introducing additional variables, constraints and relaxations. Second, alternating update is then used to solve Problem 2, the analysis shows that weights of update converge to a critical point with O(1/k) rate. The update is quite similar with dlADMM, with the same convergence result. It is not convergence to global optima. Update Thanks for the rebuttal.<BRK>The motivation for this paper is as follows: Why SGD:1. As the authors suggest: DLAM performed competitively for the Fashion MNIST dataset. Convergence proof has been done by others, but the assumptions of their proofs cannot be applied to problems involving deep neural networks, which are highly nonsmooth and nonconvex2. (Not fully proved theconvergence.Most of the work is based on some assumptions)Main Assumption: Activation functions are quasilinear functions. 2.Transform the Neural network optimization problem into inequality constrainedproblem (new point of view)3. The regularization like dropout, batch norm can not be added into this algorithm.
Accept (Spotlight). rating score: 8. rating score: 8. rating score: 6. <BRK>The paper gives a big picture view on training objectives used to obtain static and contextualized word embeddings. "There is nothing as practical as a good theory", and the authors confirm this statement: their theory suggests them to modify the training objective of the masked language modeling in a certain way and this modification proves to benefit the embeddings in general when evaluated on standard tasks. A minor comment is that the mutual information I(., .) being a function of two variables suddenly became a function of a single variable in Eq.(1) and in the text which precedes it.<BRK>At last,  after unifying multiple models under one common learning objective, the authors propose to mix two different losses which is strange (the effect of the second term is slightly studied in the experimental section) without allowing us to understand why it is important to have this second loss function and why the first one is not sufficient enough. They show that their approach works quite well. I have a very mitigated opinion on the paper. But the way it is presented and explained lacks of clarity: for instance in Section 2, some notations are not well defined (e.g what is f?)<BRK>This paper first gives a concise yet precise summary of maximizing one of variational lower bounds of mutual information, InfoNCE, then it provides an alternative view to explain case by case why word embedding Skip gram, BERT, XLNet work in practice can be viewed by InfoNCE framework, thus we have a good understand for these methods. Experiments show that it is better then BERT and BERT NCE. Do you observe this in your experiments? The paper is well written and easy to follow.
Reject. rating score: 3. rating score: 6. rating score: 6. <BRK>They have shown that pixel space adversarial attack detection and defense techniques are ineffective in guarding against feature space attacks. It was not clear to me how this is being controlled in the current method.<BRK>This paper presents an adversarial attack method, which conducts perturbations in the feature spaces, instead of the raw image space. It is a bit unclear on the intuition of the constructions of Eq.(5) and (6). Therefore, the proposed approach is able to construct adversarial examples for specific images.<BRK>This paper presents an interesting new adversarial attack technique that attempts to perturb abstract features learned by the target neural net. Its main contribution is the joining of ideas developed in the style transfer literature with those from the adversarial literature. The attack that they propose targets the feature space, but no feature space detection methods are tested.
Accept (Poster). rating score: 6. rating score: 6. <BRK>This paper proposes a type of conditional Information Bottleneck (IB) that addresses the following problem: given that some features may be expensive to obtain for use in prediction, when should they be obtained such that the overall benefit outweighs the cost? A variant of the IB is proposed to model this question. However, optimization is intractable. The main application here is reinforcement learning, where an agent could compute some plan or communicate with other agents at a cost, and the goal is to solve the task more efficiently while making use of this additional information. I am not an expert on the topic, but I find that this paper is well written and tackles a basic question with effective methods that work well in practice. "Active feature value acquisition." after ?"to dynamically adjusts"  > "to dynamically adjust"  "The agent always access the"  > "The agent always accesses the"  "Tables 3a, 3b compares": no such tables in the paper  "each method acsess the"  > "each method accesses the"<BRK>At inference time, the channel capacity (d_cap, based on the standard input) can be used to decide whether to access the privileged information or not. The approach tackles a narrow subset of problems compared to the standard information bottleneck. Can the authors clarify this? The problem is well grounded in the experimental settings the authors provide results for. Furthermore, for the goal driven navigation set of experiments can the authors report comparisons in terms of sample efficiency as well   how do success rates and average task returns vary with time steps of training? WeaknessesThat being said, the paper does have some flaws / clarity issues that make several associated details confusing when judged in context of prior work. The authors should change the lines to reflect the same.
Accept (Spotlight). rating score: 8. rating score: 6. rating score: 6. <BRK>The paper introduces a novel way of learning Hamiltonian dynamics with a generative network. The Hamiltonian generative network (HGN) learns the dynamics directly from data by embedding observations in a latent space, which is then transformed into a phase space describing the system s initial (abstract) position and momentum. Using a second network, the Hamiltonian network, the position and momentum are reduced to a scalar, interpreted as the Hamiltonian of the system, which can then be used to do rollouts in the phase space using techniques known from, e.g., Hamiltonian Monte Carlo sampling.<BRK>Summary: The authors present a method for learning Hamiltonian functions that govern a dynamical directly from observational data. The basic approach uses three networks: 1) an inference network (I m not clear why this is not just called an encoder), that maps past observations to a latent p,q space in a VAE like fashion; 2) a Hamiltonian network that governs the time evolution of the system over this latent state; and 3) a decoder network that outputs the observation from the latent state. The overall notion of learning a Hamiltonian network directly is a great one, though really this is due to the Hamiltonian Neural Networks paper of Greydanus et al., 2019. This portion seemed a little bit underdeveloped in the paper, to be honest, but overall the idea of parameterizing a normalizing flow with a Hamiltonian dynamical system seems like a good one (e.g., allowing for easier large timestep inference).<BRK>The paper proposes two ideas: 1) Hamiltonian Generative Networks (HGN) and 2) Neural Hamiltonian Flow (NHF). Hamiltonian Generative Networks are generative models of high dimensional timeseries which use hamiltonian differential equations to evolve latent variables (~position and ~momentum vectors) through time (using any differentiable integration scheme). The initial latent variables are inferred using a VAE style inference network that takes a sequence of images as the input. Neural Hamiltonian Flow notes that hamiltonian dynamics are invertible and volume preserving, which is the properties you need for neural flow models. Where do they think this model can be useful? Similarly for the NHF, if I only read this paper I have no idea whether it s better than any of the other flow based models. I think the paper would benefit from being split into two papers, each thoroughly examining one idea.
Reject. rating score: 1. rating score: 3. rating score: 6. <BRK>The proposed algorithm combines a learned model free value estimate with MCTS planning. An ensemble of neural networks is used to model posterior uncertainty in the value estimate and drive efficient exploration. The proposed structure of the paper is quite nice, there is mostly a linear and logical progression of complexity in the experiments. might be obtained by choosing from (the) ensemble... It would be very helpful to clarify that the agent is given access to a simulator... so that this is not exactly the typical RL setting of sequential decision making. It s an important problem, and the core building blocks of combining model, value and uncertainty for better exploration is interesting. However, I just think the actual paper is not clear enough on the details. For that reason I have to say that I think it s a "reject" in its current form.<BRK>The authors propose to combine planning methods like MCTS with an ensemble of value functions to a) estimate the value of leaf nodes of the search tree and b) use the ensemble estimate of uncertainty to guide exploration during MCTS search. The MCTS rollouts are also used as optimization targets for the value function. Regarding the presentation of the paper:Overall, the paper seems quite rushed. Furthermore, the algorithm itself is not described in sufficient detail:  How does the  soft penalization  work? However, I have several worries that might need addressing:  It seems to me that the method relies on access to the _true_ transition and reward function and not on a learned model. This also makes the comparison against any pure model free method like PPO much less meaningful. The one main contribution seems to be a new way of how \phi_a(x) is defined. Consequently, I will raise my score to a "weak reject" to express that I think this is promising work.<BRK>The paper proposes an approach blending model based, model free methods and utilizing risk sensitivity information in ensembles as part of the value estimation and exploration process. There is a lot of work currently trying to marry the model free with model based approaches for integrated planning and learning as the authors have mentioned in the related work section of the paper and also called out similar methods and techniques. The authors have provided evidence via experiments in three environments and shown good results of using this blended approach. Code is also provided for others to further carry out explorations in this research area.
Reject. rating score: 3. rating score: 6. rating score: 8. <BRK>This paper introduces a novel way of parametrizing embedding layers based on the Tensor Train (TT)decomposition, which allows compressing the model significantly at the cost of a negligible drop or even a slight gain in performance. And this paper focuses on the input embedding layers. For the experiments, the paper just compared methods using TT layer and normal embedding layer. There are many other methods that has been proposed to compress the embedding layers, it will be good to compare with one or two other methods, such as WEST or compression based on projection layers.<BRK>This paper proposes to use TensorTrain representation to transform discrete tokens/symbols to its vector representation. The paper assumes that those embedding matrices can be compressed by assuming that the low rank property of embedding matrices. I think this is a valid assumption in many cases, and the paper shows the performance degradation according to this assumption is relatively small compared to the gain, a dramatically reduced size of parameters in the embedding stage, is substantial. I think the paper is well written and proposes a new direction to find a memory efficient representation of symbols. I am not sure the current initialization techniques, nor the training method in the paper are the right way to train a tensor train "embedding" but I expect that the authors would perform the follow up work on those topics.<BRK>This paper proposes a low rank tensor decomposition model (Tensor Train TT [Oseledets et al, 2011]) to parameterize the embedding matrix in Natural Language Processing (NLP). However, I think this is the first time that the concept has been applied to learning an embedding matrix, which is an important problem in the field. I think the paper is of limited novelty but includes interesting experimental results that helps to better understand the potential and limitations of tensor decompositions in deep learning architectures. In fact, in this experiment, the authors chose all intermediate ranks set at the same value R with the first/last ranks set to 1 and R for TT and TR, respectively, which is not a fair comparison. I think the size of the first/last 2D cores should be added. The pseudocode for the mapping one index to multiple indices is trivial and could be avoided. Thanks for taking the review report into account.
Reject. rating score: 1. rating score: 3. rating score: 8. <BRK>The authors propose to incorporate active learning into the graph neural network training and claim some guarantee on the proposed method. I have some concerns about the correctness of the proof. My worry is naively applying Hoeffding as is done in the proof only gives a bound on a fixed model, but in the theorem A_t is not fixed. Other than that I also feel the assumptions on theorem 1 are way too strong.<BRK>This paper introduces active learning for graphs using graph neural networksThe bound is not very meaningful as it requires unrealistic assumptions and is loose. Figure 2 shows that even random selection performs quite well compared to this elaborate method. This Area if research and the data sets don’t seem to have many actual real applications in the world with much impact.<BRK>The authors propose an interesting method to actively select samples using the embeddings learned from GNNs. Theoretical analysis is provided to support the effectiveness and experimental results shows that this method can outperform many other active learning methods. Although these embeddings have encoded graph structure to some extent, I would suggest explicitly incorporating the graph structure in clustering or at least comparing to a baseline on that. The proposed method conducts embedding learning and clustering in two consecutive but separate steps. 2.It would be better to provide more details about network settings (some hyperparams have already been given in the paper), and more analysis would be helpful. For example, how the number of clusters affects the performance?
Reject. rating score: 3. rating score: 3. rating score: 3. <BRK>The paper studies the problem of advantage estimation for actor critic RL algorithms. UPDATE AFTER (NO) AUTHOR RESPONSE  The authors did not post a response, so I will maintain my vote to "weak reject" this paper. A second contribution is connecting the ideas of risk sensitive and risk seeking control with ideas from psychology (regulatory fit theory). Many of these prior works include a temperature parameter for trading off risk seeking vs risk aversion (e.g., \beta in Eq 11 of [Mihatsch 2002]), which is arguably a more transparent (to the user) and easier to analyze (for the RL researcher) than the order statistics used in this paper.<BRK>This paper focuses on risk aware reinforcement learning, where an agent could be encouraged to take more risk (high reward, high variance states) or avoid risk (low variance states). Experiments on several environments show good performance of the proposed algorithm. The paper is written clearly and the approach is straightforward. It seems to work pretty well in practice, but I wonder how it compares to other risk sensitive RL algorithms (e.g.those cited in the related work section). Overall, this paper presents a simple heuristic to steer the policy towards risk seeking / risk avoiding directions, but could benefit from either more theoretical analysis or more empirical comparison with other methods.<BRK>This paper presents a modification to policy gradient methods that are computed from advantage function estimates. GAE (Schulman, 2016) proposes to take an exponentially weighted average of these estimates to compute the policy gradients. This hyper parameter is justified on the optimistic case (max advantage), as a way to prevent overtly optimistic estimates. But there are a couple concerns about the validity of the method. One concern is that the regulatory ratio is only justified for the max case, but not for the min or max abs case. I understand how it might serve as an inspiration for algorithm design, but it is a bit distracting from the technical contribution of the paper. Did you try a Rainbow style training (changing the regulatory focus periodically over training)? In some sentences, it looks as if insure was written in place of ensure.
Reject. rating score: 3. rating score: 3. rating score: 8. <BRK>Overall, I think the paper provides a solid contribution towards combining MCMC and VI by proposing a way to optimize the MCMC part. First, for Equation (4), the explanation behind "replacing" H(P_{T}) with ELBO w.r.t.P_{0} is confusing. it is stated that the constraint is needed for preventing P_{T} to be closer to P_{0}. Note that even if the expected log likelihood of the distribution is high, it does not necessarily mean that the distribution is more similar. Minor comments:  I was unable to understand why the algorithm is named "ergodic" inference. I hope the authors could better illustrate on this point. It almost seems that the brown plot does not converge in Fig 5 (a).<BRK>The paper presents a new hybrid method to unify MCMC and VI. The presented techniques are tested on synthetic datasets and with the experiments of a VAE on MNIST. The notations of \pi and \pi^* are very confusing. I guess \pi represents the marginal distribution of the last state of the MCMC chain, while \pi^* is the target distribution. How to prevent P_T from collapsing to a delta function? Also intuitively, there should be a weight balancing the two terms of the loss; why a weight of 1 is used? In equation 8, the function g_{phi} is not continuous because of the indicator function 1(). How do you back propagate through that function? It might not be suitable to set h as the entropy of the prior, as in practice prior and posterior might be different dramatically.<BRK>The presented method is very useful to deep learning in the era of uncertainty modelling, which requires the use of Bayesian inference arguments. It s a valuable improvement upon variational inference, it s novel, and the derivations are correct. The presentation is elaborate and covers all expected aspects. The literature review is up to date. The experimental results are diverse enough and convincing. The authors have considered both proof of concept experiments and deep learning architectures.
Reject. rating score: 1. rating score: 1. rating score: 3. <BRK>Therefore, the main contribution is incorporation of the mutual information into SAC. And on this front, the paper is quite weak. The continuous control environments are more interesting, but the learning curves look very unreliable. It would be paramount to make sure that the baselines are fairly represented in the evaluations. > If better sample efficiency and better generalization are claimed, then a more thorough evaluation is required.<BRK>Finally, in section 4.4 we see the first somewhat convincing experimental results. This is not sufficiently motivated. All in all there is something truly interesting in this work, but in the present state I am unable to recommend acceptance, and the amount of work required along with questions raised lead me to be fairly confident in this assessment. Moving to the connections with past work, this paper seriously abuses notation in a way that actually hinders comprehension. One of these is a joint probability for state and action, and one is an action probability conditional on a state. In the block just before Algorithm 1, many of these symbols are never defined. It is clean, but I needed more explanation to really drive the intuition home.<BRK>The paper presents a reinforcement learning method that regularizes the objective using the mutual information term. For the current form of the paper, I give "weak reject" due to the weak support of the experimental results and the unclear motivation of the method. A comment on the paper structure is that the connection to the "capacity limited" objective should be described more explicitly in Section 2.1. I do not fully understand the intuition of penalizing the entropy of \pi(a) in the context of RL. In addition, evaluation with 10 trials are preferable. If there is no previous work, please explain the details of estimating \pi(a) and how \pi(a) is approximated from samples.
Reject. rating score: 3. rating score: 6. rating score: 8. <BRK>This paper investigates the asymptotic spectral density of a random feature model F(Wx + B). In practice, we may consider an input with additional constant feature, X <  [X,1], to deal with both models in a unified manner. Pros:  This paper investigates an interesting problem and it successfully extends the existing work.<BRK>This paper analyzed the asymptotic training error of a simple regression model trained on the random features for a noisy autoencoding task and proved that a mixture of nonlinearities can outperform the best single nonlinearity on such tasks. 6.“..., they may not be large in comparison to the number of constraints they are designed asked satisfy.” should be “...  they are designed to satisfy”.<BRK>In this work, the authors focus on the high dimensional regime in which both the dataset size and the number of features tend to infinity. They analyze the performance of a simple regression model trained on the random features and revealed several interesting and important observations.
Accept (Spotlight). rating score: 8. rating score: 6. rating score: 6. <BRK>I think they have correctly identified a setting that is of interest to a broad audience and where their approach is more attractive than the alternatives. I now recommend accepting this paper. The authors develop a generic method for estimating expectations under discrete distributions based on sampling without replacement and apply it to the task of estimating gradients for training models with discrete latent variables. The proposed estimator is unbiased and the authors prove it has lower variance than the naive Monte Carlo estimator. The method derivation is generally well written and easy to follow. However, the account of the sum and sample estimator is somewhat misleading. This is my main complaint about the paper. On the subject of applicability, the experiments only test the proposed method in very low k settings, where the exponential algorithm for computing the leave one out ratio can be applied. The plots could use some improvement.<BRK>Summary: This paper introduces an gradient estimator for loss functions that are expectations over discrete random variables. The basic idea is that an estimator over a discrete distribution can be Rao Blackwellized by conditioning on the event that the discrete realization was produced by being the first sample drawn from an unordered set of samples drawn with replacement. Clarity: The paper is very easy to understand and well written. Additionally, if overfitting is an issue, I recommend the authors consider the use of regularizers, like weight decay. I see no reason that this would violate the spirit of the paper, and might make their results more compelling. Yet, from the experiments it is not obvious that this estimator adds much to the existing literature. All of these estimators, which require multiple evaluations of the gradient, are generally less common in practice.<BRK>Summary: In this paper, an unbiased estimator for expectations over discrete random variables is developed based on a sampling without replacement strategy. The proposed estimator is shown to be a Rao Blackwellization of three existing unbiased estimators with guaranteed reduction in estimation variance. What is the input/output of the estimator? Therefore, the overall degree of novelty in theory is still relatively low. I find my concerns on motivation and presentation properly addressed in the feedback and the revised paper as well. Concerning the strength of theory, however, I am still not convinced that the current analysis is strong enough to thoroughly justify the benefit of the proposed estimator. All in all, the paper is substantially improved in presentation and the proposed gradient estimator seems to be a novel and more attractive alternative to the existing ones in a number of popular DL/RL applications.
Reject. rating score: 1. rating score: 1. rating score: 3. <BRK><Strengths> + This paper addresses an interesting and practically important problem: detection of piano fingering from videos and MIDI files. + The paper reads very well. <Conclusion>Although this work is practically promising, my initial decision is ‘reject’ mainly due to lack of technical novelty and limited experiments.<BRK>In this paper, the authors proposed an automatic piano fingering algorithm, that accepts YouTube videos and corresponding MIDI files and outputs fingering prediction for each note. The overall algorithm is mainly described. However, I would like to reject this paper.<BRK>Furthermore, the experimental setting is somewhat limited, and it is not clear whether results are statistically significant. The paper is a nice piece of works which clearly articulates the objective and the subsequent discussion.
Reject. rating score: 1. rating score: 3. rating score: 3. <BRK>  This paper proposes a comparative study of out of distribution (OOD) methods for semantic segmentation. Furthermore, some other important aspects are not explained. Authors also mention that GANs and AEs are excluded to limit the scope of the paper. Nevertheless, this is not detailed in the experimental section.<BRK>The paper evaluates a variety of existing pixel wise out of distribution detection methods in the task of semantic segmentation of road scenes. Strengths:  The paper is well written with high quality visuals and plots  The paper studies an important problemWeaknesses:  The contribution seems to be rather incremental (evaluating existing methods on 2 dataset) and some related work might be missing  Although the analysis is well executed, it is not clear what the community learns from the paperAlthough I enjoyed reading the paper, I d lean towards rejection of the paper.<BRK># SummaryThis paper puts forward a study over out of distribution (OOD) detection for semantic segmentation. This dataset is not further mentioned and evaluated in the rest of the paper.
Reject. rating score: 1. rating score: 3. rating score: 3. <BRK>Summary: This paper proposes to use either relaxed mixture distributions or relaxed mixtures of matrix Gaussians as the approximate posterior for Bayesian neural networks. I m inclined to think that this paper has a future, and definitely encourage the authors to resubmit   taking into account the reviews here. Tldr: I vote to reject this paper for several reasons. The interpretation of ensembling as variational inference is both flawed and well known. Mixture distributions have been previously proposed for variational inference. I will increase my score if these issues are resolved. Alternative Bayesian methods can be used on the full network.<BRK>The authors interpret Deep Ensemble as a special type of variational inference. Based on Bayes by Backprop which uses a Gaussian approximation to the posterior, the authors propose to use a mixture of Gaussians. The proposed methods have been tested on a regression task and Bayesian neural networks. It is not surprising that Deep ensemble is equivalent to variational inference with learning the mean only. However, it has been demonstrated in the literature that BBB is able to perform fairly well on this kind of regression. I believe a Gaussian approximation is able to work well.<BRK>SummaryIn this works, the authors propose to use a concrete mixture of Gaussians as a variational distribution. However, there are some concerns. The deep ensemble method considered in this paper is just one of the existing ensemble approaches.
Reject. rating score: 3. rating score: 6. rating score: 6. <BRK>This paper proposes a novel way to denoise the policy gradient by filtering the samples to add by a criterion "variance explained". The main reason I tend to reject this paper is that the motivation of their proposed algorithm is very unclear, lack of theoretical justification and the empirical justification is restricted on PPO   one policy gradient method. The interpretation is a superficial explanation of what Eq 7 means but does not explain why I should throw out some of my samples, why high and low score means samples are helpful for learning and score in between does not? 2) This paper argues the filter condition improves PG algorithms by denoising the policy gradient. 3) The method of denoising the policy gradient is expected to help policy gradient methods in general.<BRK>Given the mean per timestep reward as a baseline, V^{ex} measures the proportion of the variance in reward that is captured by the value function. Intuitively, I can believe that learning will be more stable under the condition that no single transition leads to too great a divergence between the predicted reward and true reward. However, I do not understand the authors  assertion that this filtering procedure removes noisy samples (how can we characterize these samples as noise?). The paper compares learning accuracy over time of PPO with and without the variance explained filtering. I commend the authors on their discussion of non positive Atari results in the appendix and I agree that it contributes significantly to the paper. I think that the approach is intriguing but, in the absence of any theoretical justification for the approach, I m not sure that the empirical results are sufficiently convincing for ICLR.<BRK>This paper proposes a simple modification to policy gradient methods that relies on the variance explained to filter samples. The paper contains experiments showing empirical gains to the method, as well as some evidence that filtering rather than simply predicting more quantities is making a difference. This method is novel, afaict, and is based on interesting statistical hypotheses. There are some gains compared to a PPO baseline, but the gains are somewhat incremental and may only apply to PPO, as other PG methods aren t tested. (in terms of reward density/variance, exploration difficulty, etc.) The conclusion in particular claims much more than what is in the paper:>> "applicable to any policy gradient method", technically yes, but it remains untested>> "SAUNA removes noise", also remains to be seen. It s not obvious why we should be estimating V^ex at all, and I think this deserves more analysis.
Reject. rating score: 1. rating score: 1. rating score: 3. <BRK>The existing results are not sufficient to validate the effectiveness of the method to prevent privacy leakage. 2.Some important related works are missing. The difference between the proposed method and previous works with the same purpose should be made more clearly. So, what s the strategy to combine these two representations (concat?)? 6.The threat model in this paper needs to be made more clearly.<BRK>This paper investigates the use of variational auto encoders (VAEs) and disentanglement to create high quality data representations that hide sensitive attributes.<BRK>This paper brings to our attention the problem of privacy preserving representation learning. The authors propose to use VAE with additional regularizer terms to learn hidden representations that would encode both private and non private part as independent as possible.
Reject. rating score: 1. rating score: 6. rating score: 6. <BRK>The proposed NMLN  is architecturally identical to MLNs with the difference being the addition of the potential function. With potential functions sampled from the anonymized and the true value fragments. The intuition in this work is that by selecting the maximum entropy distribution while also minimizing it by selecting the most informative statistical information for it, we will derive an accurate probability distribution given the possible worlds. It is not exactly clear to me why the anonymization of fragments is necessary, but the authors suggest this places a greater focus on the graph structure and minimizes the model acting differently with different constants. This is possibly addressed with the molecule experiments, but more datasets would have helped in confirming the breadth of domains as claimed by the paper. It is also difficult to measure the success of their model with generative modelling as no baseline was present for the molecule experiments. I still view this paper as a positive contribution to MLN research, with a technique that is successful among the few experiments tested. Given the preliminary investigations, this is a reject from my side.<BRK>This paper presents Neural Markov Logic Networks  (NMLN), which is a generalization of Markov Logic Networks (MLN). Pros:1. no need to specify FOL rules and can potentially discover subtle relations not evident to us. 2. can be used for generation since the learned rules might be more fine grained than what we can specify. 3. it s interesting that on nations knowledge base completion problem even without constant embeddings it works fine, which shows the power of just using relational structure. Overall this is an interesting work. I think it is a natural generalization of Markov Logic Networks and works on two small problems. I am inclined to recommend this paper to the community. updates after reading rebuttal Thanks for the clarification. I don t have further questions.<BRK>I think their strong point is at the same time their weak point. ", they do not rely on explicitly specified first order logic rules." I can see how its might be useful but would be very happy to see more motivation for this. Thats a different paper but I missed the motivation for that here too. I like the honesty of the authors for saying it doesn t scale to larger problems. Regardless, I think this paper is good to push the field in that direction. This is not enough to reject the paper for me because I do believe this is pushing the field forward in a good direction. In the contribution it says "(i) we introduce a new statistical relational model, whichovercomes actual limitations of both classical and recent related models such as " I would have really liked it to have been spelt out which limitations, very specifically and concisely the paper overcomes in that section.
Reject. rating score: 1. rating score: 3. rating score: 3. <BRK>They propose a formulation that decomposes environment hypotheses into sets of pre conditions, required actions, and post conditions. Overall, I really wanted to like this paper. Up to section 3.3.1 (and   really   until I read the appendix...), the writing sort of led me to assume that (1) the "(pre condition, action sequence)  > post condition" split was a fairly standard manner of compose a hypothesis, and that (2) the templates were mostly symbolic. However after reading the appendix, I found the imposed structure to be fairly arbitrary, and the usage of natural language overkill and not necessarily well justified. Ideally, I would like to see some comparisons between this type of hypothesis and other decompositions used in previous literature, since it seems like the method exploits this particular structure quite heavily and I don t quite understand how it generalises to other tasks. 6.The final accuracy of all the experiments are shown using the max of top 5, however appendix D shows quite a significant variance for the methods. What happens if the methods are trained on more seeds? To improve this paper, I would like to see:  Better clarity on how the hypothesis setup stands to previous literature. The difference in performance on each environment with different pre training reward function (only one in show in the paper right now)  At least one more environments with significantly different dynamics, or an explanation of how the existing settings differ in qualitative terms. A baseline employing some form of memory (such as heavy usage of frame stacking or recurrency), to attempt at figuring out whether it s really not reasonable to learn the whole problem simply using RL, with ablation of pre training (which I suspect might make a significant difference). At this point, I cannot recommend the article for acceptance, but I d be willing to change my rating if the authors were to address some of the above points.<BRK>The paper looks into the problem of training agents that can interact with their environments to verify hypotheses about it. It first formulates the problem as a MDP, where the agent takes actions to explore the environment and has two special actions (Answer_True, and Answer_False) to indicate that the agent has made a prediction about the validity of the hypothesis. The authors carry out such experiments and conclude that this doesn t work. Overall, the paper is well written and the literature review section is quite excellent. However, I have reservations against the formulations that the authors used. I would appreciate it if the authors present their argument in the rebuttal. First, in the plain formulation of MDP, a policy produces an action according to the current state only. So, if the agent is trained on some hypotheses, the agent will essentially learn to identify for each h which state s that can be used to to verify h (either prove or disprove it). In the reward functions, why did the authors use C instead of just using 1.<BRK>It does this by first pretraining the agent to perform interventions in the environment which change the states of the objects of interest, and then finetuning the agent to actually make a decision about whether the given hypothesis is correct. The paper shows that agents trained using this procedure are able to not only verify the types of hypotheses seen during pre training, but also learn to verify more complex hypotheses. In contrast, an agent which is trained directly on the hypothesis verification task is unable to learn to do it. Overall, I enjoyed reading this paper and thought that it provided an interesting take on the question of how to train agents that can appropriately gather information about their environments. However, (1) the paper lacks any discussion of related work in terms of causal reasoning and partial observability, and (2) the experiments and analysis seem weak. Yet, there is no mention of POMDPs or discussion of the literature on partial observability in the paper. Second, I felt that the setup was overly complex in places making it difficult to draw conclusions, that there were a lack of comparisons, and that the analysis was not as in depth as it could have been. I also do not think it is sufficient to add an appendix with the results across multiple seeds: these results should be in the main paper. I find the plots that have both variance and max seed very hard to interpret in some cases the mean is so much lower than the max seed that the variance region doesn t overlap at all. It seems like including the pseudo natural language adds unnecessary complexity and makes it difficult to distentangle what about the problem is hard (Understanding the hypothesis?Choosing the right interventions? For example, here are a few alternate ways of rewarding the agent that seem intuitively like they could also work:Reward the agent for changing the state of any of the objects in the environmentReward the agent for changing the state of any object referenced in the hypothesisReward the agent for observing a state of the world it has not seen before (i.e.count based exploration)In other words, how important is the fact that the reward is given based on the pre and postconditions? A few minor comments:  Please state in the main text which RL algorithm you use.
Reject. rating score: 1. rating score: 3. rating score: 3. <BRK>This paper examines the problem of warm starting the training of neural networks. The paper is also well written. Why is it surprising that the magnitude of the gradients of the "new" data is higher than at a random initialization? There is no clear hypothesis towards answering the problems proposed in the paper. There is also no analysis, which places the burden on the numerical experiments to demonstrate something interesting, and the experiments seem sparse and small scale.<BRK>This paper conducted an empirical study on why training with warm starting has worse generalization ability than learning from scratch. The paper is interesting, however, it has something unclear to me, as explained below. 1)	The scale and diversity of the study can be improved. were not included in the study. However, I kind of feel that the experimental setting has some fatal problems, which makes the experimental results not convincing.<BRK>Models trained from scratch on the whole dataset have better performance than "warm started" models, which are trained with weights initialized from training using part of the available data. We can also see that the choice of hyperparameters, necessary for the best performance, levels benefit in time from "warm starting." The core idea of the paper is the investigation of various possible causes of difficulty of "warm start" to reduce training time without damaging a generalization performance. Maybe it could be useful to show graphics of loss function.
Reject. rating score: 1. rating score: 1. rating score: 1. <BRK>Regarding clarity/polish:I am generally not super picky about these things, but there does have to be some standard. This paper looks very hastily put together, especially pages 7 and 8. People have mostly settled on using FID for this. > It is also difficult to know whether the generated distribution is close to the true distribution, and this is often observed by human eyes. Regardless, nobody really uses human evaluation anymore   so this is just not correct. The experiment uses a single run each of the baseline and DG GAN, when it s well known that GAN training runshave inter run variance larger than the difference in score reported in Fig 1 and 2.<BRK>I vote to reject the paper at this stage, mainly because of the following three points:1) The motivation is unclear and overall structure of the paper is confusing. It should be better motivated why one should use the duality gap as an upper bound for the "F distance". in the introduction.<BRK>This paper proposed to use the duality gap sup_f V(f, g*) – inf_g V(f*, g) as a metric for GAN training. 1) The duality gap is only an upper bound of the F distance. However, the converse is not necessarily true: even if the algorithm starts with the true distribution, the duality gap may not be zero. Thus the metric is not a proper metric. For the proposed metric, it is also impossible to solve sup_f V(f, g*) and inf_g V(f*, g) to reasonable accuracy.
Reject. rating score: 3. rating score: 3. rating score: 6. <BRK>That being said, the gains are not huge, which does make me think about the potential computational overhead. Additionally, the paper would be significantly improved by fleshing out the theoretical motivation for the Meta Critic approach in more detail. What are the underlying reasons why we would expect it to generically improve single task RL? I don t think that R3A1 is particularly clear and it is an important concern. As a result, I believe the source of the non stationarity in this case is the fluctuating parameters. My best guess is that this is the reason why the meta loss does not converge. However, I still totally agree that this is a major concern that is very much under addressed. I also agree that I found the comments about what the meta critic is doing unconvincing. The authors provided a few different kinds of explanations of what the model could potentially be doing, but this approach to the answer really highlights  how the theoretical benefits of this approach remain unclear. I think it should be possible to directly verify some of these theories with well designed experiments.<BRK>The approach is combined to DDPG, TD3 and SAC and is claimed to convey superior performance (or learning speed) over these state of the art actor critic algorithms. The second cue is that appart from the rllab tasks (ant and half cheetah), it is not clear that the meta critic approach brings some gain in the end of training. by using": missong wordp6: asmyptoticp9: we removing So I m wondering what exactly MC is doing and I would like to see a more detailled analysis. Isn t this more or less what the meta loss does, when looking at Fig.6?To me, unless the authors give a clear answer to the points above, the paper should be rejected as it does not provide clear enough evidence and justification in favor of the proposed meta learning approach. Any idea why? It seems that in many papers experiments are stopped before performance collapses, and a strong study about this phenomenon is missing as the authors don t want to show this. is not a sentence (no main verb).<BRK>In this paper, the authors propose a novel way, namely, meta critic, which utilizes meta learning to learn an additional loss for the policy to accelerate the learning process of the agent. Overall the proposed method is novel and the research direction is a very interesting one to explore. Eqn (3) and Eqn (4) explains the key idea of the proposed method. Eqn (3) describes the meta learning problem as a bi level optimization problem where the agent is updated with the main loss L^main with data d_train, in addition, it s updated with L^aux where the loss is learned and parameterized by \omega. Thus the meta loss is whether the L^aux helps the learning process or not. Hope that the authors could address the following issues in the rebuttal:1) Investigate why DDPG with meta critic gets much more improvements than TD3/SAC;2) Show SAC (or TD3) could get better performance on harder task.
Reject. rating score: 1. rating score: 3. rating score: 3. <BRK>There is only a trajectory level supervision provided which specifies which skills are present with no specification of the order in which they appear. They empirically show that their model can achieve decent skill level classification scores on multiple environments provided that there is a large variety of demonstrations provided. I found the experimental setup unclear. The claims made throughout the paper are not supported empirically or theoretically. There is not enough evidence for me to assess the significance of the proposed method and know whether this is indeed useful in practice.<BRK>This paper introduces a weakly supervised learning approach for trajectory segmentation, which relies on coarse labelling about the occurrence of a skill in a demonstration to segment trajectories. However, I have concerns about it s feasibility when a large number of skills are present. Confusion matrices, or precision and recall metrics, are required to avoid this. Unfortunately, due to the lack of evidence that the proposed approach is able to learn effectively, I am inclined to reject this paper. Clarifying the bounds, and presenting stronger evidence (beyond accuracy) would make this paper stronger.<BRK>This paper tackles the problem of learning to label individual timesteps of sequential data, when given labels only for the sequence as a whole. The authors take an approach derived from the multiple instance learning (MIL) literature that involves pooling the per timestep predictions into a sequence level prediction and to learn the per timestep predictions without having explicit labels. In addition, although the superior pooling method (which already exists in the literature) does outperform the alternatives evaluated here, the results are somewhat underwhelming, at only ~35 60% validation accuracy. So I view the novelty of the approach to be fairly low.
Reject. rating score: 3. rating score: 3. rating score: 8. <BRK>This paper presents a method for evaluating latent variable generative models in terms of the rate distortion curve that compares the number of bits needed to encode the representation with how well you can reconstruct an input under some distortion measure. However, my main concern remains: if rate distortion in an individual model is a useful method for evaluating generative models, you should compare it empirically with other metrics that have been proposed for this purpose (e.g.precision recall). Additionally, while the theoretical novelty of getting the full R D curve from a single AIS run is very cool, I m skeptical of the practical utility as a metric for generative models due to the computational costs of AIS (4 7 hours for 50 images on MNIST). How would this change the resulting rate prior distortion curves? * Should clarify that q_k(z|x) correspond to points along R_p(D)* The results in Eqn 14 17 showing you can tractably estimate distortion and get an upper bound on rate using the AIS derived distributions are very cool!<BRK>The authors propose an annealed importance sampling (AIS) method to compute the rate distortion curve efficiently. In experiments, the authors compare the rate distortion curves for VAEs and GANs and discuss the properties of rate distortion curves. The method for computing the rate distortion curves of deep generative models is interesting and the rate prior distortion curve is promising as a performance measure. Although the findings in the experiments are interesting and insightful, they are still preliminary and further investigations are desirable.<BRK>Such generative models are composed of a prior over latent variables and a decoder that maps latent variables to data. The paper describes a method that estimates an upper bound on this rate distortion curve using annealed importance sampling. In general, such models are hard to evaluate and compare with each other. The method is well motivated and backed by theoretical results. What prior / posterior models were used in the experiments?
Accept (Poster). rating score: 8. rating score: 6. rating score: 3. <BRK>The paper is interested in robustness w.r.t.adversarial exemples. The authors note that:* features reflecting the global structure are more robust wrt adversarial perturbations, but generalize less;* features reflecting the local structure generalize well, but are less robust wrt adversarial perturbations. You might want to discuss the relationship between the proposed approach and the multiple instance setting (as if the image was a bunch of patches).<BRK>In this paper, the authors proposed a new approach to improve the robustness of CNNs against adversarial examples. The difficulty here is that existing adversarial training algorithms tend to bias CNNs to ignore local features and to capture only global features. The authors demonstrated that combining RBS with the existing adversarial training algorithms can lead to robust CNNs. I found the paper well written and the idea is easy to follow. Especially, the use of RBS seems to be an interesting idea.<BRK>The work suggests reshuffle images blocks of adversarial examples during adversarial training, in order to improve the generalization performance on benign and adversarial test samples. The assumption claims that robust models rely on global structural features, and non robust models rely on local features. Thus, the work tries to learn local robust features, by cutting and reshuffling the image blocks. Overall the idea is interesting and the paper is well written .
Accept (Poster). rating score: 6. rating score: 6. rating score: 6. <BRK>In this paper, the authors proposed a new algorithm   expected information maximization (EIM)   for computing the I projection of the data distribution to the model distribution, solely based on samples for general latent variable models, where the paper only focus on Gaussian mixtures models and experts.<BRK>This paper propose EIM an analog to EM but to perform the I projection (i.e.reverse KL) instead of the usual M projection for EM. The motivation is that the reverse KL is mode seeking in contrast to the forward KL which is mode covering. As the author point out, likelihood shouldn t be the right metric since you are now minimizing the reverse KL I would have liked the authors to spend some more time on the right way to evaluate and actually use that new metric.<BRK>The paper presents an algorithm to match two distributions with latent variables, named expected information maximization (EIM). Specifically, EIM is based on the I Projection, which basically is equivalent to minimizing the reverse KL divergence (i.e.min KL[p_model || p_data]); to handle latent variables, an upper bound is derived, which is the corresponding reverse KL divergence in the joint space. To minimize that joint reverse KL, a specific procedure is developed, leading to the presented EIM. Overall, the paper is in good shape wrt the logic and the writing. Other detailed comments are listed below. As Eq 4 is for matching two joint distributions, discussions/comparisons should be made to reveal the novelty of the presented EIM over existing methods such as [2], etc.
Reject. rating score: 3. rating score: 3. rating score: 3. <BRK>This paper shows (theorem 1) that data augmentation (DA) induces a reduction of rugsity on the loss function associated to the model. Here rugosity is defined as a measure of the curvature (2nd order) of the function. However, the two concepts seems to be different because the authors empirically show that directly reducing the rugosity of a network does not improve generalization (in contrast to DA). Thus, rugosity is not responsible for generalization, which is the interesting property that we care about. The Hessian based rugosity analysis of DA is correct, but it does not help to understand the generalization performance or any other useful property of DA.<BRK>The paper aims to explain the regularization and generalization effects of data augmentation commonly used in training Neural Networks. It suggests a novel measure of "rugosity" that measures a function s diversion from being locally linear and explores the connection between data augmentation and the decrease in rugosity. Cons:  The main contribution of the paper, in my view, is the suggestion of using rugosity as a explicit regularization for training Neural Networks. Nevertheless, all the results in the paper show a negative impact of this on the test accuracy which is contradicting to the proposition. This result has been discussed in section 5 but without much evidence to the explanations mentioned. The connection is very interesting but I believe further work is needed to explain those negative results on test accuracy.<BRK>In addition, there have been many recent studies on showing that gradient penalty / Lipschitz regularization are useful for achieving better generalization and adversarial robustness, see e.g.[a,b,c].The results in this paper on showing that regularizing rugosity does not improve accuracy seem to contradict with the conclusion of these prior studies. Comments:It is quite interesting to see that the rugosity measure proposed in the paper captures at least some aspects of the implicit regularization effect of data augmentation both in terms of theory (i.e.Theorem 1) and practical observations. My feeling is that rugosity is mostly a measure of the smoothness of the function parametrized by the neural network.
Reject. rating score: 3. rating score: 3. rating score: 6. <BRK>The paper is well written and easy to follow. The theoretical analysis is strong, I think. 2.There are two stronger assumptions in the paper, but they are never verified in the experiments. 3.There are no validation experiments for Thm. 1.<BRK>The paper could make a better job motivating the sparsity constraints, as it cites papers from 10 years ago which are not representative of state of the art. Does the algorithm lead to better generalization accuracy or worse? The proof contains an assumption that it is unclear that has been explained in the paper. In the supplementary material, after eq.25, the factor sigma is being introduced and the paper says “we assume that there exists a constant factor sigma making this inequality true”. Definition 1 does not really define hard thresholding.<BRK>  Update after author responseThanks for the clarifications and edits in the paper. Potential improvement  The current result in Theorem 1, which is building on a similar proof technique as the original SVRG paper, has the annoying property of requiring the knowledge of the condition number in setting the size of the inner loop iteration. 2) In Figure 1, which "minimum" is referred to and how is it found?
Reject. rating score: 6. rating score: 6. rating score: 6. <BRK>In model based reinforcement learning methods, in order to alleviate the compounding error induced in rollout based planning, the paper proposes a distribution matching method, which should be better than a regular supervised learning approach (i.e.minimizing mean square error). Experiments on continuous control domains are presented to show the algorithm’s advantages. The author compares the proposed method with several well known baselines. The proposed approach is interesting, however, there are some issues. The author should include discussions regarding model correction methods. 2.The experimental results do not reflect the key issue the author attempts to resolve. MBRL algorithms are very diverse, there are different ways of learning a model and of using a model. I read the author s response and take a look at other reviews. I reconsidered the contribution of the paper and found that my previous concerns may not be that important.<BRK>The paper proposes a type of model based RL that relies on matching the  distribution of (s,a,s ) tuples rather  than using supervised learning to learn an autoregressive model using supervised learning. Your framework corresponds to learning a matrix \Xi P, while standard autoregressive models would just learn P. Knowing one gives information about the other   you can go from \Xi P to by normalizing the rows and go from P to \Xi P by computing the stationary distributions. On the other hand, you seem to claim in Figure 1 and in the introduction that your framework is qualitatively different from standard autoregressive models, but the above analysis suggests you are simply approximating a slightly different object, without much of an argument about why this is preferable. In particular:  Proposition 1 says that we will learn a perfect model given infinite data. 3.In the experimental section, the Ant plot shows no learning for your method (MI). MI performs well when initialized and does not seem to learn anything (the curve is flat). I will re evaluate the paper if the above doubts are cleared up during the revision phase. The current version has severe writing problems, which make the writing unclear. is usually optimised with l2 based error"<BRK>This paper considers the model based reinforcement learning (MBRL)problem in which one of the main parts is to learn the transitionprobability matrix of the underlying MDP (called Transition Learning  TL). The motivation is that if the transition model can be learnt fromsome real world trajectories, the agent can improve by interacting withsimulated environment built from the learnt transition model; hence,the overall number of real world interactions is reduced. Given a policy $\pi(a|s)$, and a transition $T(s’|s,a)$, the occupancymeasure $\rho(s,a)$, defined in Eq (2) in the paper, can beinterpreted as the discounted distribution of state action pairs inthe rollouts. These trajectories are then used in policy optimizationto obtain a new policy. The theoretical part istechnically sound (there might be an error in the proof of Proposition1 but it is fixable). The experimental part is also sensible, althoughit would be good to include the performance comparisons for othertasks in MBBL. Main   comments/suggestions:  MBBL (Wang et al.2019) has many other tasks (18 in total, the  authors only include 4 in this work). The claim at the end of Proposition 1’s proof (in side the proof), in my assessment, is not  established. Nevertheless, the statement of Proposition 1 only claims the  sufficient condition so this is fixable. It looks to me that they are taken from WGAN paper but with no explanation. Two references “Syed et al.(2008a)” and “Syed et al.(2008b)” are actually the same.
Accept (Poster). rating score: 8. rating score: 6. rating score: 6. <BRK>  Update after rebuttalThanks for the clarifications; I have updated by score and recommend acceptance. I like this paper, I am leaning towards acceptance.<BRK>This paper considers the problem of the need for memory efficient optimizers given the increase in model complexity. + show tradeoff among training convergence and memory in the optimizer.<BRK>  Update after rebuttal  I did not have any major concerns about the paper in my initial review, only some suggestions for improving the presentation. The authors have addressed most of these issues in their revision. I would like to keep my score as it is. I am not very familiar with prior work in this literature, but the proposed approach seems simple and sound.
Reject. rating score: 1. rating score: 3. rating score: 6. <BRK>The paper presents a method to detect out of distribution or anomalous data points. They also argue for using “large” initializations in the first layers and sin(x) as the activation function for the final hidden layer. Additionally while Fourier networks have lower confidence, that does not necessarily mean they are more accurate estimates of uncertainty.<BRK>The proposed approach also requires more theoretical justification. I m not an expert in this area but I do find this paper interesting. The extrapolation problem of ReLU networks is an interesting point. I don t know previous works that point out this for out of distribution detection but it s worth figuring out if this observation has been made in the adversarial robustness community. I do have several concerns, summarized below:* On page 3 the fourier transform is motivated by that RBF networks "do not generalize as well as ReLU networks". I m willing to increase my score if the authors addressed my concerns.<BRK>I have read the reviews and the comments. The authors claimed that this leads to an out of distribution detection which is better than either of them. This paper proposes a method for the uncertainty estimates for Neural Network classifiers, specifically out of distribution detection. However, it would strengthen the paper if the authors compared against other approaches such as:   Predictive uncertainty estimation via prior networks, NeurIPS 2018.
Reject. rating score: 6. rating score: 6. rating score: 8. <BRK>In contrast to other efforts that examine information encoded in weights, this work emphasizes the effective information in the activations. This characterization is further related to information in the weights, and a theoretical justification is made for what this means with respect to properties of generalization and invariance in the network. This, however, is a matter of personal bias as I don t typically produce papers that are principally theoretical contributions in my own work. Overall, the content of the paper seems sound and the theoretical and empirical justifications seem well founded but I also can t claim to be an expert in this area.<BRK>The paper mentioned many interesting links. 2.Relating the Fisher information and the stability of SGD to I(w; D). About the rating:This is basically a good paper but I have a few concerns: 1. A large fraction of this paper are taken from Achille and Soatto (2018), Achille et al.(2019).2.In terms of impact, the paper is somehow incomplete   it only demonstrates that the Fisher information is important, but the insights didn t lead to any substantial improvement over the current deep learning framework. In my opinion, defining "information in the weights for the task D" by KL(Q||P) is inaccurate. The weights themselves are information, which form a representation or a lossy compression of the data (which is also an information). 4.Could you elaborate on the footnote 3?<BRK>The authors measure information in the weights of a DNN as the trade off between network accuracy and weight complexity. The main result is that models of low information generalize better and are invariance tolerant. The paper is very well written and concepts are theoretically well documented. In Definition 3.1 for the ‘Information in the Weights’, how does the complexity of the task vary with \beta?
Reject. rating score: 3. rating score: 3. rating score: 6. <BRK>The introduction is too short and lacks context. The authors extend this work and apply it to verify robustness of some VAE and BEGAN like models. First proofs given for generative models. Page 6 : attribute detector... described below  > in appendixAfter seeing the authors changes in the manuscript the paper does look better, but similarly to Reviewer 2 I still judge it difficult to understand.<BRK>Decision & supporting arguments:Conceptually I found the paper very appealing, and it tackles an important problem in generative modelling. However I have some concerns with respect to the paper in its current state:1) It is not clear to me why the attribute consistency score, a key component in the paper, is a good measure of consistency in generative models.<BRK>Still, I tend to accept this paper for its potential to become a good metric for generative models. However, the current version is quite difficult for me to understand, and I guess it is difficult for a broad range of audiences without background in program analysis.
Accept (Poster). rating score: 6. rating score: 6. rating score: 6. <BRK>This paper proposes a novelty detection method by utilizing latent variables in auto encoder. Based on this, this paper proposes two metrics to quantifying the novelty of the input. The metrics proposed in this paper are intuitive and interesting. The experiments shown in Table2 is very convincing, and it could be better to extend Table3 to include other datasets (STL,OTTO, etc. )<BRK>####################This paper considers deep autoencoders (AEs) for the unsupervised novelty/anomaly detection task and proposes to extend the standard AE anomaly score, given by the reconstruction error between the input and output in the original data space, to also utilize the reconstruction errors of the hidden activations in the AE network. One should expect SAP and NAP to improve over the standard AE since both methods include the original data space reconstruction errors as well. I think they are rather straightforward. (ic) I think the proposed method begs for an ablation study of subsequently adding the reconstruction errors of additional layers. (ii) The experiments indicate that a proper normalization of the hidden activation reconstruction errors is crucial for improving detection performance.<BRK>The paper proposes a new method for novelty detection that is based on measuring the reconstruction error in latent space between layer of the encoder. To aggregate the reconstruction error from all layers of the encoder, two methods are proposed SAP (simple aggregation along pathway) and NAP (normalized aggregation along pathway). SAP is simply the sum of the squared norm, while NAP performs decorrelation and normalization of the magnitude.
Reject. rating score: 1. rating score: 1. rating score: 1. <BRK>Overall, I think there are some interesting elements to this paper but it is currently not ready for publication. I recommend weak reject for three reasons: limited novelty, the experiments as they stand are not very convincing and the writing needs work. #### ExperimentsAlthough the experiments show some improvement, they are not that convincing. The improvements do not seem statistically significant for Swimmer and Walker, and the asymptotic performance is worse than PPO for Hopper. The first 5 pages are devoted to this and the proposed method is only introduced on page 6! A short discussion of the general idea behind model based RL and its improved sample efficiency, the issue of the policy exploiting model errors, and some references are sufficient. They main idea is very simple and is summarized in Equation 13.<BRK>The authors evaluate the proposed algorithm on simulated robotic locomotion environments in MuJoCo, and the results show that the proposed method has better same efficiency compared to baseline methods in some environments. However I do find a number of shortcomings that need to be addressed. Pro:1.The idea for this paper is really well presented. 2.I’m not convinced about some of the performance of some of the baseline methods presented in this paper. There are many other model based RL algorithms developed recently, and it would be important to compare to these methods. However, I am still not convinced about the novelty of the proposed idea and the magnitude of performance improvement.<BRK>This paper presents a new model free + model based algorithm, MBPGE, that trains a policy using a policy gradient algorithm on top of the learned models. Secondly, the preliminary section spans more than their approach, which hints that the original contribution is too incremental. All of the actual method is just combining section 2.1 and 2.2. The authors extend the paper to 9 pages when there was no need for it. Overall, I don’t think that the contributions of this paper are enough for publication. At this stage, there is not enough contribution in terms of novelty nor delta in performance.
Reject. rating score: 1. rating score: 6. <BRK>The authors claim (without citation) that training generative models in the presence of noisy data is challenging. In addition to this, I don t understand the relevance of some of the results in the paper.<BRK>Summary: This well written paper presents an effective way to remove outliers for deep generative models, provided examples are ranked along their centrality. While the authors provide an excellent introduction to this diversity and clearly differentiate their own flavor of ‘cluster curriculum’, I am wondering if they would not have been better off by describing the proposed approach in terms of outlier removal, especially has it has very little in common with the original idea of curriculum, which is a learning progression designed by the teacher. Nevertheless, the paper is very clearly written and reads well in the present form. The technical presentation is excellent. by taking some distance, which I assume is Euclidean?
Accept (Talk). rating score: 8. rating score: 8. rating score: 8. <BRK>In addition to the comparison between CATER and three action recognition datasets (Kinetics/UCF101/HMDB51) as presented in Table 3., it would be more interesting to see how video understanding models that are specifically designed for those video VQA datasets will perform on CATER. This paper proposed a new synthetic dataset (CATER) for video understanding. They further conduct a variety of experiments to benchmark state of the art video understanding models and show how those models more or less struggle on temporal reasoning. Overall this paper is well written and easy to follow.<BRK>The paper introduces CATER: a synthetically generated dataset for video understanding tasks. The construction of the dataset focuses on demonstrating that compositional action classification and long term temporal reasoning for action understanding and localization in videos are largely unsolved problems, and that frame aggregation based methods on real video data in prior work datasets, have found relative success not because the tasks are easy but because of dataset bias issues. A variety of models from recent work are evaluated on the three proposed tasks, demonstrating the validity of the above motivation for the construction of the dataset. The primitive action classification task is "solved" by nearly all methods and only serves for debugging purposes. The authors recognize that since the dataset is synthetically generated it is not necessarily predictive of how methods would perform with real world data, but still it can serve a useful and complementary role similar to the one CLEVR has served in image understanding.<BRK>This paper introduces a new synthetic video understanding dataset, borrowing many ideas from the visual question answering dataset CLEVR. Due to the inherent biases in available action recognition datasets, models that simply averages video frames do nearly as well as models that take temporal dependencies into account. In contrast, the authors show that with the proposed dataset, models without spatiotemporal reasoning largely fail. The paper should be accepted as it addresses a major shortcoming of all existing video understanding datasets. One drawback is of course the synthetic nature and limited domain of objects and actions. On the other hand, this makes the setup highly controllable and reliable. Improvements and Questions:Some relevant datasets are missing. In particular for Task 3 more frames seem to give dramatic improvement. I’m missing details on the resolution of the generated videos?
Reject. rating score: 3. rating score: 6. <BRK>This paper presents a technique for encoding the high level “style” of pieces of symbolic music. The music is represented as a variant of the MIDI format. The main strategy is to condition a Music Transformer architecture on this global “style embedding”. Additionally, the Music Transformer model is also conditioned on a combination of both “style” and “melody” embeddings to try and generate music “similar” to the conditioning melody but in the style of the performance embedding. Overall, I think the paper presents an interesting application and parts of it are well written, however I have concerns with the technical presentation in parts of the paper and some of the methodology. Firstly, I think the algorithmic novelty in the paper is fairly limited. The performance conditioning vector is generated by an additional encoding transformer, compared to the Music Transformer paper (Huang et.al.2019b).However, the limited algorithmic novelty is not the main concern. The authors also mention an internal dataset of music audio and transcriptions, which can be a major contribution to the music information retrieval (MIR) community. However it is not clear if this dataset will be publicly released or is only for internal experiments.<BRK>## summaryIn this paper, the author extends the standard music Transformer into a conditional version: two encoders are evolved, one for encoding the performance and the other is used for encoding the melody. The output representation has to be similar to the input. The authors conduct experiments on the MAESTRO dataset and an internal, 10,000+ hour dataset of piano performances to verify the proposed algorithm. ## Novelty The application is interesting, but the novelty of the architecture itself is limited. 2.By checking the music Transformer, in Table 3, it is not surprising to see that the proposed method outperforms the corresponding baselines, because no conditional information is used. 3.It is better to give some mathematical definition of music generation with specific style. I am not working on music generation but I list two CV related papers about conditional image translation, which mathematically describes "an image with specific style". 4.Considering that this is an unsupervised setting that two styles are transformed, can cycle consistency be implemented as a baseline?
Reject. rating score: 1. rating score: 1. rating score: 3. <BRK>Main comments and questions:1) The contribution of this paper is very limited: it simply applies the ensemble method to dropout (ensemble method can be applied to any existing models). 2) The paper keeps claiming that the computation will not dramatically increase because only the computations after dropout neede to be re computed if applying multiple dropout masks to the same data(batch). This is only true for the case when dropout is used in the last few layers. So the computational advantage does not hold if we apply dropout in earlier/shallow layers. 4) The dropout with 1 sample in experiments is not a standard baseline in any previous works. So I will keep my rating unchanged.<BRK>In addition, while the paper claims the method provides better generalization, the results shown in Figure 6, in which the training losses for their multi sample dropout and original dropout with duplicated samples are not significantly different, suggest that their validation error might not be significantly different either. The authors claim that this enhanced dropout technique (i) accelerates training and (ii) improves generalization by achieving lower error rates than standard dropout in training and validation sets. PAPER SUMMARY: The paper proposes a new and efficient implementation of dropout in which multiple dropout samples are obtained from a single input during training. However, the novelty of the proposed technique and their claim regarding better generalization is not well supported theoretically or experimentally.<BRK>The only confusing part of the method for me is the prediction. In the second paragraph in section 2.1, the authors state that during inference neurons are not discarded, and only one dropout sample is used for the prediction. (2) To the best of knowledge, there is at least one more paper that proposes a technique for accelerating dropout training   Fast Dropout [http://proceedings.mlr.press/v28/wang13a.html]. The method seems to be a direct competitor and should be considered as a baseline and be included in the relevant work section with further discussion. Overall, the proposed approach is heuristic, and the novelty is very limited.
Reject. rating score: 3. rating score: 3. rating score: 3. rating score: 6. <BRK>Overall, the paper is well organized and easy to read. The authors claim that early stopping is efficient to find a maximal safe set. I think it would be necessary to illustrate the maximal safe set for all the datasets. The authors are suggested to compare the proposed method with more baselines. It is unclear from the paper that if the baselines have used the clean validation sets. The dataset also includes some clean data for validation use. The authors should verify the effectiveness of the proposed method on this dataset.<BRK>This paper proposes a two phase training method for learning with label noise. On the positive side, this paper focuses on the idea of prestopping and proposes several relevant definitions to formalize their idea and come up with a heuristic algorithm. However, I believe the paper has missed several very relevant papers that provides very similar ideas. Before these two papers, [1] also observed that the learning trajectories for clean and noisy samples are different in label noise problem, and they used early stopping in their experiments to address this issue. Given these existing literatures, the contribution of this paper should be considered more properly.<BRK>This paper proposes a training strategy for robustness against label noise. The neural network will first be trained on the entire dataset with all the noisy labels. The paper discusses two important questions for the method: (1) when to early stop the training; (2) how to constuct a maximal safe set. The authors  responses to these questions are very natual but less interesting. Using the lowest validation error to early stop the training could be suboptimal, since the small validation set can not fully capture the data distribution and could make the network empirically overfit to this validation set. Overall, I think the paper is well written, the idea is clearly presented, and the experiments also seem convinceing. However, the contribution of this paper is very incremental.<BRK>This paper presents a training approach on label noise datasets and outperforms state of art methods. It defines the samples whose average probability on assigned label in recent q iterations is largest among all labels as memorized samples, in the sense of the network memorize these samples. Then authors proposed two stage method which firstly early stops at minimum validation error (or $\tau$ memorized rate), and then trains on maximal safe set that gathers memorized samples. The experiments compared several state of art approaches and showed that the proposed method benefits from early stopping and safe set. Authors also showed that the prestopping idea can also be used to improve other approaches. Authors have good experiments which evaluate on multiple datasets and algorithms.
Reject. rating score: 3. rating score: 3. rating score: 6. <BRK>The paper proposes to learn a "virtual user" while learning a "recommender" model, to improve the performance of the recommender system. * What algorithm is used to train the policy pi? Could the authors discuss this more in the paper?<BRK>Although I assume somebody well versed in the recent collaborative filtering literature would not have trouble, I had too much difficulty understanding the setup and the model to be able to recommend the paper for acceptance. Is this a set of recommendations? A probability distribution over items? In the problem statement, we have that "recommender predicts preferred items . ..and the user generates feedback to the recommender." Or whether any were of interest? What new information is in \tilde{a}_i?<BRK>The paper is essentially an attempt to incorporate a form of reinforcement learning into recommender systems, via the use of a synthetic feedback loop and a "virtual user". Regarding the merits of the technical contribution, I ll perhaps have to defer to other reviewers, but overall the contribution seems above the bar.
Reject. rating score: 1. rating score: 3. rating score: 3. <BRK>Although this paper s title contains "certified defense" and "unrestricted adversarial attack",  what I believe this paper is doing is analyzing the query complexity of query based black box attacks under simple linear models such as logistic regressions (or kernelized versions). The authors tested the query performance on two attacks: (1) the sign attack proposed by the authors and (2) the simba attack proposed by Guo et al.I have several concerns regarding this paper:1. The paper reads like the authors are actually certifying the defined defensibility metric but without a threat model to certify. In the introduction, the authors  definition of adversarial examples is "any input is considered a valid adversarial example as long as it induces the classifier to predict a different label than an oracle classifier." Therefore, the performance evaluation is not fully justified. 4.Similar to 3, the classifier setting is also uncommon. I hope the comments areuseful for preparing a future version of this work.<BRK>The paper proposes adding noise to the output of scoring function to defend from black box attacks. My main concern is about the assumption of the attacker. I don t think this is the correct assumption for the current attacks   given an example, black box attacks are trying to find some x  for each x without trying to recover  or even estimate w. Therefore I wonder why the query complexity can be linked to the complexity of estimating w and is there any further assumption you need to make? (Not saying you should try all of them, but I feel more than 1 attack is needed to justify the claim). This seems only guaranteeing there s a majority mass of w centered at w*. Condition 2: What is I ? (I didn t see the definition). Some related work: In DNN defense there are some related work on adding random noise.<BRK>The authors propose a new certified defense strategy that considers unrestricted black box attacks. The paper provides bounds for the minimum number of queries needed for the attacker to attack the classifier successfully and then the authors prove that they can devise a defender to be robust against that attack. Just claiming that none of the existing methods would work for unrestricted attacks will not work is not sufficient. The specific contributions are not quite clear with respect to the existing literature (which is reviewed in a sparse manner in the paper).
Accept (Poster). rating score: 6. rating score: 6. rating score: 6. <BRK>This paper proposes a method, which is named T(Transferable) NAS, for neural network architecture search (NAS) by leveraging the gradient based meta learning approach. T NAS learns a meta architecture in order to adapt to the new tasks through a few gradient steps. In that case, different architectures is used for different tasks as opposed to the baseline algorithms. This paper proposes an incremental approach which is a combination of the existing algorithms. I like the extensive empirical work of this paper.<BRK>In this paper the author propose a combination of the neural architecture search method DARTS and the meta learning method MAML. T NAS, the method proposed by the authors, applies MAML to the DARTS model and learns both the meta parameters and meta architecture.<BRK>Summary:Current neural architecture search (NAS) methods work in the mode of what is called S1 in this paper: given a dataset, search for a new architecture from scratch for that particular dataset. This paper proposes using MAML style metalearning for learning meta architectures across many meta training tasks so that given a new test task (dataset) the meta architecture is a few search steps away from a near optimal one. Hence the name Transferable NAS (T NAS). Comments:  I really like the premise of the paper. So please bear with me:1.
Reject. rating score: 1. rating score: 3. rating score: 8. <BRK>Here is my summary of unresolved concerns written after the discussion period. This work has been substantially improved during the rebuttal process, and some of my concerns are addressed. But there are still major issues, as raised in my [last comment]( https://openreview.net/forum?id BJlXgkHYvS&noteId r1xAnokijS ), that remains unanswered. Denote this matrix as . Original Review This paper presents a generalization bound based on Fisher information at a local optima, and proposes to optimize (an approximation to) it to get better generalization guarantees.<BRK>This paper provides a metric to characterize local minima of deep network loss landscapes based on the Fisher information matrix of the model parameterized by the deep network.<BRK>This paper contributes to the deep learning generalization theory, mainly from the theoretical perspective with experimental verifications. According to the authors, this simple metric, which is the log determinant of the Fisher information matrix, can characterize the generalization of a DNN. In section 5.1, there has to be some remarks on the intuition and related works on the flatness of the local minimum that is related to generalization.
Accept (Poster). rating score: 8. rating score: 8. rating score: 3. <BRK>The only concern that I see in the paper is that the Deterministic Function Module is not explained very well. First it converts the explanations into logical formulas. This logical formulas are then exploited for partitioning the dataset into two datasets: labeled dataset and unlabeled dataset. Then, NMET relaxes the logical formulas for labeling unlabeled examples by exploiting a neural architecture that uses four modules to deal with different types of predicates.<BRK>This paper explores using natural language explanations as auxiliary training data for NLP tasks. Hence, I believe the paper is above the acceptance threshold, and recommend for weak acceptance. However, I would also like to note that the paper has a few limitations.<BRK>Can you elaborate on how z_q is constructed? I m curious to see what would happen with more explanations. Is there an explanation for this? I think that you mean "should we" makes it sound like this is a question, but there is no question mark, and it makes more sense as a statement. should be "a very" not "an very"."
Reject. rating score: 3. rating score: 6. rating score: 6. <BRK>In summary, I am mostly worried about the novelty of the paper, and wondering how the model is getting trained, and the comparison with other compression techniques. What is the main difference between this method and all the other tensor decomposition based methods for CNN compression? There are so many tensor decomposition based methods for CNN, and seems Falcon belongs to one of them. The one (maybe) special for Falcon is that it only decomposes along one dimension.<BRK>Based on EHP, the paper develops depthwise separable convolution to compress CNNs, and extend it to a rank k approach with further improved accuracy. Some analysis is provided about the operation equivalence. The experiments are extensive as well.<BRK>Moreover, for the order of performing depthwise convolution and pointwise convolution, it also needs experimental support. The authors give a detailed interpretation of the depthwise separable convolution method via EHP. 2.The paper is well organized and easy to read.
Reject. rating score: 1. rating score: 1. rating score: 1. <BRK>In particular, a set of base functions are given in hand and the goal is to obtain the right composition of these functions which fits the target function. The main contribution of the paper is to introduce a selection layer, which enhances sparse connections in the network.<BRK>This paper presents White Box Network (WBN), which allows for composing function blocks from a given set of functions to construct a target function. The main idea is to introduce a selection layer that only selects one element of the previous layer as an input to a function block.<BRK>The explanation of the papers are quite verbose. "It is different from other neural networks as it not only approximates a target function, but also constructs and reveals its structure." It is claimed that $\ell^1$ minimisation will allow to avoid... sparse and sharp operators: "This causes Wˆ (l) to be extremely sparse and sharp, and it can be an obstacle for shifting the function blocks from one ordering to another." This goes against my intuitions/knowledge, could the authors point me to a reference? If yes, this should be commented. Also, the Table is not discussed in the text...
Reject. rating score: 6. rating score: 6. rating score: 6. <BRK>This paper considers the problem of transferring word attributes between words. There is a lot of discussion ofelementary ideas such as reflection about an affine plain and involutions. The key technical idea in the paper is given in the definition of the objective function in equation (16). The experimental results are limited to three attributes one of which   the capital/country attribute   seems a relation not an attribute. But the most serious problem with this paper is a lack of references to related work. I would start with the following reference and track papers that reference it. The citation I suggested has been added but with inadequate acknowledgement. Based entirely on the results I have raised my score weak accept.<BRK>It would be interesting to learn your comment on that. It is also interesting to learn how this reflection based attribute transfer can be applied to the same word, but with embeddings that play different role in a model: e.g.input and output embeddings. In fact, input and output embeddings are located in the same space, they can be considered as pairs of words with 1 attribute flipped (i.e.role in a model, input >output). This is an interesting edge of the problem.<BRK>Summary: This paper proposes a method for binary word attribute transfer based on reflection. It applies a single reflection based mapping that relates the locations of two vectors in a Euclidean space by a hyperplane, and results in an identity mapping when it is applied twice. The paper also proposes a pretty smart idea: the mirror functions are parameterized to take advantage of the fact that inversions differ, even for the "same" word attributes. I am giving the paper a weak accept, because I think idea is really fun, and definitely has legs. It would be a cool extension. To be more specific, despite your Fig 3a: "actress" may be feminine, but "actor" is clearly neutral (anyone can be described as an "actor" but only women can be described as "actress" in most cases); thus "actress" is the gendered one. Do your zs for each attribute correlate at all?
Reject. rating score: 3. rating score: 6. rating score: 6. <BRK>What is the specific question/problem tackled by the paper? This paper tackles the problem of learning language conditioned policies from reinforcement learning. This experimental completeness is itself a contribution. Additionally, although the authors do not discuss this, this method is actually agnostic to the particular modality (e.g.text) of the goal space and could be used anytime the goal space differs from the state space. I would interested in seeing how this method would work in the room2room environment (or some other more complex task).<BRK>If the language was more complex (and not limited to a small set of template instructions), would the THER approach still be reasonable? Or in the notation of the paper, how is f(s,g) implemented?<BRK>Introduction: It thus leads to the following questions: are we eventually making the rein forcement learning problem harder, or can we generate learning synergies between policy learningand language acquisition? > it really doesn’t seem like the point of instruction following is that. Wouldn’t this be prohibitive? Can the authors introduce other baselines. But I think for a successful ICLR paper, we would need 1 2 more meaningful baselines.
Reject. rating score: 1. rating score: 3. rating score: 3. <BRK>This paper proposes a semi supervised and adversarial training process to exploit the value of unlabeled faces and overcome the limitation of a linear 3DMM and the nonlinear models proposed early (Tran & Liu (2018), Tran et al (2019)). This paper should be rejected because:(1) the experiments are not representative enough and the results are controversial,(2) this paper does not clearly demonstrate how they exploit the value of the unlabeled training images,(3) the creative progress of this paper is not typical compared to the early nonlinear model (Tran & Liu (2018), Tran et al (2019)). The loss functions are also not convincing enough:1) How to choose or initialize the value of lambda center in the Face recognition loss?<BRK>Overview:This paper introduces a model for image based facial 3D reconstruction. The proposed model is an encoder decoder architecture that is trained in semi supervised way to map images to sets of vectors representing identity (which encodes albedo and geometry), pose, expression and lighting. I also find the promise of “disentangled” representation a bit over stated, as the albedo and base geometry still seem to be encoded in the same “identity” vector (see related question below). In addition, there is no numerical ablation study that would demonstrate the actual utility of the main contributions (such as adversarial loss): there are qualitative results but they are not very convincing. This is indeed the case, but it does not seem like here the authors fully overcome this issue: they do have additional weakly supervised data, but they still strongly rely on linear 3DMM supervision (p6, “pairwise shape loss”, “adversarial loss”), and do not seem to provide experimental evidence that the model will work without it.<BRK>The authors develop a semi supervised training scheme to fully exploit the value of large amount of unlabeled face images from unconstrained photo collections. Experimental results on MICC Florence and AFLW2000 3D verify the efficacy of the proposed method. My concerns regarding this paper are as below. Since speed is very important for real applications. Based on my above comments, I give the rate of WR.
Reject. rating score: 1. rating score: 3. rating score: 3. <BRK>Summary:This paper proposes a hybrid VAE GAN model, called the latent space renderer GAN (LSR GAN), with the goal to “imagine” the latent space of a VAE, and to improve the decoding and sampling quality of a VAE. This is not a very scientific statement and can be perceived as insulting for various reasons. Please rephrase this. Decision: rejectThis paper contains incorrect claims. It is surprising that the reconstruction error does not cause overflow as log (0)  >   infinity. Other sections:  Section 3 contains a very long interpretation of the minimum description length (MDL).<BRK>The paper proposes a new method for improving generative properties of VAE model. Concerns:1) The main concern about this paper is the inaccuracy and very general statements without theoretical or empirical justification. However, I still think that the paper does not give new insights about VAE model and has poor experiment justifications of their statements. Considering MDL interpretation of VAE it is not new (see [1]).<BRK>However, I must say that I just fundamentally disagree with motivations and some of the statements made in the paper. The argument seems to be that likelihood based training of generative models does not, by design, encourage realistic looking images (indeed it is true that good likelihood does not *necessarily* imply good generative models). The decoder is by definition a generative model. I do not agree that "information about the precise shape of an object is probably not compressible". Finally, I hope I am not coming across as nitpicking or overly combative, but I am genuinely confused as to the problem that this paper is addressing.
Reject. rating score: 3. rating score: 3. rating score: 6. <BRK>This paper presents a clear approach to improve the exploration strategy in reinforcement learning, which is named clustered reinforcement learning. Although the paper is generally easy to follow and the motivations of the equations are clear,  the analysis of the results is missing and thus the paper provides very limited insights on the behavior of the proposed method. 4) In the experiments, the authors compare with other methods on only five Atari games.<BRK>This paper proposed a clustering based algorithm to improve the exploration performance in reinforcement learning. Similar to the count based approaches, the novelty of a new state was computed based on the statistics of the corresponding clusters. On the other hand, the motivation and explanation of this method are not well presented. 4.In the experiments, the authors claimed that the code for TRPO Hash is provided by its authors. Do you have any explanation?<BRK>Some suggestions are as below:1. Is the method senstive to the used clustering method. The contribution of this paper is about the state clustering for exploration in RL which can further reflect the novelty and quality. In summary, I ack that the idea is effective but seems straightforward. It would be better to present some theoretical analysis.
Accept (Poster). rating score: 6. rating score: 6. rating score: 6. <BRK>In the context of language modeling, the former is to approximate a distribution (over an extended alphabet) whereas the latter is to approximate a conditional distribution (given the input). It is a variant of top k sampling where the smallest k is selected to ensure the combined likelihood is no less than p. The paper centers on claiming and showing that the generated samples are of higher quality and more diverse than common alternatives such as beam search, pure sampling, top k sampling, and low temperature sampling. While overall I think the proposed method is sound as an alternative to other heuristics such as beam search, I have reservations on the presentation and arguments made in the paper. Pros:1.NS is sound as a heuristic sampling method. 2.The comparison with beam search (BS) is not well motivated. BS is devised to find the maximal sentence and it is not stochastic. If the estimate is wrong on the low probability tail, then so is the estimate on p(head)   1 p(tail) by virtue of p being a probability measure.<BRK>Contributions:This paper studies an important problem, i.e., how to find a good decoding strategy for open ended text generation. Compared with top k sampling, the key idea behind the proposed method is to sample from the dynamic nucleus of tokens containing the majority of the probability mass. Experiments demonstrate that nucleus sampling is an effective decoding strategy in practice. I enjoyed reading the paper. Both quantitative and human evaluation are provided. However, given the comprehensive evaluation, and high writing quality, I lean to accept this paper due to its empirical contribution.<BRK>Similar to Top k sampling, Nucleus Sampling truncates the probability distribution of the words in the vocabulary. Instead of re normalizing the probabilities for the top k words, Nucleus Sampling re normalizes the original probabilities for the words with values above a pre chosen threshold p. Some quantitative and qualitative results show that the proposed sampling method can generate long form texts with some nice properties. Pros:The problem addressed in this paper is highly interesting, and the proposed method is simple and intuitive. This happens only because our current neural language models are not well specified for generating long texts and modeling long range contexts. With a large k, there is no technical barrier that prevents Top k sampling from generating the sentences produced by Nucleus Sampling. In summary, although the studied problem in this paper is highly interesting, the proposed Nucleus Sampling is not technically significant compared to Top k sampling.
Reject. rating score: 1. rating score: 1. rating score: 3. <BRK>This paper proposes two new architectures for processing set structured data: An RNN with an accumulator on its output, and an RNN with gating followed by an accumulator on its output. While sensible, this seems to me to be too minor a contribution to stand alone as a paper. Additionally, I believe the experimental tasks are new, and as a result all implementations of competing techniques are by the paper authors. This makes it difficult to have confidence in the higher reported performance of the proposed techniques.<BRK>In this paper, the authors propose two RNN based models to learn non additive utility functions for sets of objects. However, the novelty of the paper is limited and the empirical support of the proposed models is insufficient. The author might want to provide a short introduction to Choquet integral and elaborate on the connection with the proposed models. The proposed models seem very basic and do not have much novelty. Questions:* For RCN and DCR, how to decide the ordering of phi_i, given that they are the objects of an unordered set? * It would be helpful it the authors can also provide the number of parameters of the baseline models in Tables 1, 2, and 3.<BRK>This paper studies non additive utility aggregation for sets. The authors propose two architectures. However, the experimental comparison is not fair, the description of the model (e.g.how Choquet is integrated into the model and help to learn “intermediate meaningful results”) is not clear, some claims are not true. First, the authors claim that they are the first to combine Choquet integral with deep learning. These need to be further clarified. So, I have some doubts about the experimental results.
Reject. rating score: 3. rating score: 3. rating score: 6. <BRK>This paper claims that learning prior from the data could achieve superior performance than using a standard unit Gaussian prior. Experimental results further show that the proposed method could achieve a lower or comparable negative log likelihood compared to other VAE variants using a complex hierarchical architecture. However, it would easily lead to an overfitting model with bad generalization, especially for a noisy dataset. It is claimed in the abstract and conclusion that one latent variable is used for learning the RealNVP prior while the authors use the words "shallow" (refers to few latent variables) in the experiments. It is listed in the related work that "Huang et al.(2017) applied RealNVP (Dinh et al., 2017) to learn the prior", which means the idea using the learned RealNVP prior is not a new idea. Therefore, what is the contribution of this paper? A detailed discussion is needed to elaborate on the differences from previous methods using a prior learned from the data.<BRK>The authors propose to use learned RealNVP with a shallow VAE instead of deep hierarchical VAE, which will not hurt the performance. The authors conduct thorough experiments to backup their claims and hypotheses. 2.The proposed method seems like a combination of previous studies, making the paper more like a technical report. 3.One advantage claimed by the authors is that only one latent variable has clear semantic meanings, which is not explicitly supported by the experiments.<BRK>While most of VAE research focuses on building more powerful encoder and decoder architectures, these results show that focusing on a learned prior distribution is as important. However, while there are a considerable number of experiments in this paper, for me to increase the score I would like to see at least some of the experiments suggested above, since they could help better understand the behavior of VAEs with learned priors and make this an even more impactful paper. * Can the authors clarify the differences between their model and that of Huang et al, which also uses real NVP priors?
Reject. rating score: 1. rating score: 6. <BRK>  SummaryThis paper studies the sample elicitation problem where agents are asked to report samples. The goal is then to evaluate the quality of these reported samples by means of a scoring function S. Following previous related works, the authors use the equivalence between maximizing the expected proper score and minimizing some f divergence. Is there another (broad) family of functions for which the computation of the argmin in Equation (4.3) is more tractable in practice?<BRK>This paper proposes a sample elicitation framework to tackle the problem of eliciting credible samples from agents for complex distributions. The authors suggest that deep neural frameworks can be applied in this framework for sample elicitation through the derivations. The authors also show the connection between the problem of sample elicitation and f GAN. However, some problems in the proof on sample elicitation should be clarified or carefully explained.
Reject. rating score: 1. rating score: 3. rating score: 6. rating score: 6. <BRK>This paper proposes two improved strategies for fine tuning XLM (a multilingual variant of BERT) for cross lingual NLI. However, I am not convinced at all by this reasoning, as it still relies on the translation of the English labeled data into the other languages. The authors claim that "Urdu (ur) is an unrelated language" and "Swahili (sw) is loosely between French and Urdu in terms of relatedness to English", which they use to justify why Urdu behaves differently in their experiments. Swahili and English belong to completely different language families, and from what I know their grammar is very different. This is not relevant at all, but I would suggest the authors to find a different acronym instead of XD, which happens to be a widely used emoticon.<BRK>It results in multiple models, one for each language. In addition, they also proposed a method based on distillation where only the english targets are used. While the model doesn t require the label data for the other languages, the scores remained similar to the previous experiments. It compares favorably, obtaining 4 points of improvement over SOTA in the zeroshot setup. Pros: the motivation for having only one model is interesting the results are promisingCons: one of main motivation of the paper is to achieve zeroshot as opposed to previous work. the paper was not always easy to follow and would benefit from more clarity. I didn t see any significance measurements and it would be important to add them. Overall, using all the data together seems like a natural and effective approach to and achieves good results through only one model.<BRK>Current state of the art results in multilingual natural language inference (NLI) are based on tuning XLM (a pre trained polyglot language model) separately for each language involved, resulting in multiple models. What are the main contributions of the paper? [Moderately novel] Cross lingual knowledge distillation approach that uses one and the same XLM model to serve both as teacher (for English sentences) and student (for their translations into other languages). The approach does not require end task labels and can be applied in an unsupervised settingWhat are the main results? Combining XD with multilingual tuning is not effective in improving average results or even in case of target languages2. Only 4 languages were used while combining XD and MLT3. Findings, methods and experiments are not strongly novel.<BRK>First, the authors propose to train a model for natural language inference (NLI) on multiple languages simultaneously. The authors show that their approach is superior to training individual models for each language. Second, they introduce cross lingual knowledge distillation (XD), where the same polyglot model is used both as teacher and student across languages to improve its sentence representations without using the target task labels. The paper seems okay to me and the experiments seem solid. This paper could further be improved in the following ways:  A more detailed investigation which combination of languages improve performance (and why?).
Reject. rating score: 3. rating score: 3. rating score: 6. <BRK>** Paper summary **In this paper, the authors propose a new re ranking mechanism leveraging document level information. The authors focus on X >Y translation and Y >X is a model used for re ranking. Specifically,(1)	Two translation models X >Y and Y >X are trained, where X >Y is a document Transformer and Y >X is a sentence transformer. (2)	Train a language model P(Y) on document level corpus (rather than sentence level LM). Compared to the paper “the Neural Noisy Channel” (Yu et.al, 2017), the authors use document Transformer and document level LM for re ranking, which is of limited novelty. Does it mean that the "proposal" component do not work? 4.Many models are used in this framework.<BRK>The idea is to use a language model on the target side and a reverse translation model to choose the best document level translation. The authors implement this idea using a reranking model that rescores 50 candidate translations generated by a standard Transformer model for forward translation. This is interesting work and the experimental results demonstrate the effectiveness of the approach. However, I am concerned about the (missing) comparison between the proposed approach and the approach that combines backtranslation and a document level translator (e.g.Doc transformer).<BRK>Summary:The paper describes a noisy channel approach for document level translation, which does not rely on parallel documents to train. The approach relies on a sentence level translation model (from target to source languages) and a document level language model (on target language), each is trained separately. Experiments show strong results on two standard translation benchmarks. Comments:   The proposed approach is strongly based on the neural noisy channel model of Yu et al.2017 but mainly extends it to context aware translation. In general, I think the paper is well written and results are compelling.
Accept (Poster). rating score: 6. rating score: 6. rating score: 3. <BRK>This paper studies a fast algorithm for choosing networks with interleaved block types – BlockSwap. The teacher student network is used here to learn compressed network on a budget. The insightful experiments thoroughly discuss and compare the proposed method.<BRK>The assumption that the number of blocks in the student is the same as in the teacher seems constraining. Do the authors have an explanation for this? Methodology:  From the paper, it is not entirely clear how sampling under budget constraints is achieved.<BRK>1) in the introduction, it s not clear what attention transfer means   2) why did they specifically choose these four blocks? 4) (3.2) ``consider a choice of layers $i   1, 2, \dots, N_L$" what is $L$ and why does the number of layers depend on $L$? 5)  (3.2) Why is that specific $f$ chosen? Since the paper suggests the Fisher potential is a crucial part of their method, they could provide more theoretical justification about this choice.
Reject. rating score: 1. rating score: 1. rating score: 3. <BRK>It is of course a bad idea to model a Gaussian with a Cauchy distribution (the way it is always a bad idea to have a mis specified model), but I would actually argue that the isotonic regression based approach by Kuleshov et al., 2018 is more robust against model mis specification due to its higher flexibility and thus a more powerful approach for pos hoc calibration. Most importantly, I have major concerns regarding the lack of in depth evaluation.<BRK>It is motivated by a flaw in the diagnostic proposed by Kuleshov+ 2018 (abreviated K2018 below), as explained around eq4, and proposes a replacement diagnostic for uncertainty calibration quality (sec 3 before 3.1). It then presents experiments (sec4) to demonstrate that the motivating flaw can be evidenced by their diagnostic, and fixed by their recalibration method where it makes sense (ie where predicted uncertainties are not random, i.e.statistically independent of the empirical uncertainty). Nevertheless I believe the paper should be rejected for the following reasons.<BRK>This paper focuses on the calibration for the regression problem. First, it investigates the shortcomings of a recently proposed calibration metric [1] for regression, and show theoretically where this metric can be fooled. I suggest the authors add more details about this part. But this calibration  method is not related to the proposed metric which is claimed it has better calibration clarity. The paper should investigate the shortcomings of the previous metric in the same setting as the new proposed metric.
Reject. rating score: 3. rating score: 6. rating score: 8. <BRK>This paper re organized the high dimensional 1 D raw waveform as 2 D matrix. Log likelihood could be calculated in parallel. The number of required parameters was desirable to synthesize high fidelity speech with the speed faster than real time. In general, this paper is clearly written, well organized and easy to follow. In the experiments, a small number of generative steps was considered. This is because short sequence based on autoregressive model was used.<BRK>This submission would benefit from a discussion of limitations of your approach. I believe there is a great deal of interest in the use of normalising flows in the text to speech area. I believe this submission could be a good contribution to the area. The mean opinion scores (MOS) seem to approach one of the standard baselines with significantly worse inference times though at the expense of increasing the number of model parameters from 6M to 86M parameters whilst gaining only 0.2 in MOS. The submission would have benefited from discussion about model complexity/expressivity and it s impact on MOS for WaveFlow, WaveNet and other approaches. It seems the order should be 3,4,(5),1,2,(5).<BRK>The experiments are reasonably convincing, although they could be improved. ## Original reviewThis paper presents the WaveGlow model, a generative model for raw audio. The model is evaluated and compared with related work on an objective evaluation (Log likelihood) and a subjective evaluation (MOS), and is shown to be a trade off between memory footprint, generation speed and quality. I think this paper should be accepted, for the following reasons:  The theoretical framework presented is novel and significant, as it provides a unified view of the two main approaches for neural waveform generation.
Accept (Poster). rating score: 8. rating score: 6. rating score: 6. <BRK>In this paper, based on the effectiveness of early stopping in the training of noisily labeled samples, the authors proposed two intuitive (and novel) regularization methods: (1) regularizing using distance to initialization (2) adding an auxiliary variable b_i for every input x_i during training. In terms of theory, the authors showed that in the NTK regime, both regularization methods trained with gradient descent are equivalent to kernel ridge regression. Moreover, the authors also provided a generalization bound of the solution on the clean data distribution when trained with noisy label, which was not addressed in previous research. Overall, the paper is very well organized and well written. The contribution of the paper is significant, and numerical results also vindicate the theory developed in the manuscript. I recommend accepting the paper. Is this going to cause any problem in practice if data augmentation is used?<BRK>This paper studies the topic of learning with noisy labels, in particular, classification problem where the labels are randomly flipped with some probability. The main technical contributions of this paper are two folds: 1) proof of generalization bounds for kernel ridge regression solutions that depends on the clean labels only. 2) two regularization techniques that are shown to be equivalent to the kernel ridge regression when the neural networks approach the neural tangent kernel regime. Even though the paper states that the primary advantage of the proposed methods is simplicity, it would still be good to have some empirical comparison for reference. I like that the paper has a section to explicit check whether the neural networks used in the experiments are in neural tangent kernel regime.<BRK>Contributions:  Propose two simple regularization techniques for learning from a noisily labeled dataset. Significance:Since proposed methods are in some sense related to the early stopping for the (stochastic) gradient descent, the developed theory is useful in understanding the generalization ability of over parameterized neural networks falling into NTK regime. A result (Theorem 5.1) for the regression problem with the squared loss is not so surprising because the generalization error of gradient descent in a high dimensional space (e.g., over parameterized NNs) or an RKHS (i.e., infinite dimensional model) has been well studied and the generalization error is composed of the (constant) variance and the distance between the model output and the true label. However, I basically like this paper and I think this paper makes a certain contribution to understanding the effect of over parameterization. A few questions:  Usually, the regularization parameter goes to zero as the number of examples increases. Conversely, the regularization parameter in the proposed methods also increases. However, the generalization bound by Theorem 5.3 is still affected by $\lambda$. Update: I thank the authors for the response. This paper is of good quality.
Reject. rating score: 1. rating score: 1. rating score: 6. <BRK>This paper attempts to address the problem of out of distribution detection with generative models. To do this they assume they are given batches of OOD examples or batches of in distribution examples, and they detect whether the batch is in  or out of distribution. Normally we try to detect if an example is in  or out of distribution. Small notes:> given their successful generalizing on a test dataset.<BRK>The paper makes the observation that likelihood models trained with batch norm assign much lower likelihoods to "training batches" of OoD data (batch norm statistics computed over over minibatch) than evaluation batches of OoD data (batch norm statistics over entire training set). One issue with comparing this method to most other OoD detection works is that it considers OoD detection on *batches* of (all OoD data) or (all in distribution data). As soon as the problem is changed to "classify between OoD batches" and not single samples, there are a large number of possible statistical tests one can perform to perform OoD (T test between likelihoods of each batch) and the problem becomes *much* easier. In some ways, this makes things more well defined (hard to compare distributions when one of them is just a single sample from an arbitrary distribution). This is consistent with your batch normalization experiments: in training mode, the likelihood is computed from mean activations over a batch of OoD samples, several of which probably contribute to the low likelihoods. In other words, I think there is a mistake made here: it is the phenomenon that *batch likelihoods*, not *batch norm*, that is responsible for this method working well.<BRK>This paper tackles the out of distribution detection problem and utilizes the property that the calculation of batch normalization is different between training and testing for detecting out of distribution data with generative models. The example in Figure 2 gives a good intuition of how mis specification can happen. The first is the thorough observation of the likelihood changes between different modes of batch normalization.
Accept (Poster). rating score: 6. rating score: 6. rating score: 6. <BRK>This paper proposes a new invertible flow based graph generation model. The paper also proposes an implementation of molecule lead optimization combined with a RL framework. Overall, the paper is written well. I feel no difficulty in understanding the main idea and the equations in the paper. More descriptions or references are required. The performance on the property optimization (Table 3) seems brilliant. However, there is no discussion why the combination of the autoregressive flow and the RL performs greatly, compared to baselines. + Overall, a good paper. well written, easy to understand. The novelty lies in the iterative generation process, naturally combined with the autoregressive flow.<BRK>This paper proposes a generative model architecture for molecular graph generation based on autoregressive flows. The main contribution of this paper is to combine existing techniques (auto regressive BFS ordered generation of graphs, normalizing flows, dequantization by Gaussian noise, fine tuning based on reinforcement learning for molecular property optimization, and validity constrained sampling). Most of these techniques are well established either for data generation with normalizing flows or for molecular graph generation and the novelty lies in the combination of these building blocks into a framework. Overall, the paper is very well written, nicely structured and addresses an important problem. The framework in its entirety is novel, but the building blocks of the proposed framework are established in prior work and the idea of using normalizing flows for graph generation has been proposed in earlier work (see [1] and [2]). Nonetheless, I find the paper relevant for an ICLR audience and the quality of execution and presentation of the paper is good.<BRK>In line of this I am raising my score to Weak Accept. This paper proposes a new molecular graph generative model (GraphAF) which fuses the best of two worlds of generative networks   reversible flow and autoregressive mode. Further, it also demonstrates that additionally if the chemical properties are optimised during training with reinforcement learning policy then GraphAF outperforms all the prior works. From the table 2, it seems to me every prior method works pretty well on important metrics. There is very little room for improvement. 3) The novelty of the model is limited. This is clearly not sufficient to model complex graphs. And in its current form it is not clear how one can extend to multi layer flow.
Reject. rating score: 3. rating score: 6. rating score: 6. <BRK>This paper proposes a meta learning algorithm Generative Teaching Networks (GTN) to generate fake training data for models to learn more accurate models. In the inner loop, a generator produces training data and the learner takes gradient steps on this data. In the outer loop, the parameters of the generator are updated by evaluating the learner on real data and differentiating through the gradient steps of the inner loop. They also suggest weight normalization patches up instability issues with meta learning and evaluate this in the supervised learning setting, and curriculum learning for GTNs. To me, the main claim is very surprising and counter intuitive   it is not clear where the extra juice is coming from, as the algorithm does not assume any extra information. The actual results I believe do not bear out this claim because the actual results on MNIST and CIFAR10 are significantly below state of the art. The disparity on CIFAR seems to be less egregious but the state of the art stands at 99% while the best GTN model (without cutout) achieves about 96.2% which matches good convnets and is slightly worse than neural architecture search according to https://paperswithcode.com/sota/image classification on cifar 10. This does not negate the potential of GTNs which I feel are an interesting approach, but I believe the paper should be more straightforward with the presentation of these results. The current results basically  show that GTNs improve the performance of learners with bad hyper parameters. On problems that are not as well studied as MNIST or CIFAR10 this could still be very valuable (as we do not know what performance is good or bad in advance). The paper should present a wider set of experiments to make this claim convincing. How does GTN   Full Curriculum and Shuffled Batch parametrize the order of the samples so that it can be learned? At a high level it would be very surprising to me if the way forward for better discriminative models was to learn good generative models and use them again for training discriminative models, simply because discriminative models have proved thus far significantly easier to train. If this work does eventually show this result, it would be a very interesting result. At the moment, I believe it does not, but I would be happy to change my mind if the authors provide convincing evidence. Is it assumed to be the same as the inner one (but using real data instead of training data)? I think this should be made explicit in the method section. There are some additional experiments in other settings such as RL and unsupervised learning. Both seem like quite interesting directions but seem like preliminary experiments that don’t work convincingly yet. The RL experiment shows that using GTN does not change performance much. Given that these final experiments are not polished, the claim in the abstract that the method is “a general approach that is applicable to supervised, unsupervised, and reinforcement learning” seems to be over claiming.<BRK>Summary:The paper proposes Generative Teaching Networks, which aims to generate synthetic training datafor a given prediction problem. The authors demonstrate its use in an MNIST prediction taskand a neural architecture search task on Cifar10. I do not find the idea compelling nor the empirical idea convincing enough to warrant acceptance atICLR. Detailed Comments: At a high level, the motivation for data generation in order to improve a given prediction problem is not clear. From a statistical perspective, one can only do so well given a certain amount oftraining data, and being able to generate new data would suggest that one can do arbitrarily betterby simply creating more data   this is not true. Indeed, the proposed approach does not do better than the best performing models on MNIST. The authors use GTNs in a NAS problem where they use the accuracy on the generated images as a proxyfor the validation accuracy. While Table 1 indicates that they outperform some baselines, I do not find them compelling. Thiscould simply be because random search is a coarse optimization method (and hence the proposed metricmay not do well on more sophisticated search techniques). On a side note, why is evaluating on the synthetic images cheaper than evaluating on the      original images? What is the rank correlation metric used? Post rebuttalHaving read the rebuttal, the comments from other reviewers, and the updated manuscript, I am more positive about the paper now. I agree that with reviewer 2 that the proposed approach is interesting and could be a method to speed up NAS in new domains. I have upgraded my score to reflect this. My only remaining issue is that the authors should have demonstrated this on new datasets (by running other methods on these datasets) instead of sticking to the same old datasets. However, this is the standard practice in the NAS literature today.<BRK>The proposed Generative Teaching Networks (GTNs) are networks that are trained to generate training data for other networks and are trained jointly with these other networks by back propagating through the entire learning problems via meta gradients. They also show how weight normalization can help stabilize the training of GTNs. The main idea of the paper is quite simple and it’s nice to see it works well. I’m actually surprised it has not been proposed before, but I am also not very familiar with this research area. For these reasons, I lean towards accepting this paper, although I have a few comments that I would like to see addressed for the camera ready version. However, for MNIST and CIFAR it looks like the models being used may not be that good, as it’s quite easy to obtain better performance than the results shown in the paper. I would be curious to know why the authors did not use a better architecture and also what the architecture they used actually is. I am unfortunately not familiar with neural architecture search to be able to evaluate their experimental setup in that case. Regarding the curriculum used in Section 3.2, my understanding is that the full curriculum approach has an additional computational cost vs the no curriculum approach, as you have to learn that curriculum. This would allow us to see whether learning a curriculum this way is in fact practically useful. It may just as well be that it is too expensive and training without it is faster. The authors show example images generated by GTNs and, as they also mention, these images do not look very realistic. It would be good to have some explanation/analysis around this. (e.g., thinking in terms of support vector machines, do these images lie in or close to the margin of the classifier?). I would also like to see more details on the actual architecture used for the experiments as I feel that the paper does not provide enough information to reproduce the results.
Reject. rating score: 1. rating score: 3. rating score: 6. <BRK>If we already have that, it is unclear what gains do we get from the proposed method. Overall:		An interesting objective function, Learn not only option set but also the number of options needed and incrementally learn new options. The options are learned offline by learning to solve a small number of tasks. It would be useful to provide such motivation in the introduction. Atari experiments are limited in nature in that they show only two games. However, they do not mention what are “novel tasks for Breakout/Amidar” in this context. The work could benefit by comparing with the aforementioned baselines.<BRK>The paper proposes a method for learning options that transfer across multiple learning tasks. The paper could also do a better job of situating the approach with regard to existing option learning approaches. This aspect is largely absent from the practical part of the paper, however. Since this paper aims to minimise option terminations that seems to be the most logical comparison. In the experiments, the multi task transfer is not emphasized. The influence of the KL penalty isn’t really examined in results beyond looking at performance. It would be good to indicate explicitly how it depends on the distribution of tasks and what the expectation is taken over. While the paper cites a number of option learning approaches, it could do a better job of situating the research within the literature.<BRK>This paper proposes a new option discovery method for multi task RL to reuse the option learned in previous tasks for better generalization. During the offline training, they add one option at a time and move onto the next option when the current loss fails to improve over the previous loss, which enables automatically learning the number of options without manually specifying it. Experiments are conducted on the four rooms environment and Atari 2600 games and demonstrate that the proposed method leads to faster learning on new tasks. Overall, this paper gives a novel option learning framework that results in some improvement in multi task learning.
Reject. rating score: 1. rating score: 3. rating score: 6. <BRK>Paper Summary:This paper extends TapNet, a few short learning approach, to the setting of semi supervised few shot learning. For that, it borrows the idea of soft labeling from Ren et al 2018, a semi supervised few shot learning algorithm. Experiments over mini and tiered imagenet compare the approach with alternatives.<BRK>This paper presents a semi supervised few shot learning method that combines the TapNet architecture with the soft clustering approach in the original semi supervised few shot learning method of Ren et al.2018.Results are reported on mini ImageNet and tiered ImageNet, demonstrating superior results to Ren et al.2018 and some more recent work.<BRK>Ren et al.experimented with inference only baselines: meta learning happens only using the labeled subset, and the proposed clustering approach only takes place at meta test time. Experimentally, they outperform recent approaches to semi supervied few shot classification on standard benchmarks, though not by far. (This differs from the reported TapNet baseline in that at meta test time it would make use of the proposed semi supervised refinement). In this case this would amount to meta training a standard TapNet and then performing the proposed refinement only in test episodes.
Reject. rating score: 1. rating score: 1. rating score: 3. <BRK>The motivation for the model and its description are not clear at all. Then you have to hand code a completely different architecture. Given these concerns, I have decided to keep my score as it is. Then by all accounts, the convolution of these should be an N dimensional vector. How?Please clarify this. This is not explained in the paper beyond a vague description.<BRK>In particular, for some tasks, it is not clear why the proposed architecture addresses the targeted symmetry. Note that convolution with a filter of width one followed by pooling is exactly invariant to the symmetric group. However, there are major issues:  I found it challenging to identify a novel contribution.<BRK>The task is to translate a verb number pair into number repetitions of the verb. Curiously, the recurrent baseline seems to perform better than 0% accuracy (if still poorly) on the original SCAN task which is much harder than the proposed task in this paper. Also, for tasks 2 and 3, the motivation behind using convolutions is not as clean as in task 1. Finally, as mentioned above, I still cannot intuitively understand why convolutions in the forget architecture would learn about symmetry related to structured repetition produced by a CFG. Hence, more analysis or a better motivation would have helped.
Reject. rating score: 1. rating score: 3. rating score: 3. <BRK>Overall the paper is poorly presented and difficult to follow. Despite this the method does seem to work remarkably well, and the Jacobian idea is clearly very promising.<BRK>While this might be an important problem (I am not sure), the paper is not written and organized well which makes a through evaluation very difficult. It needs to be defined.<BRK>I believe the author s contribution can be valuable, and I particularly appreciate the effort to investigate theoretical properties. As is, the case is not sufficiently convincing to be accepted, but I encourage the authors to improve the paper. Detailed review:Nota bene: This review is a late reassignment.
Reject. rating score: 3. rating score: 6. rating score: 8. <BRK>The paper addresses the acute problem of irrelevant discrimination in adversarial imitation learning methods. Ideally, the discriminator in such methods should extract task dependent features and base its discrimination rule upon them only. However, it seems that the informal improvements (augmentation and early stopping) play a crucial role in the success of TRAIL (...It drastically improves the baseline GAIL agent, and is necessary to solve any of the harder tasks..."). This is a well known problem, that indeed received little attention in previous work. I also find it problematic that the construction of the invariance set is not straightforward ("...the selection of the invariance set is a design choice..."). I.e., the paper offers general guidelines on how to solve imitation tasks but not a concise algorithm. There are also some presentation issues, e.g cluttered graphs, that can be addressed. I believe that this is a dominant problem that received little attention so far. If an improvement was evident in such a comparison, then it would have been easier for me to understand the significance of the proposed method. 6.The invariance set is referred to extensively before it is first defined.<BRK>#####This work considers imitation learning, i.e.the problem of learning an unknown reward function from expert demonstrations. Given that the invariant set $\mathcal{I}$ includes environment samples not relevant for the task, TRAIL thus aims to prevent discriminator overfitting to task irrelevant features (e.g.object or agent appearance) and regularizes the discriminator to extract more salient, task relevant features of expert behavior. I think this work could be accepted since it clearly demonstrates a key issue of GAIL (tendency to overfit to task irrelevant features) and proposes a simple solution for this issue that empirically proves to be effective. I especially appreciate the comprehensive ablation studies that isolate effects from data augmentation and actor early stopping. The paper overall is well written and easy to follow. Some questions that I have:(1) Could you elaborate more on how to select the invariant set $\mathcal{I}$ in practice? The paper proposes random policies or initial frames, but as I understand experiments are only reported for the latter design choice.<BRK>The paper proposes an extension of the adversarial imitation learning framework where the discriminator is additionally incentivized not to distinguish frames that are different between the expert and the agent in irrelevant ways. The method relies on manually identifying a set of irrelevant frames. The paper correctly identifies an important shortcoming of GAIL and proposes a sensible and generic way to overcome it. The experiments are well designed and corroborate the claims made in the paper. EDIT: I acknowledge reading the other reviews and the author response and stand by my initial assessment.
Reject. rating score: 1. rating score: 3. rating score: 6. <BRK>If a one score comparison is not enough, the right way to go is to provide multiple scores. The principled approach followed to achieve the objective is solid and elegant. Post rebuttal: Thanks to authors for their effort for clarifications. Another major weakness is that the paper lacks a quantitative evaluation scheme for its success.<BRK>However, the advantages of the proposed model over WAE and VLAE (S.Zhao et.al 2017) are less obvious to me. It is a bit hard for me to tell whether the hierarchical latent variables help to improve quantitative results, generate better images, or learn intuitive hierarchical representations, which is the main reason that I go to mild rejection.<BRK>This paper presents a deep, latent variable model for unsupervised data modeling problems. The problem with such latent, deep generative models is that they are difficult to train reliably. Experimental results are demonstrated on various image datasets and the latent codes are demonstrated to have an interpretable meaning.
Reject. rating score: 6. rating score: 6. rating score: 6. <BRK>The authors propose  to use randomized tensor factorization in the weight space as a defense to adversarial attack, which builds upon the existing works on using randomization on the weights or activation as a defense methods. Pros:1.The idea of using randomized tensor factorization for dense is novel2. Cons:1.I don t understand why using randomization in the latent space of the weights can retain the classification accuracy on clean data. The authors say that this is because both the weights and the activations are not sparse.<BRK>Several defense techniques against adversarial attacks have been proposed, mainly adversarial training (train on perturbed inputs) and introducing random perturbation to the weights or activations of the network. I think this paper can be relevant to the community but I am not confident that this is an important contribution. The idea is interesting and addresses the problem of sparsity artifacts in randomized defense strategies, but it does not appear clearly why using tensor decomposition techniques is a sound approach for designing robust networks (besides overcoming sparsity artifacts). I believe there may be more fundamental (theoretical, principled) arguments to motivate the approach, but this is not explored in the paper: the idea is interesting but not supported by much theoretical insight. Why not do the same with a simple low rank matrix for example? Maybe this is because I am not familiar with the adversarial defense literature. The idea is interesting and definitely worth exploring but to me a more thorough discussion and analysis of why tensor decomposition techniques are relevant is missing.<BRK>The high level idea of this work is to reparameterize the network parameters W of each layer with low rank tensors, where the factor matrices are injected with randomization through randomly sampled sketching matrices. Xie et al., Feature Denoising for Improving Adversarial Robustness, CVPR192. Considering tensor factorization with randomization for network robustness makes a lot of sense but overall the experiments of this paper are not well conducted towards comparative studies with other SOTAs, although ablation study shows the considerably improved robustness from the proposed method. The main concerns of this paper lie in several aspects:1. It seems that the authors did not report their comparison to recent SOTAs (such as Lin et al, 2019) comprehensively enough, nor were the benchmark measures (missing several other attacks, especially black box ones), datasets and backbones fully aligned. It is unclear how much the architecture of a backbone can impact the fairness of comparison. 2.The authors failed to cite and compare to recent two SOTAs (listed above) which conduct large scale experiments with bigger models. And there is no discussion about the extendability/generalizability of the proposed method to these data and models.
Reject. rating score: 3. rating score: 3. rating score: 8. <BRK>This paper proposes a new scoring function for OOD detection based on calculating the total deviation of the pairwise feature correlations. The explanation could be either theoretical or empirical, while the latter can be a set of carefully designed ablation studies. 6.Why are the networks used in figure 1(a) and (b) different? The clarity of the paper is good.<BRK>The paper proposes a strategy for detecting out of distribution samples based on feature representations obtained via neural networks. The studied problem is an important problem and the experimental results show that the proposed strategy leads to some performance gains in comparison to reference methods. However, in my view the main drawback of the study is that it is based on an ad hoc methodology whose theoretical foundation is not quite clear. I cannot entirely grasp the motivation for this, as looking at the feature correlations is a bit more indirect than looking at the features themselves. It would be good to provide the justification of this choice.<BRK>This paper uses Gram matrices for OOD detection. However, this paper should more accurately reflect the contribution: this helps with far from distribution examples, not near distribution yet OOD examples. For instance, I used their code and found that their technique leads to an AUROC of 79.01% when using CIFAR 10 as the in distribution and CIFAR 100 as the OOD set. This is much worse than currently existing techniques. This paper makes a solid stride in improving the detection of garbage inputs, but the paper should modify its message so as not to suggest this helps with all currently considered OOD detection tasks. There are different senses of state of the art and these should be qualified. Is it important to do the min and max with training examples instead of validation examples? Table 1 is deceptive. The title is confusing.
Accept (Talk). rating score: 8. rating score: 8. rating score: 6. <BRK>This paper was very clearly written and easy to follow. The “Main Contributions” section was excellent as well as it allows the reader to quickly understand what the paper is claiming. I’m concerned that the subset of the test set the authors are using for evaluation isn’t representative of the entire test set. I would be interested in seeing how network size affects the performance of your technique. Overall, this is a great paper, with some interesting results presented in a tight, clear manner.<BRK>Summary: the paper introduces a novel protocol for training neural networks that aims at leveraging the empirical benefits of adversarial training while allowing to certify the robustness of the network using the convex relation approach introduced by Wong & Kolter. The discussion of the relatively weak performance of previous provable defenses on page 3 is a bit vague, e.g.the statement that "the way these methods construct the loss makes the relationship between the loss and the network parameters significantly more complex than in standard training", thus causing the "resulting optimization problem to be more difficult". The key ingredient is a novel algorithm for layer wise adversarial (re )training via convex relaxations.<BRK>Summary: This paper provides a promising new general training methodology to obtain provably robust neural networks (towards adversarial input perturbations). The definition of latent adversarial examples seems to suggest that it’s the gap between the actual set of activations corresponding to the input perturbations and the convex hull. The paper is overall well written and the algorithm is clearly described. But this is true even for normal certified training, and not sure what changes in the new training procedure.
Accept (Poster). rating score: 8. rating score: 6. rating score: 3. <BRK>This paper seeks a solution to the problem of performing imitation learning when the dynamics of the demonstrator are different from the dynamics of the imitator. The problem of dealing with different dynamics between a demonstrator and imitator is an important, but often overlooked problem in imitation learning. The authors cite the work by Ayatar on imitation learning from observing YouTube videos. Given the extensive experimental results showing the efficacy of this method I recommend that the paper be accepted. The success of BC is interesting. I thought the experiments for different action dynamics was very nice. From later discussion it appears you mean take the \phi that results in the supremum. What if actions are multidimensional, with different ranges where some actions are not important? AIRL also tries to learn a state based reward that is disentangled from the dynamics.<BRK>Review for "State alignment based imitation learning." Summary: This paper addresses the problem of learning from demonstrations where the demonstrator may potentially have different dynamics from the learner. Pros:+ The problem that the paper solves is fairly relevant, and some experiments (such as cross morphology imitation learning) are promising in concept. [4] points towards such ideas as well (noted on page 43). Further, ideas of deviation correction in the imitation learning domain have been addressed before in [5]. 2) The choice of approach (in particular, the use of the Wasserstein distance to match state distributions, and the manner of learning a local prior by training an autoregressive Beta VAE) are lacking motivation, and it is unclear if or why these choices are the best way to approach the problem. This ablation seems crucial to assessing the paper, and is lacking a deeper analysis. It appears that this is simply the addition of the KL between the policy and the prior $p_a$ to the global alignment objective (subsumed into $L_{CLIP}$), hence the reviewer questions whether this can indeed be treated as a novel contribution of the paper.<BRK>They achieve this by two objectives: one local, the other global. The local objective aligns the next state to be close to the expert s next state in each transition by first training a VAE on the expert demonstrations, and using the trained VAE in conjunction with a pretrained inverse dynamics model to compute the action that the imitator needs to imitate. The paper claims that using these two objectives results in a method that outperforms existing inverse reinforcement learning and behavior cloning approaches in settings where the imitator and expert dynamics differ. Decision:I recommend the paper to be rejected. How is this different from GAIL where we match state visitation distribution (instead of state action visitation distribution?) Why is it better/worse? The toy example in the introduction was good but the experiments did not reflect the complexity of that example.
Reject. rating score: 1. rating score: 3. rating score: 8. <BRK>In these three ways I feel this paper is not a good example of exploring the new problem of ensemble learning, even if you think that problem is a worthy one. This paper addresses the problem of combining separately learned action value functions into a single, no longer learning, action selection algorithm. A new method is proposed, called TDW, based on weighting the learner’s action values according to their recent squared temporal difference errors. Does this work make a contribution to the field of reinforcement learning? (This is also a requirement of asynchronous methods, so this criticism applies to them as well.) If a method requires this, then it is not a general method. It compares an ensemble of 10 learners with a single learner who only has as much data as one of the 10 learners in the ensemble. It is a real problem that an early proponent of combining learners makes an unfair comparison with non combined learners. The third way in which this work on ensemble RL agents is not very good it that its ensemble methods are not well suited to the task. That is, if one was going to combine learners, then there are many interesting natural ways to do this, and these are not done. It would be natural for each learner to keep track of how much experience or confidence it has in each part of the state space.<BRK>1.The paper suggests an ensemble method that leads to better performance over the execution of a single agent. The final (behavior) agent is a weighted average (or majority vote) of the members, where weights are determined by the accumulated TD error of the agent in the episode so far. The TD error serves as a confidence measure of the ensemble members in their predictions. 2.Assume that one of the Q functions, Q_i is Q*. However, it is not difficult to construct an example where the optimal policy is to go to the uncertain parts of the state space. Therefore, TDW favors determinism over uncertainty. 5.The paper links between certainty (as reflected in the TD error) and performance. If the performance criterion would be risk sensitive, e.g.CVAR, then I could agree more with the claim (maybe then performance is linked with certainty). However, certainty and performance do not go together, at least not when talking about the expected return criteria. 6.Out of curiosity: since the method requires calculating the max over Q.<BRK>Overall, this is a very interesting paper and, I think, would make a great addition to ICLR. The authors are addressing a frequently overlooked and under discussed aspect of RL: namely the instability of training that occurs when a Q function becomes decreasingly familiar with areas of a state space while it focuses on a particular "trajectory" or solution to a task. One of which is not investigated in the experiments. The experiments are ample and the approach is compared to a baseline and an alternative approach. The experiments are informative and show the superiority of the proposed approach. The literature review is good but must add two relevant works. It could be removed. Your experiments indicate that the uncertainty value causes the "preferred" ensemble member to switch during the game of breakout.
Accept (Poster). rating score: 8. rating score: 8. rating score: 3. <BRK>This paper proposes a method called AugMix, which is intended to improve model robustness to data distribution shift. AugMix appears fairly simple to implement. Additionally, a Jensen Shannon Divergence consistency loss is applied during training to encourage the model to make similar predictions for all augmented variations of a single image. Overall, I would tend to vote for accepting this paper. The method is simple yet effective, the paper is very well written and easy to follow, and experiments are extensive and, for the most part, convincing.<BRK>The paper proposes a novel method called augMix, which creates synthetic samples by mixing multiple augmented images. Coupled with a Jensen Shannon Divergence consistency loss, the proposed method has been experimentally, using CIFAR10, CIFAR100, and ImageNet, shown to be able to improve over some augmentation methods in terms of robustness and uncertainty. The idea of the approach is simple, and should be easy to be implemented. The evaluation of the proposed method is currently based on experimental evidence. I therefore have adjusted my evaluation score accordingly.<BRK>The weighted combined images would be added to the original image in convex combination. Later they train the network with adding the Jensen Shannon divergence for the posterior distributions of augmented images as the consistency regularizer. The level of experiments are wide and cover different aspects. The authors conducted the experiments for a wide range of model datasets to show the validity of their ideas. In this paper, it is mentioned that AugMix is a data augmentation method that generates data to add to the training set and after training with data augmentation, the method would be more robust to other distortions that can be added to the datasets. The method should be tested for many model datasets specifically, to support improving the  uncertainty under the domain shift idea like the paper [1].
Accept (Talk). rating score: 8. rating score: 8. rating score: 8. <BRK>Quality, Clarity, Novelty, ImpactThe paper is clear and well written, with a nice introduction to the information bottleneck method.<BRK>To better understand the proposed methods, I have a small suggestion:  Try betas in a broader range including very small betas, e.g.[0.0001/k, 0.001/k,....,1/k,10/k], for both Table one and visualization. It turns out, the proposed two architectures are better (at least alternative) choices to the other existing attribution methods. In the paper, the beta used for calculating the per sample bottleneck is among [100/k , 10/k, 1/k].<BRK>It would also be a good addition to the related work. This is a very nice paper in all the ways listed above and it should be accepted! Significance: This method could be quite significant.
Reject. rating score: 3. rating score: 6. rating score: 6. <BRK>This concept could be used to explain some empirical findings in this paper. When more than two views are presented, the learning objective is a sum over all possible combinations of two views. 3.The theoretical justification is not as strong as the generalised CCA. The paper mainly presented a simple yet effective method for self supervised learning from two views, and the generalisation is a sum over all possible combinations of two views.<BRK>This paper proposed a new self supervised learning methods by utilizing contrastive predictive coding technique. The proposed algorithm is more effective than existing self supervised learning algorithm. The presented results are encouraging.<BRK>This interesting paper on an important topic; however, its readability could be dramatically improved, especially for the reader less familiar with the problem. In order to make the paper more accessible, the authors should reorganize the introduction by breaking it down into two parts:1) a more traditional introduction   one intuitive paragraph about multi view coding  one intuitive paragraph with an illustrative example on how the proposed approach will help solve a problem; at the same intuitive level, compare and contrast it with existing approaches   one intuitive, in detail paragraph on how the proposed approach works  one paragraph summarizing the main findings/results 2) a second, new section, that will turn the current Figures 1 & 2 into a complete description of an illustrative example (the current, detailed "captions" are a good start, but they should be fleshed out into a full, detailed section of the paper)
Reject. rating score: 3. rating score: 3. rating score: 3. <BRK>Multihead attention and multi hop versions are also derived. Connection of this formulation to the existing GAT method is also discussed. Adding a rescaling on each node seems a minor structural change based on existing GCN framework.<BRK>3.The node adaptive encoder is more suitable to be called a new activation function. Then ablation study on different activation functions should be made.<BRK>This paper is well written.
Accept (Poster). rating score: 8. rating score: 6. rating score: 3. <BRK>This paper rigorously proves that if a deep linear network is initialized with random orthogonal weights and trained with gradient descent, its width required for convergence  does not depend on its depth. The theoretical contribution of this paper is very important. Although the theory in this paper is developed for linear networks, it still has important guidance meaning in practices in more areas of deep learning. And the paper is well written and easy to read.<BRK>This paper studies the role of initialization for training deep linear neural networks. However, my main concern about this work is its novelty, especially for the proof techniques used in the current paper. It seems that most of the proofs are similar to the previous work Du & Hu (2019), and the main reason that it can remove the dependence of $L$ seems to be Lemma 4.2, which can be derived using the orthogonal property of the initialization.<BRK>This paper studies the convergence of deep linear networks under orthogonal initialization. It would be interesting to see an empirical comparison of the proposed initialization and the Gaussian initialization. The difference is the orthogonal initialization compared with the Gaussian initialization in Du & Hu (2019). My biggest concern is that this paper seems to be very similar to Du & Hu (2019) in many places.
Reject. rating score: 3. rating score: 3. rating score: 3. <BRK>Strengths: The authors proposed spatially shuffled convolution to use the information outside of its RF. The idea is straightforward and easy to understand. I especially like the visualization analysis of the receptive field and the layer ablation study suggested that all layers can benefit from the proposed operation, though more for middle and higher layers.<BRK>Specifically, the authors argued that the receptive field (RF) of each convolutional filter should be not constrained in the small patch. 2.The paper is poorly written in general. However, the poor presentation and the weak experimental results and analysis make the paper overall a one under the bar of the venue. I would suggest the authors revise the paper with more well motivated formula and more solid experiments and analysis in the next submission.<BRK>The approach is motivate by a very high level discussion of biological brains. Improvements are claimed on Cifar10 but results are not near the state of the art.
Reject. rating score: 6. rating score: 6. rating score: 6. <BRK>This paper proposes a new clustering method, called CNC, which is composed of two step procedures. It would be more convincing if the NMI for other methods are also reported.<BRK>This paper presents an end to end approach for clustering. The model is trained by minimizing a differentiable loss function that is derived from normalized cuts. The paper follows the general setup of deep clustering: map data to a feature space while maintaining data distributional characteristic, and make data clustering friendly in the feature space. However, deep clustering has been around for quite a few years. It might be time to move on to more challenging benchmarks.<BRK>The method proposed in this paper is very simple and appears to work well, and so this paper represents an important contribution. In general, it might be better to just present the objective as a relaxation of the normalized cut objective.
Reject. rating score: 1. rating score: 1. rating score: 3. rating score: 8. <BRK>The paper presents a method for detecting rumours in text. Early on in the paper, the authors claim that this method:1) is more accurate in rumour detection than prior work, 2) can provide explainability, and3) does not need labelled data because it is trained on synthetic "non rumour looking rumours". This is not valid. The gene classification task does not contribute to the evaluation of the rumour detection method. The claim that the method does not need labelled data because it is trained on synthetic "non rumour looking rumours" is shaky, because 1) one could train the method on labelled data, and 2) it is not clear how "non rumour looking rumours"  are guaranteed in the synthesis phase (how are they defined?how are they evaluated to be "non rumour looking rumours"?<BRK>This paper has been well written and easy to follow. Contributions of novelty are limited. The problem of explainable rumor and fake news detection has also been well studied. Therefore, this piece of work is more a marginal extension of existing solutions. The rumor dataset is very small for a typical deep learning model. I am also curious about how many rumors in the dataset are generated by replacing words.<BRK>The model can further highlight the words that are responsible for the rumor accusation. My questions:1) The task modeled is essentially a word replacement detection problem. Is this equivalent to rumor detection? Are those choices irrelevant or critical? 3) I would like to see more discussion on the nature of errors from those models, but it s lacking in the paper. This could be critical to understand the model’s ability and limitation, esp given that it’s not looking at supporting evidences from other sequences.<BRK>In this paper, the authors proposed an interesting model to solve rumor detection problem. It achieved excellent performance on two kinds of dataset. The term ‘layered’ was a little confusing to me at the very beginning, though it is strengthened in many places around the paper. Another question is about the extended dataset with generated data, are they generated using the same distribution from G of the final model? I would like to see this paper accepted to motivate future works on fake news detection and rumor detection..
Reject. rating score: 6. rating score: 6. rating score: 6. <BRK>This paper focuses on how to partition a bunch of tasks in several groups and then it use multi task learning to improve the performance. E.g., algorithm 1 is important and should be in the main context. The experiments are based on the assumptions that the network structures (how parameters are shared across tasks) are fixed.<BRK>This submission studies how to group tasks to train together to find a better runtime accuracy trade off with a single but large multi task neural model. The authors perform an extensive empirical study in the Taskonomy dataset. I like Section 6.1 in particular, which shows the difference between multi task learning and transfer learning. My main concern is that the competition between different tasks may stem from the limited capacity of the model during training.<BRK>This paper works on the problem if training a set of networks to solve a set of tasks. The authors try to discover an optimal task split into the networks so that the test performances are maximized given a fixed testing resource budget. Training more than one networks for a few tasks is definitely a valid idea in real applications and related to broad research fields. I believe this problem setup requires a larger task set. It is unclear how better a network can do than a human in one minute or should we expect learning the task split is better than manual design. I am confused by the comparison to Sener and Koltun. If it is changing the number of channels for a single network, I believe it can be improved more.
Reject. rating score: 1. rating score: 3. rating score: 6. <BRK>The authors should revise and make it clear to the reader why we should care about this problem. Since the motion between images is artificially generated what guarantees are there that the model is learning to capture realistic motion behavior? Aligning with V1 is interesting but it does not come into play in the applications of the approach or the analysis so I am not sure why I should care. I am not sure I understand the motivation for the approach.<BRK>The recommendation of this work is based on the following reasons. First, the motivation of the proposed method is not convincing. Second, the experimental results are not sufficient to demonstrate the effectiveness of the proposed model.<BRK>The hypothesis in this paper is that the primary purpose of the cells of the V1 cortex is to perceive motions and predict changes in the local image contents. I found the hypothesis and formulation reasonable, the numerical results are supportive, it s actually interesting to see that the proposed model s motion prediction outperforms the other dedicated models. Here are some issues could be addressed further:1.
Reject. rating score: 3. rating score: 3. rating score: 6. <BRK>The goal of the paper is well stated and well motivated. In other words, the assumption is too strong to claim the goodness of the proposed scheme. The theorem statements holds for this case. Therefore, the hypothesis underlying in this paper better estimation of the gradient will lead to a better performance may not be true. At least the numerical experiments provided in this paper do justify this hypothesis. In a sense it is reasonable to evaluate the effect of the proposed modification in the baseline ES. However, since the baseline ES algorithm is not really efficient on tasks such as the one conducted in Figure 2, the usefulness of the proposed approach is not tested. Please specify in what sense the linear time version of the CMA ES do not work well and provide the evidence (references).<BRK>This paper is about improving the quality of surrogate gradients. Their proposal in guaranteed to find a descent direction. This assumption does not hold almost for all problems where random search strategies would be of use. I must add that the authors also acknowledge the severity of these assumptions. Such a study would have shed light on the computation time and efficiency. The authors solve MNIST problem, which can be quite efficiently solved with a variant of (accelerated) gradient method. Solving it with ES and then improving the result with the proposed approach is not satisfactory. Reinforcement learning experiments could have been noteworthy but the authors have solved quite small problems and did not compare their results extensively against contender approaches like augmented random search.<BRK>The paper proposes an additional mechanism for generating surrogate gradients by simply using previous gradient estimates as surrogate gradients, and derives a convergence rate for when this iterative estimator will approximate a fixed, true gradient (e.g.for linear functions). Finally, the paper applies the estimate to two tasks: MNIST classification and robotic control via reinforcement learning, demonstrating improvements on both compared to standard ES. I think this is a nice contribution, and I enjoyed reading this paper, with one major caveat regarding some of the experiments. The paper is clearly written. 14 as another method for using surrogate gradients in optimization. For both examples (MNIST and RL), it is crucial to add the algorithm from that paper as a baseline. It would be nice to compare the orthogonal epsilon to the N(0,I) epsilon case. Use a more semantically meaningful subscript than `our` for the proposed gradient estimate.
Reject. rating score: 1. rating score: 1. rating score: 1. <BRK>The idea is to train multiple GANs on subsets of the data, and once in a while use these to generate new data aid the training of the final model. Why is the proposed method good?Is it optimal in any sense? In recent years there have been quite some work on learning of data augmentation which isn t cited in the paper. Intuitively, I would expect that it is hard to train a GAN (or another generative model) than it is to train a classifier, so why is it s a good idea to augment the dataset using GANs?<BRK>This paper proposes a new approach to data augmentation using GANs, called Parallel Adaptive Generative Data Augmentation (PAGDA), in which the following procedure is used:  (1) First, GANs are used to generate different batches of data points based on the starting training set, and then these batches are added back to the training set  (2) This is done using an ensemble of K GANs where each GAN is trained on K 1/K portion of the training set, and the resulting samples are sent to the training sets of the other K 1 GANs  (3) The generators are sampled from proportional to their (exponentially normalized) inception scoresOverall, this paper unfortunately should be rejected due primarily to inadequate experimental evaluation. Specifically, the paper proposes a new method (and a very complex and seemingly arbitrary one at that), in a space where there are many pre existing approaches, and then neither (a) compares to any pre existing baselines or approaches (other than the most trivial of manual data augmentation strategies), nor (b) experimentally justifies or explores any of the design choices in the extremely convoluted method design. How well does it work? How exactly is the sampling done?).<BRK>This paper proposes an method based on multiple GANs trained on different splits of the training data in order to generate additional samples to include to the training data. I rated this paper as weak reject because this paper is weak on several aspects  In related work many similar approaches are missing (see below). The main novelty of the paper seems to be the use of multiple GAN on different splits of the data, which seems a bit limited. The experimental evaluation is limited (see below).
Accept (Poster). rating score: 6. rating score: 6. rating score: 6. <BRK>The paper proposes an imitation method, I2L, that learns from state only demonstrations generated in an expert MDP that may have different transition dynamics than the agent MDP. This approach maximizes a lower bound on the likelihood of the demonstrations. Overall, I enjoyed reading this paper. It would be nice to include a behavioral cloning (e.g., BCO) baseline in the experiments. For example, how dependent is the method on the diversity of trajectories \tau generated by the agent in line 5 of Algorithm 1?<BRK>In the new environment (where we we aim to conduct imitation learning), the expert demonstrations are unavailable, making the objective function (5) unavailable. To deal with this, the authors derive a lower bound to replace (5). In the experiment section, the performance of the proposed method is clearly demonstrated. The submission offers few discussions on the error may induced by replacing (5) with the lower bound. Will such an error decrease or converge to 0, as we iterate according to the algorithm?<BRK>Summary:The manuscript considers the problem of imitation learning when the system dynamics of the agent are different from the dynamics of the expert. The paper proposes Indirect Imitation Learning (I2L), which aims to perform imitation learning with respect to a trajectory buffer that contains some of the previous trajectories of the agent. Instead, the performance of the expert which is currently not shown would be better suited as a baseline level. Similarity is hereby measured by the score of a WGAN critic trained to approximate the W1 Wasserstein distance between the previous buffer and the expert distribution. Decision:I think that the submission is below borderline in its current state. Supporting Arguments:  Novelty / Significance: Imitation learning from state only observations under dynamic mismatch is an important problem and a promising approach for training robots by non experts. The description of MaxEnt IRL seems quite wrong. The submission merely states that the buffer is a priority queue structure of fixed lengths, where the trajectory priorities are given by the average state score based on the Wasserstein critic. Evaluation:The presentation of the experimental results seems odd.
Reject. rating score: 1. rating score: 6. <BRK>Even for these two datasets the authors use very outdated networks, e.g., the best error rate for CIFAR 10 is in order of 6 percent. 1) The initial learning rate for CIFAR 10 and CIFAR 100 are different, respectively 0.1 and 0.0003. The use of such small initial learning rate for CIFAR 100 is not motivated especially given that it is usually in order of 0.05 or 0.1 when resnets are considered. 2) The authors don t compare to cosine annealing without restarts which is a pretty strong baseline. 4) The proposed method has its own hyperparameters which greatly influence the results as shown in the appendix. I suspect that setting these hyperparameters is exactly what controls the slope of the learning schedule. Overall, the results are not convincing. The authors show that the previous adaptive approaches don t work well on the CIFAR datasets (despite the fact that their authors claimed the oppositve) and I don t think that the paper contains enough material to avoid the situation that futures approaches will claim similar things about the current study.<BRK>In this paper, the authors introduce a hypergradient optimization algorithm for finding learning rate schedules that maximize test set accuracy. The proposed algorithm adaptively interpolates between two recently proposed hyperparameter optimization algorithms and performs comparably in terms of convergence and generalization with these baselines. Overall the paper is interesting, although I found it a bit dense and hard to read. The proposed algorithm seems interesting however, and the experimental results look quite impressive. This seems to indicate that MARTHE is somehow more sensitive to beta than the other variations used. Why is this a reasonable thing to do? Otherwise, it doesn t feel like a fair evaluation? To the best of my knowledge, the final achieved accuracies achieved with MARTHE however seem quite competitive with the best results typically reached with tuned SGDM on the convolutional nets used in the paper. 3.The learning rate schedules found by MARTHE seem to be somewhat counterintuitive. 4.Is it possible to provide some sort of estimate of how much computation MARTHE requires compared to a single SGDM run? It would really improve the paper if the readability was improved, as well as if larger experimental results were included.
Reject. rating score: 3. rating score: 3. rating score: 3. <BRK>Summary:The paper proposes a flow based model for generating molecules, called Graph Residual Flow (GRF). "MintNet [...] derives the non singularity requirements of the Jacobian of the (limited) residual block"Also vague and not informative. Suggestions for improvement:"The decoders of these graph generation models generates"  > generate"The state of the art VAE based models [...] have good generation performance but their decoding scheme is highly complicated"In what way is it complicated?<BRK>However, I have two main concerns:  Conceptual novelty seems to be limited. The model is based on graph residual flows (GRF), which is a graph variant of normalizing flows. Experimental results do not seem to suggest improvements over the state of the art. Other comments:  The detailed pseudocode of Algorithm 1 isn t really necessary considering it is just a fixed point iteration.<BRK>ICLR 2018, Syntax directed generative model for structured data. The author thinks that GraphNVP only update one row each time, which is less efficient, and the model can only cover a limited number of mappings.
Reject. rating score: 3. rating score: 6. rating score: 6. <BRK>2.To model dynamics, the paper proposes a residual update rule inspired by Euler’s method to solve ODEs. The paper further contains experiments illustrating features of the model such as disentangling dynamics and content, and interpolation of dynamics in the latent space. The idea of predicting video using state space models is interesting and promising. I therefore suggest rejection in its current version. 2.The advantages of the residual update rule are not made clear enough. The parallels to the ODE literature seem tenuous. The main advantage described in the paper is the ability to synthesize videos at different frame rates, but interpolation over such short time horizons is not a hard problem. At least, the paper should compare to existing methods for frame interpolation. More experiments to show the advantage of the proposed update rule would be helpful. For example, Fig 2 (b) shows that the proposed dynamics model is better than an MLP or GRU on deterministic Moving MNIST, but is this also true on real datasets, which have much more complex dynamics? The metrics are probably dominated by relatively uninteresting features such as the quality of the static background.<BRK>Contributions: this submission proposes a video pixel generation framework with the goal to decouple visual appearance and dynamics. The latent dynamics are modeled with a latent residual dynamics model. Empirical evaluations on moving MNIST show that the proposed residual dynamics model outperform MLP or GRU. On more challenging KTH and BAIR datasets, the proposed method achieves on par or better quantitative performance with previous methods, and have nice qualitative results on content "swap" and dynamics interpolation. Assessment:  To my knowledge, the proposed model is novel for video generation. It would also be interesting to see evaluation on more challenging datasets, such as Human3.6M. Therefore I recommend weak accept of the submission.<BRK>The paper proposes a video prediction model which explicitly decouples frame synthesis and motion dynamics. This is a very subtle change (compared to the current models) that can result in higher quality predictions. On the main proposed method, it is a very subtle but reasonable change. This is where the paper can be improved. For the experiments, although they are quite comprehensive, there is still room for improvement. Is it from a different architecture or the separation of dynamics? Overall, this is a well written paper with clear motivations and goals. I find the impact of the paper to be marginal (given the quality difference with already existing models) which can be improved by emphasizing more on other aspects such as disentanglement.
Reject. rating score: 3. rating score: 8. <BRK>International Conference on Machine Learning. The ideas presented in the paper are interesting, but I have concerns about the scalability of such an approach. Some parts of the experiments section does not seem clear to me, Does the proposed approach use a network per task? One major concern is that the only high dimensional experiment is a swimmer and it is not immediately clear how much do we gain there. I would recommend evaluation in a variety of high dimensional domains such as other instances in Mujoco, and visual domains. In particular, the proposed ideas would make a stronger case if the baselines included other multitask hierarchical agents such as [4] for example. A discussion including some of the missing relevant related multi task literature would also be helpful [1,2,4,5,6].<BRK>This paper is about learning hierarchical multitask policies over options. Once the prior is learned, it is fixed. The parameters of the posterior policies are adjusted via an adapted version of A2C. I liked the flow and the organization of this paper. However, when the ratio is 1 or less than 1, the value of (6) would increase, and both cases would have made the posterior more like the prior. * The term 1,2,3 in (6) are weighted equally by beta and cannot be fine tuned to desired trade offs. Misspelling and typos:* page 5: optimized is misspelled in "Details on how we optimiszed..." * Appendix C.1: should there not be a superscript dash on A_{\pi_i} since the superscript dash carries the meaning that the term is a constant.
Reject. rating score: 3. rating score: 3. rating score: 6. <BRK>Motivated by biological visual systems, this paper investigates whether the representations of convolutional networks for visual recognition are invariant to identity preserving transformations. The main weakness of this paper is that the approach is mostly data augmentation, which is standard. However, these loss functions are widely known and applied as a baseline in metric learning. In my view, the novelty of this paper lies in the application of standard approaches to a standard vision problem. Due to this, I feel this contribution is not sufficient for ICLR.<BRK>This paper introduces an unsupervised learning objective that attempts to improve the robustness of the learnt representations. This approach is empirically demonstrated on cifar10 and tiny imagenet with different network architectures including all convolutional net, wide residual net and dense net.<BRK>The paper proposes to explicitly improve the robustness of image classification models to invariant transformations, via a secondary multi task objective. The idea is that the secondary objective makes intermediate layer representations invariant to transformations of the image that should lead to the same classification. This is an interesting idea (I am not specialist enough on image classification to know whether this idea has been tried before.) and highly relevant to this conference. I can think of three changes to the paper that would flip my review to a Strong Accept:* Try using the multi task objective as a pre training, followed by fine tuning without multi task objective. Therefore, it is reasonable to expect an improvement in the classification performance if e.g.the batch size or the learning rate schedule are better tuned for this new learning objective." I wish you had used a different symbol. Do you actually need the concept of "invariance score" at all?
Reject. rating score: 3. rating score: 3. rating score: 3. <BRK>This paper introduces a new method for training a classifier that simultaneously optimizes for a fairness criterion and robustness to data poisoning. The method is shown to increase measures of fairness and reduce inaccuracy on poisoned data relative to classifiers that only consider accuracy or fairness. Extensive results are shown for both synthetic and real benchmark data sets. I would lean to reject for the following reasons: 1) the problem is not well motivated.<BRK>The fact that this is the first fairness related method that additionally deals with robustness, makes it also difficult to judge the performance of the method. This paper combines adversarial fair training with adversarial robust training. The paper is clearly written and technically sound.<BRK>This paper proposes to use a GAN style approach for training a classifier that is robust to data poisoning and can achieve a pre specified notion of group fairness. Experimental results are insufficient to argue improvement over the AD. I think an interesting direction could be to consider data where labels are subjective. Lastly I think that the discussion of the prior related work on fairness is incomplete. This paper exclusively covers group fairness, which indeed has been shown to have some disadvantages.
Reject. rating score: 3. rating score: 6. rating score: 6. <BRK>I think it is an interesting paper, but not enough as a conference paper, maybe a workshop paper. My score for this paper is weakly rejected because (1) the concept of self supervision is not first proposed by this paper. In the paper, it mentioned because the result is not better, but the author should still provide them.<BRK>According to the last paragraph of Section 6 this experiment should be feasible. Overall, I think it’s a good paper presenting a thought provoking idea. Pros:+ The paper is well written and easy to read.<BRK>More qualitative   analysis in this direction would be useful to appreciate the   proposed approach. The paper is written very well and the technical development andmotivations for each decision are well discussed and argued. The experiments are interesting and not overstated.
Reject. rating score: 1. rating score: 6. rating score: 6. <BRK>Summary: This paper proposes an uncertainty measure called an implied loss. It seems the Bayes ratio and Bayes factor to be a different thing. 2.INTRO: first contribution, accurate estimates of the probability that the classification of the model on a test set is correct. I appreciate the authors  effort to modify the paper. I feel that the structure of the modified version is better than the first version. I found the caption is hard to understand. Despite all that, I do like the idea of introducing the Bayes factor in this paper. The authors have modified many of my concerns, but still several of them were not addressed. I also emphasized the comments for parts that are unrelated to clarity (please see below). For these reasons, I decide not to change my score. Below are my comments after reading the rebuttal (which some of them may be overlapped with the issues that were not addressed in the revised version).<BRK>Passing by, the authors propose a measure based on the norm of the gradient to detect adversarial examples that seems to work well. On the other hand, I found the paper difficult to follow because the contributions are scattered over the paper and the appendices without clear link. The paper also lacks a conclusion. Overall, it seems to me that with a bit of restructuration, the paper would be an interesting contribution   for instance in terms of the assessment of dropout variance vs direct model entropy. after author rebuttalThe rebuttal answer most of my concerns, and I raised my score to weak accept. Overall, even if there is no clear evidence that one method to evaluate uncertainty is consistently better than the others, I feel that the idea of using Bayes factors to compare uncertainty measures is possibly impactful.<BRK>This paper shows that the maximum softmax probability is useful for uncertainty estimation on in class data and not just for detecting out of distribution data. They argue this with the Bayes Ratio, which here is an uncertainty estimation quality measure that seems worth exploring more for assessing the quality of uncertainty estimation techniques. This paper is currently borderline, in that what it proposes is simple, but perhaps too simple. Update: The other reviewers are concerned about lack of clarity which is separate from why I like the paper.
Accept (Poster). rating score: 8. rating score: 6. rating score: 1. <BRK>The paper introduces an additive parameter decomposition (APD) approach to continual learning in the sequential task classification setting and evaluates it across a number of dimensions, including task order robustness, which is comparatively less well researched. Performance is on par with STL, also in terms of robustness to order. Order robustness measures are introduced and motivated. Could you please report some results in the experimental setting used by one of your baselines in the original paper? I am inclined to recommend acceptance due to novelty of order robustness analyses and competitive properties of the method, but I would like clarifications to my experimental questions.<BRK>The paper proposes a training framework that: (i) can efficiently handle catastrophic forgetting in a large number of tasks(ii) is robust to the ordering of the tasksThe high level idea is to decompose learning parameters into two sets   one set that depends on the task and one set that is task agnostic. In this context, it would be helpful to know the variance associated with the other reported results as well. But none of their baselines have this mechanism built in. Without such a comparison, it is difficult to comment on the benefits of the approach. EWC [2] and the authors  results indicate that regularization in the parameter space does not work as well as regularization in the function space.<BRK>Therefore, the results are still not convincing to me. However, we agree that the term ‘large scale continual learning’ may be misleading and have renamed the paragraph to ‘scalability to large number of tasks’ in the revision. But none of their baselines have this mechanism built in. Without such a comparison, it is difficult to comment on the benefits of the approach." Moreover, the architecture used for this experiment is LeNet which oversimplifies the problem to address. [Authors  response:] The only hyperparameters required for APD are \lambda_1 and \lambda_2, which controls the capacity of the task adaptive parameters and model drift respectively. We directly followed all experimental settings on the paper and the code of the authors (https://github.com/joansj/hat).
Reject. rating score: 3. rating score: 3. rating score: 3. <BRK>The idea seems well motivated and somewhat new though not revolutionary,and the new data sets are nice (though see comments below about how I m not qualified to evaluate them),but I don t understand why the proposed algorithm wasn t evaluated on existing data sets as well,and I don t understand why it wasn t compared against other algorithms that purport to do the same thing. This paper proposes a new algorithm for synthesizing images and controlling various attributes of the images, but this has certainly been done in the past,so why can t your technique be compared to those prior techniques? Detailed Comments:> More importantly, only using 5% of the labelled data significantly improves the disentanglement quality. >  Generative adversarial networks (GANs) (Goodfellow et al., 2014) have achieved great success at generating realistic images, such as StyleGANStrange sentence   styleGAN is not itself a realistic image. Fig 1 is good. > AC StyelGAN>  FCStyleAGN> by symmetryI don t follow this part. It s possible I m misunderstanding something about the description of this, however.<BRK>  A.Summary  This paper proposes to train a new conditional GAN model that allows for controllable image generation by changing the input factors of variations (e.g.object color). I voted for Weak Reject because this paper presents a fairly incremental advance over what has been done (e.g.HoloGAN, StyleGAN, AC GAN). Therefore, the claim that only 5% of supervised labels is required may not carry over to larger scaled datasets with larger scene variability. Plus, I don t see any baseline whatsoever being compared with the proposed methods here. The author wrote "Our work extends the above works by scaling up the disentanglement learning to high  resolution images, and emphasizing the importance of supervision in controllable generation."<BRK>The authors demonstrate that AC StyleGAN performs well in fully supervised settings achieving the desired conditional generation, but, when controlling only a subset of factors, does not correctly disentangle. While the paper has a variety of contributions (most notably introducing two new datasets and modifications of StyleGAN) and interesting results (such as relatively high performance with only 5% labeled data and the disentanglement issues when controlling a subset of factors), the core contributions of new datasets and architectures are not validated or analyzed rigorously enough. In order to consider acceptance based on the value of the architectures proposed in the paper, there should be some direct evidence that the proposed Style GAN modifications AC/FC, outperform prior architectures. 2) Given the core contributions of new architectures for disentanglement, the lack of any results on real, non synthetic, datasets in order to validate these (for instance, CelebA) which significant prior work on disentanglement has been evaluated on is an unfortunate omission. 3) The authors claim their method is effective in the semi supervised setting and demonstrate this by showing relatively strong performance in a low data regime. The addition of an ablation / analysis demonstrating the contribution of the second term in settings where labels are also available would strengthen the author s claims and fully demonstrate the model is an effective semi supervised learner.
Reject. rating score: 3. rating score: 6. rating score: 8. <BRK>The method makes sense, however the choice of space in which the sample selection isbeing done is not well motivated or validated. Summary:The paper proposes a method for sequential and adaptive selection of training examples to bepresented to the training algorithm. The selection happens in a latent space, based on choosingsamples which are in the direction of gradient of the loss in the latent space.<BRK>This paper proposes a novel training framework which adaptively selects informative samples that are fed tothe training process. The adaptive selection or sampling is performed based on a hardness aware strategy in the latent space constructed by a generative model. The idea is intuitive and easy to follow. i.e., for a class of images, which images should be informative?<BRK>Overall, the paper is very well written and easy to follow. During training of a DNN framework, samples are selected in the latent space and then decoded via the Decoder in VAE to generate the input for DNN framework. The major difference is that this work can model the sample distribution and thus select samples based on the gradient w.r.t.the samples in the latent space. This is very novel to me.
Reject. rating score: 3. rating score: 3. rating score: 6. rating score: 8. <BRK>This paper aims to solve the unfaithful generation problem for a specific data to text generation task, i.e.wikibio dataset. Furthermore, the experimental results are not convincing. This score is used in both training and testing. Overall, I think this is an interesting idea.<BRK>The authors propose several approaches to making a data to text generation system more precise, that is, less prone to hallucination. In particular, they propose an attention score, which attempts to measure to what degree the model is relying on its attention mechanism in making a prediction. This paper is well motivated, timely, and it presents several interesting ideas. However, I think parts of the proposed approach need to be better justified. It would be nice to also verify empirically that this is important.<BRK>This paper studies the problem of data to text generation so that the generated text stays truthful to the data source. The idea of the paper is use a learned confidence score as to how much the the encoder decoder is paying attention to the source. The problem is of great pragmatic interest. 3.There are some importance baseline missing, such as [1], [2], [3][1] Marcheggiani, Diego, and Laura Perez Beltrachini.<BRK>This paper presents a method for conditional text generation that has higher factual precision, minimizing hallucination of facts. The method involves predicting confidence of generation at each time step and using this confidence measure to skip tokens during generation and calibrate output probabilities in test time. The presented results are pretty good! Human evaluation of models is notoriously difficult, more details would give some more weight to the results. I think this is a well written paper with thought out experiments. Could be an appendix.
Reject. rating score: 3. rating score: 3. rating score: 6. <BRK>The authors propose a model (CAST) which takes advantage of the additional context to perform the style transfer. Overall, I am not convinced that the context is useful in the proposed tasks (Reddit context and Enron context), for 2 reasons:* In the examples shown in Table 4, it seems possible to complete the proposed contextual style transfer tasks without the additional context (i.e., "Do y’all interface with C/P"  > "Do you all interface with C/P" for formal >informal transfer). I don t think there are ablations on all components of the model (i.e., the style classifier).<BRK>The paper proposes a new task for text style transfer, based on the idea that the the surrounding context of a sentence is important, whereas previous such tasks have only looked at sentences in isolation. 2) it s not clear to me what is meant by "style" in this paper. (which classifier?) I also feel that the collection of the data is a contribution, and this could be a useful dataset to the community.<BRK>The input context is the main highlight of this new task. The paper proposes a model that can use both parallel and non parallel data. In addition to the existing non parallel corpora, two datasets with crowdsourced parallel sentences were collected. Review  The major contributions of this paper are the new task setting and the accompanying parallel corpora. The proposed method is a natural extension of previous style transfer methods. If I am not missing anything, the model and the experiments are likely correct. In addition to the surrogate metrics, the paper also presents human evaluation, which strengthens the results.
Reject. rating score: 3. rating score: 3. rating score: 3. <BRK>This work injects a multi head co attention mechanism in GCN that allows one drug to attends to another drug during drug side effect prediction. The motivation is good with limited technical novelty. The paper is well written and well organized. It could be also benefited by including VGAE.<BRK>In this paper, the authors proposed a method to extend graph based learning with a co attentional layer. 2 According to the paper, bond types will be encoded as e_{ij}. Combining the two methods do provide insights into understanding the interactions between graphs and get really good results on DDI prediction, but the novelty is limited. This might cause some confusion in real application.<BRK>The paper presents a model to classify pairs of graphs which is used to predict sided effects caused by drug drug interactions (DDI). The contribution of this work is to add attention connections between two graphs such that each node operation from one graph can attend on the nodes of the other graph. But works from the benchmark experiments are not explained. I think the GNN would be more powerful if both nodes "i" and "j" are input into the edge operation. In summary, the main contribution of the paper is to add attention connections between two graphs.
Reject. rating score: 3. rating score: 6. rating score: 6. <BRK>Weaknesses of the paper:  Much of the presentation is vague or opaque. The details of the audio processing are omitted: STFT parameters are stated, but not the sampling rate.<BRK>The priors are well motivated by the dynamics of audio. * Good ablations and quantitative comparisons to baselines. * The scaling of the technique is not supported by the current experiments.<BRK>This paper introduces deep audio prior (DAP), which uses CNN S prior to perform classical tasks in audio processing: source separation, denoising, texture synthesis, co separation.
Accept (Poster). rating score: 8. rating score: 8. rating score: 6. <BRK>The authors emphasized the importance of the universality and limitation results in the introduction and paragraph after Corollary 3.1. DecisionThis paper gave us a new approach to analyzing the expressive power of graph NNs. For now, I am tending to accept the paper. How powerful are graph neural networks?<BRK>Lanczosnet: Multi scale deep graph convolutional networks. This may open a new direction for the community. 2, The depth and width dependency results are novel in the context of GNNs. I am not an expert of distributed computing and I did not check all the proofs thoroughly.<BRK>This paper studies theoretical properties of GNN in particular their expressive power. ###The authors added experiments supporting their theoretical results. I am upgrading my rating to weak accept. This paper makes a connection between GNN and the locality notion developed in distributed computing.
Reject. rating score: 3. rating score: 3. rating score: 6. <BRK>Unlike the original work, the proposed method introduces several improvements: 1. an updated loss function, that adds distance constraints between *all* the particles of the object. 2. recurrent training, when the predictions are fed to the inputs. 3. adding dropout. Writing:The paper is relatively well written and easy to follow. In addition they report results for trajectory sampling (qualitative) and model free RL, where using their model as a stochastic simulator seems to have positive impact on agent training. As for the experimental evaluation, it seems like important baselines are missing, and the model seems to be very sensitive to hyperparameters (see questions). It seems like there is a common concern about the novelty among reviewers: improvements over HRN are quite incremental. Although authors provide a verbal justification for not comparing to another strong baselines, I do not see why would it not be possible to compare methods in the similar settings, even though that baseline might be more limited.<BRK>SummaryThere has been work on deep learning based forward dynamics model to learn the dynamics of physical systems. In particular, the hierarchical relation network (HRN) as proposed by Mrowca et al.(2018).However, HRN is deterministic. This paper builds on top of HRN. It demonstrates the two techniques improve the efficiency and performance of model free reinforcement learning agents on several physical manipulation tasks. StrengthsLearning physics models which accounts for the multi modal nature of the problem is very important. Graph convolutional dropout is one method to deal with the multi modal nature of the problem. The tasks evaluated are not very sophisticated. There should be a discussion on the different type of methods to account for uncertainties, e.g.bayesian neural networks and how they differ in terms of multi modal predictions.<BRK>The question is relevant and interesting. The authors bring in two improvements to the HRN model by (Mrowca et al., 2018). The description of the system is very verbose, but a concrete description of the system is only in the references, which make the manuscript hard to read and it does not fit well in a conference where one cannot assume that the audience is not an expert of this particular subfield. One could use the system in Figure 1 as a more thorough example. The results looks good and valuable, altough the images provided are quite small.
Reject. rating score: 3. rating score: 3. rating score: 3. <BRK>Summary:  key problem or question: assessing / improving the robustness of object detectors to image corruptions (simulated fog, frost, snow, dragonfire...);  contributions: 1) a benchmark (obtained by adding image level corruptions to PASCAL, COCO, and Cityscapes) and an experimental protocol to measure detection robustness , 2) extensive experiments quantifying the severe lack of robustness of multiple state of the art models, 3) experiments showing that data augmentation via style transfer (Geirhos et al, ICLR 19) improves robustness at little cost (at most  2% performance degradation on clean COCO images). What is, in the authors  opinion, the main differentiator of this submission? What is the human robustness for the new corruptions not present in Geirhos et al 2019? The paper is well written and I enjoyed the multiple pop culture references to Game of Thrones. What is the impact of the choice of style images on robustness induced by stylization?<BRK>This paper introduces a  benchmark to assess the performance of object detection models when image quality degrades. Three variants of detection datasets, termed PASCAL C, COCO C and Cityscapes C, are introduced that contain a large variety of image corruptions. Further, this work shows that a simple data augmentation trick of stylizing the training images leads to a substantial increase in robustness across corruption type, severity and dataset. The paper is well written and easy to follow. However, my main concern is the novelty in that the proposed approach is just an extension of [1]. [1] introduced corrupted versions of commonly used classification datasets (ImageNet C, CIFAR10 C) as standardized benchmarks.<BRK>This paper presents a benchmark for measuring robustness to input image corruption in object detection settings. The paper proposes a benchmark for this task, and proposes a simple data augmentation technique for this task. 3.I like the proposed simple data augmentation procedure and the experimental finding that data augmentation with such a procedure leads to models that are robust to held out, previously unseen perturbations. The paper derives them from an earlier paper called "Benchmarking neural network robustness to common corruptions and perturbations", and thus I am not even sure if the proposed set of perturbations should be viewed as a contribution of the current paper. Similarly, the proposed method is simple and intuitive (which is good), but it will help if there were more comparisons to set the paper in context of related work.
Reject. rating score: 1. rating score: 1. rating score: 1. <BRK>The proposed method can be technically flawed. Adding a consent to the max will not guarantee privacy unless you account for the privacy cost for testing whether the distance of f(D) is larger than 2. Since the whole privacy analysis of PATE is based on the privacy guarantee of the Noisy ArgMax, the epsilon calculated here is voided.<BRK>This paper studies the teacher ensembles setting for differentially private learning. This paper proposes to add a constant c to the largest count before perturbing and releasing the counts. Because of this, the proposed method cannot guarantee the amount of differential privacy as the paper claimed.<BRK>To improve the privacy utility tradeoff, this manuscript proposes a voting mechanism used in a teacher student model, where there is an ensemble of teachers, from which the student can get gradient information for utility improvement. I can understand that by adding the large constant C, the identity of the maximum count could be preserved with high probability, leading to a better utility on the student side. Hence it is not clear to me whether the final composition will still be differentially private?
Reject. rating score: 3. rating score: 3. rating score: 6. <BRK>This paper studies data augmentation in the regime where labels for the augmented datapoints are known. The results of this study are then used to motivate their “X regularization” method, a semi supervised learning algorithm which they test on the task of improving accuracy of adversarially trained models. The authors report improvement in accuracy of adversarially trained classifiers on CIFAR 10 when X regularization is applied. While I think the paper could be potentially interesting to the ICLR community, I am currently leaning towards rejection for the following two reasons: (i) I am confused by significant chunks of the theoretical derivations;  (ii) the paper is hard  follow at several places, seems to be hastily written, and not well placed in the existing literature. Also, is it true that the green points in the middle plot were not chosen at random? I would suggest including some benchmark results (even if this only means restating numbers from other papers with citation).<BRK>This paper describes situations whereby data augmentation (particularly drawn from a true distribution) can lead to increased generalization error even when the model being optimized is appropriately formulated. On balance, I find the theoretical investigation and explanation for "how does this happen" to be more compelling than the presentation of X regularization.<BRK>Motivated by these issues, the authors propose "X regularization" which requires that models trained on standard and augmented data produce similar predictions on unlabeled data. The paper includes a few experiments on a toy staircase regression problem as well as some ResNet experiments on CIFAR 10.Review: Overall I think this paper provides an interesting perspective on when and why augmented data can increase test error. I think the main ways the paper could be improved are a) better explanation of connections between the theoretical assumptions and practice and b) better treatment of "X regularization".
Accept (Spotlight). rating score: 8. rating score: 8. rating score: 6. <BRK>The authors model A SGD as a dynamical system, where parameters are updated with delayed gradients. This is a nice paper with a large number of interesting theoretical and experimental results, and I believe it should be accepted. Is it meaningfully different than just lowering the learning rate?<BRK>I like the idea to observe trajectories ``leaving minimum . The authors also propose shifted momentum that utilize momentum for asynchronous training. Strength:  Theoretical formulation and analysis in this paper is nice and elegant. Provide theoretical insights of A SGD with momentum, which is important.<BRK>The authors introduce a theoretical model for delayed gradients in asynchronous training. Authors derive stability bounds for pure SGD (learning rate needs to decrease with delay) and for SGD with momentum, where they introduce a nice momentum formulation that improves stability. I am very grateful for the authors  response.
Reject. rating score: 1. rating score: 1. rating score: 3. <BRK>This manuscript develops a metric learning with non Euclidean error terms for robustness and applies in to data reduction to learn diagnostic models of Alzheimer s disease from brain images. If it was done using standard paired t test, this is incorrect are the folds are not independent.<BRK>This paper proposed an unsupervised method to make better usage of the inconsistent longitudinal data by minimizing the ratio of Principal Components Analysis and Locality Preserving Projections. 2.The paper is poorly written. If the same theorem has already been proved by previous work [Wang et al., 2014], it is not necessary to prove it again in this paper.<BRK>The motivation of this work is to address the issue of inconsistent and missing data in longitudinal clinical studies. The main goal of this work is to learn a dimensionality reduction representation by combining locality preserving projection (LPP) and and the global pattern by principal component analysis (PCA). The authors proposed a method to overcome the issue of a direct computation of alfa.
Reject. rating score: 1. rating score: 1. rating score: 3. <BRK>The authors never compare their system against a simple baseline when one just overrides the actions of the agent. Pros:+ In some cases, images produced by the system look appealing.<BRK>The addition of constraints is technically very simple.<BRK>The authors should provide more examples of why this particular capability is relevant and how it leads to interesting outcomes.
Reject. rating score: 1. rating score: 6. rating score: 6. <BRK>This paper proposes a method for semantic segmentation using "lazy" segmentation labels. Lazy labels are defined as coarse labels of the segmented objects. The proposed method performs better than the same method using only the weakly supervised labels and the one that only uses the sparse labels. The novelty of the method is very limited. It is a multitask UNET. The method is compared with one method using pseudo labels. However, this method is not SOTA. The method should be compared on standard and challenging datasets like Cityscapes, PASCAL VOC 2012, COCO, KITTI...<BRK>Here is one that can be cited:Label super resolution networks (https://openreview.net/forum?id rkxwShA9Ym)Major comments:  The motivation for the specific structure of the multi task blocks is not clear  The object boundaries labels can be noisy (i.e s(2) can have noise). Can they comment on the applicability of the prior work suggested above? Can this work be used for segmentation and prediction on crop data? Results:  It seems as if the improvement over the PL baseline (pseudo labels) is incremental? Can they give a more thorough comparison in terms of human effort? It is interesting to note that only 2 images give 0.82. What is the performance of MT U net without the SL images (i.e without task 3)? Table 2 does give some intuition, but authors should add another row with multitask WL  Table 3: How well does MDUnet do with 9.4% SL data?<BRK>With some fairly standard assumptions and simplifications, the loss in Eq.3 becomes rather straightforward (weighted cross entropy)The actual network architecture described in Section 3.2 takes a standard U Net as a starting point and modifies it in a fairly targeted way for the different expected types of annotations. These annotations (Section 3.3) are cheaper than full labels on a same size dataset; it is not completely clear, however, if the mentioned scribbles need to capture each instance in the training set, or if some can also be left out. The experimental evaluation is done reasonably well, although I am not familiar with any of the presented data sets. Finally, unmodified U Net is by now a rather venerable baseline, so I’m also wondering how the proposed multi task learning could be used in other (more recent) architectures, i.e.whether the idea can be generalized sufficiently. While I think the ideas per se have relatively minor novelty, the combination seems novel to me, and that might warrant publication.
Reject. rating score: 3. rating score: 3. rating score: 6. <BRK>In this paper, the authors present a new adaptive gradient method AvaGrad. We are also concerned about the novelty of the results, and believe most of these results have appeared in previous work. This is especially important given that this paper proposes a new optimizer. However, I have a few concerns about the paper, which I will list below. The convergence rate analysis for this case is fairly simple, and I am not sure if analyzing RMSProp/Adam in this setting should be considered a significant contributions of the paper. b) How is the optimal hyperparameters (learning rate and damping, i..e, epsilon, parameters) selected? My score reflects this. Adaptive methods for nonconvex optimization. A few more minor comments:1. But this statement seems misleading since it is not clear whether the worse rate is due to the analysis (which gives an upper bound) or the algorithm? After discussion with other reviewers, we are still not convinced that the hyperparameter tuning in the experiments (especially the baselines) is rigorous enough.<BRK>Summary:This paper proposes a new adaptive method, which is called AvaGrad. The authors first show that Adam may not converge to a stationary point for a stochastic convex optimization in Theorem1, which is closely related to [1]. They then show that by simply making $eta_t$ to be independent of the sample $s_t$, Adam is able to converge just like SGD in Theorem2. It is good to see some research directly addresses this problem. The results on the image datasets seem too good to be true. Implementation Issue:***Many implementation details in the below discussion are different from the paper (e.g.hyperparameters and network architecture). The performance seems the same as Adam. Delayed Adam even leads to a *divergence* problem, especially with a large learning rate (0.03). The divergence problem never happens when using Adam and AdamW with the same hyperparameters.<BRK>In this paper the authors develop variants of Adam which corrects for the relationship of the gradient and adaptive terms that causes convergence issues, naming them Delayed Adam and AvaGrad. While I currently do not recommend acceptance, I am open to changing my score after considering the author comments! Overall, while this area of analyzing Adam and proposing modifications is a popular and crowded subject, I believe this paper may contribute to it if my concerns are addressed. This paper is clearly written and has reasonable experimental support of its claims. If the optimizers are unique enough, including AdaShift in your experiments would be very useful for demonstrating their differences. The synthetic toy problem is much appreciated, more papers should start with a small, interpretable experiment. If momentum was used, was the momentum hyperparameter tuned? You mention validation performance in the main text, so this is just double checking. Given that the best performing values seem to be on the edges of the ranges tested, I would be curious if the trend continues for more extreme values of alpha and epsilon (one could sparsely search along the predicted trendlines.)
Reject. rating score: 3. rating score: 3. <BRK>Aside from this the main contribution of the paper is a study on optimising the step size in gradient methods.<BRK>ADMM is adopted to solve the optimization problem. This paper should be rejected because (1) the proposed method does not show the convergence rate improvement of the gradient method with other step sizes adaptation methods. The limit grid search range could not verify the empirical superiority of the proposed method.
Accept (Poster). rating score: 6. rating score: 6. rating score: 3. <BRK>The paper proposes a fast algorithm to train a NN under orthogonalityconstraints on the weights for each layer. Using a new retractionthe paper proposes an adaptation of SGD or ADAM on the Stiefelmanifold. The idea of the paper is to use a truncated fixed point iteration toobtain the Cayley transform. The idea is seducing but Isee some difficulties due to this approximation. Theorem 2 proves that the relative gradient tends to zero butthis does not control how "orthogonal" are the obtained weights. Basically I fear that the proposed iterative solver movesthe iterates away from the manifold although some empiricalresults in Table 5 suggest otherwise. Figure 1 should be replaced or complemented by a convergence plotin running time.<BRK>Summary:This paper proposes an efficient method to perform Riemannian optimization over the Stiefel manifold using an efficient computation of the Cayley transform via fixed point iteration. While simple, the method is to my knowledge novel. Overall:1) In abstract, "This amounts to Riemannian optimization on the Stiefel manifold". In general, I felt that this paper was missing related work which enforces orthogonality constraints in deep learning. Could you please clarify how this is computed in practice? 3) I am not convinced that the second change made to the Adam algorithm is reasonable (at least, not if we wish to continue calling the algorithm Adam). 4) I am a little unsure of the motivation behind the experiments. I did not understand the proposed explanation that that the orthogonal weights do not affect eachother during backpropagation   could you please clarify? Even if this were true, why would this encourage exploration? There is existing literature which suggests that orthogonal networks will be easier to train (see e.g.[5] and others). Minor:  Section 2, TYPO: "non suquare parameter matrices". The momentum $M_t$ is used below Equation 2 before it is defined.<BRK>Post Rebuttal Update  Assumption one in the paper states that the derivative of the objective function must be Lipschitz continuous   the authors use ReLU in all experiments whose derivative is not Lipschitz continuous. The authors address this concern with the following (page 6 in paper)"... For some models using ReLU, the derivative of ReLU is Lipschitz continuous almost everywhere with an appropriate Lipschitz constant in Assumption 1 , except for a small neighborhood around 0, whose measure tends to 0. Optimizing neural networks while restricting the parameter matrices to remain orthonormal/on the Stiefel manifold is said to lead to faster convergence in terms of the number of epochs, and could reduce overfitting. On the theory side: If the authors want to rely on the assumption that for a measure zero part of the domain the function is not Lipschitz, then the authors should adjust assumption 1 to loss functions whose derivatives are locally Lipschitz continuous, and this will require a redo of the proofs under this new assumption. So the claim that it does not matter in practice is not completely supported. Decision:Weak reject. The empirical validation of the proposed method is insufficient. The authors claim that Cayley Adam catches up with Adam after 12000 seconds which might be true for cifar10 (not really visible in the figure) but is definitely not true for cifar100 where Adam seems to be better all the time. The authors should clarify this and experiments should be done with other activation functions that do not violate the assumption. The total number of epochs is also the same for all methods. The following issues are more minor, but I would still like to see them addressed. 1) In the last section of the related work, the work by Wen & Yin is said to be “not suitable for training common deep neural networks”.
Reject. rating score: 1. rating score: 1. rating score: 1. <BRK>While this paper tackles an interesting problem. The technical approach is unfortunately too outdated and obvious and not quite the level of ICLR.<BRK>You should explain what the so on is in a paper submitted to a conference. Particularly, before performing negation handling and stemming, the algorithm converts each Amharic word to its consonant vowel form." There is almost no organization to the text of the work. This paper is quite difficult to read.<BRK>This paper introduces a method combining the lexicon based feature and character ngram model for handling negation in Amharic sentiment classification. The experiment only compares with two baselines in single dataset, which is also not convincing.
Accept (Poster). rating score: 6. rating score: 6. rating score: 6. <BRK>This paper describes a method to learn data augmentation policies using an adversarial loss. In AutoAugment, an augmentation policy generator is trained by reinforcement learning. Instead, the current paper proposes to train the policy generator and the classifier simultaneously. The policy generator is trained adversarially to find augmentation policies that increase the loss of the classifier. This leads to a significant speedup compared to classic AutoAugment. The proposed method yields improved performance compared to AutoAugment at ~1/10 of the computational cost, which is impressive.<BRK>The authors propose a method for adversarial data augmentation which jointly trains the target network and the augmentation policy network. The authors claim that such learning set up prevents overfitting and reduces computational cost with respect to competitors. * The rationale behind the idea is properly introduced and justified * The formulation is clear and sound* The results show improvements over competitors in terms of accuracy and computational cost* The contributions seem very incremental. * I miss further discussion regarding the benefits of the GAN approach against pre training policies.<BRK>This paper proposes a technique called Adversarial AutoAugment which dynamically learns good data augmentation policies during training. An adversarial approach is used: a target network tries to achieve good classification performance on a training set, while a policy network attempts to foil the target network by developing data augmentation policies that will produce images that are difficult to classify. In its current state, I would tend towards rejecting this paper. ICLR, 2015EDIT: The authors have addressed the majority of my concerns, and as such I have increased my score from a 3 to a 6. Secondly, there are many grammatical errors in the paper which hamper readability. 2) The paper could benefit greatly from some revision of the grammar. I would recommend either having a friend or colleague read it over, or even using an automated grammar checking program, such as Grammarly.
Accept (Poster). rating score: 8. rating score: 6. rating score: 3. <BRK>This paper presents a novel improvement in methodology for learning code execution (at the level of branch predictions and prefetching). I would vote to accept this paper. Taken at face value, the results are impressive, although I am not familiar enough with this field to assess the fairness of comparison against the baselines.<BRK>The paper proposes using Graph Neural Networks to learn representations of source code and its execution. They test their method on the SPEC CPU benchmark suite and show substantial improvement over methods that do not use execution. The downstream consequences of their methods are improved methods for example for branch prediction. This is demonstrated via transfer learning in an algorithm classification problem.<BRK>I am no expert at optimizing code performance, so please take my review with a grain of salt. (b) Binary encoding of features leads to better performance in comparison to categorical and scalar representations. The results show that the proposed method outperforms existing methods on standard benchmarks in the program execution community. Given lack of expertise in the area of program execution, I cannot judge the significance of the performance improvements reported in the paper. Given my current concerns, I cannot recommend acceptance. I might change my ratings based on the review discussions and the author’s responses to the above questions.
Reject. rating score: 3. rating score: 3. rating score: 6. rating score: 8. <BRK>The episodic few shot learning setup from Vinyals et al.(2016) provides a small dataset for each task, partitioned into a support and a query set; at test time, only the labels for the support set are provided. However, this exposes a potential problem with the formulation: As the authors state in the submission, "...the prior P_i in each task has to be chosen independently of the sample set S_i", in alignment with prior works in data dependent PAC Bayes bounds [Catoni, 2007; Parrado Hernandez et al., 2012; Dziugaite & Roy, 2018].<BRK>In this work, the authors introduce PAC Bayesian generalization bounds for Meta Learning. The novelty of the bound in Thm. Instead of applying McAllester’s bound twice (as done by Galanti et al.2016, Amit and Meir 2018), the authors employ Catoni’s bound (a different variant of PAC Bayes) twice. Therefore, the relationships between different arguments are unclear to me. 1 in the current paper.<BRK>### Summary​This paper proposes a tight bound in generalization to new tasks in meta learning framework, by controlling the task prior with Local Coordinate Coding (LCC) prediction. "Model agnostic meta learning for fast adaptation of deep networks." The baselines compared with are other PAC Bayes bounds and successfully justifies the contribution.<BRK>This paper discusses a theoretical analysis of localized meta learning. The first contribution of the paper is to construct a PAC Bayes generalization bound for the case when the hyper prior and the prior on the new task are both isotropic Gaussian. Can you elaborate more on this?
Reject. rating score: 3. rating score: 3. rating score: 3. rating score: 3. rating score: 8. <BRK>They use a parameter tieing scheme to enforce this invariance. I don’t find the paper to be particularly well motivated. Relational DBs are not necessarily a setting where you would always want equivariance. Further, the idea of concatenating all tuples of a relational DB to be passed through a particular feed forward layer is infeasible for all but the smallest datasets. Scaling issues aside, the experiments do not actually show that this method outperforms any reasonable baselines such as a simple tensor factorization. I also found it particularly hard to follow the descriptions of the methods. A few specific points:  The text and notation in the beginning of section 2 could be a lot clearer. I had to read these paragraphs multiple times to pick out precisely what you were trying to say. This explanation could be helped a lot by improving the figure. A lot of the context and background that was sent to the appendix should be included in the main paper, particularly the relation to related workedits:double ‘the’ in abstract “linear complexity in the the data “<BRK>The paper handles the permutation invariance of the entity relationship data entries by tying the weights of linear parameterization together. Heavy math was used to describe the proposed idea, and experiments were performed on simple problems. The main idea of the paper seems to be equation (3) and the associated method of tying weights in the linear layer. In both theory and experiment, the paper does not demonstrate the advantage of the weight tying method compared to an untied baseline. This weakens its necessity and motivation. This paper also has very weak relationship with deep neural networks, while the word  deep learning  was mentioned in both introduction and conclusion. In general, it is a weakly motivated and math excessive paper.<BRK>This paper proposes Equivariant Entity Relationship Networks, the class of parameter sharing neural networks derived from the entity relationship model. The paper is well written and well structured. 2.Representative examples, e.g., the Entity Relationship diagram in Figure 1, are used to demonstrate the proposed algorithms. Are there state of the art algorithms that have been proposed by other researchers to be used as baselines in the experiments? The authors only take synthesized toy dataset in their experiments. Are there other real world datasets to be used in the experiments? What are the motivations of embedding like this? Are there other embedding techniques, e.g., Matrix Factorization and Skip gram frameworks like that in Word2VEC, can be used for your purposes?<BRK>This paper introduces a parameter tying scheme to express permutation equivariance in entity relationship networks that are ubiquitous in relational databases. Results on the generality of the tying scheme are presented, as well as some experimental validation. In contrast to this earlier paper, which builds its argument step by step with great clarity (helped by illustrations), the current paper would prove hard to read to an ICLR audience, since most of its language and methods borrow more from the database theory literature. In addition, the experimental validation leaves much to be desired. In the synthetic data experiments (e.g.Figure 3), there is no guidance as to how to judge the quality of the resulting embedding. All in all, even though the proposed approach could usefully extend the state of the literature, the paper in its current form would need additional work before recommending acceptance at ICLR.<BRK>This paper defines a parameter tying scheme for a general feed forward network which respects the equivalence properties of a vector encoding of relational (ie database) data. One suggestion I have is that some of the details from the Appendix should be moved into the paper proper, most notably the details of the experiments in Appendix H. The paper is surprisingly well written and comprehensible given the complexity of the math. I learned a lot from reading this paper.
Reject. rating score: 3. rating score: 6. rating score: 8. <BRK>In contrast, this work shows that under a sensible definition of adversarial risk, there is no trade off between standard accuracy and sensible adversarial accuracy. So, minimizing sensible adversarial risk (assuming you are given f^B) is exactly equivalent to minimizing adversarial risk. So I feel sensible adversarial risk is not the reason behind such high accuracies. Could the authors explain what is the reason for such good adversarial accuracies reported in the paper?<BRK>For example, the VAT approach published in PAMI. The reviewer would like to see more comments about this. There are two concerns with the paper from the reviewer. (1) The paper states that there is always a trade off between between robustness and standard accuracy in the adversarial learning work, this seems arguable to the reviewer.<BRK>This can be seen as a variant of gradient clipping that reduces the influence of examples with a very high loss. The authors show that their modification yields an 8   9% improvement in robust accuracy on CIFAR 10, which gives state of the art performance. I find the empirical improvements achieved with their modification of PGD style adversarial training very interesting and recommend accepting the paper. Moreover, there are additional experiments the authors can conduct to investigate the performance of their method more thoroughly. Further comments:  I encourage the authors to release their code and pre trained models in a format that is easy for other researchers to build on (e.g., PyTorch model checkpoints).
Accept (Poster). rating score: 8. rating score: 6. <BRK>This paper presents results on Dictionary Learning through l4 maximization. The authors base this paper heavily off of the formulation and algorithm in Zhai et. al.(2019) "Complete dictionary learning via l4 norm maximization over the orthogonal group". The paper draws connections between complete dictionary learning, PCA, and ICA by pointing out similarities between the objectives functions that are optimized as well as the algorithms used. The paper further presents results on dictionary learning in the presence of different types of noise (AWGN, sparse corruptions, outliers) and show that the l4 objective is robust to different types of noise. Finally the authors apply different types of noise to synthetic and real images and show that the dictionaries that they learn are robust to the noise applied. Overall this paper makes significant contributions by extending the work in the paper referenced above to noisy dictionary learning settings and I would vote to accept based on these results.<BRK>This paper explores the recently proposed $\ell^4$ norm maximization approach for solving the sparse dictionary learning (SDL) problem. Furthermore, focusing on the MSP algorithm for solving the  $\ell^4$ norm maximization formulation, the paper highlights the connections of this fixed point style algorithm with such algorithms for PCA and ICA. Unlike PCA, surprisingly, the MSP algorithm is shown to be robust to outliers and sparse corruption. Overall, the paper makes a nice effort towards better understanding the relatively new $\ell^4$ norm maximization approach and its connection with other well understood problems in the literature. That said, the reviewer feels that, in the current form, the results in the paper are not novel enough to warrant an acceptance to ICLR. The analysis of the MSP algorithm in the presence of noise, outlier, and sparse corruption is not comprehensive enough. Also, it is not clear how interesting the outlier formulation presented in the paper is.
Reject. rating score: 1. rating score: 1. rating score: 1. <BRK>At last, the paper is short of technical contents and the figures occupy a lot of the space.<BRK>Main comments:Quality: This paper reads like a project report rather than a typical ICLR paper. Originality: The novelty of this work is very limited, if any. If the main contributions lie it the construction of the device, I suggest a resubmission to a device related conference.<BRK>The violations occur in the 4th line of the abstract, the last paragraph of Section 1 (Introduction), and the first paragraph of Section 2 (System Description). For this reason, I am not providing a review of this paper.
Accept (Poster). rating score: 8. rating score: 6. rating score: 6. <BRK>This paper proposed scale equivariant steerable convolutional neural networks that is able to preserve both the translation and scaling symmetry of the data in the representation. Overall, this is a very good paper. The newly proposed scale convolution is the most general way of achieving scale equivariant representations. Experiments are convincing and justifies the usage of the proposed architecture in dealing with multi scale inputs.<BRK>This is confusing as a reader. (2.4) The expansion of the filters to a diagonal structure is described in the "implementation" section. The most significant improvement for the STL 10 dataset was obtained by the SESN B variant. (2.5) It was not immediately obvious how $\psi_{\sigma}(s, t)$ was related to $\psi_{\sigma}(x)$. Overall I found the paper to be thought provoking and well executed. (1.2) I would have preferred to see the approach demonstrated on a task which possesses scale equivariance, such as semantic segmentation. However, it should still be scale equivariant, except for boundary effects in scale? What is the parameter N_S in this experiment compared to the number of scales S? Issues with clarity:(2.1) The explanation of equation 10 is not clear.<BRK>The paper describes a method for integrating scale equivariance into convolutional networks using steerable filters. Even so, I still find the method concise and of interest, with the basics evaluated, even if some of its unique advantages may have been better explored. The method is evaluated using MNIST scale and STL 10, with convincing results on MNIST scale and bit less convincing but still good results on STL 10.
Reject. rating score: 3. rating score: 3. rating score: 6. <BRK>This paper presents a general framework for sentence generation using a BERT like model. It is fine that the authors focus on empirical results of translation quality but then I would like to see more strong and extensive evidence that supports the use of such approximation. It is nice to see existing decoding strategies represented in a generalized framework, but I was a bit disappointed that the authors do not seem to address the most critical problem in using a BERT like model for sentence generation, namely, how to find the most likely sentence in a probabilistically sound way.<BRK>This is an interesting and important research direction because not only would it result in better and context sensitive greedy/approximate MAP decoded outputs, but also opens up opportunities for parallelization of the decoding procedure which is difficult to achieve with left to right decoders. I think the paper would be strengthened with results with a more sophisticated learned index selection procedure in addition to the heuristics investigated in this paper.<BRK>This paper proposes a generalized framework for sequence generation that can be applied to both directed and undirected sequence models. It could be further extended. The presentation of the paper is also clear. 4."Rescoring adds minimal overhead as it is run in parallel"   it still needs to run left to right in sequence since it is auto regressive.
Reject. rating score: 3. rating score: 3. rating score: 3. <BRK>This point is also related to the spatial coverage used in the paper. I hope the comments are useful for preparing a future version of this work. for the non disjoint experiments, the difference between A and B is that the subsets contain different instances.<BRK>This paper tests generality and transfer in visual navigation tasks. Each experiment compares two highly similar tasks   as the authors themselves acknowledge   in ways that do not obviously connect to realistic transfer scenarios, such as transferring to a new environment. The paper is called "Insights on visual representations for embodied navigation tasks," but I am left wondering what those insights are. e.g., one room or multiple rooms?<BRK>The ultimate question that this study tries to answer is an interesting one. However, I believe that the results are not strong enough to support the claims that are stated in the paper and the limited scope of the environments tested does not make a convincing case that the results will be generalizable much beyond these scenarios.
Reject. rating score: 3. rating score: 3. rating score: 8. <BRK>This paper proposed a local VAE based on the model agnostic meta learning concept. Images generated based local VAE are shown to be better than those generated bu \beta VAE in general in terms of generation quality and disentanglement/compactness/informativeness. This paper shows an interesting idea of viewing neighbourhood of a data point as a task to adopt the meta learning concept to  train VAE which gives superior performance. However, this paper falls short regarding its overall organisation of the paper and the way that different concepts being articulated which make it hard to follow and read. It seems to me that very substantial effort is needed for the revision to reach the ICLR standard. The introduction section should be carefully rewritten to explain clearly the notions of locality and structural relationship. As far as I can see in the subsequent sections, the locality/structural relationship seems to be referring to the neighbourhood either in the input space or the latent space.<BRK>Authors of this paper propose to utilize the embedding model for the other local structure as a weak form of supervision based on the insight that the real world datasets often shares some structural similarity between each neighborhood. The Local VAE is proposed to have the different model parameters for each local subset and train these local parameters by the gradient based meta learning. Local VAE incorporates local information by using prior distributions of local parameters in VAE. There are several concerns:1. In section 2, authors discussed LLE. It is unclear the purpose of the section 2.1? K nearest search should not be a computational problem for large datasets by using fast approach.<BRK>However, the local learning approach is limited to using only a simple model, because complex models require a large amount of data and extended training time. The derivations in the paper are correct. It is reasonable to expect that each local subspace of the dataset shares some structure since most dataset tends to be governed by the consistent rules of the real world. The numerical experiments show that the locality enables the model to achieve the disentangled representation for each subspace without any label information. The paper just extends the objective function of the VAE to have different parameters for each local subset. In consideration of extended training time in the complex model, the paper doesn t provide an evaluation of the efficiency of their proposed model. This paper in its current form is already fairly good.
Reject. rating score: 3. rating score: 6. rating score: 6. <BRK>This paper investigates the aleatoric uncertainty and epistemic uncertainty in machine learning. The evaluation was performed on benchmark regression tasks. The evaluation of the robustness against out of distribution and adversarially perturbed test data was performed. 2.A model with smaller number of parameters was proposed. Total evidence and model evidence were defined. The derivation of these evidences should be clarified. 2.Theoretical justification for related methods could be improved.<BRK>This paper proposed deep evidential regression, a method for training neural networks to not only estimate the output but also the associated evidence in support of that output. The main idea follows the evidential deep learning work proposed in (Sensoy et al., 2018) extending it from the classification regime to the regression regime, by placing evidential priors over the Gaussian likelihood function and performing the type II maximum likelihood estimation similar to the empirical Bayes method [1,2]. [3] A. Malinin and M. Gales. Predictive uncertainty estimation via prior networks. I hope that the authors could spend more efforts clarifying their ideas, especially the derivations in Section 3.2 and 3.3. It could be better to call out the dependence on the input explicitly. What is the originality of the L p norm here? In practice, which p value should be used? In the experiments, which p value was used? Was a different RMSE computation used in this work?<BRK>This paper proposes a novel approach to estimate the confidence of predictions in a regression setting. "Evidential deep learning to quantify classification uncertainty." Pros:1.Novel approach to regression (a similar work has been published at NeurIPS last year for classification [3]), but the extension of the work to regression is important. 3.The presentation of the paper is overall nice, and the Figures are very useful to the general comprehension of the article. Cons:1.The theory of evidence, which is not widely known in the ML community, is not clearly introduced. I think that the authors should consider adding a section similar to Section 3 of Sensoy et al.[3] should be considered. I believe that the article would greatly benefit from a more thorough introduction of concepts linked to the theory of evidence. 3.(p.3) Could you comment on the implications of assuming that the estimated distribution can be factorized?
Reject. rating score: 1. rating score: 3. rating score: 6. <BRK>This paper presents a framework for learning hierarchical policies using a latent variable conditioned policy operating at the low level, with model based planning at the high level. The authors should add citation to this paper and clarify the differences between their work and this work. While the idea is well motivated, this paper should be rejected, primarily due to a lack of experimental results. Latent space policies for hierarchical reinforcement learning. Second, the title and method section would imply that the method is self supervised, specifically in how the latent dynamics model is learned. Lastly, and most importantly, there are no comparisons to prior work. The results as presented do not actually support that the proposed method performs better than existing work.<BRK>The authors propose the model in the control as inference framework and introduce two additional latent variables: one that represents a latent state (z) and another that represents a latent action (h). Overall, I found the paper difficult to follow and some of the reasoning a bit unclear. The experiments seemed limited in scope, given that the authors discuss reinforcement learning in general, but only provide results on the reconstruction error when doing imitation learning. is just a simple supervised learning problem, it had the lowest reconstruction error but the computed action from such the internal model couldn’t make the humanoid walk." with plots. While the authors state that "h can be interpreted as high level commands," but if it is inferred at every time step, why is this a "high level" command?<BRK>*This work implements a hierarchical control scheme for a high dimensional control problem (locomotion using a humanoid body). Does the choice of planner matter at all? My main concerns have to do with presentation, but I think they are relatively significant concerns. Algorithm 1 indicates that the learning of the low level controller will be done jointly with the learning of the latent model and planning using the high level, learned intention space. If I ve understood this correctly, I d be reasonably interested in this result. The authors simply say that there are imitation tasks but do not walk through what these terms refer to.
Reject. rating score: 3. rating score: 6. rating score: 6. <BRK>These images could be even more realistic if the norm bound of the attack is fairly small, and would still be able to attack specific classifiers. They seek to generate examples that are adversarial for a specific classifier, and they do so by using class conditional GANs and a fine tuning loss. The paper proposes using GANs to generate unrestricted adversarial examples. Because the authors want to attack a specific network, they could have (1) generated realistic images using a pre trained GAN (2) used a norm bounded attack on the specific classifier and the generated GAN images.<BRK>Although still not perfect, some of the unrestricted adversarial examples on ImageNet are surprisingly good to the sense that they may be used as practical attacks. original reviews  This paper proposes a novel method on generating unrestricted adversarial examples by finetuning GANs. They demonstrated that their attack is harder to mitigate using adversarial training, produces unrestricted adversarial examples faster than existing methods, and can generate some unrestricted adversarial examples for complex high dimensional datasets such as ImageNet. It is more convincing to have some quantitive results of that.<BRK>This paper presents a GAN architecture that generates realistic adversarialinputs that fool a targeted classifier. The novelty is that they finetune the generator itself during training, themethod can be applied to a variety of GAN architectures, and the method is fast.
Reject. rating score: 3. rating score: 3. rating score: 6. <BRK>For the interpolation algorithm (I regard this is the true technical contribution of this paper), the authors used an annealing mechanism to use different weights and functions at different stages of training. This paper claims to propose a general entropy regularized policy optimization paradigm. However, there are still some minor problems in the paper. The essence is that after MLE pre training, different optimization algorithms are used in different stages, and this should be the focus of the article.<BRK>This submission belongs to the field of sequence modelling. The unified view presented I believe is interesting and could be of interest to a large community. Unfortunately this submission has two issues 1) presentation and 2) experimental validation. I find experimental results a bit limited and not entirely conclusive as it seem that MT provides the only strong experimental evidence. I find quite hard to interpret the significance of difference, for instance, between 36.72 and 36.59 in ROUGE 1.<BRK>This paper presents a formalism of entropy regularized policy optimization. They also show that various policy gradients algorithms can be reformulated as special instances of the presented novel formalism. Overall, the paper is well written and the derivations and intuitions sound good. Hence, I suggest a week accept for this paper.
Reject. rating score: 3. rating score: 6. rating score: 6. <BRK>However, some claims (in introduction or in section 3) are stated more strongly than they are supported by direct experimental evidence. ... the first work on sampling molecular conformations for molecules of arbitrary size and shape based on deep learning. The algorithm presented in the paper attempts to sample appropriately the equilibrium distribution of conformations for each given graph structure (at fixed chirality, if I understood correctly). Indeed, the paper presents a rather extensive comparison of the new model with 2 other ones, on a large data set. This is not addressed here. In section 2.3: only a single molecule output is taken from each distance matrix.<BRK>The model learns to generate, when conditioning on a molecule graph, the distribution of distances between each of the atoms and its second and third neighbour. I think that authors should specify the dimensionalities of each variable and function in the paper. 5.I could not understand if and how the model is able to handle graphs with different number of nodes, and different number of distances among them. Also, I like the fact that some of the limitations of the method are stated. In general, I think that the proposed model solves good enough the problem that is designed for.<BRK>The paper proposes a generative model for generating molecule with three dimensional structure. Given a graph, it leverages a variational autoencoder to embed the distance between two atoms into latent vectors (encoder) and then generate distance between two atoms based on the latent vector. What is the variable "x"?
Reject. rating score: 3. rating score: 6. rating score: 6. <BRK>It comes to a variety of conclusions including that TD(0) does not induce much generalization, that TD(0) does not induce as much generalization as supervised learning, and that the choice of optimizer and objective changes the behavior according to their generalization criteria in various ways. While I agree that this is plausible, there is nothing empirical or theoretical to support this claim in the paper or in the references. This is a significant problem since this assumed connection underlies everything in the paper. It is also not clear whether this measurement is necessarily important for the same reasons as above. Conclusions:  In general the results are presented in plots that do not give clear implications and are difficult to read. I understand that we cannot expect completely clean results on such empirical questions, but the results would be potentially much more convincing and clear if the hypotheses were clearly stated and then one plot could summarize the result with respect to each hypothesis. Again it is not clear how this temporal consistency of updates should be interpreted in terms of the goals of the RL algorithms. The results would become more interesting if the metrics used could be connected back (either empirically or theoretically) to an objective.<BRK>Moreover, memoization performance has also been measured. There are also additional surprising results about optimization. The empirical study is rather complete and significant. It raises interesting questions for the community and states some clear open problems. Hence, I believe it is impactful. I strongly urge authors to call the observed quantity something else. There are some clarity issues in the explanation of the experiments. A clearer visualization or a better explanation would improve the paper. It would be an interesting addition to the experiments. Minor Nitpicks:  Memorization section is not clear.<BRK>Fundamentally, this is a very interesting problem. The authors experimentally analyze this issue through the lens of memorization, and showed that it can be observed directly during training. This paper is very written, and well organized. The experiments are quite solid. However  I may be not capable in judging the novelties and contributions of this paper, since I did not conduct research on this topic.
Reject. rating score: 1. rating score: 3. rating score: 3. <BRK>This paper presents a single image super resolution method. The motivation is real and practical. Yet, the proposed remedy in the paper is not an answer either. The paper fails to provide comparisons with the top methods in the NTIRE 2019 single image super resolution challenge leaderboard.<BRK>Second, a low to high network is trained to produce a super resolution image. Beneﬁting from the GMSR dataset and novel training strategies, the proposed low to high network could thus deal with the real distortions and restore ﬁne details. However, there are some problems as follows:1. The rationality of this setting should be given. 3.There are some formatting issues in the paper,   1.<BRK>According to the proposed framework and dataset, promising results are achieved for realistic image super resolution. [Weaknesses]  The novelty of this work is relatively limited. The idea to use a high to low network to model distribution of real world low resolution images has already been investigated in several works, like Bulat et al.(2018) and Zhao et al.(2018).However, no comparisons are conducted with these two works.
Accept (Poster). rating score: 6. rating score: 6. rating score: 3. <BRK>The paper proposed and studied the unsupervised federated domain adaption problem, which aims to transfer knowledge from source nodes to a new node with different data distribution. To address the problem, a federated adversarial domain adaption (FADA) algorithm is introduced in the paper. Overall, the problem studied in the paper is interesting, theoretical analysis on the error bound is provided in the paper, and the effectiveness of the proposed method has been validated in various datasets. Although the technical contributions of the paper are solid, I still have several concerns about it. 2.It would be better if the author(s) can provide some complexity analysis of the proposed algorithm.<BRK>This paper introduces an unsupervised federated domain adaptation (UFDA) problem and proposes a new model called Federated Adversarial Domain Adaptation (FADA) to transfer the knowledge learned from distributed source domains to an unlabeled target domain. This paper uses a dynamic attention mechanism by leveraging the gap statistics to transfer distributed source knowledge. An extensive empirical evaluation is performed on UFDA vision and linguistic benchmarks.<BRK>The authors present a novel algorithm for dealing with domain adaptation in the setting of federated learning (classification, specifically). the notation in the federated adversarial alignment section is unclear: what *exactly* are the model coefficients Theta that are being updated? the representation disentanglement process is intricate, and only vaguely addressed. The problem considered is of interest, and the approach is novel and interesting.
Accept (Poster). rating score: 6. rating score: 6. rating score: 6. <BRK>Summary:The paper presents a new set of metrics for evaluating disentangled representations in both supervised and unsupervised settings. The empirical support for some of the claims (e.g., superiority to (Eastwood & Williams, 2018)) is a bit weak, but other strengths mentioned above largely make up for that. (3) As opposed to much of prior work, these metrics do not require any training, which can be considered a plus as they do not require adaptation for each (sub)domain.<BRK>I don t think this paper should be longer than 8 pages. I think the paper serves as a great primer for people who are not familiar with disentangled representations, and also proposes a necessary vocabulary for understanding the trade offs of different representation disentangling methods. The paper is too long, you could cut the final paragraph of S2.1 without losing anything (that s half a page already).<BRK>This paper defines precise semantics of disentanglement representations and presents evaluation metrics to evaluate such representations. Also, for the paper to be more self contained, I think the authors should include a short discussion about models that they compare in the experiments. I think this paper addresses an important problem of quantifying and measuring disentangled representations.
Reject. rating score: 3. rating score: 3. rating score: 3. <BRK>I would like to thank the authors for the detailed rebuttal. My concern is generalizing something that is not correctly describing the object we should model (KGs). I found the classification of different methods interesting, and should be made more clear in the experiments. We cannot have a sense of the significance of the results without knowing how many runs were executed, how they were executed (e.g., k fold cross validation, bootstrapping), and their standard deviation. I dispute that the paper proves that the definition of groups emerges purely from the nature of the knowledge graph embedding (KGE) task. I would also recommend following the emerging literature in graph representation learning using group theory, of which KGs are but a special case. > I clarified it, since I was talking about the entity embedding. I did not see a revised version of the manuscript. I will raise my score because the paper could be published as a niche paper. This is all very confusing." The paper is well written and an interesting read. Unfortunately, I think the paper overstate its claims. I understand why this is true for KGE but this is not true for KGs, which only shows that KGEs may not be the right method to represent KGs moving forward.<BRK>This paper start merely by studying the graph reconstruction problem and prove that the intrinsic structure of this task itself automatically produces the complete definition of groups. it seems to be a novel result. Based on this result, one could construct embedding models that naturally accommodate all possible local graph patterns, and the paper also shows a few simulations. My main concern is that, while the focus on this work is the theoretical finding, there is no rigorous statement of it as a theorem. As a result, I am not exactly sure what the proofs in the appendix is trying to show. For the algorithm section, I feel that it is also lacking in the sense that there is still no automatic way to choose which group to embed. It is also unclear what is the purpose of the simulation section.<BRK>The authors approach the problem of representing knowledge graphs from a top down perspective, in which they argue for certain fundamental desiderata for hyper relations (composition, inversion) followed by properties that define a mathematical group. I found the paper extremely difficult to follow. This means one can devise arbitrary properties and restrictions on that family. It s not clear to me what motivates selecting a (abelian) group, where inversion, closure, identity, associativity, and commutativity are demanded to be properties of knowledge graph embedding models. Given this overarching family of models, the authors proceed to identify existing models as certain choices of that family.
Accept (Spotlight). rating score: 8. rating score: 8. rating score: 6. <BRK>################################################################################DecisionI vote for accepting this paper, since as far as I know this paper gives a novel insight to the overfitting problem in meta learning, and has formulated the problem formally with theoretical insight, and given a working solution with strong experiment results and clean comparative studies. Sometimes sentences are very confusing to readers. (1) The term "mutually exclusive" is confusing because the view point example the paper gives seems to be mutually exclusive (each task has its own kind of data, hence "exclusive").<BRK>This kind of overfitting is formalised as the memorization problem. Concerns:  The key hypothesis that the proposed meta regularization method is based on   is that a model with memorization tends to be more complex. What is the basis for such an assumption? The paper does not comment on this, and this would have been useful to know. There have been recent efforts that have attempted addressing overfitting in meta learning. There are some minor typos in the work, which would benefit from a proofread.<BRK>Summary:In this paper, the authors propose a new method to alleviate the effect of meta over fitting. The designed method is based on the information theoretic meta regularization objective. The problem is important in the meta learning field. + The motivation for this paper is clear. The framework and the derivations are straightforward. It would be better if the authors explain more.
Reject. rating score: 3. rating score: 3. rating score: 3. <BRK>In this paper, the authors proposed a training method called global momentum compression (GMC) for distributed momentum SGD with sparse gradient. The authors propose GMC, a training method for distributed momentum SGD with sparse gradient communication. I think in general the ideas and efforts of the authors in proving the convergence rate of distributed momentum SGD with *gradient sparsification* is interesting and important. 2.In the experiments, the authors focus on momentum SGD for image classification tasks. Regarding the assumptions in the paper, I think assumption 2 need some validation / support to show that it is a proper one. But it should be more convincing to empirically show the magnitude of u comparing to the magnitude of gradient g in Equ.<BRK>The results are correct, and the experiments illustrate that the proposed approach can make a difference (albeit, modest) in the quality of the resulting model. The main point I find dissatisfying about the theoretical results of the paper are the use of Assumption 2. The memory vector is a parameter of the algorithm. Overall the results are potentially interesting. I would have given a higher rating if the assumptions didn t appear to be so strong, and if the experimental results demonstrated a more substantial difference with DGC.<BRK>The author propose a method called global momentum compression for sparse communication setting. Since GMC only changes DGC setting from local momentum to global momentum, no modification is involved in the compression part of DGC. Although they theoretically prove a new version of DGC, it s just a minor modification and no significant performance improvement as shown from their empirical results. It s better to include more related algorithms for comparison (like quantization methond: QSGD or signSGD).
Reject. rating score: 3. rating score: 3. rating score: 3. <BRK>UPDATE:I thank the authors for proactively engaging with the review process and improving the paper. After considering the other reviews and discussions with other reviewers, I also share the concern that the simple MEGA D baseline performs very well, with little additional gain from the full MEGA approach (only on the many permutations case that was introduced in the rebuttal). I know this is part of the contribution, and I am certainly an advocate of simple techniques that yield strong results. A very simple direct method is proposed, as well as an angle based approach. 4) The related work section is quite thin, and there are several other works that could be cited; currently they seem to be focused on just "gradient similarity based continual learning" with a few other continual learning works.<BRK>  Summary of paper  The paper proposes a new loss balancing approach for lifelong learning. The proposed method dynamically balances the old task loss (from an episodic memory) and the new task loss based on their magnitude. However, the rebuttal did not address well the question why the simple MEGA D variant outperforms prior state of the art besides promising to publish code. Although the method outperforms state of the art, a very simple baseline of their method also outperforms state of the art, making the contribution ineffective. (1) page 3 "In this case, the weights are only optimized for the current task while ignoring previous tasks which leads to catastrophic forgetting." Overfitting to the episodic memory is a common problem. (5) missing "∇" on the denominator in (eq.7).Experiment:      The only ablation (the direct approach) is statistically indistinguishable from the proposed method. One can only assume that the experiment is problematic. Completely uses hyperparameter from an unpublished paper. Comparison with other loss balancing papers.<BRK>The results are on four popular benchmarks and they also test on Many Permutations, which is an important finding as well. I wonder if the authors tried modifying any of the baseline approaches in this way as well. The direct approach seems like a straightforward extension of experience replay with a special learning rate for current examples vs. buffer examples that is adaptive to the relative loss.
Accept (Talk). rating score: 8. rating score: 8. rating score: 8. <BRK>In this paper the authors proposed a framework for off policy value estimation under the scenario of infinite horizon RL tasks.<BRK>This paper proposes a new estimator to infer the stationary distribution of a Markov chain, with data from another Markov chain. The proposed method could work in the behavior agnostic and undiscounted case, which is unsolved by the previous method.<BRK>Main contributions:This paper generalizes the recent state of the art behavior agnostic off policy evaluation DualDice into a more general optimization framework: GenDice. Connection of theory and experiment:I have a major concern for the theory 1 about the choice of regularizer $\lambda$.
Reject. rating score: 3. rating score: 3. rating score: 6. <BRK>The applicability of the approach is limited to the requirement that multiple adversarial samples be present for accurate detection. The paper is considering a very interesting problem and provides the suitable application of an approach previously developed for pattern detection. The approach is motivated by p value statistics of the activation patterns in the deep learning network under the "null hypothesis" of a non anomalous input. My rating is "weak reject" because the explanation of the subset scanning approach is not clear.<BRK>The authors apply linear time subset scanning to find groups of anomalous inputs and network activations. If I put 100 of the same (or very similar images) would this be something that this method could detect? Major comments:Overall this is quite interesting work, and seems like a promising method to detect groups of anomalies. A value of \epsilon 0.02 was used in experiments for BIM.<BRK>The paper is the first paper, in my knowledge, that introduces the problem of identifying anomalous (or corrupted) subset of data input to a neural network. Why should this be the test? To achieve this, the problem is posed as that of subset detection and subset scanning approaches are adapted for the same. This has not been experimented with.
Reject. rating score: 3. rating score: 3. rating score: 3. rating score: 3. <BRK>This paper proposes a mechanism for identifying decision states, even on previously unseen tasks. Decision states are states from which the option taken has high mutual information with the final state of that option, but low mutual information with the action at a time step, given the current state. An intrinsic reward based on an upper bound of the relevant mutual information speeds up learning in similar environments that the agent has not encountered. A key contribution of this work is extending the notion of goal driven decision states to goal independent decision states. The authors also introduce an interesting upper bound on the mutual information between options and final states. The authors provide an empirical evaluation that supports their central claims. I recommend this paper be accepted because it contributes an interesting theoretical result, a definition of decision state that does not depend on extrinsic rewards, and an algorithm to find such decision states. * The partial observability claim is not substantiated by the experiments. The options in this paper terminate based on a time horizon, but there is no mention of whether the intra option time step is included in the state or bottleneck variables.<BRK>Overall I think the approach is well motivated and interesting, but the resulting implementation takes too many unmotivated modifications to make work, and the results aren t terribly convincing despite this; as such I currently vote for it s rejection. The empirical evidence isn t terribly convincing. The qualitative results are also a bit lacking. This would at least allow us to see if the discovered decision states correspond to those that are optimal under your metric. Your rebuttal lessened my concerns about the using (x,y) and only using an LSTM for the policy. I agree these are largely orthogonal issues, and since they were consistent with their baselines, that is fine. However, the response to the unintuitive nature of the "decision states" is less convincing. If all you care about is the downstream task performance, why even show the qualitative results or impose the semantics of "decision states" on the learned representations? The sandwich bound is novel in and of itself; I understand the need to relate to prior work, but I actually think dropping the language around "decision states" (maybe outside of the algorithm s motivation) and talking purely in information theoretic terms would improve the paper. Your response to [R3] on the setting of Beta hyper parameter seems to not be supported by the results.<BRK>This paper draws connections to many prior works such as VIC, diversity is all you need (DIAYN), and InfoBot and shows results on MiniGrid and MountainCar. Main Comments:While this is an interesting paper, I did not find the experimental section to be convincing enough for publication at this stage. However, if I understand correctly, this method still requires to specify a prior over the options, so it is not clear why DS VIC would be preferable to InfoBot. If the empirical results showed a more robust and significant gain in performance on more diverse or complex tasks, I would be willing to reconsider my judgement regarding the significance of this work. I am slightly confused by the interpretation of that plot because (1) it seems like the model does not detect "all decision states" (e.g.intersections) . and (3) the model doesn t seem to be very consistent about what it considers a to be a "decision state". For a fair comparison, it would be good to show how the curves look after all (or at least .more of the models) converge. Plus, the comparison does not seem fair given that the the numbers are reported before the baselines converged. Please include more details about this stage and how you ensure the comparison was fair.<BRK>Testing this theory could help strengthen the paper. The paper is interesting and provides some interesting theoretical results which adds to the body of work on variational intrinsic control, and unsupervised reinforcement learning. Is it possible that too few seeds were used to estimate the standard error on the mean? That being said, I do not think the paper is ready for publication at this point in time. Unfortunately, the results here are mixed, with the mutual information based reward statistically outperforming count based bonus (which it builds on) only when the optimal hyper parameter $\beta$ (controlling the strength of the regularization term) is known. This somewhat breaks the narrative of unsupervised decision states. J is not defined anywhere. There is also an important missing baseline which is glossed over. It is also not clear from the main text was is being plotted in Figure 3, requiring the reader to go through Appendix 4 to understand the visualization (without any references to this appendix in the main text).
Reject. rating score: 1. rating score: 6. rating score: 6. <BRK>This paper proposes a method for simultaneously imputing data and training a classifier. The proposed method is a simple combination of dictionary learning and classification. I did not see much novelty here. 2.There exists a capacity mismatch between data imputation and classifier. The paper claims that the method is proposed for training deep neural networks.<BRK>This manuscript studies supervised learning with incomplete observation of the features, assuming a low rank structure in the full set of features. The contribution here is a different one. Added to a linear model, it contributes representation learning, and hence can bring some of the appealing properties of deep architectures. With these two comments, I would like to know better how the work positions itself to supervised dictionary learning: the paper is cited (albeit with an incorrect citation), but the positioning is not explicit.<BRK>The proof is clear. However, why wasn’t the result used in the numerical section? After all, this doesn’t seem to inform your actual learning scheme and is not used to evaluate your results. It would be nice to see a numerical validation/illustration of this result. That makes no sense, but it became clear when I got to section 2.b) Please use markers and dashes etc. Or is it trivial that, given one such dictionary, there exists a second one?
Reject. rating score: 3. rating score: 3. rating score: 6. <BRK>This paper addresses the problem of learning and using useful skills through unsupervised RL. The proposed method is, if the primitive skills found by DIAYN are labeled w_i, to find skills z_[i,j,k] which corresponds to the kth state while transitioning from w_i to w_j. The approach is not well motivated. I am not confident that I understand how the transitional skills are defined. While the paper has improved and is on a good track, I do not think it is yet ready for publication.<BRK>General descriptionThe paper tackles the problem of transition between different skills in hierarchical reinforcement learning. General remarks,The problem tackled and the proposed idea are interesting; however, I am not fully convinced by the derivation and the experiments of the paper. Footnote 1 page 2, a log is missing in the MI definition.<BRK>The proposed LTS approach is compared againt a state of the art DIAYN approach on several benchmark control tasks. The authors clearly state that using 3 transtional skills fails when evaluated on 50 transitional skills. Can this be elaborated on? 4.Related to 1 above, I found it difficult to relate the features used for the tasks with the actual tasks themselves.
Reject. rating score: 1. rating score: 3. rating score: 6. rating score: 6. <BRK>The authors argue that to optimize the  proxy for image likelihood  has two advantages: First, the landscapes of the surface are more smooth; Second, a latent sample point in the regions that have a low likelihood is able to generate desired outcomes. This paper has three major flaws and should be clearly rejected. First, the novelty of this paper is trivial, in my opinion, the Eq.2 is the only contribution of this paper. Second, the experimental results are not convincing, almost all the methods proposed after 2015 have better performance compared to these baseline methods. Third, there are a lot of claims in this paper have been made without clarification, I have huge troubles in understanding certain sentences.<BRK>Authors extend the invertible generative model of Kingma et. This seemed critical for convergence of image inverse tasks. The use of Glow prior was shown to be beneficial for all inverse tasks. While the proposal demonstrates improved empirical performance, it seems to be the only contribution of this paper. Taking an existing model and applying it to a problem where similar extensions have been tried (GAN etc) does not seem quite worthy of a full paper.<BRK>Recent work has shown that GANs can be effective for use as priors in inverse problems for images such as compressed sensing, denoising, and inpainting. The approach and application is very natural, although it’s a bit surprising that using the likelihood as a prior term directly did not work very well. The results of this work show that invertible generative models have utility for inverse image problems even when the quality of raw samples is substantially below GANs. In my opinion the main advantage in this method is not on having low reconstruction error on observed pixels, which becomes less of a problem for more powerful GAN models, but rather the good performance on out of domain data which is somewhat surprising. The authors are reasonably thorough, testing their model on a variety of problem settings and perform ablation studies on hyperparameters. It might also be interesting to know whether the good performance on out of distribution inputs is due to the exact invertibility or the log likelihood objective, although I would guess that it is the latter.<BRK>This paper investigates the performance of invertible generative models for solving inverse problems. Since one of the main arguments in the paper is how the lack of representation error benefits the Glow prior compared to DCGAN prior, it would be interesting to see the representation error quantitatively for the DCGAN results and how it contributes to the total error. They demonstrate results on problems such as denoising, inpainting and compressed sensing. The contribution of this paper is that they use a pre trained invertible model as a prior in various tasks not known in training time and support their technique with experimental results and therefore I would recommend accepting this paper.
Reject. rating score: 1. rating score: 1. rating score: 1. <BRK>###Summary###This paper tackles the transfer learning problem with the double blind unsupervised domain adaptation, where either the source or the target domain cannot observe the data in the other domain, but data from both domains are used for training. The high level intuition of this paper is based on the observation that in some practical settings, the transferring source data to the target domain is restricted due to the privacy policy. The proposed model is trivial as it only contains the auto encoders and L2 loss. The datasets used in this paper are not the standard domain adaptation benchmark. It would be nice to see how does the proposed model work on standard domain adaptation benchmarks such as Office31, VisDA, Office Home, DomainNet, etc. The paper proposes a transfer alignment network (TAN) which comprises two autoencoders, one trained on the source domain and one trained on the target domain. In the domain adaptation phase, the model leverages an aligner to transfer the output of the target encoder to an aligned latent variable. I would like to discuss the final rating with other reviewers, ACs. The paper proposes to compare the TAN with three baselines: S(UL):  a stack of encoder and a neural network classifier trained using source data and tested on target data without finetuning. S(UL) T(U): a model retrains the S(UL) with the target unlabeled data. ### Novelty ###The experimental setting proposed in this paper is interesting. The experimental results in this paper are weak. 2) The paper is applicable to many practical scenarios since the data privacy in the real world application is critical. The claims of the paper are verified by the experimental results.<BRK>This paper proposes a TAN model for double blind UDA problem, which supposes that partial data drawn from source domain is unlabeled and the target domain is completely unlabeled. The double blind domain adaptation has no signficant difference. From the model, I could not observe the technical novelty, although the authors focus on the "Aligner". The aligner can actually be removed, by jointly training a domain shared encoder in Step 3 when the unlabeled target data is used for training. In other words, step 4 can be removed and in test stage, the output of encoder means the domain aligned feature representation. Additionally, the source classifier is trained independently from the unlabeled target data. Although the domain aligned feature representation can be learned, it is still risky to directly apply the source classifier for unlabeled target data. I also have concerns on the experimental datasets. Why not use the benchmark visual datasets? The proposed model lacks of comparisons with many state of the art models.<BRK>In this paper, the authors claimed to address a new domain adaptation setting under double blind constraint, to meet the privacy requirement. Though the problem itself seems real and interesting, the solution in this work makes the problem quite trivial. In fact, any other domain adaptation method can be applied to address the problem, while the authors even did not compare with existing UDA methods which can be intuitively adapted. For example, the most widely accepted DA algorithms, such as MMD and DANN, can be adapted to minimize the distance between the target encoder and the source encoder by training the weights for the target encoder. In this case, the aligner introducing more parameters is not even necessary. S(UL), directly applying a model trained on a source dataset, has already achieved as competent results as TAN. The datasets used are at a toy level from UCI. More profound discoveries are expected on DA benchmarks. The paper needs significant proof reading, as there are many grammatical errors and typos.
Accept (Poster). rating score: 8. rating score: 8. <BRK>Summary:This work is tackling the problem of predicting variable sized heterogeneous hyperedges in hypergraphs. Originality: The main contribution here is to propose a more flexible formulation to improve over the fixed MLP used by DHNE. Feng, Yifan, et al."Hypergraph neural networks." Proceedings of the AAAI Conference on Artificial Intelligence. Overall I think the rebuttal addresses my major concerns and I will raise my score.<BRK>This paper proposes a new graph neural network model capable of performing tasks over hyperedges of variable type and size (i.e.different number and types of graph nodes connected by different hyperedges). They experimentally verify its effectiveness over the previous state of the art on several datasets and tasks. This is perhaps the minimal case which their method can address.
Accept (Poster). rating score: 6. rating score: 6. rating score: 3. <BRK>  *Synopsis*:  The main contribution of this paper is the development of estimated mixture policy (EMP), which takes ideas from the new off policy policy evaluation infinite horizon estimators (i.e.Liu) and from a recent development in more traditional importance weight approaches using regression importance sampling (i.e.Hannah).This new method provides a nice extension of Liu s algorithm to many policies, and to when the policy is unknown. I would like if your theorems were restated in the appendix, for ease of reading. Q2 How would BCH do if we used the mixture policy as the behavior policy in the multiple behavior policy case?<BRK>After rebuttal:Thank author for the clarification. The new version looks better and I tend to accept the paper in the current version. This paper provides a algorithm to solve infinite horizon off policy evaluation with multiple behavior policies by estimate a mixed policy under regression, and follows the same method of BCH. The intuition of using an estimated policy comes from Hanna et al.(2019) which shows that an estimated policy ratio can reduce variance even it introduces additional bias. Technical Concerns:The major concern I have is in continuous case, it is almost impossible to pre assume a model for learning the mixed policy $\hat{\pi_0}$. You should replace  for all  to $\forall$ when writing equation, like equation before (2), equation (6) and equation in proposition 4.<BRK>The authors propose here a method for off policy policy evaluation (OPPEval), to use the reinforcement reinforcement learning on infinite horizon problems from previously collected trajectories. A key question to this paper (and the OPPEval field) is to evaluate their methods  more consistently in closed loop agent experiments   after training on the historical data.
Reject. rating score: 3. rating score: 3. rating score: 6. <BRK>The paper introduces a 3D object reconstruction/completion algorithm that utilizes a simple decoder from features generated using PCA of SDF. The overall presentation of the paper is decent, and the network structure of the proposed approach is reasonable. The experimental results make sense, and it s nice to see the performance is reasonable as well. If so, why not use this symbol in Eq.(2) as well? In general, I find it s a bit hard to follow when only 2 3 paragraphs are used for describing the proposed approach. It ll be good if the authors can elaborate on the approach in a more thorough manner.<BRK>This paper studies the problem of learning the feature representation for predicting the 3D shape of objects, from a single image or a point cloud. The proposed approach performs PCA on the SDF field. The authors claims that this approach trains faster and is easier to scale, while showing competitive performance compared to state of the art methods. The paper is generally easy to read, but with some details missing. And I found the discussion of the results to be insufficient. Figure 2 not referred in the main text.<BRK>Thank the authors for the response. I am still in favor of the idea   applying simple, old school method into a new problem, and I also agree with R1 and R2 that the paper is currently lack of details and experimental results. SummaryThis paper presents a new method for 3D shape reconstruction, based on SDF (Signed Distance Function) and PCA. I lean to vote for accepting this paper since the idea is simple but novel, and it achieves good performance. The idea is effective. This is impressive since such a simple method can improve the performance this much. Weaknesses  More analysis could be provided about how do the authors choose SDF.
Reject. rating score: 1. rating score: 3. rating score: 3. <BRK>Summary:This paper conjectures that normalizing flows are fundamentally limited due to the architecture assumption that the generative function g is continuous in x. I do not think this is fair, since models like MAF do provide exact values. Localised generative flows are proposed as a solution and consist in modeling the generative model as a continuous mixture of bijections.<BRK>The paper introduces a straight forward way to expand the flow models by considering mixture of flow distributions. The intuition of the paper is weak and heuristic. 2) For the mixture p(X), will the proposed method generate samples concentrated on one or some of the components? Why or why not? For example, (https://arxiv.org/abs/1805.11183) also uses a hierarchical model with continuous auxiliary variables and a marginalization similar to Eq.(4) in this paper.<BRK>Formally, this is achieved by introducing a conditional random variable $U|Z$. Overall, I think this is a generally interesting contribution to the normalizing flow literature that I expect to spark further research. However, there are some rough edges to this paper.
Accept (Poster). rating score: 8. rating score: 6. rating score: 6. <BRK>Review: The paper proposes a new framework (TREMBA) for black box adversarial attack. The method utilizes a pretrained source network to learn a low dimensional embedding, it then searches efficiently within the embedding space (using NES) and produces an adversarial perturbation that can attack an unknown target network. TREMBA produces perturbations with high level semantic patterns, and is easily transferable to different target architectures. I like the exhaustive evaluation and comparative study done in the paper.<BRK>This paper proposes a new black box adversarial attack method called TREMBA, in which the search for the “adversary” is done in a reduced space z. Summary of its contributions: 	A attack method that improves query efficiency of black box attack 	Produces perturbations that are effective across different networks 	Improves attack success over SOTA defended networks In general, the paper is very well written, with clear mostly clear exposition and sufficient experimental verification.<BRK>This paper proposed a new method for black box adversarial attacks which tries to learn a  low dimensional embedding using a pretrained model and then performs efficient search within the embedding space to attack the target network. The proposed method can produce perturbation with semantic patterns are easily transferable. It can be used to improve the query efficiency in black box attacks.
Accept (Poster). rating score: 8. rating score: 8. rating score: 6. rating score: 3. <BRK>The paper presents a novel and interesting way to measure the margin in the context of deep networks that removes the exponential dependency of depth in the corresponding generalization bounds. The key idea is to stabilize not only with respect to the input perturbations (which most existing works do and end up obtaining an exponential dependency), but simultaneous perturbations at every layer. I really like the fresh idea of simultaneous perturbations based analysis. It will be great if theorem 3.1 can further be simplified in notation and details just to present the main result that linear dependency is achieved via the lower bound for all layer margin and to show improvement over existing bounds. As per the authors of WRN, the optimal architectures are not very deep. Any comments towards this might help.<BRK>The paper proposes a new generalization bound for deep neural networks and develops a regularizer which optimize quantities related to the bound and improve generalization error on competitive baselines. Otherwise it’s hard to gauge how useful it is. Similarly, a generalization bound can be derived based on such all layer margins. This paper shows that it is indeed the case with a much more sophisticated yet intuitive definition of margin that depends all hidden layers at the same time. Other than the nice theoretical properties outlined in the paper, the most exciting property of the all layer margin is that it can be directly optimized which the paper shows at the end. It would be good to either adopt a more explicit notation or have some extra clarification. Likewise, since the proposed AMO is applied to residual networks, I want to hear the authors thought on this.<BRK>The paper presents a bound on the generalization error of a deep network in terms of margin at each layer of the network. The starting premise is that extending the existing margin generalization bounds to deep networks worsen exponentially with the depth of thenetwork. Recent work which removed that exponential dependency isclaimed to require a more involved proof and complicated dependence oninput. The paper provides a new bound that is simplerand tighter. Finally, they present a new algorithm motivated by their bounds, thatmaximized margin on all layers.<BRK>This paper proves generalization bounds for Neural networks in terms of all layer margins. This new quantity allows to show generalization bounds that scales as sum of complexities of each layer and inversely scales with the all layer margin. However I don’t think this is possible in the worst case. The paper need to make it clear that the bound only avoids explicit exponential dependence on depth. However, the bound again depends on a hard to compute quantity   \kapp^{adv}, making it less clear about the utility of such a bound. ** Post response comments **Authors have clarified some of my earlier concerns. Because the current draft does not address these yet, I am leaving my score at 3.
Accept (Talk). rating score: 8. rating score: 8. rating score: 8. <BRK>This paper extends the original PSRO paper to use an $\alpha$ Rank based metasolver instead of the projected replicator dynamics and Nash equilibria based metasolvers in the original. The main contributions that the paper seems to be going for is a theoretical analysis of $\alpha$ Rank based PSRO compared to standard PSRO. The paper performs empirical experiments on different versions of poker. Then the paper tacks on the MuJoCo soccer experiment as a teaser for ICLR crowd. Overall the paper is quite interesting from the perspective of multiagent learning and I would lean towards accepting. Does this have to do with intransitivities in the game?<BRK>Review Update (18/11/2019)Thank you for the detailed replies and significant updates to the paper in response to all reviewers. I think the paper has improved significantly through the rebuttal stage and therefore the update in my score is also significant to match the far larger contribution to the community that the paper now represents. This paper considers alpha rank as a solution concept for multi agent reinforcement learning with a focus on its use as a meta solver for PSRO. The theoretical contributions help further the community s understanding of alpha rank but the method remains somewhat disconnected from other recent related literature. 2) The preliminary MuJoCo soccer results in Appendix G significantly increase the relevance of this work to the ICLR community given the prior publication of this environment at ICLR 2019. 3) Appendix A includes a brief literature survey.<BRK>Specifically the paper establishes connections between Nash and α Rank in specific instances, presents a novel construction of best response that guarantees convergence to the α Rank in several games, and demonstrates empirical results in poker and soccer games. The authors make sure to put these results into context, but given the clarity of the results in the toy domains I would have expected clearer takeaways from the empirical results as well.
Accept (Spotlight). rating score: 8. rating score: 6. rating score: 6. <BRK>Please explain x1 < > x2 in eq.8In figure 1b, consider adding the finite width limit prediction for the training dynamics. Section 2.3 empirically shows that these bounds are pretty tight. time as a function of width n is O(n^ 1), and thus 0 for n >infinity. WHY I AM ACCEPTING THIS PAPERThe paper adapts FDs and cluster graphs, which is a potentially very useful tool for other wide network researchers, and could accelerate research in this whole sub field. If I understand correctly, Feynman diagrams (or cluster graphs) are only used here to calculate correlation functions for deep *linear* networks. Then, other results establish that the width dependent asymptotic behaviour for linear networks holds as is for nonlinear networks, and these results with FDs constitute Conjecture 1. This is not explained.<BRK>This paper proposes a new tool based on Feynman diagrams to analyze wide networks (e.g., feed forward networks with one or more large hidden layers or CNNs with a large number of filters). The method is presented as a conjecture. tighter bounds for gradient flow of wide networks  an extended analysis of SGD for wide networks  a formalism for deriving finite width correctionsThe study of (infinitely) wide networks has been active over the last few years. As such, improving our theoretical understanding of wide networks and especially properties of finite width networks, which is what this paper explores, seems significant and potentially very impactful.<BRK>The main part of the work revolves around Conjecture 1 which assesses how the correlation function scales depending on the number of connected components of even/odd size of the constructed cluster graph. The authors then study training dynamics of wide networks under gradient flow and (stochastic) gradient descent and present empirical evidence to support their claims. of a deep LINEAR network, however, the activation functions are non linear. (3) What is the actual relevance of the ORDER of the correlation function in Conjecture 1 and the relevance of the cluster graph. Can the authors motivate this more clearly, or provide some more intuition on this construction? Overall, the paper presents some quite interesting results, but the authors could be more clear on the relevance of those results and its implications.
Reject. rating score: 3. rating score: 3. rating score: 3. <BRK>This module can be used to replace ConvLSTMs in previously published video prediction models. It expands ConvLSTMs by adding a "previous history" term to the ConvLSTM equations that compute the IFO gates and the candidate new state. This history term corresponds to a linear combination of previous hidden states selected through a soft attention mechanism. To support the claims, they conduct empirical experiments where they show that the proposed model outperforms previous video prediction models on KTH (up to 80 frames) and the BAIR Push dataset (up to 25 frames). To sum up, the paper does not adequately address how the proposed model allows for longer term video generation. There are important remaining issues with the experimental section and the conclusions reached from their results, and therefore I still think the paper is below the acceptance bar.<BRK>This paper presents a new RNN unit based on ConvLSTM for long term video prediction. Also, the authors might include more compared models on the pushing dataset. The major novelty of this paper is the LH STM unit, which applies a temporal attention approach to historical hidden states. 2.The authors mainly compared the proposed network with the Context VP model in the experiments (Fig 5, Fig 8, and Table 3), which is not enough.<BRK>Summary:This paper proposes a new LSTM architecture called LH STM (and Double LH STM). They show that their architecture outperforms previous in the PSNR, SSIM and VIF metrics. Why retrain for longer sequences? Can the authors comment on why they don’t just do this? Is long term prediction limited to how many previous states you can store in the GPU? The fact that these metrics look good in this paper can be due to blurriness. There is clear evidence of blurry predictions in the comparison Figures in the main paper and supplementary material. If the authors successfully address these issues, I am willing to increase my score.
Reject. rating score: 3. rating score: 3. rating score: 6. <BRK>An interesting paper on the role of regularization in policy optimizationIn this paper, the authors study a set of existing direct policy optimization methods in the field of reinforcement learning. Does it mean that deploying BN, results in a more sensitive algorithm? while I agree with the authors on their first reason which is quite commonly known, I might hesitate to make the second statement (2)Generally, I found this paper an interesting paper and appreciate the authors for their careful empirical study. But I found the contribution of this work to be not significant enough. Most of the statements and claims in this paper are well know in the community, especially among deep learning practitioners.<BRK>The paper provides an empirical study of regularization in policy optimization methods in multiple continuous control tasks. Their findings suggest that L2 and entropy regularization can improve the performance, be robust to hyperparameters on the tasks studied in the paper. However, I am leaning to reject this paper because (1) the experimental finding is not well justified (2) the experiments are missing some details and do not provide convincing evidence. There are some missing details which makes it difficult to draw conclusion:1.<BRK>This paper investigates the use of conventional regularizers for neural networks in the reinforcement learning setting. Various regularizers are tried, including l2/l1 regularization, entropy regularization and dropout in a combination with a few standard deep RL algorithms such as TRPO, PPO and SAC. > "policy network but not the value network." Overall, I find this paper to be a solid empirical study of regularization in deep reinforcement learning. I would be willing to increase my score based on the authors  response to the following points:1) In section 6, the last two sentences ("For A2C, TRPO, and PPO ... so further regularization is unnecessary.") In particular, which discrepancy between the sampling policy and the optimization policy is being referred to here? Given the success of L2 regularization, it could be possible that weight decay is even more effective.
Reject. rating score: 1. rating score: 6. rating score: 6. <BRK>In this manuscript, the authors analyze the dynamics of training deep linear neural networks under a generalized family of natural gradient methods that apply curvature corrections. They first show that the learning trajectory (direction of singular mode dynamics) in natural gradient descent follows the same path as gradient descent, while only accelerating the temporal dynamics along the path. 1.The results of the paper are limited to specific deep linear neural networks which are not practical. 2.The paper lacks a cohesive introduction that includes a literature review on this very rich area of research. 3.The paper is not clearly written and is missing many details. I discuss my comments on the manuscript in details next. •	The analysis performed in the paper cover the deep linear case and no intuition is provided on how these results can help in understanding the more general non linear case. By citing [Saxe et al.2013], the authors claim that the effect of curvature of loss landscape on deep learning dynamics can be well captured by deep linear networks. However, this result does not hold even with the simplest non linearities.<BRK>In this paper, the authors study the training dynamics of natural gradient descent with linear DNNs. The paper is well presented and the derivations of analytical solutions are clear. Although the analysis is limited to the linear case, it does give some insight into the behaviors of curvature corrected gradient descents. For example, the Hessian of each layer ofResNet will be close to orthogonal and how will that affect the NGD? Overall I think this is an interesting paper with strong results and vote for accepting.<BRK>Authors analyse curvature corrected optimization methods in the context of deep learning. They build their analysis on Saxe et.al.s work. They show that curvature corrected methods preserve properties of SGD. They also show the disadvantages of layer restricted approximations. The paper looks to deep learning from a dynamical systems perspective and hence their experiments are fitting to this framework. However, my only concerns it the use case. I believe this is a big missing part of the paper.
Reject. rating score: 3. rating score: 6. rating score: 6. <BRK>"However, the first explanation has a large MI with the input document where "great" occurs a lot." Perhaps you should wait to introduce this equation until you have first explained how z relates to x and t. As it is, z is not clearly defined. Why not use a state of the art model for IMDb? The phrasing makes it sound like the prediction of the model somehow depends on a logical step based on whether there are any negative/positive words found. I find the lack of a comparison to some kind of attentional method somewhat glaring in the IMDb example, since I would expect that many classifiers with attention would simply attend to the same words. The same can be said for the MNIST example regarding an attention map. It does seem that there is key information like the definition of approximator fidelity in the appendices which is crucial to actually understanding the paper.<BRK>My main concern though is the MNIST experiment: what I would have expected was the following: cognitive chunks are shown to participants and they need to guess the correct number (just like in the IMDB experiment). Additionally, there is a (hard) constraint, making sure that only a fixed (small) number of chunks is used. Both factors together, according to the paper, constitute a “good” (i.e.brief but comprehensive) explanation which allows for interpretability and attribution of the black box system’s decision. The theoretical properties of the IB objective are appealing for producing interpretable data summaries. ) Experimental evaluation, where human judges rate the “interpretability“ of various state of the art attribution methods. It might be interesting to explore these possibilities for VIBI as well in the future. My main issues with the current paper are (I) interpretability and comprehensiveness are not necessarily the same as maximum compression of maximally relevant information, (II) the method (in theory) depends strongly on the quality of the approximator, this is currently not mentioned and not explored, (III) the experimental section is currently not very strong, in particular the MNIST experiment. Overall, I personally think that the main idea of the paper is interesting, mature and fleshed out enough for a publication, however, the experimental section is somewhat lacking and (II) is missing from the current manuscript. I am therefore slightly leaning towards suggesting a major revision of the paper, but I am happy to be convinced otherwise by the other reviewers and the authors during discussion/rebuttal. [3] The deterministic information bottleneck. In the limit, the bottleneck variable captures a minimal sufficient statistic, i.e.a maximally compressed version of all relevant information   for finite beta, the bottleneck approximates such a minimal sufficient statistic.<BRK>The paper proposes a method to learn an explanation of black box systems from its outputs. The method is based on the information bottleneck as the objective function is designed to measure mutual information between input x, system output y, and narrowed information of input t. t is constructed by filtering x with maintaining interpretability of y, so that it is finally assumed as the explanation of the system extracted by the proposed method. Maybe the paper is focused on only tasks that the predictor does not generate much information, such as classification. It is still unclear how the proposed method work when it is applied to the output rich models, i.e., the model should keep as much information as inputs. The paper also conducted some experiments by changing the strategy, but it is still unclear what is the important criteria. Could you explain how this eqn. of p.v may be undefined in the main text. Is there any reason?
Reject. rating score: 1. rating score: 3. rating score: 3. <BRK>[Summary]The paper proposed to use gradient based presentation in deep neural networks to capture missing information unavailable in the activation based network representation. It is claimed that the missing information that could not be encoded during learning from training set can be revealed by taking the gradient of an example with respect to the model parameters. Based on this idea, a new learning algorithm is proposed to combine both the conventional loss for activation based presentation and gradient based regularization. As an illustration, the method is evaluated on anomaly detection benchmarks with gradient based representation as part of the anomaly inference.<BRK>The authors present an interesting idea: creating representations based on gradientswith respect to the weights to supplement information missing from the training dataset. Their explanation is clear from a high level, however, the paper as a wholelacks rigorous justification. The main proposal appears to be a modification of the loss function, but the paper may benefit fromdiscussing implementation details (for example, during training vs. testing). Finally, the Figures (2 and 3 in particular) are not clear and need more explanation given inthe captions. The method looks particularly promising at anomaly detectiontasks   the authors may have a more clear paper if they focus on this aspect.<BRK>The authors consider gradients of some loss as a feature representation. The premise on which the paper builds is that gradient information is more relevant than activations. Is it the gradient of the loss XE of the classifier w.r.t.an arbitrary label? This is inacurate, J is not minimized to compute theta and phi: the sum over J(x_i; ...) is minimized. J is just an examplar loss. "with our proposed method which will be described in the following section" : I would avoid this kind of "movie" narrative which tries to create a "cinematic buildup".
Accept (Poster). rating score: 8. rating score: 6. rating score: 6. rating score: 3. <BRK>Summary: The paper considers the problem of recovering the span of the latent variables of a neural network with various activation functions. The authors consider ReLU activations   in which case they can recover at least "half" of the row span with ~kd queries to M(x), and smooth activations   in which case they can approximately recover the row span in poly(k,d) queries. Evaluation: This is a strong submission.<BRK>This paper is interesting to me in terms that it provides a systematic approach to generate adversarial samples for a given black box neural network system. Though this is the side product of the paper. 2.When we are trying to recover the span of A, how can we judge if or not M(.) In general, A^{k by n} does not guarantee to be with rank min(k,n).<BRK>is there a stopping condition that can be tested? ** EvaluationWhile the content of the paper lies a bit off my expertise, my impression is that this is a solid technical and theoretical contribution.<BRK>The authors propose two algorithm, one for the case of ReLU activations and onefor the case of differentiable activations, and theoretically prove that they canrecover the span under certain assumptions. I find it somewhat odd that it is possible to recover a rank 80 subspace with100 samples. I do agree that input obfuscation is somewhat different from adversarial attacks. This should be clarified and the specific lemmas that allow this referenced.
Reject. rating score: 1. rating score: 1. rating score: 3. <BRK>This work design a framework to predict valence and arousal values in the emotional impact. I have to guess the technical details based on the context. For example, in Equation 1, how to estimate y_{i}? It is a classification problem here, why use MSE loss rather than cross entropy loss? Are you suggesting that sometimes H() will be about zero? Figure 2 is not clear enough for me. What concerns me the most is the novelty of this work. The authors claimed they propose a progressive training strategy for effectively training and information fusion.<BRK>This paper tackles the problem of affective impact prediction from multimodal sequences. The authors achieve state of the art performance by using (i) a two time scale temporal feature extractor, (ii) progressive training strategy for multi modal feature fusion, and (iii) pretraining. They divide a long video sequence evenly into several clips. It seems working well in practice. However, from the explanation, it s not clear why this strategy is good for the *complementary* fusion. However, enough ablation studies on the two time scale structure and proposed training strategy are not provided to demonstrate that the proposed method does provide complementary multi modal feature fusion, which is claimed as a contribution. It is a combination of existing ideas (but giving good performance). The proposed training procedure is also weak to be considered as a scientific contribution.<BRK>This paper proposes a framework to predict valence and arousal tasks in videos. The framework mainly employs LSTM in a two time scale structure to take multimodal inputs. In general, the proposed framework groups well studied techniques to solve a well known task of multimodal learning. Although the proposed framework is technically sound, the proposed "residual based training strategy" and "long temporal fusion" are kind of trivial or lackluster. I can hardly identify any significant contributions that support a publication in top machine learning conferences such as ICLR.
Reject. rating score: 1. rating score: 3. rating score: 3. <BRK>This paper explores a very interesting idea: can a model learn what variables are and how to use them? The clearest win over standard networks is on the simple synthetic experiments. Unification seems to be implemented as a form of attention (or self attention) where the model can control the degree to which a symbol acts as variable.<BRK>The paper overall is well written and structured. Cons:Despite its superiority over plain baseline, the paper does not provide thorough comparison with other state of the art methods on reasoning related tasks. Some of the technical details regarding the choice of hyperparameters are missing.<BRK>The figure and explanation of the UMN are not very clear. Experiments on the sequence and grid datasets demonstrate the data efficiencyof this approach. While this is encouraging, these are very simple toy tasks. Would be helpful to state whether babi contains co referent expressionsand how these might affect the model. What is the strongly supervised case?
Reject. rating score: 3. rating score: 3. rating score: 3. <BRK>In the paper, the authors proposed CrossGO, an algorithm for finding crossing features useful for prediction. The idea here is that, if a feature has a crossing with some other features, its contribution in the saliency can vary across different inputs. Thus, by looking at the variation of the saliency, one can find candidates features for feature crossing. In the last step, a simple logistic regression is trained using the candidate crossings, and the effective crossings are selected using a forward greedy feature selection. ### Updated after author response ###The authors have partially addressed my concern by adding FM/HOFMs as the experiment baselines, which I greatly appreciate. Thus, the sparse interaction models need to be taken into consideration as well.<BRK>Cons:1.The setting of the threshold for filtering feature fields is somewhat heuristic. The authors should provide some explanations on its setting. 2.The experiments are somewhat not convincing. However, I do not find it in this paper. Although the authors mention that on wide datasets, AutoCross simply cannot work, on narrow datasets, a time complexity comparison should be provided. A comparison with baseline methods on the number would be better to show the advantage of the proposed method.<BRK>The candidate set of cross featurescan be exponential and is handled by the proposed scheme by utilizingthe gradient based importances of the features in a (deep) neuralnetwork. Features with large discrepancies in their local and globalinterpretations are used as the seed set of candidate features forgenerating new cross features, and the final step performs a featureselection to further reduce the final set of cross features. While the proposed scheme does present a way to improve the accuracyof interpretable models, I am currently recommending a reject for thefollowing reasons (given the higher standard recommended for papersover 8 pages):  While this paper does consider some baselines, it seems to be  missing some crucial baselines that address the same (or very  similar) problem. This can potentially make the  proposed feature generation scheme somewhat unstable, and the  interpretations from the subsequent models might not be as  interpretable as they seem. "Featureengineering for predictive modeling using reinforcement learning."
Reject. rating score: 3. rating score: 6. rating score: 8. <BRK>This work proposes a novel approach for visualizing the predictions of neural network models on pairwise tasks, e.g.predicting whether two images are similar. The authors also quantitatively show the benefit of their method. My concerns:1) While showing performance on WSL is appealing as it allows for a way to quantitatively evaluate the method, I wonder if there are other ways to evaluate, that explicitly measure the quality of the proposed technique for allowing interpretability and understanding of the base model s performance. 2) The Triplet vs MS experiment is interesting, but I m not sure this is the most convincing way to show that this proposed visualization technique is better than something else. Further, how would Grad CAM do on the same task? 3) The retrieval experiment only shows qualitative results, and again, no baseline is compared.<BRK>  Summary This paper presents a simple method that draws visual attention of deep embedding networks for metric learning. In addition, the proposed method seems also independent of the loss function used for metric learning, thus it can be applied to most of existing deep embedding networks to understand their behaviors in a qualitative manner. The major concern of mine is its weaknesses in clarity and technical novelty. However, I still believe this submission is valuable since it addresses a relatively new and timely topic, the proposed method is simple yet effective, and the applications of point specific attention (i.e., "cross view pattern discovery" and "interactive retrieval") are all interesting and practically useful. Comments[Pros]1) The motivation and implementation of the point specific attention are convincing. 2) The point specific attention enables not only image to image retrieval, but also more elaborate understanding about a pair of images and their similarity. 3) The proposed technique is model agnostic and loss agnostic, thus can be applied to most of existing deep embedding networks for image retrieval. The way to compute the point specific attention map is not clearly described in Section 3. It is hard to understand the contents in Figure 6 as they are not clearly illustrated. The experimental and implementation details of the last two applications are not given. For example: (cross view pattern discovery) how to compute the angle error, and how much the proposed technique is sensitive to the position selected on the query image, (interactive retrieval) how to compute the similarity between images given a specific region of interest on query. 3) More qualitative results on the last two applications should be presented, even in an appendix, to convince future readers. Especially, in the case of "interactive retrieval", more results are demanded as quantitative performance analysis seems not straightforward.<BRK>This paper proposes a visualization method for deep metric learning, which derived by analyzing the inner product of two globally averaged activations. The proposed method can generate an overall activation map which highlights the regions contributing most to the similarity. Also, it can generate a partial activation map that lights the regions in one image that have significant activation responses on a specific potion in the other image. Further, the metric learning architecture is extended to Grad CAM map, and the problem of Grad CAM map is pointed out. Experiments on weakly supervised localization, model diagnosis, and the applications of the proposed decomposition model in cross view pattern discovery and interactive retrieval are promising. Overall, this paper is well written, and contributions are good. In Sec.1 and Sec.2.2, the authors wrote the Grad CAM has been used for visualization of re ID (Gordo & Larlus (2017)). However, this paper seems to be not the works of Grad CAM nor re ID. However, there are no explanations for this reason.
Accept (Poster). rating score: 6. rating score: 6. rating score: 6. <BRK>paper summary:The authors claim that likelihood based generative models are not as robust to noise as general consensus claims them to be. To prove this authors make use of adversarial, ambiguous and incorrectly labeled in distribution inputs. 2) They also demonstrate how class conditional generative models have poor discriminative power. Cons:1) The experiments section is written very poorly. This section relies heavily on the supplement making it hard to read due to the constant back and forth between the results and details of the experiments.<BRK>The authors investigate the marginal and conditional terms of the likelihood objective and demonstrate empirical results that the model fails to capture the outliers. While there is room for improvement to further develop the method, the current version of the paper would be a good contribution to the community. Summary:This paper investigates some limitations of the conditional generative models (or generative classifiers).<BRK>This paper demonstrates some theoretical and practical limitations on the use of likelihood based generative models for detecting adversarial examples. They provide compelling empirical evidence that while conditional normalizing flows trained on MNIST can be effective in detecting and defending adversarial attacks, these models trained on CIFAR10 are not. This could use some clarification. My only major complaint with the paper is that it is not clear to what extent the theoretical and practical problems are related.
Reject. rating score: 3. rating score: 3. rating score: 6. <BRK>Summary: This paper endeavors to develop some theoretical understanding of why attention mechanisms improve the performance of neural networks. Note that this criticism does not apply to section 3.2 where the attention mask is specified as a function of the input. Finally I would suggest some more direct discussion of how the results apply to models which are used in practice.<BRK>This paper studies the loss landscape of two layer neural networks on global  and self attention models. First, the authors need to provide more detail about the baseline model, especially for the experiment on 5.1.1. The authors also need to clarify the meaning of the "number of training samples" in the tables. Do the reported results average over multiple runs? Given the theoretical part is a contribution, the experiments at the current version do not support their claim fully.<BRK>This paper studies attention and self attention networks from the theoretical perspective, giving the first (as far as this reviewer knows) results proving that attention networks can generalize better than non attention baselines. The results cover fixed attention (as we understand both single layer and multi layer) and self attention in the single layer setting. On the experimental side, the paper introduces L1 loss on the attention mask. This is (to the knowledge of this reviewer) a new idea and it would be interesting to see larger models (e.g., a Transformer on a translation task) trained with this additional loss.
Reject. rating score: 3. rating score: 3. rating score: 3. <BRK>This paper analyzes convergence of asynchronous methods on general non smooth and non convex functions (typically arising from deep leaning). A major concern to me is that the assumptions made in the analyze may be too restrictive. The paper would be of great interest if the global shared memory asynchronous model is made more precise in the main body of the paper.<BRK>This paper proposes a model to study asynchronous stochastic subgradient methods for minimizing a nonsmooth nonconvex function. Did you measure the average time to compute a block gradient, and could you report it in the paper? The main drawback of this paper is that the experiments seem to suggest that, although asynchronous methods do a good job optimizing training metrics (loss, accuracy), the models they train do not generalize as well as using synchronous SGD. In this case one would expect to be able to use a larger learning rate too (since there is less noise).<BRK>Specifically, my sole question to the authors is how does their analysis take into account (sub)gradients that are delayed *and* not in the same half space as the current non biased estimator of the gradient ?
Accept (Poster). rating score: 6. rating score: 6. rating score: 6. <BRK>The paper presents a hierarchical reinforcement learning method for coordination of multiple cooperative agents with pre learned adaptable skills. These skills are learned via a maximum entropy objective where diversity of behaviour given each skill is maximised and controlled via a latent conditioning vector. The method proposed outperforms the baselines reported. Overall, this paper addresses an interesting problem and can be impactful with the caveat for some clarifications and analysis. The problem setting used to test the method is a multi agent setting where it is crucial that skills are adapted to enable cooperation. I found the environments and the problem setting generally interesting and important for testing the proposed method. 2) The set of skills for the two agents are selected by the meta policy in every T_low steps.<BRK>This paper aims to achieve multi agent coordination by composing diverse skills learned by augmenting individual subtask objectives with DIAYN style diversity bonuses. Once individual diverse skills are learned for the subtasks, the agents are combined by a meta agent to coordinate multiple distinct robots to achieve a shared goal. This is a good application of low level skill learning to multi agent coordination. I have settled on a weak acceptance, because the approach is simple and seems scalable, but the acceptance is weak because the method relies on specifying the subtasks in advance. If this didn t work, would perhaps a larger latent skill vector, or a large discrete set of DIAYN skills, have made it work?<BRK>This paper provides a specific way of incorporating temporal abstraction into the multi agent reinforcement learning (MARL) setting. Specifically, this method first discovers diversified skills for every single agent and then train a meta policy to choose among skills for all agents. The second key question is if TA under the multi agent system is special, then why the DIAYN method, which is proposed under the single agent setting, could be directly used in the multi agent setting?
Accept (Poster). rating score: 6. rating score: 6. rating score: 6. <BRK>This paper studies the generalization error bounds of stochastic gradient Langevin dynamics. The convexity of the loss function is not assumed. The generalization bound proposed in this paper applies to some existing problem setups. Also, the authors proposed the generalization bound of the continuous Langevin dynamics. This is an interesting paper. The Bayes stability is a significant contribution of this paper, and the theoretical analysis of the SGLD with non Gaussian noise distribution will have a practical impact. Besides, the role of zero data point, i.e., f(w,0) 0, was not very clear. What made the generalization bound so loose? In this paper, the developed theory was a general purpose methodology.<BRK>In this paper, the authors provide new generalization analysis of (stochastic) gradient langevin dynamics in a nonconvex learning setting. The results are largely based on and improves the analysis in Mou et al.(2018).In more details,  Theorem 11 improves the corresponding generalization bound in Mou et al.(2018) by replacing the uniform Lipschitz constant by the expected empirical gradient norm, which can be smaller than the Lipschitz constant. The authors also argue this can distinguish normal data from randomly labelled data with experiments. A drawback is that the results are only applicable to gradient methods in Section 4, i.e., using all examples in the gradient calculation. It would be interesting to see how the generalization bound would be for the stochastic counterparts. In practice, the regularization parameter should be set to be small enough to achieve a small test error. Therefore, eq (8) may not be quite interesting. After rebuttal:I have read the authors  response. I would like to keep my original score.<BRK>This paper aims at developing a better understanding of generalization error for increasingly prevalent non convex learning problems. For many such problems, the existing generalization bounds in the statistical learning theory literature are not very informative. The paper employs a framework that combines uniform stability and PAC Bayesian theory to obtain generalization bound for the noisy gradient methods. For gradient Langevin dynamic (GLD) and stochastic gradient Langevin dynamics (SGLD), using this Bayes Stability framework, the paper obtains a generalization bound on the expected generalization error that scales with the expected empirical squared gradient norm. The paper then extends their results to the setting where an $\ell_2$ regularization is added to the non convex objective. These bounds subsequently provide bounds for GLD as well. The obtained generalization bounds are informative as they appear to capture the trend in the generalization error. The results in the paper are interesting and novel. This is not the case, e.g.see [1].As noted by the authors, some of the bounds obtained in this paper share similarities with one of the bounds in Mou et al.as all these bounds contain the expected empirical squared gradient norm. Could authors comment on extending their results to hold with high probability and how it would affect their bounds? I have decided to keep my original score unchanged.
Reject. rating score: 3. rating score: 3. rating score: 8. <BRK>This paper proposes the unknown aware deep neural network (UDN), which can be used to discover out of distribution samples for neural network classifiers. The results provided in this paper seem positive. However, I think the main idea is not well explained, and the experiments provided are not sufficient.<BRK>Experiments on CIFAR 10 and MNIST against CIFAR 100, SVHN show that the method has an improved rejection accuracy while maintaining a good classification accuracy on the test set. The topic of the paper is interesting and the approach seems to be solid, however the experiments are not so convincing.<BRK>This paper proposes a neural network architecture for image classification, which can more accurately recognize the unknown class that is not presented in the training data than the prior work. I am not an expert in image classification. Thus I cannot judge based on the novelty of this paper. In other words, the proposed method makes sense intuitively and is validated on several datasets. The response addressed my questions and concerns.
Accept (Spotlight). rating score: 8. rating score: 8. rating score: 6. <BRK>Overview: This work is an interesting work to understand the generalization capabilities of a two layered neural network in a high dimensional setting (samples, features and neurons tend to infinity). I believe this is an interesting work that needs to be accepted. Summary: The work shows that in two layered neural networks with non linearity1) the double descent phenomenon of the bias variance decomposition may be observed when the second layer weights are optimized assuming that the first layer weights are constant. However, the risk is independent of overparametrization.<BRK>The authors study the generalization error of two layer neural nets, where an asymptotic point of view is taken. 1.If only the second layer is optimized, they observe the double descent phenomenon. I see the following strengths of the paper. This is a very well written paper with a clear message. In my view, this is an interesting contribution, which should be accepted.<BRK>This paper provides exact bounds on the risk when training a two layer neural network in an asymptotic regime. The asymptotic regime is considered by making all of $d$, $h$, $n$ go to $\infty$, in a way that the ratio $d/n$ approaches $\gamma_1$ and the ratio $h/n$ approaches $\gamma_2$. In other words, a "double descent" phenomenon is not observed in this setting. Recommendation:I recommend "weak acceptance".
Accept (Poster). rating score: 8. rating score: 8. rating score: 6. <BRK>## Updated reviewI have read the rebuttal. The key novelty of this model is to preserve long range memory in a compressed form, instead of discarding them as previous models have done. The paper also introduces a new benchmark for long range dependencies modelling composed of thousands of books. The presented approach is significant, as modelling long range dependencies is an important milestone in sequence modelling. The new benchmark is a good addition. The comparison with the relevant literature is thorough and well done. The experiments are convincing and demonstrate the viability of the approach, although some aspects can be improved (see below). can the authors comment on that? In Section 5.6.2: can the authors justify why the attention weights were split in only 6 bins? The speech analysis section (5.7) is not very insightful.<BRK>This paper proposes a way to compress past hidden states for modeling long sequences. The outcome is a versatile model that enables long range sequence modeling, achieving strong results on not only language model tasks but also RL and speech. For testing and evaluating the modeling of really long context sequence modeling, the authors introduce PG 19, a new benchmark based on Project Gutenberg narratives. That said,  it seems like a huge amount of resources were spent on this work alone. Overall, I am voting for a weak accept. While this paper is more incremental and novelty may be slightly lacking, I think the breadth of experiments and competitive results warrants an acceptance. Several issues and questions for the authors:1) Why are the results on PG 19 not reported in a Table format? 2) The authors mention that this memory compression architecture enables long sequence modeling.<BRK>This paper investigates a so called "compressive transformer" approach. The idea is to compress distant past memories into a coarse grained representation while keeping a fine grained representation for close past memories. A variety of compression techniques and training strategies have been investigated in the paper and verified using tasks from multiple domains including language modeling, speech synthesis and reinforcement learning. Particularly, the authors propose a new benchmark PG 19 for long term sequence modeling. Overall, I found the work interesting and experiments are thorough and strong. What is the mathematical formulation of the problem? (What is the difference between different colors?What is the difference between sequence, memory, and compressed memory? Despite of the strong experimental presentation, lacking the technical details has significantly hurt the quality of the paper. P.S.Thanks for the rebuttal.
Accept (Poster). rating score: 8. rating score: 6. rating score: 6. rating score: 3. <BRK>Summary of the workThis paper proposes to automatically search quantization scheme for each kernel in the neural network. The author provided implementation on real world FPGAs using bit serial spatial architecture, which plays particularly nice with the algorithm. This something that should be highlighted in the discussion. The second potential weakness is the close relation between the method and HAQ. i have read the authors  response.<BRK>The paper proposed a method for network quantization. Similar with the work of "HAQ: Hardware Aware Automated Quantization with Mixed Precision"(CVPR 2019), the proposed method is based on reinforcement learning. The contribution of the work is on the kernel wise quantization, i.e., assigning different bitwidth to different kernels in one layer.<BRK>SummaryThis paper proposes a network quantization method. Even though the extension from layer level to kernel level is straightforward, the improvement is significant and meaningful. The experiment result demonstrates its efficiency in real applications. The algorithm of the paper is similar with previous work HAQ, which is based on DRL to guide the search procedure. Thus, the novelty of algorithm is somewhat weak. I think some algorithms can be applied into kernel level quantization directly.<BRK>This paper proposes a new method for quantizing neural network weights and activations that uses deep reinforcement learning to select the appropriate bitwidth for individual kernels in each layer. * The text that details the hierarchical RL approach with multiple controllers that is core to the new AutoQ approach is extremely hard to follow, with many undefined symbols. Major comments:* Based on the analysis of kernel wise results, it seems that a very simple strategy that chooses QBNs based on weight variance could be sufficient to achieve good performance.
Accept (Poster). rating score: 6. rating score: 6. rating score: 6. <BRK>The paper proposes a method called FNA (fast network adaptation), which takes a pretrained image classification network, and produces a network for the task of object detection/semantic segmentation. 2.The concrete parameter remapping scheme is not entirely novel. Overall I find the method is effective and experiments convincing and I recommend weak accept in my rating. 3.The results are quite impressive.<BRK>This paper provides a new technique to adapt a source neural network performed well on classification task to image segmentation and objective detection tasks via the author called parameter remapping trick. The technique results in improvements in both performance and training time. To the best of my knowledge, the experiments setting is sensible and the results are good.<BRK>In this paper, the authors take a MobileNet v2 trained for ImageNet classification, and adapt it either (i) semantic segmentation on Cityscapes, or (ii) object detection on COCO. From a narrative perspective, one of the main selling points is not needing to perform any expensive ImageNet pre training; however, a pre trained MobileNetv2 is being utilised. The stuff in Table 6 is pretty interesting however, if convoluted. How important are these? I appreciate that this can be expensive however. Very specific to one network choice  Lack of error bars or comparison to random search. A comprehensive ablation study, along with error bars, and another choice of seed network would do much to strengthen this paper.
Reject. rating score: 3. rating score: 6. rating score: 6. <BRK>This paper propose to modify the existing work [1] of self training framework for graph convolutional networks. Cons:1.The proposed framework makes modification on the existing work, which is a good extension but the novelty is limited. 2.The gap of the experiment results between the proposed method and the baseline methods are quite small.<BRK>The paper proposes an approach for learning graph convolutional networks for inferring labels on the nodes of a partially labeled graph  when only limited amount of labeled nodes are available. The main idea here consists in relying on self training to get a better coverage of labeled nodes enabling learning with less deep models, this translates to a simple and intuitive algorithm. Although the difference with previous approaches do not look like a huge step, the method seems to be quite justified empirically and achieve real good results wrt state of the art.<BRK>The authors do not change the GCN but extend the self training portion as per the prior GCN paper by introducing Dynamic Self Training that keeps a confidence score of labels predicted for unlabelled nodes. # CommentsThis is a very interesting paper in terms of looking at the effects of changing the self training framework to better utilise the underlying structure. As such we can exploit information from other nodes that are yet to be labelled. 1.As the self training is going on, are there different computational costs or are they about the same?
Reject. rating score: 3. rating score: 3. rating score: 6. <BRK>Summary: The authors propose quantize the weights of a neural network by enabling a fractional number of bits per weight. They use a network of differentiable XOR gates that maps encrypted weights to higher dimensional decrypted weights to decode the parameters on the fly and learn both the encrypted weights and the scaling factors involved in the XOR networks by gradient descent. More extensive and thorough experiments could improve the impact of the paper. For instance, authors could compress the widely used (and more challenging) ResNet 50 architecture, or try other tasks such as image detection (Mask R CNN). Justification fo rating:The proposed method is well presented and illustrated.<BRK>It looks at binary codes for quantization instead of the usual lookup tables because quantization approaches that rely on lookup tables have the drawback that during inference, the network has to fetch the weights from the lookup table and work with the full precision values, which means that the computation cost is remains the same as the non quantized network. The benefit of the gate is that the bit sequence can be shorter than the binary weights which means that the code length can be less than 1 bit per weight. The paper also proposes an algorithm for end to end training of the quantized neural networks. NoveltyThe idea of using logic gates for dequantization is interesting and (as far as I know) novel. Kudos to the authors, I really enjoyed reading it. Significance/ImpactThe paper is motivated by the high computation cost of working with full precision values. The evaluation section lacks experiments that evaluate the computational savings. The baselines should include quantization methods based on lookup tables, and there should be a comparison of computational costs. I would strongly recommend including the computational cost of each method in the evaluation section.<BRK>This paper proposed a fractional quantization method for deep net weights. It adds XOR gates to produce quantized weight bits compared with existing quantization method. With both designs, the proposed method outperformed the existing methods and offered sub bit quantization options. The use of XOR gates to improve quantization seems novel. With 1 bit quantization, it outperformed the state of the art. The results seem thorough and convincing.
Reject. rating score: 3. rating score: 3. rating score: 6. <BRK>This paper proposes a framework to learn correlated drug drug interaction based on structured prediction energy networks (SPEN). The core idea is to model the dependency structure of the labels (multi label) by minimizing a designed energy function. The graph energy is designed as MLP over the mean of all nodes embeddings, where the nodes embeddings are obtained through a graph convolutional network. The authors tested on two DDI datasets and the result shows improvement compared to several baseline methods. The proposed method uses structure prediction energy networks to model such dependency. The formulation for semi supervised SPEN could be defined more clearly and worth elaboration.<BRK>This paper presents a graph neural network for drug to drug interaction (DDI) prediction, which explicitly models link type correlation. Basically, the drug to drug interaction prediction problem is a specific type of link prediction task, with drugs as vertices and interaction as edges, and the authors propose a graph neural network with an energy based formulation where the link types are encoded as the graph edges. The authors validate their method against feedforward GNNs on two DDI prediction datasets, and achieve significantly improved performances. Cons  Using energy based formulation for graph neural network is not novel, and thus the paper lacks novelty in methodology point of view. Thus I suggest the authors to perform a more extensive comparative study. It would be better if the authors included actual examples of drug drug interaction predicted by the proposed model and an existing model, for further analysis and interpretation.<BRK>Abstract:Understanding drug drug interactions (DDI) is an important task in drug development and prescription management. The authors proposed a new graph energy neural network (GENN) for DDI prediction. They used an energy function based on graph neural networks to capture the dependencies among edge types in DDI graphs. However, this conclusion can not be supported by Table 1, in which GENN only shows trivial improvement comparing to GNN for the most reported metrics. Minor comments:In general, the paper is well written and easy to follow, except for sections 4.2 and 4.3. As mentioned above, the notations used in the last sentence of the second paragraph of section 4.3 seem to be inconsistent with Eq.(4)
Reject. rating score: 1. rating score: 1. rating score: 3. <BRK>This paper proposes to change the sampling scheme of evolutionary strategies to sample from a mixture of distributions (instead of the standard way of sampling from one Gaussian). 2) The authors claim asymptotic convergence, specifically they claim “Therefore, one might show that the proposed algorithm converges with a high probability.“. Overall, this paper is not ready for acceptance at a venue such as ICLR. Although this seems to provide slightly better performance, the contribution is incremental and more importantly the paper lacks clarity, especially regarding the theory part.<BRK>This paper is motivated to improve the performance of zeroth order stochastic search by incorporating a gaussian mixture as the candidate solution sampler. However, it is not mentioned in the paper. The experimental results are compared only with the two baselines of the proposed algorithm. I have to say that the experimental design of this paper must be reconsidered to conclude and derive the goodness of the proposed algorithm.<BRK>Moreover, authors use EXP3.P as an online learning mechanism. The idea and intuition is definitely interesting. Moreover, most of the existing adaptive optimization method convergence guarantees can be extended to DFO in a straightforward manner. I think it is not hard to prove the main claim; however, the reasoning provided in the paper is incomplete. It is clear from the results that the adam style Hessian method outperforms the proposed method.
Reject. rating score: 3. rating score: 3. rating score: 6. <BRK>The paper proposes a method to handle Mahalanobis metric learning thorough linear programming. Why is the approximation needed?<BRK>However it is unclear to me what is the state of the art of this field from this paper and its novelty. The conclusion of the paper is missing.<BRK>It is always good to the hardness of a problem before evaluating an approximation algorithm for the problem. The paper mentions some references. 3.Did you define combinatorial dimension before using this in Lemma 2.1? I think the paper ports an interesting result from LP type problems into the context of distance learning that people may find interesting and may encourage further work.
Accept (Poster). rating score: 6. rating score: 6. rating score: 3. <BRK>Therefore, I would consider weakly accepting the paper. However, the motivation for choosing this policy is unclear, and there is no mathematical derivation of the gradient value. In conclusion, this paper generalizes linear symmetric quantization to all parameters in order to deploy the network on specialized integer neural network processors for efficient inference.<BRK>The submission proposes to train a linear symmetric quantizing function for integer processors. I found it hard to extract a complete picture of how the proposed approach operates: from the leftmost diagram in Figure 1 I can infer what high level steps are involved, but I wouldn’t know how to re implement the approach from the textual description itself. Can the authors clarify? At this point, are only the weights and activations quantized in the network? Section 3.3 gives the impression that there is a single quantization parameter shared by all parameters and activations, but Equation 3 uses different alpha values for the weights and activations.<BRK>This paper focuses on the quantization of ConvNets. And when performing quantization, it is important to consider the hardware implementation. Details on how to deal with scaling factors, how to deal with biases, and so on, can have significant influences on the overall performance. However, the main concern of the paper is that the methods adopted in the paper are too plain. As a result, I would not recommend acceptance for publication.
Accept (Poster). rating score: 8. rating score: 6. rating score: 3. <BRK>This paper describes a method for segmenting 3D point clouds of objects into component parts, with a focus on generalizing part groupings to novel object categories unseen during training. The system is described well, and shows good performance on a nicely motivated task. A few more ablations would have been nice to see (in questions below), as might more qualitative results. Overall, the method is presented and evaluated convincingly. Does this improve or overfit to the training categories?<BRK>This paper proposes a method for part segmentation in object pointclouds. The abstract says that locality "guarantees the generalizability to novel categories". This is an overstatement, since "guarantees" implies some theoretical proof, and also since the paper s own results (in Table 1 and 3) indicate that cross category generalization is far from addressed, and depends partly on the categories used in training (shown in Table 2). Can the authors clarify and elaborate on this please? It seems like many of the pairs can be rejected outright early on, such as ones whose centroids exceed some distance threshold in 3D.<BRK>This paper studies the problem of part segmentation in objects represented as a point cloud. The paper hypothesizes that top down approaches do not generalizes well to new categories because they end up overfitting to the global context. Evaluation on unseen object categories is an underexplored topic, and the paper is generally well written. I think the submission can be an above threshold paper if the questions are addressed. Table 4 shows some ablation study in an attempt to justify the proposed design, but I think it should be more thorough.
Reject. rating score: 3. rating score: 3. rating score: 6. <BRK>The paper proposes a hierarchical regularization framework based on hierarchical hyperspheres. In particular, the paper tackles the problem of diversity promoting learning. I vote for reject for the following reasons:  The paper is hard to read in general. The paper also does not really give an intuition of why (or what contexts) one of the proposed regularizers would be better than the others. etc...Given the fact that the improvements do not seem significant compared to a single baseline, a proper evaluation with standard deviation should be provided. The "well defined hierarchical information which is categorized by an expert as mentioned in the manuscript" can also be exploited by hyperbolic representations and should then also be compared (as baseline).<BRK>## SummaryThe paper tackles the problem of promoting diversity in the weights of deep neural networks. The paper argues that hierarchical learning and hyper spherical learning are important in addressing this problem. The paper provides experiments on CIFAR 10 and CIFAR 100 where the improvements of using such regularization is visible but not sufficiently significant. ## Overall feedbackI found the paper is well motivated and the proposed approach to be interesting. The improvments of the proposed approach also seems quite marginal. I had a hard time to find exactly what different regularization terms are, e.g., E, H, L2.<BRK>This paper proposes a regularization strategy motivated with principles of hierarchical, hyperspherical and discrete metric learning. Pros:1: I think the paper is well organized and motivated, the regularization of parameters in deep neural network is one of the center problem for effective learning. 2: The proposed strategy is effective with their experiments, various datasets and objective metrics are adopted to validate the regularization. 2: There would be better to show its performance using larger dataset such as ImageNet or COCO detection.
Reject. rating score: 3. rating score: 6. rating score: 6. <BRK>This paper proposed a novel text explanation method which extracts keywords that are essential to infer an answer from question. The experiment is also weak, considering the results is conduct on 100 samples, there might be significant variance. al.2018], who collected the explanations, this paper directly use the coco captions dataset as the source for the explanations. One of my major concern about this paper is it actually generate relevant captions with respect to the question and answer instead of real explanations.<BRK>The explanations are further used for verifying the yes/no answers. The results of VQA 2.0 is pretty good. For example, in Fig.3, it shows an explanation (premise) of "something (not the vegetable) on a plate" and the hypo is "there are vegetables on the plate". It does not directly provide evidence of the answer. It would be better to mention this detail in the paper.<BRK>The paper proposes a novel method for explaining VQA systems. First, it extracts keywords from the question. Finally, 3) checking through linguistic inference whether the explanation sentence can be used as a premise to infer the question and answer. It is potentially helpful to future work on textual explanations and explainable AI in general. While it is debatable whether this is optimal, the proposed approach provides valuable insights on what constitutes a good explanation. However, the proposed approach also has noticeable weaknesses.
Reject. rating score: 1. rating score: 1. rating score: 6. <BRK>Summary:The goal of this paper is to train neural networks (NNs) in a way to be robust to adversarial attacks. The authors formulate training a NN as finding an optimal controller for a discrete dynamical system. This formulation allows them to use an optimal control algorithm, called method of successive approximations (MSA), to train a NN. Comments:  Although the problem studied in the paper is important and the approach is interesting, it seems the paper has been written in rush and in my opinion is not ready for publication. It is not clear what has been done before and what is the contribution of this paper. The experimental results are not convincing. There are strong claims in the paper such as "experiments show that our method effectively improves deep model s adversarial robustness", this is too strong, given the quality of the experiments of the paper. I do not see this in the paper. Overall, I think this paper requires a major revision in order to be evaluated better and to be more useful for the community.<BRK>Neural Networks are vulnerable to adversarial perturbations. This paper proposes a method that based on optimal control theory that uses semidefinite programming. This is a quite popular topic in Adversarial training recently, there has been a few works in that line. There are several typos in the paper and writing of this paper requires more work. There are several typos in this paper, for example STOA, should be SOTA (in the Section 6.) In its current state, this paper looks very rushed. As Yiping Lu pointed out, the PMP statement in this paper is also wrong. I would recommend the authors to go over the paper carefully and resubmit to a different venue.<BRK>The paper contributes to the robust training of neural networks as follows:  1) The paper uses the theoretical view of a neural network as a discretized ODE to develop a robust control theory aimed at training the network while enforcing robustness;  2) Such an objective is achieved by introducing Lyaponov stability and practically implemented through the method of successive approximations;  3) Empirical evaluation demonstrate that the newly introduced method performs as well as the SOTA in terms of defensive training. The paper is well written and proposes a well motivated and theoretically original strategy to robustly train neural networks against adversarial examples. The strength of the paper is definitively in its theoretical section, it would be really great to see an empirical improvement improvement on the SOTA. However, I do not believe the paper should be penalized for only matching other algorithm as it relies on a tractable and principled theoretical analysis.
Accept (Poster). rating score: 6. rating score: 3. <BRK>* SummaryThe work considers sparse and short blind deconvolution problem, which is to inverse a convolution of a sparse source (such as spikes at cell locations in microscopy) with a short (of limited spatial size) kernel or point spread function, not known in advance. This is posed as a bilinear lasso optimization problem. The work applies a non linear optimization method with some practical improvements (such as data driven initialization, momentum, homotopy continuation) and applies it to 3 real imagine problems. In Fig 1 an explanation of the axis and projections would be needed.<BRK>This paper analyzes some of the optimization challenges presented by a particular formulation of "short and sparse deconvolution" and proposes a new general purpose algorithm to try to alleviate them. The paper is reasonably clearly written and the experimental results seem impressive (as a non specialist in this area). However the experimental investigation on real data does not compare to a baseline, which seems like an essential part of investigating the proposed method. What does the square boxed convolution operator in (8) mean? It seemed like a lot of the paper is taken up with a recap of the results presented in Kuo et al.(2019), and it didn t always seem clear exactly what the relevance of these results were to the present paper. Minor comments:The abstract says "We leverage... sphere constraints, data driven initialization". In the abstract, a reference for the "due to the spectral decay of the kernel a_0" claim would be helpful. At the start of section 2, it would be helpful to give a one sentence description of the unifying theme of the section. I didn t understand the relevance of the sentence "Under its marginalization... smaller dimension p << m." to the present paper. In section 3, under "momentum acceleration", it would be helpful to justify the claim that "In shift coherent settings, the Hessian... ill conditioned...".
Accept (Poster). rating score: 8. rating score: 8. <BRK>Since by definition, forecast is meant to represent the change of profit, which can be negative? The paper is clear, concise, and well written, and hence I do not have any overarching comments.<BRK>2.Throughout the paper there is a confusing mix between ideas from economics and ideas from accounting. The need for a class of games with such properties is motivated by a discussion of the pathologies of smooth games under simultaneous gradient ascent. However, I do have some criticisms:1.
Reject. rating score: 3. rating score: 3. rating score: 6. <BRK>This paper performs empirical study on the influence of overparameterization to generalization performance of noisy or networks and sparse coding, and points out overparameterization is indeed beneficial. 1.Overparameterization is better than underparamterization and exact parameterization is not surprising. As the number of parameters goes to infinity, the model can eventually remember all the training data, and has poor generalization. I am expecting some theoretical analysis for tasks simple as noisy or and sparse coding, or some experiments for more complicated (deep) models need to be done, to make the paper more solid. The authors do address my comment #1. I agree that overparameterization improves recovery is a new finding. I think the analysis can be more in depth to make this paper more interesting. I would like to raise my score a bit to a "neutral" score, but given the current scoring system I ll just keep my score.<BRK>This paper investigates benefit of over parameterization for latent variable generative model while existing researches typically focus on supervised learning settings. The motivation of this paper is interesting. On the other hand, I have the following concerns on the significance of the paper. If there were thorough investigations on more modern deep generative models, then the paper would be stronger. This is an empirical study, but if there was theory to support the empirical observations, then the paper was more convincing. Hence, I think what investigated in this paper can be discussed by relating sparse coding theories. However, there is no theoretical justification on the experimental results. Summarizing the above arguments, the insight obtained in this paper is a bit weak. More ablation study and more experiments on general models will clarify what is going on in the over parameterized model for latent generative models.<BRK>The paper “aims to be a controlled empirical study making precise the benefits of overparameterization in unsupervised learning settings. ” The author’s empirical study is comprehensive, and to my knowledge the most detailed published work on this to date. As the authors point out (and I agree), the paper constitutes a compelling reason for theoretical research on the interplay between overparameterization and parameter recovery in latent variable neural networks trained with gradient descent methods. I would argue that “making precise” is too strong for what the paper actually delivers.
Reject. rating score: 1. rating score: 1. rating score: 1. <BRK>I would not like to sound offensive, but this paper is clearly below the standards of the conference, and outside any academic orthodoxy for the matter:  It is only 3 pages long including references, and does not even follow the conference template. The problem tackled is defined as "word prediction", which seems to be some form of language modeling. The proposed method combines HMMs with n grams and morphological and POS features, which are all well established and should be considered more of a (nowadays outdated) baseline. There is no proper evaluation: the experimental settings are not described, and only one number is reported, with nothing to compare to.<BRK>Summary:This paper proposes to predict word sequences for Amharic language  a language spoken in Eastern Africa. It proposes to use HMMs with POS tags and morphological features to perform this prediction task. It is clearly a very early stage work and not in the scope of ICLR. This paper should have been desk rejected as it needs more work before it is fit for publication. The authors should consider looking at RNN based methods such as LSTMs for this task.<BRK>*What is this paper about? *The authors propose a method to incorporate POS tags into a language model to improve its performance in Amharic language. *It tackles and interesting problem. This referred results are not shown in the manuscript though. It is also very short, not going in details about the used techniques.
Accept (Poster). rating score: 6. rating score: 6. rating score: 6. <BRK>The authors propose a new approach of deep probabilistic subsampling for compressed sensing, based on Gumbel softmax, which is interesting. What are the drawbacks of using deep learning in this context, e.g.related to non convexity? Does the method both work for underdetermined and overdetermined problems (number of data versus number of unknowns)?<BRK>The paper proposes a learning based adaptive compressed sensing framework in which both the sampling and the task functions (e.g., classification) are learned jointly end to end. The paper is very well written. This baseline was also missing in image reconstruction. 2) Compressive Sensing incorporates vast literature of algorithms focusing on different aspects of improvements; algorithms focused on classification and inverse problems.<BRK>This paper introduces  a novel DPS(Deep Probabilistic Subsampling) framework for the task adaptive  subsampling case, which attempts to resolve the issue of end to end optimization of an optimal subset of signal with jointly learning a sub Nyquist sampling scheme and a predictive model for downstream tasks. The model is not well motivated and the optimization algorithm is also not well described. What is the Gumbel max trick? 5.Does the proposed approach work in real world problems? It is now acceptable.
Reject. rating score: 1. rating score: 6. rating score: 8. <BRK>This works presents a method for inferring the optimal bit allocation for quantization of weights and activations in CNNs. The formulation is sound and the experiments are complete.<BRK>This work nicely proposes a new theoretically sound unequal bit allocation algorithm, which is based on the Lagrangian rate distortion formulation. Surprisingly, the simple Lagrange multiplier on the constraints leads us to the convenient conclusion that the rate distortion curves for the weight quantization and the activation quantization have to match. First of all, the paper is not about 2bit quantization.<BRK>Very good paper that studies the error rate of low bit quantized networks and uses Pareto condition for optimization to find the best allocation of weights over all layers of a network. The theoretical claims are strongly supported by experiments, and the experimental analysis covers state of the art architectures and demonstrates competitive results. This needs to be clarified further.
Reject. rating score: 3. rating score: 3. rating score: 6. <BRK>This paper investigates the question of internal consistency in emergent communication. I think coming up with methods/ architectures that combine these two capabilities is an important research direction. I also think paper is very well written and structured, and it reads very well. However, I have several concerns with the paper. First, half of the results center around the hypothesis ‘internal consistency helps agents generalize to unseen items’. While ultimately this hypothesis is disproven, it’s unclear as to why this might be expected in the first place. I also have mixed feelings about the use of ‘self play’ to enforce internal consistency, and how it relates to the core result of the paper: “the proposed constraints enable models to generalize learned language representations across communicative roles, even in the case of where the agent receives no direct training in the target (test) role”. Thus, I do not recommend acceptance in the paper’s current form.<BRK>In an emergent communication setting, the paper proposes three methods for encouraging internal consistency for dialog agents   an agent speaks/listens to others as it would expect to be spoken/listened to. The paper concludes that the internal consistency constraints do not improve models’ generalization to novel items, but improve performance when agents are tested on roles they were not trained for. The experiments support the above conclusions on the small datasets used in the experiments. The paper is in general well structured and is clear and easy to follow. The contributions of the paper are rather incremental. I do not think this paper presents enough contributions.<BRK>The paper analyzes if enforcing internal consistency for speaker listener setup can (i) improve the ability of the agents to refer to unseen referents (ii) generalize for different communicative roles. Though paper advocates through some of its results that self play is helpful in generalization across roles via internal consistency, without multi step experiments, qualitative and quantitative analysis of what is happening and why there is so much variation, the paper is weak in its current form. Below I discuss some of the concerns in detail:Without multi step evaluation, it is hard to gauge the extent to which self play for internal consistency help in generalization of the roles. I still believe that without multi step communication, the work is as useful as it can be in current form. I think most of what you got is correct for multi step, see second para for more details in this response.
Reject. rating score: 1. rating score: 3. rating score: 6. <BRK>Generalizing self attention to work across dimensions of a multi dimensional time series is a good idea, and the experiments in the paper seem to support its effectiveness. The paper does provide some ablation results to compare their three forms of modified self attention, which is good. However, I have to vote to reject the paper. My primary issue with the paper is its tone. While generalized forms of self attention are a good idea, the paper strongly emphasizes that it is a novel idea. If the paper changed its tone from purporting to construct a novel method of self attention (which I do not believe it does), to being an empirical study of the utility of self attention models for doing time series imputation I would be much more willing to accept it, though as a purely empirical study the bar would be high on the standard of the experiments, the reported experiments are on rather small datasets. Also, the paper could use an edit for grammar.<BRK>This paper proposes a Transformer based model with cross dimensional self attention for multivariate time series imputation and forecasting. Handling time series data with missing values is quite important, and the authors design the novel cross dimensional attention mechanism which is reasonable and performs well. Especially, the authors compare several recent RNN based models. However, I do have a few questions and concerns. This may explain why the performance on short term forecasting is relatively unsatisfying. About the experiments:Two datasets of the three mentioned in the main paper have M 1, which degrade the proposed model from 3 dimensional to 2 dimensional. The overall idea is relatively easy to follow, while some detailed descriptions should be added or clarified. The numbers are helpful, but it would be better if the results are computed based on the hyperparameters (e.g., T, L, M, d_V, etc).<BRK>This paper investigates different ways of implementing 3D attention and extensively evaluates the performance of them. The proposed problem is a practical problem and I think the main contribution of this paper is valuable. However, there is always a trade off between having access to the entire history and the ability to be used in streaming settings. Also, the authors should report the actual run time of the algorithms on the real data (beyond Table 1 and compared to the baselines). The main criticism of the experiments is that the datasets are very small. Also, if you have run the experiments and not copied the reported data, make sure to indicate that in the paper. The ways that the authors decompose the attention tensor reminds me of the works in different tensor decomposition algorithms in spatiotemporal data [1, 2].
Reject. rating score: 1. rating score: 3. rating score: 3. <BRK>This work searches neural architectures in an embedding space, which is continuous. The key idea of searching in embedding space has already been proposed and applied in  Luo et al.2018.The authors do not differentiate this work from Luo et al.2018, and I didn t find any essential differences between them. 2.The proposed method does work well. It is not compared with latest algorithms including NAO and DARTS.<BRK>Summary:This paper borrows the idea of word to vector from NLP and applies it in reinforcement learning based Neural Architecture Search (NAS). Then it performs reinforcement learning based Neural Architecture Search(NAS) in the architecture embedding space. Because it performs architecture search in a continuous space, a CNN based controller is used instead of a RNN controller. The result of the proposed method on CIFAR 10 is comparable with other popular NAS approaches. Besides, there is no comparison with more recent and related important methods such as DARTS and the method proposed by Luo et al.(2018).Actually its performance is not as good as Darts or the best performance reported in ENAS.<BRK>The paper proposes an interesting idea to perform Neural Architecture Search: first, an auto encoder is pre trained to encode/decode an neural architecture to/from a continuous low dimensional embedding space; then the decoder is fixed but the encoder is copied as an agent controller for reinforcement learning. The controller is optimized by taking actions in the embedding space. Many RL methods, such as DDPG, can handle continuous actions;3. The paper should compare with NAO.
Accept (Poster). rating score: 6. rating score: 6. rating score: 3. <BRK>This paper proposes the idea of using differential privacy (DP) to improve the performance of outlier and novelty detection. Based on the above comments, I think the paper can be accepted if there is room for it. I find this a bit confusing. The paper also shows empirically how DP can help improve backdoor attack detection.<BRK>This paper leverages differential privacy’s stability properties to investigate its use for improved anomaly and backdoor attack detection. Thus the contributions of the paper are not substantial. The paper also tackles an important and timely problem that is relevant to the ICLR community. Under this view, this paper essentially says that “differential privacy leads to disparate impact on model accuracy/loss”.<BRK>Interesting topic but lacks of novelty#Summary:The paper proposes that by applying differential privacy, the performance on outlier and novelty detection can be improved. However, the paper proposes to use differential privacy to the model training process, which is not in the settings of a backdoor attack.
Reject. rating score: 3. rating score: 6. rating score: 6. <BRK>The paper deals with the problem of text classification when the number of class is large (17000) and most of the classes do not have examples in the training set. The paper proposes to use adversarial methods and the hierarchical organisation of the classes to improve current models. The description of each bloc is clear enough independently  but many question remains on the global picture. This is somehow unususal for a text classification problem where usually the classes are not defined by a description. 3.1 is related to feature extraction for the clinical document, but it is also said that this processing is also apply to class description. Results : the claim "our methods improve the F1 score from nearly 0 to 20.91% for the zero shot codes" is not true : state of the art models such as Xian2018 and Felix2018 are already over 20% F1.<BRK>This paper addresses the problem of zero shot prediction for ICD codes for medical notes. The classification of ICD codes is trained on the GAN extracted features. The feature extraction model takes the word embeddings from the input text and passes them through a CNN; in addition, attention is used for the encodings of all the ICD codes. I had to refer to the paper to be able to appreciate the differences. The GAN based feature generation seems to be new in this paper. * Siblings ICD codes mean codes that share an immediate parent? * Table 1. seems to cite the related work results, but it doesn t seem to include the results of the proposed method.<BRK>The paper proposes a method to do zero shot ICD coding, which comes down to determining which elements from a set of natural language labels (ICD codes) apply to a given text (diagnostic summary). The main problem is that many codes have zero or very few examples. This generator is trained using a GAN, which moves the few /zero shot problem to training the discriminator. Additionally, a keyword reconstruction loss is used, based on the idea that the keywords of the corresponding ICD code can be reconstructed from a good feature vector. The paper is written and executed well; the setup, which involves a fair number of components, is described and motivated clearly. All in all, I would argue for the paper to be accepted.
Accept (Spotlight). rating score: 8. rating score: 8. rating score: 6. <BRK>The authors present MetaBO, which uses reinforcement learning to meta learn the acquisition function (AF) for Bayesian Optimization (BO) instead of using a standard constant AF. 10.Section 4, ‘the state corresponds to the entire functions’. 11.Section 4: replace ‘not to be available’ by ‘unavailable’. I therefore consider the paper as borderline. Chen et al does not depend on a GP and is therefore more scalable.<BRK>This paper proposes a framework for meta learning neural acquisition functions for the Bayesian optimization of various underivable functions. The paper is well written and it proposes a novel direction for research. Aren t you also biasing the distribution by adding the local maxima to the set of ξ (Xi)?<BRK>Summary: The authors propose a meta learning based alternative to standard acquisition functions (AFs), whereby a pretrained neural network outputs acquisition values as a function of hand chosen features. On [Chen et al., 2017]:  To the best of my knowledge, these RNN based methods only require the gradient of the loss function. Another baseline: EI with multi task GP? The cubic scaling should be fine for, e.g.,  xxx 20  multi task variants.
Reject. rating score: 3. rating score: 3. <BRK>I acknowledge reading the rebuttal of the authors. My point was this paper would make a good submission to ICLR if it was better motivated presented and explained to a wider audience. The hamiltonian for example for the Wasserstein distance $H(\rho_t,\Phi_t) \frac{1}{2}\int||\nabla_x \Phi_t||^2+ \mathcal{E}(\rho_t)$. Some experiments of the particle based method are shown on synthetic experiments and in bayesian logistic regression. Review: Contribution/ Clarity:The main contribution of the paper is in deriving the accelerated gradient flow for the wasserstein distance this was also addressed in a recent paper [Taghevia and Mettha 2019]. The technical contribution is interesting but given that this field of flows in probability space is still not very well spread in the ML community, I wonder if ICLR is the best fit for this type of work. since MCMC and BM method lead to similar result what is the advantage of the wasserstein accelerated flow? one could also implement also an accelerated langevin dynamic<BRK>The accelerated gradient flow is developed by leveraging a damping Hamiltonian flow. The paper focuses mainly on the case with the Wasserstein metric and provides a convergence analysis. There is not enough motivation and explanation for the ideas presented. Adam is not relevant to the Fisher Rao natural gradient while K FAC is just an approximation method and isn t a good reference for demonstrating the effectiveness of the natural gradient. Having said that, I believe this paper can be an important contribution if the authors invest more time on refining its presentation and if more thorough experimental studies are conducted.
Reject. rating score: 6. rating score: 6. rating score: 6. <BRK>Summary: This paper proposes selective activation RNN (SA RNN), by using an update coordinator to determine which subset of the RNN’s hidden state dimensions should be updated at a given timestep. The proposed loss term is then a sum of the original objective (e.g.classification) and a weighted sum of the probability that each dimension will be updated for each timestep. For example, in prior work, the empirical evaluations were on much larger scale datasets such as Wikipedia [Shen et. I would be very interested to see how this training procedure fairs when evaluated on much more complex tasks, and would make the results about computational speedups at train/test time much more convincing. Questions:  I’m curious if you tried different types of gradient estimators to get around the non differentiability rather than the straight through estimator.<BRK>Pros:Overall, I think the idea of this paper is clear and the whole paper is easy to follow. The experiments clearly show the advantage of the proposed method claimed by the authors. For example, in “This way, representations can be learned while solving a sequential learning task while minimizing the number of updates, subsequently reducing compute time.” two “while”s are not elegant and there should be an “In” before “this way”. In “We augment an RNN with an update coordinator that adaptively controls the coordinate directions in which to update the hidden state on the fly”, the usage of “in which to” is not right.<BRK>A main problem with RNN is to update all hidden dimensions in each time step. The authors proposed selective activation RNN (SA RNN), which modifies each state of RNN by adding an update coordinator which is modeled as a lightweight neural network. The coordinator, based on the incoming data, makes a discrete decision to update or not update each individual hidden dimension. The idea proposed in this paper is interesting and it is presented very well.
Reject. rating score: 3. rating score: 3. rating score: 3. <BRK>This paper proposes a method for generating dialogue responses that are not generic. The authors propose maximizing correlation between latent representations of responses and prompts, such that the response is encouraged to contain information relevant to the prompt. I am skeptical about the early stopping criteria, which involve human evaluation.<BRK>[Summary]This paper proposes a dialogue generation model that learns a semantic latent space. In this paper, the authors tackle the generic response issue. The authors add an objective that maximizes the correlation between prompt and response features to the existing autoencoder structure, which prevents the generation of generic responses.<BRK>1.Summary: The authors proposed to alleviate the generic response problem in open domain dialog generation by generating a response to a prompt from a semantic latent vector. 3.Strengths:3.1 This paper is well written. 4.2 The experiments in this paper do not look thorough enough.
Accept (Poster). rating score: 8. rating score: 6. rating score: 6. <BRK>The paper studies adversarial training "at scale", i.e., on the ImageNet dataset. Hence the results of the paper may be useful for the wider robustness community. To achieve this goal, I strongly encourage the authors to release their models in a format that is easy to build on and experiment with for other researchers (e.g., PyTorch model checkpoints).<BRK>This paper introduces two properties of adversarial training observed from abundant empirical results. Based on the discoveries, the authors propose plausible explanations as well as new methods to gain higher adversarial robustness. The two properties are as follows1. There are some “, ,” (double commas) in the appendices.<BRK>This paper reveals some interesting properties of neural networks when trained adversarially at ImageNet scale. It therefore becomes not that important to study the effect of Batch Normalization on training with both clean and adversarial images. It seems missing in the paper. Finally, though it seems that deeper networks are more robust,  the robustness might be a misconception caused by gradient vanishing.
Reject. rating score: 3. rating score: 3. rating score: 6. <BRK>To me, if the work could be changed to compare against works which are not so tightly constrained, not for the purposes of holding it to the same standard but to understand it’s relative standing, or to better justify the very strict constraints which somehow, despite out of distribution detection being a popular upcoming topic, apparently only has one other paper that matches it. After reading the other reviews and comments, I appreciate the effort by the Authors, but it looks like the paper still needs some work before being ready. The authors evaluate the method broadly and against the state of the art and provide a thorough explanation of the background material and formulation. For this reason, I am borderline unless that caveat is addressed as described below, in which case I would be happy to accept.<BRK>Again, weights can be validated by adversarial samples to satisfy the constraints. Also, the performance of their replication of the prior method is far lower than reported. In this kind of case, I suggest the authors to put {the numbers in the original paper} as well as {their replication} and claim that they fail to replicate the number. Ideally, if their method is evaluated in the same condition, it should outperform prior works in any case. ** post rebuttal end **  Summary:This paper proposes an out of distribution detection (OOD) method under constraints that 1) no OOD is available for validation and 2) model parameters should be unchanged. However, the method for comparison is not properly set. Though the idea is interesting, I am skeptical about the effectiveness of the proposed method.<BRK>This paper proposes a new framework for out of distribution detection, based on global average pooling and spatial pattern of the feature maps to accurately identify out of distribution samples. The motivation of the proposed approach is clear, and the method seems novel. However, the experiments could have been done in a more complex setting. The out of distribution samples pose a danger in safety critical applications such as autonomous driving, for example a car deployed in environment that it has not seen during the training might crash. So, it would be interesting to see the performance of both baselines and proposed approach in those settings where inputs are similar in nature but very different in some aspects. I do not necessarily see something wrong with the paper, but I m not convinced of the significance (or sufficient efficiency) of the approach.
Reject. rating score: 1. rating score: 1. rating score: 3. <BRK>This paper uses first order logic (FOL) to help reduce so called “superficial” information/semantics that is less relevant to the judgement of natural language inference relations. I do not recommend it for the conference.<BRK>This paper presents an approach to treat natural language inference using first order logic, and to infuse neural NLI models with logical information to be more robust at inference. However, by extension, this makes s_1  > s_2 a tautology, which is the  true  notion that the authors are looking for. In summary, while I am sympathetic to the aim of grounding neural models in explicit notions of semantics, this paper shows such a lack of awareness of previous literature that I cannot recommend acceptance.<BRK>This paper tried to reduce superficial information in natural language inference (NLI) to prevent overfitting. It utilized the first order logic to explain what is superficial information. I’d like to see some discussion in the paper on this.
Reject. rating score: 1. rating score: 3. rating score: 6. <BRK>But in the current form, the paper has less value to be published in ICLR. This paper proposed "F pooling" for Frequency Pooling, which is a pooling operation satisfying shift equivalence and anti aliasing properties. Spectrum pooling has been used in the community of computer vision and machine learning. The reviewer encourages the authors to make further new developments and have a more comprehensive literature review.<BRK>This will help to make this paper more self contained. In light of this, some justification and explanation shall be provided for using this criterion for optimality. 3.The experimental study is weak. Also, from the three Tables in the experimental part, the improvement of F pooling over AA pooling (developed by the main reference of this work) does not seem to be significant or consistent.<BRK>This paper proposed a new pooling method (Frequency pooling) which is strict shift equivalent and anti aliasing in theory. The experimental results are actually less impressive than what are claimed in contribution and conclusion. It is good to me that the authors discuss one limitation on the imaginary part of output and I would like to hear more on other potential limitations for this method. also, if the authors can explain more on sec 2.5 it will be helpful.
Reject. rating score: 1. rating score: 1. rating score: 3. <BRK>LDMGAN: Reducing Mode Collapse in GANs with Latent Distribution MatchingSummary:This paper proposes a modification to the VAE GAN model where mode coverage is encouraged by passing samples G(Z) through the encoder and minimizing the forward KL between E(G(Z)) and the prior over Z. There are formatting errors in the PDF, such as at the top of page 8.<BRK>I also think that the presentation is not clear enough. The notation is not clearly defined also. Also by looking at the provided samples, I think the quality of the results are far away from what is obtained with state of the Generative models currently. Overall, I think the idea is sound and sensible, but I don t think that results are convincing enough for a conference paper. Furthermore, the writing needs to be improved.<BRK>It is argued in this paper that GANs often suffer from mode collapse, which means they are prone to characterize only a single or a few modes of the data distribution. However, this paper has the following major concerns: 	（1） Though somewhat new, the novelty of this paper may be incremental to me. It looks like a combination of VEEGAN and AAE. （2） The paper tested the proposed algorithm with a 2D Synthetic dataset. Such discrepancies were also observed in Figure 3. From the figure, it seemed that LDMGAN improved the sample quality and diversity compared to GANs, but it is still prone to characterizing only a single or a few modes of the data distribution.
Reject. rating score: 3. rating score: 3. rating score: 3. <BRK>The reason why I think the authors should add such theoretical proof is that it seems that the idea to construct new prior matrix instead of old one to avoid the catastrophic forgetting is not related to deep neural network at all. Summary:This work provides a memory efficient nonlinear bandit algorithm based on deep neural networks. The experiment settings and results are convincible. More specifically, the algorithm in this work only uses part of history information to save the memory usage.<BRK>And experiments are conducted to show that the proposed method is resilient to catastrophic forgetting and can achieve good cumulative reward results. However, I found several weak points as follows. 2.This algorithm works with neural network based features, but it is in nature not scalable as shown in the complexity analysis (linear dependence on action number). Overall, this is a reasonable paper. However, on the one hand, as an algorithm mainly works under bandit settings, it is a lack of theoretical support.<BRK>Overall the paper is well written and explained clearly. The result of this paper is interesting. 1.The results in Section 4.2 seem to be following the setting in Riquelme 2018. These datasets are all in a supervised learning setting. It is a bit disappointing that the proposed method is not tested on RL datasets. a.There are other methods in the literature to overcome catastrophic forgetting of neural networks, e.g regularizing the update of the network.
Reject. rating score: 3. rating score: 3. rating score: 6. <BRK>Notations and claims are clear. The idea of a multi view generation model based on restricted kernel machines is interesting. It would be interesting to see complexity analysis to evaluate the computational costs. Overall, I do not recommend this paper for publication. The experimental results are not satisfactory, and the paper needs improvements in that regard.<BRK>In this manuscript, the author(s) extend the work to a generative model setting to achieve multi view generation   a generative model that can explain correlated variables from a common subspace. The manuscript is well written and easy to follow and the algorithmic details are clear. From the above empirical results point of view, I do not think this manuscript is ready for publication, despite what I see as the elegance of the framework.<BRK>a) if the main idea of the paper that the authors propose some new method for generative modeling of multi modal data, then the authors should make significantly more diverse experiments and ablation studies. The authors of the considered paper 1) generalise a multimodal variant of the Boltzmann machine from [1] (which uses a special cross product term to take into account dependency between modalities) to the case of kernel machines,2) demonstrate on several typical datasets that using explicit deep network features it is possible to model images and data with two modalities (faces/textual description of faces). I guess that the paper can be published, but only after issues in a) are addressed. Actually, this is not the case of the current work.
Reject. rating score: 1. rating score: 3. rating score: 3. <BRK>Then, the algorithm chooses state action pairs of the batch data that have returns larger than constant times the upper envelope. Also, I believe the paper should be compared to Bear Q learning as well, as it is very easy to implement and outperforms BCQ by a large margin. While there is no theoretical guarantee on the performance of the algorithm, the design of the algorithm does not follow the usual design the other researchers follow. The way of reporting the experiment results does not seem very professional. I recommend the authors to consult with some other researchers who have publication experience. These are some points that I could not understand:1. Also, why are the standard deviation in the Figure 2 and the Table 1 so different?<BRK>(9) Why do different batches with different seeds and the same algorithm lead to widely different results for batch RL? Is it because of the off policy RL methods used to collect the data, is it due to the batch RL method used? My decision is influenced by two main reasons:(1) Although the simplicity of the method is apparent and a very desirable feature, the authors don t highlight situations where this can lead to bad policies. Even though I pointed out a very specific case, one could think of many other cases where the proposed approach might result in a bad policy. (2) Experimental results are a little unsettling. In any way, this needs to be addressed. Also, as raised in the previous point I think using these Mujoco locomotion environments is not convincing enough to claim that BAIL is a viable competitive batch RL approach. I am curious(4) The paper makes some subjective statements such as "BEAR is also complex", which is not substantiated well enough. Refrain from making such statements(5) Not comparing to BEAR because their code is not publicly available is a contentious reason.<BRK>The authors propose to estimate a smooth upper envelope of the episodic returns from the dataset as a state value function. Recommended decision:The direction of imitating "good" actions from the dataset is interesting. But the solution proposed by the authors is wrong in principle and cannot be simply justified by "good empirical performance". The actual "smoothness" also depends on the other term in the loss (same lambda does not indicate same smoothness in different objectives). I appreciate the motivation that the authors try to validate the use of their objective to learn a "smooth upper envelope" but most of these statements are somewhat trivial and/or wrong section 4.1 does not actually deliver a valid justification. Although it is fine to use a conservative estimate it would be better to be explicit about this and explain why this may not be a concern. The baseline online trained policy should not contain noise for a fair comparison. That said, in batch RL setting it is not necessary to compare to online trained policy because it is a different setting. But if the authors want to compare to those, choice of baseline should be careful.
Reject. rating score: 1. rating score: 1. rating score: 3. <BRK>In this paper, the authors performed an empirical study on the importance of neural network weights and to which extent they need to be updated. Some observations are obtained such as from the third epoch on, a large proportion of weights do not need to be updated and the performance of the network is not significantly affected. Overall speaking, the qualitative result in the paper has already been discovered in many previous work, although the quantitative results seem to be new. There is no deep discussion on the intrinsic reason for this, and what is the most important factor that influences the redundancy of weight updates. The authors came to the conclusion that from the third epoch on, no need to update most of the weights. Why is it?<BRK>Particularly, the authors show that freezing weights which have small gradient in the very beginning of the training only results in a very slight drop in the final accuracy. This paper should be rejected because (1) the paper only provides some empirical results on freezing network network weights, I don t think there are much insights and useful information; (2) To my knowledge, the phenomenon that only a few parameters are important has been observed before by many papers. Given that, I vote for a rejection.<BRK>The paper presents the empirical observation that one can freeze (stop updating) a significant fraction of neural network parameters after only training for a short amount of time, without hurting final performance too much. The observation that weights can be frozen is somewhat interesting, although similar findings have been reported before. The authors mention that fully parameterized models are expensive to run, but they don t demonstrate any speed ups using their approach. Such speed up would also not be expected since the forward pass of the algorithm cannot get faster by freezing weights, and the impact on the backward pass is limited. I d be willing to raise my rating if the authors can convince me of the usefulness of their algorithm.
Reject. rating score: 1. rating score: 3. rating score: 3. <BRK>This paper proposes Tree search Policy Optimization (TPO) algorithm for tasks with continuous action spaces. It does not have the same sample efficiency as other off policy methods (like SAC, DQN, DDPG), and it does not need to face the same difficulty like importance ratio corrections. 1.This paper is basically a special case of dual policy iteration methods (as mentioned), with some heuristics to make MCTS work in continuous action spaces. Overall, this paper has no contributions on theory. Actually, I was wondering how can TPO work with other true off policy algorithms like the mentioned SAC and RERPI. The maximum timestep is 4M.<BRK># SummaryThe paper proposes to use MCTS for fine tuning a policy in continuous control tasks. # DecisionThere are some concerns regarding the novelty of the proposed method. Therefore, I currently refrain from recommending this paper for publication. Clearly, if one starts with a good policy, it can only get better from there. So, one can directly start with a PPO pretrained policy. The algorithm is not compared to any other approach. Are there no similar methods?<BRK>This paper proposes a hybrid approach that combines MCTS with policy optimization. The proposed hybrid framework enables MCTS planning on continuous action problems. The method is demonstrated on continuous control tasks such as Humanoid with high dimensional and continuous actions. In addition, some of the descriptions in the paper are unclear that makes it hard to understand. The policy network is trained using a gradient based approach. It would be more valuable if the authors pay some discussions for this limitation and demonstrate it in experiments.
Reject. rating score: 1. rating score: 3. rating score: 3. <BRK>This paper studies the implicit regularization in deep learning under the over parameterized setting. To my knowledge, implicit regularization or implicit bias statements in prior works cited in this paper are all about the convergence to a specific solution for underdetermined problems. In comparison, Theorem 2 just gives some bounds that holds for every sgd iteration. I cannot see any connection between Theorem 2 and implicit regularization. Therefore the implicit bias of (stochastic) gradient descent for DNNs in the over parameterized regime is essentially implicit bias of (stochastic) gradient descent for linear models (for square loss). The remark “in most cases, the authors used GD to derive their results by the NTK analysis” is also not convincing.<BRK>In particular, the authors propose a novel technique called "random walk analysis" to study the nonlinearity of the neural network with respect to the input data points. After reading the authors  response, I still think that the generalization results in this paper are not significant and important, and how the theoretical results can be related to the implicit regularization.<BRK>This paper suggests a new technique to analyze the implicit regularization caused by ReLU activations. Generalization to more complex activation functionsIf I understand correctly, the interpolation technique between two points only works for ReLU functions. Prior work on generalization of SGDI was really expecting a discussion about how the generalization bound derived in this paper compares to prior work, e.g.Hardt, Moritz, Benjamin Recht, and Yoram Singer. Brutzkus, Alon, et al."Sgd learns over parameterized networks that provably generalize on linearly separable data." And many others…For instance the bound derived in Hardt et al.is also of the order O(n^ 2). Noise SGDMy understanding is that the authors assume that the noise of SGD is Gaussian.
Reject. rating score: 3. rating score: 3. rating score: 6. <BRK>The paper provides cases where the action value function is difficult to approximate and is much more difficult than the dynamics of a model. 1.In term of the motivation, those cases listed in the paper are interesting, but they are not representative. In fact, the Dynamics can be far more complicated and it is still an open problem regarding how to learn the Dynamics. The method itself is not novel and it basically down weights the bootstrap estimate. It is very intuitive that some appropriate combination between the two can yield better performance. I believe we can still expect a large NN to approximate the action value function very well.<BRK>Based on theoretical and empirical analysis, the authors propose a new model based RL algorithm that said to improve task performance. Overall, the paper pursues an interesting and ambitious problem on the interplay between model based and model free approaches, and the expressivity of the representation of dynamics, policy and value functions. However, the results in the theory part is drawn based on analysis on a very simple and special task. Therefore the theoretical results can not be considered general for all MDP cases. In addition, these results are not surprising.<BRK>This paper presents a mainly theoretical argument comparing the expressivity of model free and model based RL methods contrary to analysis in the past which usually relies on sample complexity. I think this paper should be cited. 2.In terms of the experiments, it is hard to understand the significance and connection to the theory. The theory talks about the expressive power of Q functions, which suggests that we should look at only the asymptotic performance on these tasks, but most of the results are similar to MBPO or SAC in terms of asymptotic performance, although with a different learning speed, which could have to do with different factors. 3.This paper shows the existence and provides a constructive proof for a family of MDPs where expressing optimal Q functions is exponentially harder than expressing dynamics. For example, on the gym benchmarks, we would expect fairly not so complicated Q functions   although they might take longer to learn. 4.There is also a divide between learning Q*, and learning a policy that optimizes Q* reasonably well.
Reject. rating score: 3. rating score: 3. rating score: 6. <BRK>The paper proposes an algorithm for mitigating poisoning attacks in federated learning settings and compares it , on four different datasets, against state of the art baselines.<BRK>This paper presents an approach to robust federated learning that uses robust regression to weigh all the model parameter coefficients in order to achieve the robustness. They test the robustness of these algorithms to label flipping (MNIST, CIFAR 10), backdoor attacks, and multiplicative gaussian noise corruption of the model coefficients. Overall the paper presents an interesting and novel approach to robustness in FL, using a robust regression estimator to aggregate the model coefficients.<BRK>The paper proposes an aggregation algorithm, based on repeated median regression and residual based weighting to defend federated learning from adversarial attacks. Experiments are shown to demonstrate the method robustness against label flipping, backdoor and Gaussian noise attacks.
Reject. rating score: 1. rating score: 3. rating score: 3. rating score: 3. <BRK>The paper models the neural architecture search (NAS) as a network pruning problem, and propose a method to sparsify the super net during the search of architectures. "Bayesnas: a bayesian approach for neural architecture search". Q2."adaptive stochastic natural gradient method for one shot neural architecture search". This paper has comparable performance, but it is also much faster. From Tables 2 & 3, the proposed method is not better than STOA on the accuracy or number of parameters.<BRK>This paper proposes a channel pruning approach based one shot neural architecture search (NAS). The results on CIFAR 10 are reasonably good, but the results on ImageNet are not competitive to other NAS works. Therefore, I recommend the authors compare this technique with other pruning techniques, such as NetAdapt (https://arxiv.org/abs/1804.03230 ) and AMC (https://arxiv.org/abs/1802.03494).<BRK>This paper aims to search a sparse but competitive architecture with using a single fixed type of operation by proposing a channel level neural architecture search (CNAS). If we just compare CNAS R or CNAS W, they are not better than baselines. Main argumentCNAS is a straightforward combination of NAS and pruning. As the author said in the section 2.1, CNAS method can be seen as two separate processes: training a supernet like one shot NAS methods and then conducting pruning on the found supernet using Taylor expansion criteria. Many related works are missed in the paper. From the results of the experiment, the improvement of CNAS is not convincing.<BRK>This paper aims to propose a novel framework for neural architecture search. The sparsity can also be achieved on the level of channels. First, the proposed method is quite straightforward and can be viewed as a quick extension of existing structures. Second, the reported results do not seem to be promising since the improvement was marginal. It is also very difficult to tell whether the contribution is brought by the proposed sparse structure or the adoption of autoaugment since the baseline methods are not applied with it. Third, the paper has not been well written and there are grammatical mistakes throughout the manuscript.
Accept (Talk). rating score: 8. rating score: 8. rating score: 6. <BRK>This manuscript presents a number of algorithmic techniques to reduce the computational and space complexity of Transformer, a powerful and very popular deep learning model for natural language processing (NLP). The techniques presented in this manuscript seem to be very reasonable and the experimental results also indicate that they are effective. My major concern is that the authors shall present more detailed experimental results. In addition to bits per dim, it will also better if the authors can evaluate the performance in terms of other metrics.<BRK>This paper presents a method to make Transformer models more efficient in time and memory. The proposed approach consists mainly of three main operations:   Using reversible layers (inspired from RevNets) in order to prevent the need of storing the activations of all layers to be reused for back propagation;   Using locality sensitive hashing to approximate the costly softmax(QK^T) computation in the full dot product attention;  Chunking the feed forward layers computations to reduce their cost. The problem approached by the paper is interesting and the proposed approach is novel to the best of my knowledge. It seems to me that the same operations can be achieved with different Q and K. Indeed, doing so, the authors slightly reduce the capacity of the model. The reported results can be made stronger by reporting average/error bars across several trial to show consistency.<BRK>This paper presents an attempt to reduce the memory complexity of Transformers. It presents a LSH based self attention mechanism, along with reversible adaptation of Transformers. The Locality sensitive hashing scheme reduces complexity from L^2 to L which is pretty neat. I think the LSH based attention quite novel and is a natural solution to reducing the complexity of the self attention module. I am mainly concerned if the Reformer works on tasks such as machine translation or other NLP tasks. Can this be done in an end to end differentiable manner? 4) Can the authors present some results on other tasks? While neat, I think other tasks (e.g., MT or QA) can be investigated to further ascertain that the LSH attention works well.
Reject. rating score: 3. rating score: 3. rating score: 6. <BRK>I am not sure about my assessment of this paper. The authors note that when beta 0, NovoGrad becomes layer wise NGD with decoupled weight decay. The paper includes a set of experiments containing different methods and their hyperparameters. It is not the case for NovoGrad because of its gradient normalization.<BRK>The idea is from the LARS and AdamW. I am worried about the novelty of this paper. Although the idea is straightforward, the proposed method may be helpful for the community. It is just a combination of AdamW and LARS.<BRK>The authors acknowledge that weight decay decoupling is already present in the literature, so it appears that the only contribution is the layer wise normalization. The motivation for this modified version of AdamW are unclear, but the empirical results are convincing and rigorous. Proof of convergence even in a deterministic convex setting are missing, so the reader has to extrapolate correctness from previous work on adaptive gradient descent. On the other hand, the proposed algorithm is tested on a great variety of tasks, using state of the art models.
Reject. rating score: 3. rating score: 6. rating score: 6. <BRK>1.The significance of the theoretical analysis in Eq.(1) to Eq.(4) needs to be better explained. The rating is 3: Weak Reject considering that the novelty is limited and the experimental study is weak.<BRK>What is supposed to be the output of it? Overall, I think this paper is of good quality and proposes an interesting method for this crucial task of source separation. The figure 1 is very useful, but I found it not clear enough and too small.<BRK>The authors propose complex valued neural networks to perform audio source separation in the Fourier domain.
Accept (Poster). rating score: 8. rating score: 6. rating score: 6. <BRK>The authors propose a Graph to Sequence Reinforcement Learning Model for Natural Question Generation, evaluated on SQuAD benchmark in for Question Generation. An interesting aspect of the work is related to the Graph2Seq model, and the use of the Reinforcement Learning to fine tune the model.<BRK>The paper seems not well motivated. I have a few questions regarding the model and experiments. First, a reasonable baseline could be using Transformer based sequence to sequence model. I think you should justify why graph structure is important in your experiment.<BRK>This paper focuses on improving the performance on the task of natural language generation. To this end, they propose a  graph to sequence (Graph2Seq) model for the task of question generation which exploits the rich structure information in the text as well as use reinforcement learning based policy gradient approach to address the exposure bias and inconsistency between test/train distributions in cross entropy optimization setup. The proposed model achieves state of the art results on question generation, which are further validated with human evaluations. However, the importance of this module is not well studied in the experiments section.
Reject. rating score: 1. rating score: 3. rating score: 6. <BRK>Yes, it is true that there are no enough studies that has studied this problem in a principled way. There is not much novelty in this work. The rest of the paper is mere brute force. They compute the RSA between two tasks, at multiple predetermined layers of a model and whichever layer shows a lesser RSA, they start branching there. Very surprising that the authors did not spend a good analysis over the computation time of the proposed approach. As far I can understand, the computation of the proposed approach is going to be really expensive. a.Train a task individual DL model on each of the taskb. I really dont like the comparison made in this paper with NAS   NAS really is a search optimization problem, finding new architectures. However, the technique proposed in this paper is more of a brute force comparison at pre determined layers to compute which is the better of determined subset of n layers. There is no takeaway for me a researcher from this paper.<BRK>This paper proposes a novel soft parameter sharing Multi task Learning framework based on a tree like structure. The idea is interesting. However, the technique details, the experimental results and the analysis are not as attractive as the idea. The proposed method is a simple combination of existing works without any creative improvement. Besides, there is not enough analysis about the idea this paper proposed. The intuition that more similar tasks share more parameters probably cannot always ensure the improvement of MTL.<BRK>The main idea is to first train a specific, standalone encoder/decoder network for each task. Subsequently, a task affinity factor is computed by looking at the similarity (or, more likely the dissimilarity) of an holdout set of images feature representations. Knowing these dissimilarities (based on RDM), one can cluster the tasks and make similar tasks share more of their layers. The experiments provided are extensive and cover most of the current multi task learning methods and interesting problems. I especially like the idea of formalizing the dissimilarity between tasks using RDM. First, except for CelebA, the experiments provided use ResNet50 with only 4 different "anchor point" in the network. CelebA provides a more complex case, but it also requires to change the method from an exhaustive search to a beam search (end of Sec.3.2).Doing so get us back to a kind of greedy approach, precisely what was advocated against the paragraph before (in the discussion about Lu et al.2017).Second, the fact that task affinities are computed a priori leads to the following conclusion: "this allows us to determine the task clustering offline". Out of curiosity, did you consider other correlation coefficients than Spearman? I am not sure the impact would be very high, since it can probably be replaced by an architecture exhaustive search if the number of tasks and branching points in the network are low, but the formalism of the approach is welcomed.
Reject. rating score: 3. rating score: 3. rating score: 8. <BRK>In this paper, a tensor decomposition method is studied for link prediction problems. The model is based on Tucker decomposition but the core tensor is decomposed as CP decomposition so that it can be seen as an interpolation between Tucker and CP. The performance is evaluated with several NLP data sets (e.g., subject verb object triplets). (A) The idea of combining CP and Tucker is not new.<BRK>* Summary:The paper introduces a novel tensor decomposition that is reminiscent of canonical decomposition (CP) with low rank factors, based on the observation that the core tensor in Tucker decomposition can be decomposed, resulting in a model interpolating between CP and Tucker. The authors argue that a straight application of AdaGrad on this decomposition is inadequate, and propose Ada^{imp} algorithm that enforces rotation invariance of the gradient update. * Comments:Although the approach is well motivated, the paper has many ambiguities that need to better clarification. 1.Tucker decomposition results in lower dimension factors, "d" in the paper. Please provide further explanation on this part. First, the paper does not explain the meaning of evaluation metrics.<BRK>The authors present a new way of decomposing 3 order tensors which uses interpolationbetween the Tucker and CP decompositions, called CPT. The authors also provide a new optimization algorithm called ADA imp, for learning this decomposition,which is a variant of Adagrad adapted to their settings. The paper is overall interesting, clearly written and well motivated. The authors also show favorable experimental results on two knowledge base datasets, with improved loss vs. #parameter used tradeoff. does this mean there is some structure that must be present in the tensor? The authors present their method in the context of knowledge base completion, thus for tensors of order 3, but it is not clear if any of the components they proposed indeed specialized for this problem, or is it a contribution to general tensor decomposition.
Accept (Poster). rating score: 6. rating score: 6. rating score: 6. <BRK>Some of the comparisons (HIRO) do not agree with the results in the HIRO paper. More detail should be included in the caption. In the paper, it is noted that HRL has high sample complexity and needs lots of training data. I like the idea of using an attention model to help pick learn a weighting for the combination of a number of sub policies. This information is very important to make sure the method is not overly biased to the composition of them. Can this contradiction be explained? For the HIRO comparison was the system also using the composite policies there were pretrained?<BRK>Functionally, this is equivalent to regular RL using domain knowledge to engineer the action space. This paper addresses the hierarchical RL problem of combining multiple primitive policies (pi_1, …, pi_K) into policies for more complex tasks. The encoded state and the hidden states of the RNN are used to output an attention weight over each primitive. This method is evaluated on several mujoco tasks, such as making a cheetah jump hurdles by combining “forward” and “jump” primitives. Why should the primitive’s action be encode in order?<BRK>This paper presents an approach in which new tasks can be solved by an attention model that can weigh the contribution of different base policies conditioned on the current state of the environment and task specific goals. I found the paper interesting and well written. Additional comments:  Where is the training graph for the Composite SAV applied to the “ant fall” task? Algorithm 1 should probably be moved to the main text.
Reject. rating score: 1. rating score: 3. rating score: 3. <BRK>Also, I think it is not appropriate for ICLR since it assumes knowledge of quantum computing that most people at this conference would not have, and I as a reviewer do not possess, and hence cannot evaluate this paper.<BRK>), or whether this is some fully classical variant of the algorithm being tested. Overall, while I like the idea of parameter estimation on a quantum computer, I could not recommend accepting the paper in its current form.<BRK>The authors present a quantum algorithm for expectation maximization on a quantum machine, which is poly logarithmic in the size of the dataset (and polynomial in other parameters such as the dimension of the feature space, and number of mixture components, condition number of the data and covariance matrices, some precision/error parameters etc) per iteration.
Accept (Spotlight). rating score: 8. rating score: 6. rating score: 6. <BRK>General:The authors propose a method to train individually fair ML models by pursuing robustness of the similarity loss function among the comparable data points. The main algorithmic tool of training is borrowed from the recent adversarial training, and the paper also gives the theoretical analyses on the convergence property of their method. Pros:1.They make the point that the individual fairness is important. 2.The paper proposes a practical algorithm for achieving the robustness and the indivdual fairness. What is the empirical convergence property of the algorithm? 2.It seems like the main tools for algorithm and theory are borrowed from other papers in adversarial training e.g., (Madry 2017). 4.What happens when you use more complex models than 1 layer neural net?<BRK>SummaryThe authors propose training to optimize individual fairness using sensitive subspace robustness (SenSR) algorithm. The authors show improvement in gender and racial biases compared to non individual fair approaches. I am not sure whether this is the only work in this direction (baselines are not for individual fairness). 3.Some of the metrics in the experiments are not precisely defined such as Race gap, Cuis. 4.Some baseline models are not clearly defined such as “Project” in Table 1. 5.Not sure how Section 3 connects with the rest of the paper. What’s TV distance in introduction?<BRK>This paper proposes a new definition of algorithmic fairness that is based on the idea of individual fairness. This should have been at the beginning of section 2 in order to motivate the derivation. The idea of the fairness constraint is that by perturbing the inputs (while keeping them close with respect to the distance function), the loss of the model cannot be significantly increased.
Accept (Poster). rating score: 8. rating score: 6. rating score: 6. <BRK>This paper considers the problem of obtaining an optimal regret algorithm in a distributed setting without incurring a large communication cost. In the standard and linear MAB settings, the authors propose algorithms and show that they achieve optimal regret up to logarithmic factors with communication costs that are almost independent of the horizon T. In addition the authors establish interesting lower bounds on the communication cost to obtain sublunar regret. Overall I recommend the paper for acceptance. I understand this repeated work doesn’t affect the regret   but it is an artifact of using elimination.<BRK>The results are interesting and I do not have any objections with the paper, except that ICLR might not be the right avenue for such work given that it lacks any ideas regarding representation learning. The authors study a bandit problem where there are multiple agents (say, M) and each of the agents is playing a multi armed bandit problem for T rounds. For both these settings, the authors establish elimination style algorithms with communication. The agents can communicate with each other in order to achieve small regret.<BRK>The paper considers the problem of distributed multi arm bandit, where M players are playing in the same stochastic environment. The goal of the paper is to have small over all regret for all the players without a significant amount of communication between the players. It seems that in the MAB setting, the lower bound could be further strengthened with a log(K) factor, since removing this factor would ultimately require "dynamic epoching" which is not possible with limited communication.
Reject. rating score: 3. rating score: 6. rating score: 6. <BRK>The paper would be much stronger if it also provided comparisons to more recent models in representation learning closer to state of the art. •	Lastly, the paper needs a revision for ease of readability, as there are a significant number of grammatical errors that make it hard to read at times. Overall, the work in this paper has the potential to be a contribution to ICLR but lacks experimental completeness and clarity. Moreover, the main contribution is a better denoising autoencoder, but in the grand scheme of representation learning, it is unclear how broad of a contribution this is.<BRK>2.As an empirical work, the experiments in this work is rather small scale, using only CIFAR10 and MNIST. This paper proposes a denoising auto encoder where the input image is corrupted by adding noises to its Laplacian pyramid representation. By corrupting the Laplacian representation, which is multi scale, the corruption of the image is not local and thus more robust representations are learned. 1.Is it possible to generalize this idea to other representations of the images such as wavelets or sift, or the representations learned by other neural networks?<BRK>In the conventional DAE, the noise is directly added to the input space. This paper introduces a novel type of noise, which corrupts the Laplacian pyramid representation, and uses it to train a DAE. Several experiments are conducted showing the effectiveness of the method1) On MNIST, LapDAE provides better reconstruction images2) On Cifar 10, LapDAE provides better image retrieval results3) On Imagenet, combining with the transformation technique in [2], LapDAE achieve state of the art result4) On Pascal VOC,  combining with the transformation technique, LapDAE achieve state of the art result on transfer learningOverall, I find the idea natural and simple (simplicity of an approach is a good quality to me). The following are further comments and questions. * From the experiments, standard DAE are harder to train comparing to the proposed LapDAE. In my opinion, this suggests that the local noise are harder to remove comparing to the Laplacian noise.
Reject. rating score: 3. rating score: 3. rating score: 3. <BRK>Unfortunately, before any other comments I would like to point out that this paper is not properly anonymized. Apparently there is also an arXiv version of the paper which I was not aware of and which would be fine if this version was not cited from the README file in the submitted code. Why not present results for harder problems?<BRK>The key technical contribution is by controlling the KL divergence of the learned policy with a prior policy that was learned on other dialogs. The method is able to constrain the policy not deviate too much from the prior to avoid extrapolation error. Overall, the paper is fairly well written. The paper develops a method for learning human preferences in dialogs in off policy reinforcement learning and use KL control to avoid extrapolation error issues. The experiments are illustrative but more comparisons with the other methods will be appreciated to make it more convincing.<BRK>This paper proposes an off policy batch reinforcement learning algorithm, Way Off Policy(WOP). The authors address the extrapolation error which is a general problem of batch reinforcement learning. Moreover, I m not sure if it is novel because I think that it is not much different from KL penalize proposed in RL. (If so, a detailed explanation needs to be added in the paper.) It is shown that the normalization constant is missing in Equation (9).
Reject. rating score: 1. rating score: 3. rating score: 3. <BRK>This paper analyzed a few issues of Adam, and proposed a new variant of Adam called AdamAL. The simulation does not show the advantage. 1) Heuristic observations. One main observation is that v_{t+1}   beta_2 v_t + (1   beta_2) g_{t+1}^2 in the original Adam will be different from \hat{v}_t defined in AMSGrad. "Does it really solve the problem or dose this design violate the intuition of Adam Type algorithm"? what is the "intuition" to be violated? I roughly get the point of this sentence, but "intuition" is not a good word here. It makes the paper very difficult to read.<BRK>The authors considered both of them as specialized cases of mirror descent algorithms and provided a new algorithm AdamAL. Pros:The authors provided a novel framework to analyze Adam type algorithms by using standard FTRL framework. The authors also provided a new algorithm AdamAL to overcome some shortcomings in previous algorithms. Cons:  The novelty of this paper is limited. I suggest the authors explain more on the algorithm design.<BRK>The paper proposes to study some weaknesses of Adam and AMSGrad and propose a new method called AdamAL that is evaluated on CIFAR10. I am not from this area, but unfortunately I find this paper to not be rigorously written or organized. However, it is not the detailed types of experiments I was expecting of a paper that points out specific issues in AMSGrad.
Reject. rating score: 1. rating score: 3. rating score: 3. rating score: 3. <BRK>The main idea is to train a discriminator to detect whether the explanatory saliency map is consistent with the input. Experiments have been conducted on CIFAR10 and SVHN to validate the method. The use of SHAP as the only explanation method is not well explained. (2018).A simple unified framework for detecting out of distribution samples and adversarial attacks. Doubts on the effectiveness of the proposed method. But in this paper, the authors assume that the explanation saliency map for normal examples are perfectly correct and used as positive instances for training the discriminator.<BRK>This paper suggests a method for detecting adversarial attacks known as EXAID, which leverages deep learning explainability techniques to detect adversarial examples. Though method is well presented and the evaluation is substantial, the threat model of the oblivious adversary is unconvincing.<BRK>#WeaknessThe idea of this paper is based on the interpretation method of DNN. #Summary:The paper proposed a method that utilizes the model’s explainability to detect adversarial images whose explanations that are not consistent with the predicted class.<BRK>The paper proposes a method to check whether a model is under attack by using state of the art explainability model, SHAP. Using explainability to detect the presence of adversarial attacks seems like a nice intuitive idea and the results show that it indeed works.
Accept (Spotlight). rating score: 8. rating score: 6. rating score: 6. <BRK>Similar ideas in the last few years appeared, but only bounds on a compressed network were obtained. In contrast, the current submission gives a bound on the original (uncompressed) network in terms of the complexity of the compressed network class. The result is novel and quite interesting. Having weight matrices to be close to rank 1 seems unrealistic.I would like to see some sort of empirical evidence if the authors believe that this is the case. And for larger ranks, the bound seems to be close to VC bound.<BRK>This paper obtains a compression based generalization bound (Theorem 1) for the original network, while prior work gives bounds for the compressed network. In some cases, the bound given by Theorem 1 for the original network could be better than the bound for the compressed network. In terms of proof techniques, Lemma 2 is a general result to control the local Rademacher complexity using upper bounds on the covering numbers, which is interesting and could be useful in other problems.<BRK>This paper provides generalization bound based on the compression arguments. A key contribution is that instead of bounding thepopulation risk for the compressed model, this paper manages to give bounds on the non compressed network following a unified analysis framework. Also, this paper applies the unified framework to low rank assumption on weigh matrices and covariance matrices. The central contribution here is using localized Rademacher complexity to bound the $L_2$ norm between original network and compressed network. I would appreciate it if the authors could give a thorough response to questions mentioned above.
Reject. rating score: 1. rating score: 3. rating score: 3. <BRK>(2) The experimental section is questionable since there are missing methods in the comparison and no error bars in the experiments. Summary of the paper:  The authors propose a meta learning approach for BO. The proposed method is validated on several experiments. Detailed comments:The writing of the paper needs to be improved. The intro on Bayesian optimization has to be improved, it explain very poorly this technique. In the related problems the actual objective is unknown. The authors should consider several repetitions with different random seeds or different problems. Therefore, it is not possible to understand why the proposed approach works better. This will allow to check that the meta learning procedure is useful.<BRK>However, there seems to be multiple highly restrictive (at times impractical) assumptions in this work that are atypical of the BO setting adopted by other meta BO algorithms and not discussed, as detailed below. This does not seem to hold true. This is not true: The GP hyperparameters need to be learned and they adapt to new problems. Can the authors discuss why is this a reasonable assumption? Isn t it more natural to consider a single Bayesian neural network instead of using a neural network for the mean and a GP for the variance? Can the authors explain in greater detail how they run their algorithms (Algorithms 2 and 3) in the experiments? For example, the authors say that "WRA N starts with learned initial surrogate model".<BRK>Such technique facilitates Bayesian optimization as well, warm starting Bayesian optimization. The current paper takes a similar step, learning neural surrogate model from related tasks to warm start Bayesian optimization. The main idea is to replace the mean function of GP by neural surrogate model, so that parameterized models are used for meta training, in the framework of RETILE. Weakness   While mean function of GP is replaced by a neural surrogate model, the posterior variance of GP should be calculated. It is not clear why minimizing ranking loss makes sense in this case.
Reject. rating score: 1. rating score: 1. rating score: 1. <BRK>4e.The statistical significance of Figure 31 and Figure 32 is not provided. The new method employs a cascade/serial architecture for performing the style transfer. No experiments to explore the architecture hyperparameters. Minimal quantitative analysis. A vast majority of the results (30 of 32 figures) are qualitative comparisons and the paper is sorely lacking an emphasis on quantitative comparisons. I wish the authors dedicated more emphasis in this paper to a detailed quantitative comparison for these methods. For that matter, I would expect that some methods work better on some styles or images.<BRK>The authors proposed to mix together multiple styles by proposing two frameworks: 1) serial style transfer (SST), which combines style transfer methods in series; and 2) parallel style transfer (PST), which combines style transfer methods in parallel. Though not much studied, this topic is not new [ref 1], [ref 2]. The authors didn t provide a thorough literature review on mixing multiple styles in the related work or anywhere else in the submission. In terms of the methodology, the novelty is quite limited. are determined.<BRK>This paper proposes two ways to aggregate existing style transfer methods and shows improvements on quality and flexibility. To me this is more like an engineering effort rather than a research work. Authors should focus more on the unsolved issues in style transfer, e.g., how to do geometric style transfer (shape). The quality of results might be improved but there is little novelty.
Reject. rating score: 3. rating score: 6. rating score: 8. <BRK>For example, the subsection on Split Linearized Bregman Iteration in the "Methodology" section does not contain anything new compared to [1], and this is not clear enough to the reader. * The newly written conclusion is still incorrect, stating again that Split LBI achieves SOTA performance on ImageNet. * One important issue with the paper is that it blurs the distinction between prior work and the new contribution.<BRK>This paper is concerned with methods to enforce sparsity in neural networks (specially CNN for image classification) at training time. I recommend (other than for the length issue) this paper for publication. In particular, the use of "that" and gerunds vs present tense is mostly erroneous. the paper lacks a conclusion which should bring together several strands of argumentation  I strongly suggest publishing the source code, not just promising to release them upon request. it is insufficient to state that "official source code" was used, as in table 1; this should definitely state the (Github or other) URL where the code is available<BRK>This paper studies the problem of finding important structural sparsity of over parameterized neural networks. 2.The convergence guarantee of the proposed method for solving nonconvex optimization is established based on a novel Lyapunov function. 3.Extensive experiments show that the proposed algorithm can: (a) train over parameterized neural networks to achieve the state of the art performances on image classification tasks; (b) find sparse networks with competitive networks; and (c) early stopping plus retrain from scratch can achieve similar or better  performance than existing compression method. I believe that this work is very interesting and will be of interest to the ICLR community. The presentation of this paper is clear and the idea of coupling the gradient descent and mirror descent is very interesting. Minor comments:1.There is no definition of KL function in the main content.
Reject. rating score: 3. rating score: 3. rating score: 6. <BRK>5.The term “dynamical critical distance” is not used uniformly. On the whole, the paper is written clearly and explains its methodology in simple language. This along with other concerns mentioned above mean that I cannot recommend this paper for publication. 4.The paper fails to motivate the the utility of the concept of “Dynamical Critical distance”.<BRK>The equivalence between eq.2 and the two parts of eq.3 is not obvious. there is a link between generalization and stiffness2.) This is observed for different models on different datasets6.) Since overfitting is the point in training where train error and test error diverge it is obvious that this can also be observed with regards to "stiffness". Disclaimer: This review was done on short notice.<BRK>This submission introduces a metric, termed stiffness, to evaluate the generalization capability of neural networks. It can not support author s claims well. They demonstrate that stiffness is a useful concept for diagnosing and characterizing generalization. However, this submission focuses on generalization problem during transfer learning.
Reject. rating score: 3. rating score: 3. rating score: 8. <BRK>Summary: The authors propose a new loss function as well as an adjoining visualization for improved performance of hard negative / easy positive mining for deep triplet metric learning. Especially important since on several datasets, the model does not perform as well as others. Comments, questions, and concerns:  Overall the paper is well written and clear.<BRK>Authors analyze the gradient of easy positive and hard negative pairs and propose a second order loss for metric learning. 2.The comparison is not convincing.<BRK>The authors propose a simple modification to the desired gradients and derive a loss function that gives those gradients.
Reject. rating score: 3. rating score: 6. rating score: 6. <BRK>Each entity (or relation) is associated with a distribution over the entities (or relations) in the other KG through an exponential kernel density like estimate. Most of the key ideas have already been covered there, and I would like to see a comparison of this work with that before acceptance.<BRK>The authors aim at learning alignments between knowledge graphs. For this they use a discriminator that functions as an adversary to the parameterized triple alignment function leading to adversarial training. Furthermore, they regularize the training by maximizing a neural mutual information estimator. Several experiments seem to indicate that their approach improves over competing methods in the unsupervised and weakly supervised setting.<BRK>5.The paper is well written and easy to follow. Even though there are a family of unsupervised approaches for domain alignment, this paper is the first to solve the knowledge graph alignment problem in an unsupervised/weakly supervised way.
Accept (Spotlight). rating score: 8. rating score: 6. rating score: 6. <BRK>The paper proposes a meta reinforcement learning algorithm called MetaGenRL, which meta learns learning rules to generalize to different environments. The experiments show that the learned neural objective can generalize to new environments which are different from the meta training environments. The meta training has only at most two environments and the generalization of the neural objective function is only performed at one environment. It would be great if the authors could show more results with more meta training environments (say, 10 meta training environments) and more meta testing environments (the current setup is only with one);2. At the same time, at Page 8, Section "Dependence on V" actually acts as an example of LSTM couldn t figure out an effective variance reduction method during the short meta training time. Note that in Equation (1), r(s_1, a_t) has discount gamma^1, which is not true, I d recommend the authors to follow the time indexing starting from 0, so that the Equation (1) is correct.<BRK>There are a ton of factors at play   exploration, meta generalization, meta training, inner training, instability of ddpg, so on. They make use of a learned objective function used in combination with DDPG style update. Summary:This paper presents a novel meta reinforcement learning algorithm capable of meta generalizing to unseen tasks. Motivation:The work is well motivated and is tackling an important problem. Given that this is the majority of your paper is empirically based this is my main criticism. I hope the authors continue to work to improve this! This is strange. Further study of these factors could be useful. Hyperparameters of your baseline do not appear to be tuned (taken from appendix) where as for your method has a number of choices.<BRK>The paper proposes to meta learn the objective function of a policy gradient algorithm using second order gradients of the objective function w.r.t the state action value Q. This is an interesting approach, however, I think the experimental evidence is not sufficiently convincing. In particular, I think the most important baseline that is compared against is not RL2, but DDPG: RL2 is not designed to generalize but to learn quickly on new tasks from the training task distribution. Cheetah, Hopper and Lunar Lander are very simple environments. The authors claim that the algorithm allows sharing of exploration strategies, which I don t believe can be the case based on it s current design. I m not familiar with the relevant literature, but this seems like a strong statement which I believe should be supported by a citation. There s a fundamental trade off between the speed of adaptation and the amount of overfitting.
Reject. rating score: 1. rating score: 3. rating score: 3. <BRK>~The authors propose the addition of multiple instance learning mechanism to existing deep learning models to predict taxonomic labels from metagenomic sequences.~I appreciate the focus area and importance of the problem the authors have outlined.<BRK>The combine 2 existing approaches for taxonomic classification, with established methods for Multiple Instance Learning, namely DeepSets and an attention based pooling layer. While the domain of taxonomic classification is interesting, I find there is a lack of novelty on the machine learning part. The authors combine well established methods in a straight forward manner and while the resulting increase in performance for some datasets may be relevant in the domain, the conceptual advances are too incremental for a machine learning audience.<BRK>The authors never explain how their MIL metagenomic model should be applied to a full metagenomic sequencing dataset. This paper will be much better once the authors add in estimation from full read sets and evaluate against standard tools.
Accept (Poster). rating score: 6. rating score: 6. rating score: 6. <BRK>This paper presents  a controllable model from a video of a person performing a certainactivity. I would ask for the ablation study/additional experiments of using each component. The generated video can have an arbitrary background, and effectivelycapture both the dynamics and appearance of the person. The overall pipeline makes sense; and the paper is well written.<BRK>This paper proposes a method to address the interesting task, i.e.controllable human activity synthesis, by conditioning on the previous frames and the input control signal. To synthesis the next frame, a Pose2Pose network is proposed to first transfer the input information into the next frame body structure and object. The proposed conditioned residual block, occlusion augmentation and stopping criteria seem to help the Pose2Pose network work well. Besides, the object is also considered in this network, which makes the method generalized well to the videos where human holds some rigid object. 3.The paper is easy to follow. 2.As to the results of the Pose2Pose network, I wonder if there are some artifacts that will affect the performance of the Pose2Frame network. Otherwise, the artifacts from Pose2Pose will affect the testing performance of the Pose2Frame network. Will the proposed mask term perform better?<BRK>The paper presents an approach to extract a character from a video and then maneuver that character in the plane, optionally with other backgrounds. The character is then redrawn into the background with a neural net, and all of this is done in real time. If I had a complaint, it would be that I did not learn anything scientifically from the paper. Hence, I rate it as a weak accept.
Reject. rating score: 3. rating score: 3. rating score: 6. <BRK>The main methodology proposed in the paper is inspired by the idea of parallel tempering from physics. The mapping of the choice to the temperature value is essential for the algorithm to succeed and as described in the paper, there is no hope of replicating the algorithm as the authors are extremely vague about how to figure out this mapping. The paper does not define what a numerical value of diffusion is. Note that hyperparameter optimization is an optimization procedure and it is very unclear to me why sampling from the distribution which will look like the product distribution over the replicas is even a good idea. If it is the case this needs to be specified carefully. This is a well known intuition that for parameters like learning rate one needs to adjust them periodically to achieve good training dynamics. The examples considered in the paper   batch size, learning rate, dropout rate. I think the authors should make this distinction and state it upfront because there are many hyperparameters that we optimize over which change the target distribution and not just the noise.<BRK>This paper presents an analogous optimization algorithm that is inspired by MCMC. The algorithm maintains a series of weight replicas and sweeps them according to loss functions, based on rules from statistical physics. I may have concerns, as in training neural network, most optimization algorithms use stochastic gradient which intrinsically contain noise inside, although certain level of gradient noise helps to escape saddle point on the landscape, there is no necessary analysis whether this additional noise would contribute to the optimization positively or negatively, after all the noise level has to be bounded for the optimizer to converge. The experiments are a little insufficient, as the authors used very small datasets, and there is no comparison with other carefully tuned optimization algorithms, which, circle back to my previous concern, is that whether such an algorithm could actually outperform a carefully tuned SGD, ADAM.<BRK>This work presented an improvement of grid search algorithm for certain hyperparameters in deep neural nets training. These hyperparameters, such as learning rate and drop out, have "temperature" like meaning to control the noise injected in the training. With this analogy, the author proposed to use the idea of parallel tempering in statistical physics to allow exchange these hyperparameters during the training. The main contributions are two folds: 1. the connection between some hyperparameters to its physical perspective (temperature) and 2. the empirical validation of the proposed algorithm. The paper is clearly written and easy to follow. 1.In Line 11 of algorithm 1, what is \alpha and how to update it? 2.How do the authors come up with the values for learning rate and drop out in the experiments?
Reject. rating score: 3. rating score: 3. rating score: 6. <BRK>##### Rebuttal Response:The other reviewers seem to have understood more than me. In my opinion the writing must be adapted to be interesting to the ICLR community and the bigger picture should be highlighted more, as the bigger picture is remains quite unclear at the current state. The contributions summarize the most fundamental works of RL but do not really relate these methods to the proposed approach. Currently I vote for borderline reject as I am familiar with RL & PDE S but do not understand the motivation and intention. I am leaning towards rejection as the paper is a resubmission from Neurips and has not been substantially improved. However, I am not certain about my evaluation.<BRK>The naiveness comes from the fact that the typical reinforcement learning problem, the agent needs to decide how to choose an action. While the motivation is interesting, the author argues this work is novel due to it does not fall under supervised learning, but rather reinforcement learning. The correct category for this work would be more similar to imitation learning using WANO’s algorithm as the expert label. This is a field of supervised reinforcement learning. While the framing of the problem is perhaps novel in the space of PDE, algorithmically there needs to have a breakthrough or new invention. Therefore, I reject this paper under the ICLR conference. Some suggestions to further improve this paper: The author could add CNN and RNN structure to the prediction model. The weaknesses of the paper are the lack of diversity in comparison with other models and the paper needs some level of novel breakthrough in an algorithmic sense.<BRK>This paper proposes to use reinforcement learning for constructing discretziation stencils of numerical schemes. For RL this task requires a continuous action space, and the DDPG algorithm is used for training the policy. The tests are quite thorough and interesting, while at the same time being limited in scope. What s also missing in the current version is a more thorough discussion of inference and training performance. And due to the large size of the trained model, which has to be evaluated for every single node in the 1D mesh, it s probably also quite slow. I think this is worth a discussion in the text. One could even estimate the number of operations necessary to evaluate the model, and run a higher order WENO scheme for a "fair" comparison. This could be clarified in the text (or written out). I am somewhat on the edge with this paper   the 1D case for the two equations is carefully evaluated in the submission, and it s great to see the trained model can improve the accuracy across a fairly wide range of settings. On the other hand, there are a range of open questions, as outlined above, and it s not clear whether the approach could be easily translated to higher dimensions. I hope the authors can clarify some of these points in the rebuttal, right now I m leaning towards the positive side.
Reject. rating score: 3. rating score: 6. rating score: 8. <BRK>Unfortunately, the sole novelty in this paper is a new justification for the STL estimator. There are no experiments. Specific points:* The "AISLE framework" is simply used to point out that IWAE and RWS optimize KLs in different directions for the parameters of q. This is well known in the literature and is discussed in several of the papers cited by the authors. * There seems to be an overall misunderstanding of the difficulties associated with multi sample objectives. * Furthermore, IWAE does not necessarily require reparameterizations to deal with the high variance of its terms. * As such, your claim to "have shown that the adaptive importance sampling paradigm of the reweighted wake sleep is preferable to the multi sample objective paradigm of importance weighted autoencoders" is far too strong, especially considering the fact that experimental evidence in Tucker et al.2018 shows there are situations where either one is preferable.<BRK>Summary: This paper presents a unifying framework through which much of the recent work on maximum likelihood learning in latent variable models (variational autoencoders) via multisample variational approaches and importance weighted approaches can be understood. Strengths:   The framework is elegant and the derivations are simple. IWAE can be applied in discrete settings (see, e.g., Mnih & Rezende, 2016) and, as you show, reparameterizations can be applied in the adaptive importance sampling type algorithms to potential improve the variance. I appreciate that it wasn t the primary aim of the paper to introduce new algorithms, but I think it could strengthen the contribution to consider at least a few. Are there any other interesting divergences to consider for the proposal distribution objective? I appreciate that there might not be much consistency in terms of which methods outperform others, but large scale experiments would at least present evidence of this point.<BRK>This is in contrast to viewing it as a biased estimator of the IWAE s q gradient. This is in contrast to viewing it as an unbiased estimator of the IWAE s q gradient. STRUCTURE:The article is well written and easy to understand. NOVELTY:A different view on IWAE STL and IWAE DREG is interesting and novel (as mentioned above). The recommendation of using RWS style algorithms over IWAE as given in the abstract ("we argue that directly optimising the proposal distribution in importance sampling as in the RWS algorithm is preferable to optimising IWAE type multi sample objectives) is also not novel since this is also advocated by [5] (section 3.2: "This makes RWS a preferable option to IWAE for learning inference networks because the phi updates in RWS directly target minimization of the expected KL divergences from the true to approximate posterior"). Are there different adaptive importance sampling algorithms that could be used within AISLE that would improve on IWAE STL/IWAE DREG/RWS? EXPERIMENTS:There are no experiments in the main paper. CONCLUSION:While I really like the presentation and connections made in the paper, I m not sure what the practical takeaways are (other than use IWAE STL, IWAE DREG, RWS over IWAE which is advocated by [2], [3], [5]). I m giving this a weak accept due to the former.
Reject. rating score: 1. rating score: 1. rating score: 3. <BRK>I didn t understand equation (3). It seems to be a variant of equation (4), and seems to be in disagreement with equation (6). Second, in terms of comparisons, the paper lacks adequate related work. Then there are many new deep topic models. It was interesting that you only did one layer for your networks, i.e.,  equations (4) (6). However, your model is remarkably simple so if it works well, that is good.<BRK>This paper proposes a neural topic model that aim to discover topics by minimizing a version of the PLSA loss. There s plenty of neural topic models to compare against (you mentioned some in your related work section) but no comparison with any of those is presented.<BRK>I am unimpressed with the quality of writing and presentation, to begin with. There are numerous grammatical errors and typos that make the paper a very difficult read. In its current form, this paper is not ready for publication in ICLR. Paragraph2vec and many of its derivatives have shown significant improvements with document modelling. The authors may present some computational complexity measures to convince readers about the practical applications of the proposed models.
Reject. rating score: 3. rating score: 3. rating score: 6. <BRK>Is this comparison correct? * Claim of diversity but no evidence in the video prediction setting (frame conditional generation). If the paper is about a new method, then the novelty is lacking. Decomposing the discriminator into space and time discriminators was originally proposed by MoCoGAN. Optimally, a frame conditional model should identify all objects/background observed in the input frame and object/background dynamics if multiple frames are given. However, it is not clear to me whether there is a conclusion to be made from these results about the model size. The fact that the experiments in this paper show that the conditional model is not as good as the non conditional model is most likely due to the model and/or objective function and not the generation scenario. Do the authors have any comments on this? In addition, there is no such results on the other datasets (UCF101 and BAIR) or on the frame conditioned experiments for Kinetics 600. The authors claim that the fact that the Kinetics 600 dataset is large automatically removes the concern of overfitting.<BRK>Compared to previous video generation works, DVD GAN is the first model to present compelling qualitative results on the UCF 101 and Kinetics 600 dataset. There are also increased memory and computation costs due to the 3D nature of video and the requirement for temporal modelling. This is significant due to the scale and complexity of this dataset in comparison to other datasets in the video generation literature. The novelty of DVD GAN is also limited with respect to prior video GAN literature. This is a significant weakness given that this paper s main contribution is a discriminator component for video GAN architectures. Points of Improvement:  Given that the video discriminator is the main contribution, it should be benchmarked against previous video generator models in the literature such as TGAN [3], MoCoGAN [1] and TGANv2 [2]  Given that the reproducibility of these results (at this moment in time) is difficult outside of a few organisations, can the authors provide a more thorough exposition of the efficiency aspects of their proposed discriminator. In light of this, do the authors plan to provide access to this dataset or should we place less significance on the Kinetics results?<BRK>The paper proposes a class conditional GAN model for video generation called DVD GAN. This is similar to the MoCoGAN model, with the main difference being that the video discriminator operates on a smaller resolution video, thus reducing the dimensionality of the input to discriminate. The main contribution of the paper is to successfully train this large GAN model on the very large scale Kinetics dataset. However, there are a number of things that could be improved/minor comments:  Further details about the generator should be included in the main body of the paper, only having a figure to describe its architecture is not enough when the model and how to scale it up are key contributions. Since the community uses FVD and there is a publicly available implementation of this metric, I d suggest that the authors also include FVD scores in Table 1 to help reproduce the results. The related work section is missing many references to video prediction models, please add them to the paper. Furthermore, metrics such as SSIM as used in SVG LP can help better understand how well do the models cover the ground truth sequence for given context frames.
Reject. rating score: 1. rating score: 3. rating score: 3. <BRK>Summary:In this empirical study, the authors identify that batch normalization   a common technique for accelerating training   leads to brittle representations that exhibit a lack of robustness and are more susceptible to adversarial attacks. The authors demonstrate their results on SVHN, CIFAR 10, CIFAR 100 CIFAR 10.1 using a variety of network architectures including VGG, BagNet, WideResNet, AlexNet, etc. As presented, the experiments are not convincing. For instance, batch normalization usually accommodates higher learning rates and it is not clear if the authors adjusted the initial learning rate, learning rate schedule or training schedule accordingly. If so, it would be important to run a set of experiments with these parameters fixed as per the baseline no BN models. That said, even if the authors did run these experiments, it is still not clear if the cause of adversarial vulnerability is due to BN. In this scenario, the gains of removing batch normalization could just as well be explained by the effective change in the learning rate, and not about batch normalization itself, c.f. I have several reservations about the underlying hypothesis that requires stronger evidence to overcome. I would encourage the authors to consider this line of attack and thus, re orient their analysis and discussion accordingly.<BRK>According to the new experiment results, the vulnerability is indeed not an effect of gradient masking, thanks for the clarification. However, the new results also indicate that the finding is susceptible to both weight decay and learning rate: in Figure 16 (a): "Un PGD" < "BN PGD" before learning rate decay, andFigure 17 (a) vs (b), doubling the weight decay penalty to 1e 3 also increases the vulnerability of BN. In the experiments, the authors demonstrated a significant difference in robustness between networks with or without batch normalization layers, in varies settings against both random input noise and adversarial noise. This weakness of batch norm was explained due to the "decision boundary tilting" effect caused by the normalization. Overall, I believe the phenomenon exists, but the reasons behind requires more explanations, at least not just the batch norm. If it is true, this finding will impact almost all DNN models. This paper identifies an important weakness of batch normalization: it increases adversarial vulnerability. For this level of attack, the acc should be nearly zero. It is hard to say if this is a bad thing for batch norm. Two other suggestions:1. 2.Cannot see why the input dimension discussion contribute to explanations of the batch norm weakness.<BRK>Overview:This is an interesting work. The paper is dedicated to studying the effect of BN to network robustness. Then, they use a linear "toy model" to explain the mechanism that the actual cause is the tilting of the decision boundary. Moreover, the author conducts extensive experiments on popular datasets to show the robustness margin with or without the BN module. It is clear. And for the attacker setting, they do use the popular setting (i.e.Mardy s PGD setting) in this field which makes the results more convincing. The toy example is clear but not convincing enough. I think it is one of the important parts of this work. The observation of BN causes adversarial vulnerability is interesting but the main focus should be offering more convincing explanations. In other words, without the BN layers, the robustness of adversarially trained models will not increase in my experiments.
Reject. rating score: 1. rating score: 3. <BRK>This paper proposes a new graphon based search space. Overall, it provides some new angles for NAS search space design, but the experimental results are very weak.<BRK>The authors propose a new search space based on graphons and explore some of its benefits such as certain theoretical properties. The architecture search shares similarities with DARTS. The paper is well written and the authors consider that the typical reader will not be familiar with graphons. How do results for WS G look like if you reduce its parameters to match yours?
Reject. rating score: 3. rating score: 3. rating score: 8. <BRK>This paper proposes a novel loss function to account for imperceptible, geometry aware deformations of point clouds. Overall, the paper is a well written and could be an interesting contribution.<BRK>This paper looks at the task of (adversarially or cooperatively) perturbing a point cloud in context of a classification task. al.did use L2 norm for their perturbation case, they did investigate the Chamfer/Hausdorff distances for another scenario, and therefore the main contribution here is an additional loss term. al., as they allowed point addition, so the ‘norm of difference of point clouds’ is not well defined.<BRK>This paper describes a new targeted adversarial attack against 3D point cloud object classifiers that is robust to several countermeasures. It would also be nice to have some basic descriptions of the point cloud classifiers and the SOR countermeasure.
Accept (Poster). rating score: 6. rating score: 6. rating score: 3. <BRK>## OverviewThis paper explores how pre training a recurrent network on different navigational objectives confers different benefits when it comes to solving downstream tasks. Some evidence is provided that this difference has to do with the requirements of the task. ## Major comments/concerns  I think the presentation the pretraining objective (eq 3) could be clearer. \alpha is used to separate the two types of networks (MemNet from PosNet), which is the critical difference studied in the paper, so it would helpful to go into more detail about what \alpha controls and how it was chosen. For the first task, I am surprised that the agent is able to navigate the environment using only the eight neighboring locations.<BRK>However, the exposition and the results are clear and it is interesting how pre training can shape the dynamics of a network. This paper studies the internal representations of recurrent neural networks trained on navigation tasks. The paper shows that the pretraining method leads to differential performance when the readout layer of these networks networks is trained using Q learning on different variants of a navigation task. I am not entirely sure about the novelty or impact of the presented results.<BRK>I see the paper has a large room for improvement and the current manuscript is not convincing for publication. A recurrent neural network is incorporated and training is divided in two steps of (1) task agnostic pre training and (2) task speciﬁc Q learning. The paper is well written and clear. Visualizations can be improved.
Accept (Poster). rating score: 6. rating score: 6. rating score: 6. <BRK>Can you plot the density ratio estimate in a simple 2d example like this? Although it is still somewhat "adversarial," they need to update the kernel function extremely rarely (every 2,000 generator update steps) and the optimization seems overall much more stable. Certainly not all of these theoretical questions require answers for an initial ICLR paper, but they would be illuminating for understanding the method.<BRK>This paper proposes GRAM nets using MMD as the critic to train GANs. Similar to MMD GAN, the MMD is computed on a projected lower dimensional space to prevent the kernel struggle in the high dimensional observed space. On the other hand, contrary to MMD GAN, GRAM nets trains the projection f_{\theta} trying to matching the density ratio of p/q between the observed and the latent space. The proposed density ratio matching criterion avoids the adversarial training in MMD GAN, thus can potentially fix the two player optimization problem. Compared to adversarially training f_theta, the proposed approach could lead to more stable training potentially.<BRK>In this paper, authors propose a new generative adversarial networks based on density ratio estimation. Some important density ratio based GAN methods are missing. To the best of my knowledge, the ratio model is obtained by minimizing the distance between true ratio function and its ratio model (Something similar to (4)). But, in this paper, the authors describe the ratio is obtained by using MMD density ratio estimator. Thus, I guess the kernel model is employed.
Accept (Poster). rating score: 6. rating score: 6. rating score: 6. <BRK>This paper investigates higher order Taylor expansion of NTK (neural tangent kernel) beyond the linear term. The motivation of this study is to investigate the significant performance gap between NTK and deep neural network learning. The conventional NTK analysis only deals with the first order term of the Taylor expansion of the nonlinear activation, but this paper deals with higher order terms. It is theoretically shown that the expected risk can be approximated by the quadratic approximation, and show that the optimal solution under a quadratic approximation can achieve nice generalization error under some conditions. It is nice if there are comments on how large its expressive power is.<BRK>This paper studies the training of over parameterized neural networks. Specifically, the authors propose a novel method to study the training beyond the neural tangent kernel regime by randomizing the network and eliminating the effect of the first order term in the network’s Taylor expansion. Both optimization guarantee and generalization error bounds are established for the proposed method. It is also shown that when learning polynomials, the proposed randomized networks outperforms NTK by a factor of d, where d is the input dimension.<BRK>This paper presents an approach for going beyond NTK regime, namely the linear Taylor approximation for network function. This technique enables further analysis of both the optimization and generalization based on the nice property of quadratic approximation. Although it is an innovation in the theoretical perspective, I still want to raise two questions about the object this paper trying to analyze:1. The activation function is designed so that it has really nice property: it has second order derivative which is lipshitz. 2.The network model considered here, is modified to a randomized neural network.
Reject. rating score: 1. rating score: 3. rating score: 3. <BRK>This is demonstrating a good point that explicit regularization is inconvenient. Because sometimes people would like to achieve the best performance in practice with all the resources and hammers they could get their hands on. * As a paper that studies augmentation, it would be great to provide full details on all the details (including the hyper parameters on the magnitudes of all perturbations) of the data augmentation used, especially for the "heavier augmentation" variant. Would this technique sometimes be an implicit regularization and sometimes not? Another question is regarding dropout, which is classified as explicit regularization by the paper. The paper acknowledged that the hyper parameters are suboptimal when they change settings for explicit regularization.<BRK>It is also not justified that "increasing resolution" won t help in this case since your top 5 error is a bit too high compared with recent results. The authors compare data augmentation with explicit regularization on several image classification datasets, architectures and amount of data, concluding using data augmentations is enough to reach a on par performance with using explicit regularization. I do have several concerns about the paper. (3) The hyperparameters used for WD+dropout in the experiments are "as specified in the original papers". This does not support the main claim of the paper. (5) More experiments on other domains (e.g., NLP) can be used to strengthen the paper, since the title does not specify a modality. 3.The definitions of explicit and implicit regularization in Section 2 a bit vague. In summary, the claims are not well supported by the experiments, and I tend to reject the paper.<BRK>This paper demonstrates that for regularization, data augmentation usually works better than explicit regularization methods such as weight decay and dropout. The experiments are detailed and also includes theory explanation of why data augmentation works. From my understanding, they are two different methods to tune the model quality, and they can combined to further improve the model performance, as discussed in [1]. So this paper is not well motivated.
Accept (Poster). rating score: 8. rating score: 6. rating score: 6. <BRK>The paper is concerned with neural networks using low precision operations. Experimental results showing higher accuracy than previously reported approaches in a number of settings and for common metrics, and best to date results for other settings. In terms of improving the paper further, the authors could expand their experiment section.<BRK>The motivation of the paper is to be able to train low precision networks to a high accuracy. However, the results for lower precision are impressive. Quantization is a useful tool in model compression, and doing it well for very low precision models (2 3 bit precision specifically), is challenging.<BRK>This paper trains low precision network with quantized weights and quantized activation. The paper presents excellent experimental results on ImageNet. The paper is generally well written and easy to follow. However, there does exist quite some grammar errors, especially in abstract, which could be improved. Moreover, I would like the authors to clarify some technical details. What is the main benefits of the proposed quantization method in general? Finally, I am worried about practical benefits towards the authors  claim because the networks are not fully quantized.
Accept (Poster). rating score: 8. rating score: 8. rating score: 6. <BRK>The paper takes a model based approach to "imitation learning". One major issue with the paper is that all the main contribution seem to be in the appendix. Tell the authors the paper they should have written, rather than the paper that was submitted. Apologies for that. The terminology of Inverse RL, Imitation Learning and Apprenticeship Learning is a mess in the literature unfortunately and can t blame you not trying to fix it in this paper.<BRK>Summary:  key problem: expert like probabilistic online motion planning to reach arbitrary goals without reward shaping thanks to off line learning from expert demonstrations;  contributions: 1) an imitative planning procedure via gradient based log likelihood maximization leveraging "imitative models" q(future states | features), 2) multiple proposals to define flexible goals in this probabilistic framework, 3) a complete implementation for end to end navigation in CARLA, 4) an extensive experimental evaluation showcasing the performance, flexibility, interpretability, and robustness of the proposed approach w.r.t.the previous state of the art and several Imitation Learning (IL) and Model Based Reinforcement Learning (MBRL) baselines. The current model does not factor the influence of the agent on its environment (\phi :  \phi_{t 0}). Additional Feedback:  Figure 5 is confusing, not sure it adds much value to the paper;  typos in Appendix ("pesudocode", "baselines that predicts", "search search"). ## Update post rebuttalThanks to the authors  excellent replies and my initial inclination towards strong accept, I am happy to bump my score to 8.<BRK>The paper propose imitative models that learns goal based probabilistic models of expert demonstrations, and use this to perform test time planning and control of certain goal directed behavior. The idea is quite simple: given a set of states, learn a probabilistic model that assigns high probability likelihood to expert behavior. While I believe the method described might not be significantly novel technically, I believe the paper made nice contributions in terms of the autonomous driving application.
Reject. rating score: 3. rating score: 3. rating score: 3. <BRK>This submission introduces a semi supervised method using auxiliary network for improved semantic segmentation. The framework can work in a semi supervised setting since they can use confidence map to annotate unlabeled images to train the network. I give an initial rating of weak reject because (1) novelty in architecture design is trivial (2) the way of using unlabeled images is not new (3) experiments are not supportive (3) performance is not comparative to state of the art. I will illustrate more as below. The methods are evaluated on too many kinds of hardware and can not be directly compared.<BRK>  This paper proposes a semi supervised learning strategy for semantic segmentation of road scenes. Specifically, authors propose to include an auxiliary network that will predict the confidence (at pixel level) of the predictions on unlabeled images. These predictions (e.g., confidence map) will be based only on the probabilities obtained by the first network. Nevertheless, from Table 2 it can be observed that the proposed method ranks in the middle in terms of both speed and parameters, compared to other state of the art models. The paper contains many grammatical errors.<BRK>3.What the results would be if using all labeled data and the newly added unlabeled data? This paper focused on the problem of semantic segmentation. Then a semi supervised learning scheme with an auxiliary network is introduced to annotate the unlabeled images thus help boost the segmentation accuracy. The auxiliary network for predicting the confidence map may look interesting, while the experimental results are not convincing.
Reject. rating score: 3. rating score: 3. rating score: 8. <BRK>W3.The overall novelty of theory is limited. This misleading point needs to be clarified. S2.The paper is well organized and clearly presented in general. Actually in my opinion, the novelty/importance of this work is more suitable to be evaluated in a high dimensional learning theory intensive journal or conference rather than in a DL/RL conference.<BRK> Summary The authors study the high dimensional sparse estimation problems, which is one of the fundamental topics in both machine learning and optimization communities. The main result of this paper may be the part of (b) on Theorem 1. Any local solution is a sparse estimator. Questions or comments 1. 2.Can the results presented in the paper extend to the case for SCAD?<BRK>This is a theory heavy paper. My only concern is relevance for this conference, but other than that the results are interesting and useful. That should add to the readability of the paper.
Accept (Poster). rating score: 8. rating score: 8. rating score: 3. <BRK>When proposed candidate programs lead to divergent outputs on a new input, the paper proposes to use a learned neural oracle that can evaluate which of the outputs are most likely. The paper applies their technique to the task of synthesizing Android UI layout code from labels of components and their positions. To summarize the results, we can see that the proposed method in the paper can perform about as well as existing hand crafted constraints that guide the search process of the previous work, when training an oracle on the dataset with negative examples created by noising the positive examples. For other domains, it may be more important to evaluate the candidate program together with its output, which would make it similar to a re ranking approach. I believe this paper presents a novel and insightful approach to creating programs from imprecise specifications. Therefore, I vote to accept the paper. Some questions for the authors:  How big was $\mathcal{I}_i$ in the supervised dataset $\mathcal{D}_{S+}$?<BRK>The core idea is to generate several candidate solutions, execute them on several inputs, and then use a learned component to judge which of the resulting input/output pairs are most likely to be correct. An implementation of the idea in a tool for synthesizing programs generating UIs is evaluated, showing impressive improvements over the baseline. Strong/Weak Points+ The idea is surprisingly simple and applies to an important problem in program synthesis. I have substantial doubts that the approach would work as well in the domains of, e.g., string manipulation, Karel, or data structure transformations. My doubts are based on the fact that there are easily generalizable rules for UIs (no overlaps, symmetry, ...), whereas other domains are less easily described. Improving the readability of the paper would make me improve my rating to a full accept.<BRK>I believe it would be helpful if the performance of the neural oracle was also presented. As the whole framework depends on how accurate the neural oracle can select the correct output, it is important to evaluate this. 4/5[Strengths]*motivation*  The motivation for improving the generalization of program synthesis by augmenting examples is convincing. *novelty*  The idea of utilizing a neural network to select correct outputs to augment examples for disambiguating the possible programs is intuitive and convincing. *technical contribution*  The paper investigates a set of network architectures and ways to specify the network input for learning the neural oracle. *experimental results*  The presentations of the results are clear. The results demonstrate that the proposed framework can improve generalization accuracy. I believe it would be interesting to see if these methods could further improve the performance of the neural oracle. A more comprehensive description of the dataset is lacking. I suggest the authors randomly sample some synthesized programs (both success and failure) and present them in the paper. I believe it is important to present some examples of the given I/O pairs, initially synthesized programs (p_1), found distinguishing input (x*), candidate outputs (y), the prediction of the neural oracle (i.e.selected outputs), the augmented examples (I \cup {(x*, y*)}), and finally the next synthesized program.
Reject. rating score: 3. rating score: 3. rating score: 3. <BRK>An LSTM based extension termed VarTPSOM is also evaluated on medical time series data. If so, this would be useful to discuss prominently in the paper. If not, it is not clear to me what the authors are gaining from the variational framework. The paragraph at the bottom of page 4 that discusses the "advantages of a VAE over an AE" is not convincing to me. Most obviously, I do not believe that any of the comparisons represent the proposed method but with the VAE swapped out for some type of AE? The related work section mentions that SOM VAE and DESOM are "likely limited by the absence of techniques used in state of the art clustering methods". Is there a citation for this?<BRK>The paper proposes combining the latent space of a variational autoencoder with two losses that regularize the latent space. In the model under consideration there is assumed to be a grid of "cluster centers" or centroids that all points must cluster along. Similarly on clustering time series data from physionet, the proposed method outperforms the SOM VAE. The paper also visualizes what is encoded in the centroids. Overall this paper s contribution is the use of a VAE (rather than an autoencoder as in related work) that contains a latent space regularized to favour learning cluster structure. Minor comments, there are several places that need editing for grammar and context: * The equation at the top of page 4 needs editing within the subscripts of the summation since i is overloaded * line 29 talks about "the observed centroids", but centroids are not mentioned until much later in the paper * expand AE the first time it is used as an acronym<BRK>This paper aims to build a deep clustering algorithm with interpretable latent topology. The authors combine VAE, clustering assignment hardening and SOM. The authors further extend the model to deal with sequence input, by adopting temporal smoothness regularization and language model. For example, clustering algorithms lead to a good interpretation of “color” of images that do not need to be good at clustering the semantics of images. The learned representations are also highly structured, can be used for visualization and can be used for further clustering (though might not be as good as clustering driven approaches). How’s the qualitative difference in the topology of centroids between your method and vector quantization based approaches? 4.Some suggestions on experiments. .In your table one, it might be good to compare with some recent discrete latent variable models (as mentioned in bullet point one). The whole process would be much slower than k means/vector quantization, which seems to be not good for large enough data set.
Reject. rating score: 3. rating score: 3. rating score: 6. <BRK>In this paper, a hierarchical graph matching network, which considers both graph graph interaction and node graph interaction, is proposed. The major contribution of the paper lies in the propose of multi perspective matching function $f_m()$, which is somewhat similar to the Neural Tensor Networks proposed in [1] and utilized in [2] Although, in [2], the Neural Tensor Network is used to measure the similarity between graph level embeddings. Some other questions to be clarified:1)	Why different similarity score functions are adopted for the classification task and the regression task? Why not using other more commonly used loss for classification task?<BRK>The paper proposes an architecture for (supervised) learning a similarity score between graphs through a series of layers for node embedding, node graph matching, aggregated graph embedding, and finally prediction. The lack of any theoretical reasons for using this architecture over others (perhaps by linking it to the cut norm of the graphs) or insights as to when and on what types of graphs this architecture performs well reduces the significant of this paper.<BRK>The paper proposes a network learning similarity between graphs. In particular, the proposed method focuses on node graph cross level interaction which has not been considered by other neural net studies. The performance is evaluated by two datasets on classification and regression tasks respectively. The authors repeatedly mention that the proposed method jointly learns representation and similarity. The experiments show superior performance of the proposed methods, but the datasets are only two for each tasks. In particular, since graph classification is a popular task, evaluation on a variety of benchmarks would be more convincing. Showing an example of graph pairs, in which cross level interaction is indispensable to appropriately evaluate similarity, would be convincing.
Accept (Poster). rating score: 8. rating score: 8. rating score: 3. <BRK>This paper proposed a knowledge based meta learning framework, called ARML(Automated Relational Metal Learning) that automatically extracts cross task relations and constructs a meta knowledge graph. ARML wanted to solve the task heterogeneity problem in meta learning through knowledge graph learning using graph neural networks. The paper was well motivated and well written, which made it very interesting to read. Since then, the framework s proposal to learn it as a graph neural network is a very natural extension, which can greatly increase the performance of existing few shot learning tasks. The reviewer is very curious about the qualitative comparison of the high level structures found by the two algorithms, and I confident that this comparison will enrich the paper.<BRK>In particular, it proposes to enhance meta learning performance by fully exploring relations across multiple tasks. To capture such information, the authors develop a heterogeneity aware meta learning framework by introducing a novel architecture meta knowledge graph, which can dynamically find the most relevant structure for new tasks. My major concern is about the clarity of the paper and some additional ablation models (see cons below). The paper takes one of the most important issue of meta learning: task heterogeneity. I believe this paper will provide new insights for this field and I recommend this paper to be accepted.<BRK>This paper mainly tackles the problem of heterogeneous tasks in meta learning by proposing a new meta learning framework ARML, which contains a module extracting relations across classes and a module representing meta knowledge. After that, a modulating function is applied to a set of shared parameters, which finishes the calculation of task specific parameters. The authors empirically evaluated the proposed method on several datasets and it seems that ARML outperforms some compared methods. Firstly, the proposed method is not well motivated. I think the relation between tasks can be simply obtained from instances (CNN embeddings). 2.The meta knowledge graph lacks interpretability.
Accept (Poster). rating score: 6. rating score: 6. rating score: 3. <BRK>The authors propose a learnable space partitioning nearest neighbour algorithm where the model learns a hierarchical space partition of the data (using graph partitioning) and learns to predict which buckets a query will reside in (using a learnable model). The paper is an interesting combination of classical optimization and deep learning. In terms of writing, the model description requires significant overhaul. I would recommend an algorithm box to summarize both the hierarchical graph partitioning *and* the learnable query network. 4.3 "additional experiments" is some of the most interesting parts of the paper, I would recommend promoting these experimental results to the main paper.<BRK>The paper proposes a scheme to learn space partitions for improvednearest neighbor search by first converting the search problem into asupervised classification problem by graph partitioning thek nearest neighbor graph, and then using some machine learning modelto learn space partitions corresponding to the supervised problem. Thescheme is theoretically motivated and the empirical resultsdemonstrates the utility of the proposed scheme against variousbaselines. While the paper presents a promising new direction fornearest neighbor search with space partitioning schemes, I am somewhaton the border, leaning towards reject. This is mostly due to my lackof understanding why Cayton & Dasgupta (2007) and Li et al.(2011) arenot valid baselines and are dismissed as "hyperplanepartitions". Secondly, the authors present empirical evaluation of recursive  hyperplane partitions where the proposed scheme is the only learning  based scheme while the techniques presented in Cayton & Dasgupta  (2007) and Li et al.(2011) would have been the learning based  partitioning baseline to beat. In the original paper, the authors utilize a  recursive hyperplane partitioning scheme. Is it  another hyperparameter we need to tune over while generating these  space partitions? Is it obviously always improved?<BRK>This paper proposes a technique for partitioning (presumably large number of) points in a d dimensional, such that approximate nearest neighbor search in this space can be efficiently performed by reducing to search in a smaller partition. The authors compare this technique to, most notably, a k means based technique where centroids are used to partition the space. The experiments are sufficiently detailed and well documented. In that vein, the metric of interest must incorporate time e.g.queries per second at a fixed level of accuracy. The comparison in the paper with k means baseline with k 50 does not seem fair here. Alternatively, the authors can directly compare QPS as stated above.
Reject. rating score: 1. rating score: 3. rating score: 3. <BRK>This paper proposes an alternative mechanism of training the attention values of a sequence to sequence learning model as applied to tasks like speech synthesis and translation. Experiments with mechanical  turks indicate that their attention forcing mechanism is strongly preferred over the existing teacher forced output and attention model. Related work: Recently, many papers have directly or indirectly handled the problem of exposure bias that this paper attempts to address. The TTS experiments are only on one dataset. Even the scheduled sampling or professor forcing methods are not compared with. As such as far as the TTS task is concerned the significance of the improved quality over a baseline seq2seq method is limited. Overall, the idea proposed seems quite incremental, experiments are limited, and related work discussion incomplete.<BRK>The experiments are not very convincing (only 30 human evaluators for Speech synthesis with no other quantitative evaluation, NMT results that are not particularly promising). https://arxiv.org/abs/1707.06029Without the comparison against other related papers that also aim to supervise attention mechanisms (there are other beyond the ones I cited above)s, it is unclear how much is novel about this paper. Furthermore, it is conceptually clear to me that attention forcing fully matches the training vs generated distributions.<BRK>The downside of this paper is in the experimental results and also complexity. Attention forcing also requires a reference or ground truth alignment, which is often not available. Hence the authors propose to simultaneously train another teacher forcing model to estimate the reference alignment. Attention forcing could also be used in conjunction with scheduled sampling. How does that compare with the reported results for attention forcing?
Reject. rating score: 1. rating score: 3. rating score: 3. <BRK>This paper proposes a way to testify how much a SmoothGrad saliency can vary from the true saliency attesting to the adversarial robustness but with the goal of interpretation. At the premise of this work I do not think the paper motivates the value of such a robustness certificate. I also am not sure we need such a method. What were the negative cases where this would fail?<BRK>These results motivate the introduction of Sparsified SmoothGrad and a relaxation of this method that has differentiable elements. However, ImageNet is never revisited in the paper. The proposed methods are shown to perform as well as Quadratic SmoothGrad (Smilkov et al.2017) in CIFAR 10 experiments. Unfortunately I m not an expert in this area and I don t feel confident in having a very strong opinion about this paper.<BRK>With the current setup, there is a clear trade off between robustness and fidelity of interpretation, which the paper fails to acknowledge. The work addresses an important problem of robustness of interpretation methods against adversarial perturbations. 3.The authors present empirical evidence on just one set of sparsification parameters and K. It would be more conclusive to evaluate the robustness of the proposed variations with different values of sparsification parameters, and K. They further propose variants of SmoothGrad interpretation method which are claimed to be more robust.
Reject. rating score: 1. rating score: 1. rating score: 3. <BRK>The experimental evaluation also raises a number of questions. Overall, it does not feel to me that the paper is ready for publication. 1) Why did authors chose only one value of the learning rate and the lambda hyperparameter?<BRK>Furthermore, the numerical results are unfortunately a bit discouraging. Furthermore, the plots only consider epochs and not the running time. 2) Each update requires the solution of a nonconvex, nonlinear least squares problem which is prohibitively expensive to solve.<BRK>The authors could appeal in the theory of implicit SGD for that, e.g., [1,2,3,4]. The idea is reasonable and seems to lead to good performance. Here are some more critical thoughts about the paper:1) There is not much theoretical justification about the idea in the paper. 3) Can we explain the results in Table 1 theoretically?
Reject. rating score: 1. rating score: 3. rating score: 6. <BRK>Summary:In this study, the authors propose a new architecture for visualizing the latent space of a network. This is a subjective process and is not systematic. In particular, I would want to see a quantitative assessment of this method and compare it with other network introspection techniques. The main contribution of the paper is thus in the interpretation and analysis of the method.<BRK>Review: The idea presented in this paper is simple and easy to understand and the paper includes experiments testing its effectiveness in various settings. However, there are various issues with the paper in its current form: First, the idea of reconstructing hidden activations is not new and the paper fails to cite future work. This is incredibly important.<BRK>For Fig 6, it would also be nice to know how robust these numbers are to lambda, and include some variation for different values of lambda, or at least some discussion of how much tuning is required. Finally, they show that the representations learned with this regularization method transfer to unseen tasks better than without the reconstruction regularization. The authors tackle an important problem with a very simple regularization technique and show how it can aid interpretability in adversarial attacks and improve transferability through strengthening attention of features to new tasks and in the multi task setting. While the two problems are important, results on multiple datasets would be necessary to strengthen the paper.
Reject. rating score: 3. rating score: 3. rating score: 3. <BRK>Summary:The paper proposes to use adaptive computation time (ACT) for the number of layers (instead of the number of timesteps in the original paper) for a RNN. It is also unclear to me how many layers does the RNN baseline have and how the results changes wrt the number of the layers for the baselines, i would imagine that this would get better as the number of layers increases.<BRK>These weaknesses lead me to vote for a weak reject.<BRK>Have the authors tried to fix N_t as a constant (let’s say 2), and then perform the whole thing on the same setting again? I am wondering why this indirect way of stacking?
Accept (Poster). rating score: 6. rating score: 3. rating score: 3. <BRK>This paper describes an application of lottery ticket hypothesis to NLP and RL problems. I don t have any major complaints regarding this work and I believe it is well executed.<BRK>The paper used the lottery ticket hypothesis to study the over parameterization of deep neural networks (DNNs). The results suggest that the lottery tickethypothesis is not restricted to supervised learningThe similarity between supervised learning and RL and NLP problem is obvious from a function approximation and optimization point of view. The paper is empirical in nature, and do not offer any additional insight. The experiments are not very conclusive.<BRK>The lottery ticket hypothesis has been found effective in over parameterized deep neural networks, which provides better sub network initialization if not outperforming the original full network. This paper is clearly motivated and easy to follow. The results are interesting. The paper does not seem to propose any new algorithm, and the application of lottery ticket (late rewinding) to LSTM, Transformer, and RL looks quite straightforward. I understand this is subjective, so I leave it to the AC for further evaluation.
Reject. rating score: 3. rating score: 3. rating score: 6. <BRK>This paper works on weight quantization in deep networks. The authors propose to utilize the extra state in 2 bit ternary representation to encode large weight values. The idea is simple and straightforward. Clarification in the experiment section can be further improved. However, from the revised manuscript, the proposed method performs significantly worse than some recent quantization methods like PACT and QIL.<BRK>Summary:The paper proposes a hybrid weights representation method where the weights of the neural network is split into two portions: a major portion of ternary weights and a minor portion of weights that are represented with different number of bits. •	In the paper, it claims that the proposed weight ridge method “can obtain better accuracy than L2 weights decay”. Cons:•	The idea of using mixed bit width for neural network quantization is not new. However, the experiments in the paper only compare with basic quantization method which makes the comparison not fair enough.<BRK>This paper is about quantization, and how to represent values as the finite number of states in a low bit width, using discretization. Their approach is a hybrid weight representation method, which uses a network to output two weight types: ternary weight and sparse large weights. For the ternary weight, they need 3 states to be stored with 2 bits. The one remaining state is used to indicate the sparse large weight. I would urge the authors to revise writing to make it broader accessible.
Reject. rating score: 3. rating score: 6. rating score: 6. <BRK>This paper proposes modifications and modular extensions to the differential neural computer (DNC). The approach is nicely modular, decoupling the data modules from algorithmic modules. This enables the authors to pretrain the data modules with supervised learning and to train the small algorithmic modules with neural evolution strategies (NES). NES is a global optimization method (which may be understood as policy gradients where the parameters of the neural policy are the actions) and consequently this enables the authors to use discrete selection mechanisms instead of the soft attention mechanisms of DNC. The idea that modularity can be used to attain greater generality and domain independence has already been explored at ICLR to some extent.<BRK>This paper presents an approach called a neural computer, which has a Differential Neural Computer (DNC) at its core that is optimised with an evolutionary strategy. The paper is interesting and well written but I found that the contributions of this paper could be made more clear. First, the idea of evolving a Neural Turing machine was first proposed in Greve et al.2016, which the authors cite, but only in passing in the conclusion. Second, the idea of learned modules to allow the approach to work across different domains is interesting, but I’m wondering how novel it really is? I also had a question about the DNC training.<BRK>This paper introduces a neural controller architecture for learning abstract algorithmic solutions to search and planning problems. The method of triggering learning based on curriculum level performance is a notable feature that nicely couples generalization progress with learning, and yields insightful learning curves. Is there something fundamental about this particular decomposition into modules? However, I still think the presentation can be much improved to improve the clarity of the contribution. I would hope to see the following addressed in the final version:1. More detailed and precise discussion of how the approach relates to prior work.
Reject. rating score: 3. rating score: 6. rating score: 8. <BRK>The authors developed a supervised WTA model when training samples with both input and output representations are available, from which the optimal projection matrix can be obtained. The authors further extended the model and the algorithm to an unsupervised setting where only the input representation of the samples is available. Overall I think this paper is clearly presented with some good experiment results. 2)Is this algorithm can really work on a large scale problem?<BRK>The derivation of the closed form updates is clever and seems to be correct. Overall, I am quite favorable for this paper. That is, usually PCA is not compared with these methods since a random projection method is faster.<BRK>This paper proposed a Winner Take All model that learns a sparse binary representation for dense vectors. I understand in reality it may not hold but shouldn t affect the representation learning and downstream applications of the binary vector, but is there any work done to estimate whether it holds in general or only for d  in some range? 2.In experiments, both the supervised and unsupervised methods are compared.
Reject. rating score: 1. rating score: 1. rating score: 1. <BRK>The main contribution is proposing a deeper CNN approach, using both word and character embeddings (as well as label embeddings with attention). The paper claims improved performance over baselines. DecisionI reject the paper for 3 main reasons:1) Very misleading claims regarding establishing a new state of the art. The baselines used for comparison don t include any of the best existing published results. In particular, no mention nor discussion of Transformers (self attention) networks, including BERT and XLNet approaches, which are the state of the art in text classification. More discussion is needed to understand why the new aspects of the proposed network are importantly different from other existing approaches. With a few quick searches, I found that there are several approaches performing better than the proposed model on every dataset considered in the analysis, as you can see below.<BRK>This paper presents a multi input model for text classification. The justification of the proposed architecture is not persuasive. All the compared baselines are extremely weak. The authors need to follow the experiment setup in more recent text classification papers. Authors need at least to compare their methods to BERT.<BRK>This paper described a multi input model for text classification. This paper is a bit hard to read compared to other submissions. The paper mentioned "labels" as input to embedding layer as well as attention mechanism. Moreover, the experiment results (e.g., Table 3 and 5) did not include label. It will be more convincing if authors include additional baselines such as fine tuning BERT. Based on above reason, I would reject this paper.
Reject. rating score: 1. rating score: 1. rating score: 3. <BRK>I don t think this paper should be accepted. In my opinion, the mix of EBM and VAE is not really compelling;  and it is not clear at all to me that one gets much from the "V" in this setting. Furthermore,  the experimental results are not great either qualitatively (by the standards of generative models of images in 2019) or quantitatively (even by the standards of GAN papers). Finally, the author s claims about sets seems tacked on, and unrelated to the rest of the paper.<BRK>This is difficult to achieve with bounded determinant. the third model is the usual autoencoder. An extension to sets is proposed as well as a task showing performance for semi supervised learning. Therefore, the entropy is not bounded as the paper does not claim to have any entropy normalization here. In the remainder of this review, I will try to substantiate the claims regarding theory which lead me to an overall decision to reject the article in its current form.<BRK>In this work a novel probabilistic generative model is introduced mixing several existing frameworks: The Hierarchical Bayes Autoencoder (HBAE) can be interpreted as a cGAN with a VAE encoder. This is one of the main contributions of this work since generative modeling of sets is a challenging and unsolved task. Although there is a clear explanation of the contributions of this paper, the motivation of this study is not precisely described. The proposed formulation seems encouraging thanks to the incorporation of multimodal decoders. Although the methodology, conceptually, suggested an interesting approach, the results are not so encouraging given the quality of the images achieved by other methodologies.
Reject. rating score: 3. rating score: 3. rating score: 6. <BRK>This paper proposed an attention based document classifier based on BERT LSTM structure. This proposed model is lack of novelty, it applied the attention sum of LSTM output on top of BERT in the document classification task. The experiments are only evaluated on two less studied datasets which makes the comparison with other works less persuasive, more results on general document classification datasets are needed. Why the RNN BERT and the ATT BERT only include h2~hm but exclude h1? It is not a recurrent neural network.<BRK>The paper presents a methods to combine multiple pre trained language models using an attention mechanism in order to take the whole document into account. The effectiveness of the methods is evaluated on two long document classification tasks (Arxiv publication and patent classification) with state of the art results. I am not sure that it is sufficient for the ICLR standards. RNN are ReNN but restrained to a linear chain structure  In Introduction: ... in the  domain extremely complex data that is language ...  > I m not sure the sentence is correct  In Introduction: the last sentence should be shorten and rephrased.<BRK>This paper proposes to compare different methods to build BERT/GPT  representations of long documents, to bypass the limitation of the input size of these models. Results show that the largest improvement is the base BERT model over the previously proposed model : this aspect should be comment : what is the reason of the improvement ? Hyper parameter and Training/Testing time are reported, which is useful from a practical point of view if one should decide to implement the proposed method or not, considering the extra computational load and the relatively small improvement. The Shuffling experiment demonstrate an interesting behaviour of the models, that should be confirmed on a real dataset.
Accept (Poster). rating score: 8. rating score: 8. rating score: 6. <BRK>This paper proposes a unifield theoretical framework for node embeddings and structural graph representations, which bridgs methods like matrix factorization and graph neural networks. The theoretical analysis is sufficient and experimental results are good. The theory alo shown that the concept of transductive and inductive learning is unrelated to node embeddings and graph representations, which clears another source of confusion in the literature. In my opinion, this is a theory paper and the proof are sufficient.<BRK>The authors present mostly theoretical analysis indicating the equivalence of embeddings and structural graph representations. The authors argue that while most of the earlier work consider these to be different, they are actually the same and give theory and empirical results to back up this claim. This is not an easy paper to read, as the authors immediately jump into heavy notation without much intuition or visual aid. This continues throughout the experiments as well, where the authors are not very gentle when it comes to presentation. I gave a weak accept as I do not want my (unfortunately) weak review (due to the paper topic not being my strong point) to have a great effect on the final decision. However, it is clear that the paper can and should be better written, and the paper ideas made closer to the readers. However please note that my confidence remains as low as previously. It is unfortunate that ICLR did not do any paper reviewer matching like other conferences and that you are stuck with my weak review. But that is a different story that the organizers should make sure to address asap.<BRK>The authors define node embedding and structural graph representation based on a minimal number of requirements, which is, in turn, used to derive some useful properties and unify two seemly different concepts. The experiments on multiple tasks with multiple datasets validate the main claim that a single node embedding is insufficient to capture the structural representation of a graph. Here s a minor comment on the representation of the paper. Although the concept of node embedding and structural representations get clearer as reading, the introduction, where I couldn t find any reference of these, seems unlikely to clarify the difference between these two concepts. The familiar representation in Section 3 might be the first part where the readers could get some intuition about their differences.
Accept (Poster). rating score: 8. rating score: 6. rating score: 6. <BRK>The paper proposes a novel intrinsic reward/curiosity metric that combines both episodic and “life long” novelty. The paper covers this episodic curiosity measure and how it’s integrated with the life long curiosity metric. This paper demonstrates a novel episodic curiosity metric and a means of integrating that with a more standard life long curiosity metric. The writing is clear and the results are good and well explained.<BRK>The work is motivated by the goal of having a comprehensive exploration of an agent in deep RL. For achieving that, the authors propose a count based NGU agent, combining intrinsic and extrinsic bonuses as new rewards. An extrinsic/ long term novelty module is used to control the amount of exploration across episodes, a life long curiosity factor as its output. In the intrinsic/episodic novelty module, an embedding net and a KNN on episodic memory are applied to compute the current episodic reward. The proposed method is tested on several hard exploration games. Overall, this paper is well written. Methods and results are clearly described.<BRK>In this paper, the authors present a methodology for generating intrinsic rewards for reinforcement learning agents targeting hard exploration environments. The intrinsic reward is generated using an episodic memory module and a lifelong novelty module. Several experiments on a simple domain and on Atari domains are conducted to evaluate and compare the performance of the proposed method against the baselines. General comments:  The authors state that a novel contribution is to disentangle exploration and exploitation. I marked with (*) the ones I consider the most important.
Reject. rating score: 6. rating score: 6. rating score: 6. <BRK>This paper proposes a unifying framework, BRAC, which summarizes the idea and evaluates the effectiveness of recently proposed offline reinforcement learning algorithms, specifically BEAR, BCQ, and KL control. Based on prior work, the authors state that there are two variants of regularizations to the behavior policy, value penalty (vp) and policy regularization (pr) and three choices of divergence functions along with their sample estimate that measures the distance between the learned policy and the behavior policy. The paper empirically investigates the effectiveness of each regularization scheme as well as each divergence function and conclude that vp is slightly more effective than pr while all divergence functions have similar performances. Overall, this paper could be an interesting summary of prior works in offline RL and provide some empirical insights on the effectiveness of each building block in the previous approaches, though it neither offers theoretical explanations nor proposes a new offline RL algorithm that outperforms the existing methods under the BRAC framework. My score would be increased given some technical insights or some promising results in a relatively novel offline RL algorithm in the author’s response. While the experimental results demonstrate some interesting phenomenons such as combining vp and the primal form of KL divergence achieving the best performance and taking minimum over Q functions outperforming using a mix of maximum and minimum over the Q functions, I believe the paper would be greatly improved if the authors can provide a new offline RL method based on the BRAC that can achieve better performance than current approaches and is less incremental than simply combining vp and KL divergence.<BRK>The paper introduces a general framework for behavior regularized actor critic methods, and empirically evaluates recent offline RL algorithms and different design choices. Overall, the paper is well written and easy to follow. I appreciate the authors for their careful empirical study. I am leaning to accept the paper because (1) the experimental design is rigorous and the results provide several insights into how to design a behavior regularized algorithm for offline RL. There are some comments for the experiments. Are they negative so not reported in the figure or just missing? 3.Do you think the conclusion will change if you use training datasets of different size (e.g.much less than 1 million)?<BRK>This paper presents a framework for evaluating offline reinforcement learning (RL) algorithms. Results from a  thorough series of experiments are presented which suggest that certain details of recently proposed RL methods are not necessary for achieving strong performance. These results suggests that some of the complexity in RL design can be ignored. Clearly the authors of those previous works thought they were needed. To really help researchers design better algorithms, we need to be guided by some insight about not only what doesn t work but why it doesn t work. If so, this should be made more clear and stated prominently in the paper so that the reader knows that BRAC is, in this reproduction of previous results sense, reliable. If not, how if the reader to know that the "unnecessary" technical complexities, are truly unneccessary? Finally, the paper presents lots of results, but I did not see any mention of the statistical significance of these results. Minor issue:  In the Conclusion Section, the authors say, "Unfortunately, off policy ... is an challenging open problem." And I think the authors did a good job addressing a difficult problem.
Reject. rating score: 3. rating score: 6. rating score: 6. <BRK>GDP: Generalized Device Placement for Dataflow GraphsThis paper presents a method to assign the individual operations making up the dataflow graph of a deep neural network to a set of connected devices on which they should be executed, with the objective of maximizing runtime performance of inference. The end to end model is trained with a reinforcement learning criterion minimizing the expected placement runtime. Overall, the paper is well written, easy to follow, and addresses an important concern is scaling up neural networks to large model sizes. The proposed approach combining graph neural networks with transformer based placement network appears, at first glance, novel. (This can be given in appendix).<BRK>In this paper the authors propose an end to end policy for graph placement and partitioning of computational graphs produced "under the hood" by platforms like Tensorflow. The authors compared to prior work propose a method that can take as input more than one data flow graphs, and learns a policy for graph partitioning/placement of the operations in a set of machines that minimizes the makespan. Since the authors compare with METIS, it is worth also comparing with Scotch https://www.labri.fr/perso/pelegrin/scotch/ that is also publicly available. Update: Thanks to the author(s) for the detailed feedback. I have upgraded my score accordingly.<BRK>Summary: This work proposes to use a combination of graph neural networks (GNNs) and proximal policy optimization (PPO) to train policies for generalized device placement in dataflow graphs. That said, the results are strong and the paper is well written, so it certainly has merits as an application paper.
Reject. rating score: 3. rating score: 6. rating score: 8. <BRK>However, I think that the submission in its current form lacks experimental analysis of the proposed conditional probes, especially the trade offs on the reliability of the representation analysis when performed with a conditional probe as well as a clear motivation for the need of a language interface. I cannot recommend acceptance of the submission in its current form, but I encourage the authors to incorporate the refined motivation and add more comprehensive experimental evaluation for a possible resubmission.<BRK>The main idea is to train an independent LSTM (a Question answer decoder) so that given the hidden state of the predictive model and a question about the environment, it is able to answer the question. The authors give empirical evidence that the representations created by SimCore contain sufficient information for the LSTM to answer questions quite accurately while the representations created by CPC (or a vanilla LSTM) do not contain sufficient information. As a result, I am positive, however, I think it would be best accepted as a workshop paper given that:  The experiment are only carried out on a single environment, however, their claims are rather general.<BRK>The authors propose question answering (QA) as a tool to investigate what agents learn about the world, i.e., how much about the world is encoded in their internal states. This includes unseen combinations of seen attributes ("zero shot"), showing that, what the agents learn, is to some degree compositional. The authors investigate multiple agents and find that LSTM and CPC|A representations are no better than chance, SimCore s representations seem to be the best for the QA task, and there is still a big performance difference between SimCore and the upper bound "No SG". I think this paper is interesting and well done.
Reject. rating score: 3. rating score: 3. rating score: 3. <BRK>The main contribution of the paper is in the novel training procedure in which the target distribution and the generated distributions are mapped to a latent space where their divergence is minimised. The idea is quite interesting but there are certain problems in clarity regarding the loss terms and the precise need for them. It appears that the work is proposing a novel framework for training VAEs where instead of comparing the target and generated distribution in the data space (the standard $L_r$), the loss is minimised by minimizing the divergence between these distributions after mapping them onto a latent space. To achieve this objective the author’s design two loss terms in addition to the standard reconstruction error. Authors provided two approaches for their training method based on MLE and VAE. The notation and the derivation of the MLE approach in Section 3.3 are not clear. The main idea is quite clear but how this training procedure achieves that is not coming out clearly in this version. Thus I feel the paper, though has good content, is not publishable in the current format.<BRK>The paper considers generative models and proposes to change the VAE approach in the following way. While VAEs assume a generative model (prior on the latent space + stochastic generator) and aim at learning its parameters so as to maximize the likelihood of the (training) data distribution, the authors propose to derive the learning objective from a different view. In my understanding, they consider the two composed mappings generator(encoder) and encoder(generator) and require the first one to have the data distribution as a fixpoint and the second one to have the latent prior as a fixpoint. Starting from this idea they derive an objective function for the case that the mappings are deterministic and then further enrich the objective by either likelihood based terms or VAE based terms. I find the main idea of the paper highly interesting and compelling. Nevertheless, I would not recommend to publish the paper in its present state . The technical part is in my view very hard to comprehend. It should be possible (and I believe, is possible) to derive a simpler objective ab initio, starting from the main idea of the authors.<BRK>This paper presents a set of losses to train auto encoders using a stochastic latent code (PGA). The authors relate it to VAE and propose a variational variant of their framework (VPGA), but the initial intuition is distinct from VAEs. The intuitions behind this framework seem sound and the authors added theoretical justifications when possible. This paper seem to present a good idea that should be published, but is currently not clear enough. It is likely I didn t understand it fully and a clearer section would help. Currently, they are called encoder and decoder or generator in the text, f and g in the math, and theta and phi on figure 1. I am ready to change my rating after the rebuttal if the authors address clarity of section 3.
Reject. rating score: 1. rating score: 1. rating score: 3. <BRK>Contributions:This paper investigates the use of learning rate decay in deep neural networks. The main contribution is an empirical analysis trying to understand lr decay. They also show that the model transferability decreases through training. Similar argument has already been made in (Nakiran et al., 2019) or (Li et al.2019), which are cited in the paper, and other works such as “On the Spectral Bias of Neural Networks”.<BRK>Summary: This paper investigates the way decaying the learning rate helps the training of neural networks. However, for example learning both the proposed explanation in this paper and the fact that learning rate decay improves stability can both be true. The experimental arguments are quite vague in this paper. In terms of arguments that decaying learning rates help the neural networks to learn simpler examples first and the harder ones have already been done in other papers such as Li et al 2019 which is also cited in this paper. Can one explain why cyclic learning rates works with the hypothesis that this paper processes?<BRK>The paper investigates the role of learning rate decay in neural network training. While there are prevalent ideas of how/why learning rate decay help both optimization and generalization of neural networks, this work proposes interpretation based on pattern complexity. The question the paper tackles is a very important question in understanding deep learning that requires careful study. Also I do not agree with the claim that SGD explanation leads to training curves as in Figure 6.
Reject. rating score: 3. rating score: 3. rating score: 6. <BRK>The authors replied that the regularization considered in the paper might be treated as Lipschitz constraint for bounded data sets. Moreover, the authors said that they would add an explanation of this important point in the revision but I haven’t found any revision yet. The authors replied that the choice of discriminator is designed in tandem with the choice of generator. If they use a standard two layer ReLU network as discriminator, this would hurt the sample complexity. However, it will be more convincing to show the convergence of WGAN if the authors consider NN discriminator rather than quadratic discriminator which hardly be used in GAN. But actually this paper only considers two kinds of simplified discriminators: a (rectified) linear discriminator and quadratic discriminator, which are very different from WGAN used in practice. The analysis of two special cases are hard to be extended to the analysis of WGAN and thus can hardly help to explain why WGAN is successfully trained by SGD in practice. For the rectified linear discriminator, the regularization of the discriminator is the norm the output layer of discriminator which can be related to the Lipschitz constraint in WGAN.<BRK>In this paper, the authors attempt to prove that the Stochastic Gradient Descent Ascent could converge to a global solution to the min max problem of WGAN, in the setting of a one layer generator and simple discriminator. Since the linear discriminator and the quadratic one could be solved in one step Gradient Ascent, the author applied the standard analysis method to reveal the property of the Gradient Descent method. However, the most significant drawback of this paper is that the settings for the discriminator are too simple, which leads to the following two problems: 1) Revealing the joint distributions of two coordinates is still much weaker than the desired result of recovering the true distribution of the data.<BRK>In the response the authors clarified the contributions of this paper. I agree with the authors that the analysis of gradient descent ascent is a difficult problem, and the optimization results given in this paper is a contribution of importance. Therefore I still consider the simple discriminator and generator as a weak point of this paper. This paper studies the training of WGANs with stochastic gradient descent. The authors show that for one layer generator network and quadratic discriminator, if the target distribution is modeled by a teacher network same as the generator, then stochastic gradient descent ascent can learn this target distribution in polynomial time. The authors also provide sample complexity results. However, I think the WGANs studied in this paper are simplified too much that the analysis can no longer capture the true nature of WGAN training. First, the paper only studies linear and quadratic discriminators. When the discriminator is as simple as linear or quadratic functions, there is pretty much no “Wasserstein” in the optimization problem. Moreover, the claim that SGD learns one layer networks can be very misleading. In fact what is a “one layer” neural network?
Reject. rating score: 1. rating score: 3. rating score: 3. <BRK>Decision:The paper is studying an important topic, which is how to use likelihood based models correctly, and it warns against the misuse of such models. However this is not necessarily true. In fact, PixelCNN is not a flow model, even though the paper misleadingly describes it as such. PixelCNN can be used to model discrete random variables, whereas flow models are used for continuous random variables (flows for discrete random variables exist, but they are different from PixelCNN). In what follows, I elaborate on specific issues with the paper in more detail. "We may reasonably suspect that flows’ counter intuitive likelihood assignment is dominated by the inherent differences of pixel level statistics associated to the image semantics"This is also speculation. "Considering the weak correlation between flows’ likelihoods and image semantics, it is inappropriate to use them for OoD samples detection"Given the flawed assumption about the role of image semantics, I don t think there is evidence for that. In fact, out of distribution examples can have high probability (density). This outcome is clearly atypical, and many people would agree that it s out of distribution, but it has the highest probability.<BRK>The authors show that the changes of likelihood of an image computed by flow based image generative models have surprisingly weak correlations with semantic changes of image. The flow likelihoods are sensitive to very small changes of pixels that do not affect the semantics of an image. And the likelihoods are less robust against out of distribution inputs where we expect strong robustness compared to discriminative models. This is an interesting paper: it warns the abuse of likelihoods computed by flow models with several numerical experiments, which are simple but clearly designed to support the claims. Designs of experiments are apparently similar to those of (Nalisnick+, 2018) at the first glance. I think there is a room to improve the manuscript to clarify the difference from the Nalisnick+’s work. My understanding is that (Nalinsnick+, 2018) is interested in the OOD likelihood behaviors when datasets are swapped, and explains the behaviors of OOK likelihood based on the variance and the curvature of the dataset. This indicates the latent representations are robust against perturbations while pixel intensities are not. This seems an interesting problem for me. I’m happy to hear the authors’ opinions about this issue.<BRK>The paper studies the correlation between likelihood of flow based generative models and image semantic information, and shows that even small perturbations, like a few pixel translations or noise applied to background, significantly affect models’ likelihoods, which signals that these likelihood models cannot be used for out of distribution data detection. However, very similar observations were made in prior works [1] and [2]. In particular, the paper [2] showed that likelihood of PixelCNN is dominated by background pixels which makes the observations in section 4.2 (applying noise to background) unsurprising. The sensitivity of Glow model to even 1 2 pixel translations (section 4.1) and exploiting multi scale structure of Glow (zeroing latent variables in section 4.3) are interesting, but I believe, not enough for a full paper. I believe this can be called “data augmentation”, why introduce a new term? It would help to see experiments on other datasets, e.g.CIFAR 10, SVHN. 7.Related work section can be elaborated: please, discuss how the observations made in the paper are different from / consistent with [1] and [2]. [1] Nalisnick, Eric, et al."Do deep generative models know what they don t know?." arXiv preprint arXiv:1810.09136 (2018). arXiv preprint arXiv:1906.02845 (2019).
Reject. rating score: 1. rating score: 1. rating score: 3. <BRK>5.In section 3, the V, G of memory are not used. I am inclined to reject this paper. The authors use quite small dataset (also non standard) for evaluation. The usage of \citep and \cite are different.<BRK>The paper considers the problem of debiasing word representation in a trained language model using some sort of key value memory structure with annotations of gender that is constrained to view only a subset of keys. A formula would be useful here. (3) more complete experimental definition. What datasets exactly are you evaluating on?<BRK>While this is an important topic, the contribution is rather minor and not well presented. The results are shown for the datasets from Peru and Mexico (Chile is missing) with a larger bias amplification for the Peru dataset. The authors should perhaps consider to explain the structure of the paper in the introduction.
Accept (Poster). rating score: 8. rating score: 6. rating score: 6. <BRK>The paper introduces an approach to answering queries on knowledge graphs, called Query2Box. The idea is to work with the embeddings of the vertices of the knowledge graph as if they were kind of sets.<BRK>This paper proposes a method to answer complex logical queries in large incomplete knowledge bases (KB). 7.It is nice to see, that the model prefers boxes of different width.<BRK>The paper studied the problem of answering complex logical queries on KGs, by proposing an embedding approach that encodes the queries into hyper rectangles. In general, I like the paper due to the nice presentation and promising approaches.
Accept (Poster). rating score: 8. rating score: 6. rating score: 6. <BRK>Minor Presentation Weaknesses:* Figure 4: I think the sub figures are mis labeled in the caption. (IBR) Supervision (using image captions) followed by self play performs much worse than all other approaches. Overall the work could prove to be an interesting and useful reference point inside the language emergence literature so I recommend it for acceptance. Weaknesses * The 3rd point of section 5 is presented as a major conclusion of this paper, but it is not very surprising and I don t see how it s very useful. The self play to supervision baseline seems to be presented as an approach from the language emergence literature. I don t think this is what any of that literature promotes exactly, though it is close.<BRK>This paper investigated how two conflicting learning objectives; supervised and self play updates could be combined with a focus on visual grounded language tasks. The paper is very well written, and I really enjoyed reading it overall. One of my concerns is the lack of applications, especially on the tasks using more natural language. I agree with the point made by the authors that this work mainly focuses on investigation rather than exploitation. I think they are written in a hurry or changed a lot in the last minutes.<BRK>This paper explores the effect of ordering supervised learning and self play on the resultant language learnt between agents. 3) The conclusion in Section 7 at the bottom of page 8 that "S2P performs much worse than the other options" is contrary to previous results. Specifically, hyperparameter ranges swept over are shown but how were they then chosen from? Is this mean accumulated reward? On Page 7 there is a reference to Figure r4b, is this intended to be a reference to Figure 4b right?
Reject. rating score: 3. rating score: 3. rating score: 8. <BRK>This paper analyzes ensembling methods in deep learning from the perspective of the loss landscapes. The authors empirically show that popular methods for learning Bayesian neural networks produce samples with limited diversity in the function space compared to modes of the loss found using different random initializations. The analysis shows that while the values of the loss and accuracy are nearly constant along the paths, the models corresponding to different points on a path define different functions with diverse predictions. The paper also demonstrates the complementary benefits of using subspace sampling/weight averaging in combination with deep ensembles and shows that relative benefits of deep ensembles are higher. The experiments are described well and the results are presented clearly in highly detailed and visually appealing figures. The paper presents a thorough experimental study of different ensemble types, their performance, and function space diversity of individual members of an ensemble. In my view, the strongest contribution of the paper is the analysis of the diversity of the predictions for different sampling procedures in comparison to deep ensembles. However, the novelty and the significance of the other contributions are limited (see comments below). While it is known that deep ensembles generally demonstrate stronger performance [1], there is a trade off between the ensemble performance and training time/memory consumption. 2) It remains unclear to me what new insights does the analysis of the low loss connectors provide? It is expected (and in fact can be shown analytically) that if the two modes define different functions then intermediate points on a continuous path define functions which are different from those defined by the end points of the path. This result was also analyzed before from the perspective of the performance of ensembles formed by the intermediate points on the connecting paths (see Fig.2 right in [2]). Moreover, I would encourage authors to reformulate the statements on the connectivity in the function space such as:   “We demonstrate that while low loss connectors between modes exist, they are not connected in the space of predictions.”  (Abstract)  “the connectivity in the loss landscape does not imply connectivity in the space of functions” (Discussion)In my opinion, these claims are somewhat misleading. What does it mean that the modes are disconnected in the function space? It is true that two modes correspond to two different functions. However, it is unclear in which sense these functions can be considered to be disconnected. Loss surfaces, mode connectivity, and fast ensembling of DNNs. In NeurIPS, 2018.<BRK>The contribution of the paper is the following two findings: 1. Despite the fact that local minima are connected in the loss landscape the functions corresponding to the points on the curve are significantly distinct. 2.The points along the training trajectory correspond to similar functions. Originality and novelty. Both findings do not seem quite new. The first conclusion can be mostly derived from Figure 2 right [1]. Moreover, the difference between functions on the curve in terms of predictions is the main motivation of Fast Geometric Ensembling. The second conclusion is also not quite new and there were several approaches to overcome it e.g.SWA [2].I appreciate that the authors did a much broader investigation of this phenomena than it was done in previous works. It is known that ensembling based on dropout is worse than independent networks, but the main advantage of this and similar approaches is memory efficiency. The provided experimental results and supporting plots are also clear and contain the necessary description. I would recommend the authors to add more rigorous description of how they constructed these plots to increase clarity of the paper. Can the authors please also clarify how they derived formulas for the expected fractional difference for f^* and f functions in the section 3.2? Overall, it is an interesting paper, but the findings are not quite new. Loss surfaces, mode connectivity, and fast ensembling of DNNs.<BRK>This paper is trying to answer the question why ensembles of deep neural networks trained with random initialization work so well in practice in improving accuracy. Their proposed hypothesis is that networks trained from different initializations, although all converge to a low loss/high accuracy optimum, explore different modes in function space and therefore provide more diversity. The difference in function space is based on the fraction of points on which the two functions disagree in terms of their prediction. Second, they use different subspace sampling methods around a single optimum and demonstrate that they are significantly less diverse (low disagreement between predictions) than sampling from independent optima through diversity vs accuracy plots. Moreover, they comment on the recent observation that local optima are connected by low loss tunnels. They experimentally show that even though low loss/high accuracy path exists between local optima, these tunnels do not correspond to similar solutions in function space, further supporting the multi mode hypothesis. The authors compare the relative benefit of subspace sampling, weight averaging and ensembling on accuracy and interpret their findings in terms of the hypothesis. Overall, the paper is very well written and provides interesting insights into the multi modal structure of deep neural network loss landscapes. Even though the hypothesis of the paper is not entirely new and has been touched upon in Fort & Jastrzebski (2019), this paper contributes to the field by providing thorough experimental support and clear exposition of the idea. The paper mentions that the trends are consistent across all datasets the authors have explored. However, they only provide results on CIFAR 10 (and a limited set of experiments on ImageNet). Since the contribution of the paper heavily relies on providing experimental verification, it would be important to include at least the diversity vs. accuracy plot for the other datasets they have explored to demonstrate that this phenomenon is not specific to CIFAR 10. Additionally, I would like to add a couple of comments on the paper that are not part of my decision, but could potentially improve the paper. The diversity score introduced in the paper is simple and intuitive, however it would be interesting to see whether the results hold across different notions of function space disagreement. It is mentioned in the paper that data augmentation has been used for training the ResNet20 architecture. Some comments on the figures: in Figure 3 it is very difficult to discern any difference between different shades of red (disagreement values), and in this form the plots are not too informative. Maybe rescaling or a different way of presentation would help.
Accept (Poster). rating score: 6. rating score: 6. rating score: 6. <BRK>This paper extends existing work on certified robustness using smoothed classifier. The fundamental contribution is a framework that allows for arbitrary smoothing measure, in which certificates can be obtained by convex optimization. Sec 5.3,It is unclear from the writing whether the comparisons to previous works were done on the same ResNet architecutre with the same clean accuracy. It’s not motivated clearly. I recommend accepting this paper, but would do so with more passion if some of the comments/questions can be addressed.<BRK>Summary: This submission proposes a unified framework for black box adversarial certification. If not, I will turn down the score to 3. [ ] The theorem part contains too many details and some important parts (e.g.tightness) are in appendix. It should be re organized.<BRK>The paper introduces a generalization of the randomized smoothing approach for certifying robustness of black box classifiers, allowing the smoothing measure to be an arbitrary distribution (whereas previous work almost exclusively focused on Gaussian noise), and facilitating the certification with respect to different metrics under the same framework. The following sections were not as well organized, in my opinion, and need improvement.
Accept (Poster). rating score: 8. rating score: 6. rating score: 6. rating score: 3. <BRK>I really like the analysis of Brier Score/Log loss and MRR. It studies Brier and log loss performance of Platt scaling and isotonic regression probability calibration on WN11 for TransE and claims that better calibration yields better performance as measured by mean reciprocal rank. 2.Decision (See the updated decision in the comment below)Probability calibration is a very relevant issue, particularly in industry and when combining knowledge graph embedding models as external data in other models. This shows me that probability calibration is an important topic and I see this study as an important contribution to the field which is often mindlessly following evaluation metrics. Thus it is difficult to trust results if they are not well tied into the literature and compared against multiple datasets and models. I think adding more results would make this paper great and I would be happy to change my acceptance decision.<BRK>In this paper, the authors deal with the calibration problem in graph embedding models. They used Platt scaling and isotonic regression in the situation when there is ground truth negatives. Overall, the approach is not very innovative, but the problem they tackled is under studied. The presentation of the whole paper is ok, although it falls onto preliminary side. Since I did not identify any technical problem so far, I will vote for a weak acceptance, unless I observe more technical issues during the discussion.<BRK>This is the first work that studies probability calibration for knowledge graph embedding models. When ground truth negatives are not available they propose to synthetically generate corrupted triples as negatives and use sample weights to guarantee that the frequencies adhere to the base rate. Given that the paper s major contribution is experimental insight, and there are no major technical contributions, I would have liked to see a more in depth analysis of how some of the key hyper parameters influence the calibration of a model beyond the type of the loss, and beyond the correlation with embedding quality. 5) Platt scaling assumes that per class probabilities are normally distributed, while isotonic regression makes no assumption about the input probabilities. It would be valuable to report similar results for the other datasets in the appendix. Is there an explanation for this?<BRK>This paper focuses on the calibration of the knowledge graph embedding task with Platt scaling and isotonic regression. In this paper, the authors only apply the existing techniques (e.g.Platt Scaling, Isotonic Regression) to tackle the calibration issue, which makes a minor contribution. However, in the case of FB13 (ComplEx) and YAGO (TransE), the results are again the conclusion. I suggest the authors clearly state the experimental analysis.
Accept (Poster). rating score: 6. rating score: 6. rating score: 3. <BRK>The paper aims to improve the random forest performance by iterative constructing & feeding more powerful features that are to be used by the random forest learning processing where a random subset of features are chosen from the current feature pool when making growing/stopping decision at the current split node. The idea is new and interesting, and its usefulness has been empirically shown. It would be very helpful if the paper could shred some lights in this regard.<BRK>The paper proposes a scheme to learn representations and a randomforest model in a closely coupled manner, where the representationsare learned to simultaneously improve the predictive performance ofthe individual trees in the random forest and reduce the correlationbetween these trees. The introduction  mentions that the individual trees in the forest are "less  biased".<BRK>The comparison with state of the art Tab 3 and Tab 4 shows that for some datasets other techniques are better, did the authors draw some conclusions from that? Replicability: I think with improvements in the experimental section the method results can be replicated. How much is the overhead of building the random forest for each iteration of the learning (algorithm 1), a more detailed analysis on this is useful for understanding the method.
Reject. rating score: 1. rating score: 3. rating score: 3. <BRK>It’s riddled with typos, inaccurate notations and undefined variables (see below for a sampling). are not defined. Sec 4, Algo 1 contains the main core of the proposed algorithm, but it’s only defined for the 2D grid case. How have the baselines been implemented? How has data been split for training/testing? This is not true from the formula. Overall, the major criticisms of this paper:   The proposed algorithm is not clear. The authors need much more experimentation to bolster their claims in the paper.<BRK>The introduction (even corrected) is very abrupt and it is very difficult to understand the problem that the authors propose to attack. I only understood the adressed problem after seing which are the baselines the proposal is compared with in section 4.2. Also, the introduction does not introduce the proposal at all. The proposed approach is completely cryptic, with clearly not enough definition of the notations the algorithm deals with. From my point of view, without a full re writting of the paper, this work cannot be published in a conference like ICLR.<BRK>This paper proposed a new diffusion operation for the graph neural network. Extensive experiments have been conducted to verify the performance of the proposed method. 1.The motivation of this method is to accelerate the diffusion speed in a graph. As a result, we cannot use many layers so that the non linearity of deep neural networks cannot be fully utilized. Thus, is it necessary to accelerate the diffusion speed for graph neural network? More datasets are needed to thoroughly verify the performance of the proposed ballistic graph neural network.
Reject. rating score: 3. rating score: 3. rating score: 6. <BRK>The motivation and direction of the paper is simple and easy to follow: Take the state of the art semi supervised learning algorithm and propose an active learning version of it. It is not a straightforward combination of MixMatch and active learning, and there are some specialized techniques such as “aug” used in the design of the proposed algorithm. A more minor comment: The same issue goes for the semi supervised learning side.<BRK>The paper proposes to combine active learning techniques with MixMatch for semi supervised learning. The cost analysis of adding labeled vs unlabeled data looks interesting. And semi supervised learning has a long history and it is not restricted to recent deep learning based approaches. A thorough review can make the approach well placed in the literature. In experiments, the authors only compare with MixMatch. I suggest that the authors include the missing literature in the next version.<BRK>This paper takes a look at using active learning techniques instead of random sampling for the "state of the art" semi supervised learning (SSL) method MixMatch. An additionally interesting point is the value of labeled vs unlabeled data in this setting. In the experiments section, 262144 and 32768 iterations seem to come from nowhere. Only later did I realize that these were powers of 2.
Reject. rating score: 1. rating score: 3. rating score: 6. <BRK>This determines the optimal action in constant time with respect to the horizon. The algorithm assumes access to pre trained value and policy networks and it uses calls to these networks to prioritize the next state to be explored. Deterministic transition  Importantly, access to value network that gives noisy estimates of the optimal value function   The noise model is additive and i.i.d and satisfies a concentration inequalityAll this assumption makes the setting very simple and unrealistic.<BRK>This paper proposes A*MCTS, which combines A* and MCTS with policy and value networks to prioritize the next state to be explored. These discussions are critical to understand the merit of the proposed algorithms. And it combines A* search with MCTS to improve the performance over the traditional MCTS approaches based on UCT or PUCT tree policies. •	The complexity bound in Theorem 1 is hard to understand.<BRK>This paper presents the search algorithm A*MCTS to find the optimal policies for problems in Reinforcement Learning. In particular, A*MCTS combines the A* and MCTS algorithms to use the pre trained value networks for facilitating the exploration and making optimal decisions. The experiments verify the effectiveness of the proposed A*MCTS. But the experiments should be improved to illustrate the reasons for the hyper param setting.
Reject. rating score: 3. rating score: 3. rating score: 8. <BRK>The nice thing is that as the linear approximations get better, the contributions in this paper would continue to help. The paper makes two key algorithmic/theoretical contributions:1. An approach to obtain a better estimate of the radius of the l p ball where the NN is provably robust. This result is fairly straightforward, and relies on computing the distance of a point to the boundary of adversarial polytope. This I think is a more interesting contribution. Particularly, when the current paper refers and discusses both of the above works. Is there a specific reason to not compare (other than that the SDP based approach is not a linear approximation, and probably is much slower)? The gains in the average robustness are somewhat small, and these gains alone are not convincing without being able to see how these gains were obtained. 3.Thanks   it is much more clear now. 4.The distributions for MMR/PER seem quite similar and most of the gain seems to come at the lower end (small eps). This still remains my biggest concern   I was hoping that the distributions would diverge a bit more at larger eps, but this is difficult to confirm with the current set of plots.<BRK>This paper proposes a certifiable NN training method, "polyhedral enveloperegularization" (PER) for defending against adversarial examples. The defenseis based on the same linear relaxation based outer bounds of neural networks(KW/CROWN) used in many previous works. The paper makes a few new (but small)technical contributions:1. this paper uses a different loss function (7), which is essentially Hingeloss on the lower bounds of distance to decision boundary. The authors proposes a very small improvement to thebinary search process by setting the lower bound of search to the largestepsilon that is certifiable using the current linear relaxations obtained froma larger epsilon. In this paper, the authors instead in this case use  b /||A|| as the certifiable radius. This is merely a different way of evaluation,and I don t see this as a contribution, as the "improvement" does not come froma tighter bound. 4.As discussed above, it is probably not fair to compare PER+at with KW. A newbaseline like KW+at should also be considered. I suggest rephrasing some parts of the paper and providing moreexperimental results as discussed above.<BRK>Opinion:This paper is clearly written and I think that it s an interesting insight and that the authors do a good job at conveying its usefulness. I m happy for this paper to be accepted. This paper highlights the fact that in the case where the studied radius is not safe, it is possible to extract a safe radius from the linear bound. The authors then show that: > this can be used to make binary search to find the larger verifiable epsilon faster (by providing a lower bound on epsilon even when failing to verify, rather than simply when succeeding verification) > this can be employed during the training process to improve regularization, similarly to another previously proposed method (Croce et al.).
Reject. rating score: 1. rating score: 3. rating score: 3. <BRK>2.There seems to be a lack of novelty except combining Sinha et al’s theoretical result with GAN training objective. ### referenceAman Sinha, Hongseok Namkoong, and John Duchi. Certifying some distributional robustness with principled adversarial training. This paper reads “here we use the wasserstein metric” in “robust training over generator” subsection, the reviewer is not sure if the authors are aware of this point.<BRK>This paper proposed another way to improve GANs. The method tackled the robotness issue by requiring the generator and discriminator to compete with each other in a worst case setting.<BRK>SummaryThe present work proposes to combine GANs with adversarial training replacing the original GAN lass with a mixture of the original GAN loss and an adversarial loss that applies an adversarial perturbation to both the input image of the discriminator, and to the input noise of the generator. The resulting algorithm is called robust GAN (RGAN). Extensive experiments show a small but consistent improvement over a baseline method. DecisionThe authors do a thorough job at characterizing the proposed method using both theoretical analysis and wide ranging experimental studies. In particular, the generalization bound does not seem to depend on lambda, (which interpolates between the original GAN and RGAN).
Reject. rating score: 1. rating score: 3. rating score: 6. <BRK>The paper then shifts to the main contribution: an in depth study of a Gaussian mixture simulation along with accompanying theoretical results. This means that while samples from p_true should fall within the model’s typical set, the model typical set may be broader than p_true’s. Table 1 makes this clear by showing that only 30 40% of samples from the model fall within the typical set of p_true. The paper then makes the observation that the models do not have high overlap in their typical sets, and thus p_true’s typical set could be well approximated by the intersection of the various models’ typical sets. Applying this procedure to the Gaussian mixture simulation, the authors observe that ~95% of samples drawn from the intersection of the ensemble fall within p_true’s typical set. This paper makes some careful distinctions between the true data process, the model, and the alternative distribution, which I have not seen done often in this literature. While the text does make some compelling arguments in the Gaussian mixture simulations, some validation on real data must be provided. Besides the lack of experiments on real data, I find the paper’s material to be a bit disjointed and ununified. ____Final Evaluation:  While I find the paper to contain interesting ideas, it is too unfinished for me to recommend acceptance at this time.<BRK>This paper proposes to use ensembles of estimated probability distributions in hypothesis testing for anomaly detection. While the problem of density estimation with its application to anomaly detection is relevant, I have a number of concerns listed below:  Overall, this paper is not clearly written and it is difficult to follow. Discussion is not straightforward at many points. In particular, the objective of experiments on synthetic data in Section 3 is unclear. What is the proposal and how to evaluate it in the experiments? There are also many grammatical mistakes, which also deteriorates the quality of the paper. The distribution p should be not the ground truth but the empirical distribution. * In experiments, only a simple Gaussian mixture model has been examined. * How strong are the assumptions in Theorem 2 in practical situations? Hence the effectiveness of the proposed method is not clear.<BRK>Summary:I machine learning, we often have training data representative of an underlying distribution, and we want to test whether additional data come from the same distribution as the training data (e.g.for outlier/anomaly detection, or model checking). This paper points out that the typical set of the model may be very different from the typical set of the underlying distribution if the model is learned by maximum likelihood, in which case a test of typicality with respect to the model would be a poor test of typicality with respect to the underlying distribution. The toy example that is used to illustrate the problem is clear and illuminating, and motivates the paper well. I don t think this is true. The idea of using the intersection of the typical sets of an ensemble of models is interesting and clever, and backed by strong theoretical results. In particular, the paper proposes an idea and theory to back it up, but it doesn t really propose a practical method, and as a result it doesn t test the theory in practice. "autoregressive models, such as PixelCNN"Autoregressive models can also be used to model discrete variables in which case they can t be thought of as flows. Are the theorems relevant for small n? The model and the loss are (at least in principle) orthogonal choices. Goodness of fit testing can be used in situations other than outlier detection, e.g.for testing whether a proposed model is a good fit to a dataset.
Reject. rating score: 3. rating score: 6. rating score: 6. <BRK>The authors aim to propose new algorithms for min max optimization problem when the gradients are not available and establish sublinear convergence of the algorithm. I don t think this paper can be accepted for ICLR for the following reasons:1. For Setting (a) (One sided black box), the theory can be established by the same analysis for ZO optimization by optimizing y. 2.The assumptions A1 and A2 are hardly satisfied in ML applications, where the objective is essentially smooth.<BRK>The paper presents an algorithm for performing min max optimisation without gradients and analyses its convergence. The presented algorithm is a natural application of a zeroth order gradient estimator and the authors also prove that the algorithm has a sublinear convergence rate (in a specific sense). However, to the best of my knowledge, it has not been used in this context before and personally I find the algorithm quite appealing.<BRK>This paper considers zeroth order method for min max optimization (ZO MIN MAX) in two cases: one sided black box (for outer minimization) and two sided black box (for both inner maximization and outer minimization). Convergence analysis is carefully provided to show that ZO MIN MAX converges to a neighborhood of stationary points. (13)  is better, but not the proposed algorithm?
Reject. rating score: 1. rating score: 6. rating score: 6. <BRK>The authors extend the existing work SGC [1] to a nonlinear version, which addresses the limitations of dealing with nonlinear feature. It further extends the theoretical finding in [1] about the low pass filtering functionality of graph convolutional networks and shows its advantage in dealing with graph signal processing problem. Pros:1.This work goes into detail of the theoretical finding of SGC. 2.Authors conduct extensive experiments on multiple datasets. 2.Authors further extend the theoretical finding in [1] and verify the fact that low pass filtering functionality of graph neural network provide better noise robustness than two layer MLP. It has some novelty, but the novelty is incremental. The paper has the problem of notation missing, e.g on page 2, Fig1 is never mentioned; on page 7, the notation of "LG" is missed in Fig 5.<BRK>The paper shows the graph signal convolution in GCN based models is typically a low pass filter. To be honest, the paper is well motivated, well written. However, I have some concerns regarding the technical contributions of this paper. (3) It seems the only difference between SGC and gfNN is that gfNN is equipped with one more hidden layer. If it is the case, then the technical contribution of gfNN is trivial. I vote for weak accept, but I am fine if it is rejected.<BRK>Therefore, I think it is a little too aggressive to conclude that claim. On the other hand, although many graph neural network models take the approach of graph filtering, there is little theoretical analysis on graph NNs that take the filtering approach. This paper analyzed the learnability of filtering type graph NNs. The authors showed that a two layer GCN approximately behaves as a noise filtering + a two layered MLP (Theorem 8). Compared to them, their result admits non linearity. Finally, the authors experimentally showed that existing graph NNs do not have predictive power when useful information is in high frequency domains. For these reasons, I think this paper has contributed to a deeper understanding of graph NNs and sufficiently significant. Especially, I could not understand how the term $\tilde{O}(\epsilon^{1/4})$ is derived. In Section 3, the authors claimed that the performance of MLPs is relatively more robust to the Gaussian noise in the low frequency regime compared to the high frequency regime.
Reject. rating score: 1. rating score: 3. rating score: 6. <BRK>This paper proposes a coloring scheme that can increase the expressive power of GCNs. 1.A major concern for this method is the permutation invariant in coloring scheme. The experimental studies are weak. There should be some ablation studies to evaluate the effectiveness of the coloring scheme. The novelty of this paper is incremental and not technically sound.<BRK>This paper proposes an extension of MPNN which leverages the random color augmentation to improve the representation power of MPNN. Same GNN baselines are missing in the structural property tests as well. 4, The experimental results on the benchmark datasets are less impressive as the mean performances are close to the WL test results and the variances are considerably large.<BRK>The paper presents an interesting work, called Colored Local Iterative Procedure (CLIP), to improve the expressive power of Message Passing Neural Networks (MPNNs). They then developed a coloring scheme to improve the MPNN, and obtained superior performance on benchmark graph classification datasets as well as in the graph property testing experiments. It also does not change the expressive power of MPNN. However, why do the authors use k as a hyperparameter to select the best results in classical benchmark datasets? It seems $k$ has different meanings in different places of the paper.
Reject. rating score: 1. rating score: 3. rating score: 6. <BRK>This paper presents a method for learning to predict a depth map given a pair of images. I thought for a moment that the advantage of the paper was to be able to predict an absolute motion and an absolute depth (this is not impossible if the method is able to estimate the scale from known objects, as it is suggested from the introduction). I have several concerns about this paper.<BRK>This paper provides a depth learning architecture (global local structure) from two images. 3:  Even for single image network, eigen s method is not SoTA, the author may consider (1)  as one of the baseline, etc. Pros:1: The paper is well written and motivations are clearly explained.<BRK>This paper proposed a novel global local network, which can be trained with extremely sparse ground truth, to predict dense depth. Are there some failure cases, and some visualizations on the filter banks of the local network? After Rebuttal:I thank the author for the response.
Accept (Poster). rating score: 8. rating score: 6. rating score: 6. <BRK>The paper empirically shows that the double descent phenomenon occurs as a function of EMC. However, these are mainly hypotheses. I like the paper for its rigorous experiments and providing intutions from their observations. However, I am very new to this area of research and will only provide my review on my understanding.<BRK>I do not have much to say about the paper except that I like the sumilation, and found rather interesting. All in all, i definitely support publication. Manfred Opper (1995), can be seen here: http://www.ki.tu berlin.de/fileadmin/fg135/publikationen/opper/Op01.pdf FIG 10 From this work, it is also rather clear that increasing the number of training examples can hurt performances (this is exactly what the plot says), at least without regularisation. However, this is consistent with our generalized double descent hypothesis:…". Again this was shown (empirically) as well in the above mentioned papers and also explained in "physics terms" where the peak is like a phase transition (jamming), if one stop the dynamics.<BRK>This paper provides a valuable and detailed empirical study of the double descent behaviour in neural networks. This intuition is correct I believe. At the same time it has several issues that need to be addressed. The fact that previous work also observed this in CIFAR should be moved in the introduction.
Reject. rating score: 3. rating score: 3. rating score: 3. rating score: 6. rating score: 6. <BRK>I am not familiar with the generative model and continual learning. The paper proposes a new problem setup as "online continual compression". The paper gives a combination of many existing techniques to address the new problem.<BRK>If so, continual learning should run in an online learning way. 2)	The motivation of the hierarchy in this work is unclear. It seems a bit weird since the z_q^{i 1} is not an image and its elements are index values. This works covers related works very well.<BRK>In general, the novelty of the paper is a little bit limited and the writing of the paper is not very easy to follow. It is unclear why the stacking works for online continual compression. I don t understand it. What are the yellow rectangle parts in Figure 1?<BRK>This paper focuses on the problem of continual learning with limited memory storage. How would the form of the tasks (in case of number of classes per task) affect the results? for a model to exploit and there is not enough storage capacity to keep all the data without compression.<BRK>The idea is very simple yet interesting, the paper is a good read, and the results seem promising. The ablation study is well designed but not discussed enough. Additionally, the experiments cannot support the idea well since it is on a very special setting, the LiDAR experiment is missing quantitative evaluation, and different tasks (such as text classification, or visual tracking with only one labeled sample) might introduce different difficulties in this online learning setting.
Reject. rating score: 3. rating score: 3. rating score: 3. <BRK>SummaryThis paper propose an extention method of SGD, deep gradient boosting (DGB), which views the back propagation procedure as a pseudo residual targets of a gradient boosting problem. To apply DGB to the real CNNs, DGB is simplified to a input normalization layer, conditioned on the assumption that the convolution kernels should be small. For example, could this input normalization layer help to address the problem that BN performs bad when batch size is small? The authors mention that this layer does not have additional parameters. But as I know, the parameter size of BN is small, which downgrades the significance of the proposed method. * In the DGB experiments, the improvements of DGB compared with SGD in four datasets all seem marginal, in which DGB is slower than SGD.<BRK>The paper proposes an a new idea of treating the back propagated gradients using chain rule as pseudo residual targets of a gradient boosting problem. It is an interesting idea to look at gradients different and update weight accordingly. The paper is written in a way that it looks like two papers and connected with the same idea DGB. In all experiments, can you also report running times? For 2.1.1 MNIST, why is alpha the larger the better while the others are not this case?<BRK>The paper presents a new gradient update rule for neural net weights, which is a result of relaxing the usual SGD to a regularized linear regression between the descent direction and the original gradient. Although the method seems to work and the idea is interesting, I’m not entirely convinced by the experimental evidence. A more modern architecture would be preferable in those experiments. It might be better to reorganize the text and present everything about the method (fast approximation, CNN, interpretation as normalization) together. The authors name the proposed method “deep gradient boosting”, but I’m not entirely sure how it is related to gradient boosting. I don’t quite follow Eq.5 – There seems to be missing something.
Reject. rating score: 3. rating score: 3. rating score: 6. <BRK>The paper derives results for nonnegative matrix factorization along the lines of recent results on SGD for DNNs, showing that the loss is star convex towards randomized planted solutions.<BRK>This paper studies loss landscape of Non negative matrix factorization (NMF) when the matrix is very large. This suggests that the optimization problem would become easier as the size of the matrix becomes very large. The NMF problem is known to be NP hard. In case that the matrix X to factorize is large, the author(s) uses concentration property of random matrix to show that along any random positive matrix U,V and U’,V’, the MSE loss of NMF is convex with high probability. I think it would be better to mention this somewhere in the paper.<BRK>(2)	“The same holds along the line line…” in page 3 should be “The same holds along the line…”. 2.Figure 3 presents the loss surface of NMF on straight paths connecting two random points for 8 real world datasets. From this figure, we can see that the minima is obtained around lambda equals to 0.5. 3.Experiments utilize 8 datasets to demonstrate the good performance of the proposed planted model. These can be processed by the given sparsity. 5.Open codes about this paper.
Reject. rating score: 3. rating score: 3. rating score: 3. <BRK>This paper proposes a framework to unfold the safeguarded Krasnosel’ski˘ı Mann (SKM) method for the learn to optimization (L2O) schemes. 1.Safegarding is the key point of this paper, but the authors did not review related works on safegarding. First, SKM is proposed in Algorithm 1 with convergence guarantee established in Theorem 3.1 and Corollary 3.1. Moreover, the experiments are not convincing.<BRK>This paper presents a unified framework for parametrizing provably convergent algorithms and learning the parameters for a training dataset of problem instances of interest. I also have some comments about related work. Are all methods performing that horribly? The other L2O methods for gradient descent (see refs.below) use shared parameters instead. Please discuss this. "Learning to learn by gradient descent by gradient descent." Advances in neural information processing systems.<BRK>This paper is trying to provide a general learning to optimize(L2O) convergence theory. It proposes a general framework,  the Learned Safeguarded KM(LSKM) method, and proves the convergence of the algorithms generated by this method under certain conditions. Also I have some comments as follow:1.
Accept (Poster). rating score: 8. rating score: 6. rating score: 6. <BRK>This paper aims to improve the performance of leader agents in leader follower Stackelberg Markov games (SMG) by introducing architectural components. The authors show significant improvements over the state of the art on several problems, and also evaluate the importance of the different components of their approach with thorough ablations. The supplementary material is longer than the main paper, and almost half of it is additional results. More discussion of the concrete applications would be informative. Although I am not intimately familiar with the research area of SMGs, I expect this work will have significant impact in the area on the basis of the thoroughness of the results alone.<BRK>1.SummaryThe authors apply MARL to principal agent / mechanism design problems where selfish agents need to be incentivized to coordinate towards a leader s (collective) goal. The leader is modeled as a semi MDP with event based policy gradients and modules to model/predict followers  actions. The authors compare this approach on 4 environments with M3RL, which also solves (extensions of) principal agent problems. 4.Additional feedback with the aim to improve the paper. Make it clear that these points are here to help, and not necessarily part of your decision assessment.<BRK>The proposed model is interesting and the authors have done sufficient empirical studies to show the success of their method, as well as some necessary theoretical derivations. My major concern about this work is whether the task that this paper follows is widely studied in the reinforcement learning community, as the authors mentioned that only [1] can be used as baseline methods since the other methods can not be applied in their problem. But in general I think this is an interesting paper. This paper proposed 3 major contributions: event based policy gradient, leader follower consistency scheme and action abstraction policy gradient. The authors show the empirical performance comparison for the case with and without each of these techniques in the experiments. In addition, the proposed method performs better compared to the state of the art method (in [1]). Intuitively the proposed techniques make sense, but it will be great if more insights are provided. Also, since the authors mentioned in the appendix that two of the tasks are original from [2], but we can not apply the method in [2] as a baseline method. It is possible to modify the method in [2] to fit with the setting and compare with them?
Accept (Talk). rating score: 8. rating score: 8. <BRK>Finally, the literature review section is quite excellent.<BRK>Using this framework, the paper defines the notion of conditional F entropy and F entropy (by conditioning on an empty set).
Accept (Poster). rating score: 8. rating score: 6. rating score: 6. <BRK>Moreover, for efficiency, the authors also propose a feature based critic to replace the original one. The results are convincing. The paper is well presented, including clear background and good illustration, etc.<BRK>The paper is very polished and nice to read so that somebody can really learn from it. I don t quite understand the explanation for why REINFORCE wouldn t be effective.<BRK>In that sense the datasets used in the paper are not really large scale. However, there’s a number of inaccuracies, which should be addressed.
Reject. rating score: 1. rating score: 1. rating score: 1. <BRK>The paper presented a CRM model which is VAE with separate priors for save and unsafe modes and utilize it in  RL  for roundabout planning task. It is only performed on one task which is on getting out of the roundabout. It also does not compare with any real baseline methods. It is only the proposed method compare with the weaker version of the method. In this case,  the label is s and I believe that the latent space will be meaningful regarding s.5.. 6.Writing needs to be improved. There are grammatical mistakes here and there.<BRK>This paper proposes a reinforcement learning model to solve a task relevant to autonomous driving, and evaluates that model in a traffic simulator called Simulation of Urban Mobility (SUMO). The paper describes related work but the connection to the exact problem they are solving wasn’t 100% clear to me. I don’t see a significant enough algorithmic contribution from this paper to yield an ICLR acceptance.<BRK>This paper tackles the challenge of control with safety constraints using a learned soft constraint formulation. Then it learns a variational approximation q(z|x) to the true posterior p(z|x). They then combine this learned safety constraint with a constrained RL method, SSAC, to learn safe policies. This means that, unlike SSAC, this work does not encourage safety at all early in training. The particulars of the method used for computing whether the safety constraint is violated are somewhat surprising. Overall I think there are some interesting ideas inside this work, but it needs some improvements:1.
Reject. rating score: 3. rating score: 3. rating score: 6. <BRK>This paper studies the problem of applying attention to the problem of image captioning. The idea of applying additional structural constraints on the sparsity structure induced by Sparsemax is a cool idea, and I like the idea of incentivizing contiguous pixels to have similar attention probabilities. The major weakness I see in this work is that the authors only limited their experiments to image captioning. [Edit: After going through reviewer discussion, I updated my score to reject. I am not convinced of the motivation for sparse attention unless it is for long sequences, since otherwise the regular softmax should be able to assign 0 s to the un needed items. Moreover, for generalization one can use attention dropout which is simpler instead.]<BRK>This paper proposes two sparsifying methods of computing attention weights, dubbed sparsemax and TVmax, which appear to slightly improve objective and subjective image captioning scores. The sparsifying projections are posed as optimization problems, and algorithms for their computation, along with formula for their gradients are given. It is unclear why this fails to produce better objective scores than sparsemax, while producing better human ratings. In any case it is not clear why this should necessarily be a good inductive bias for all images, although it is plausible that it helps in some cases. Overall the paper is flawed by the lack of clarity in the motivation for the proposed methods, and the lack of retrospective analysis and understanding of why the proposed methods should improve results.<BRK>This paper produces a new method call TVmax and presents that the selective visual attention could improve the score in the Image Captioning task. Different from the fusedmax[2] which fuses attention in one dimension, the proposed method encourages the sparse attention over contiguous 2D regions. Compared with the softmax function, the sparsemax[1]  and the TVmax are able to sparse the visual attention very well. The paper also evaluates the score in both automated metrics and the human rating. I wonder whether there is a better task for evaluating the visual attention. Although the proposed method (TVmax) is slightly worse than the sparsemax in the automated metrics, it is still promising in multimodal problems. 2.Could you show some results of TVmax on the other task in order to show the effectiveness of the proposed method?
Reject. rating score: 3. rating score: 3. rating score: 6. <BRK>The paper introduces the R Transformer architecture which adds a local RNN layer before each attention layer in Transformer. It is an interesting paper still and the locality is a nice way to remedy the speed problem, but the paper lacks a true study and ablations on this main limitation. Without these results, we cannot recommend to accept this paper.<BRK>Contribution: The authors propose an architecture that combines the practices of recurrent and feed forward sequence models. i) The authors use the "same hidden size for R Transformer and Transformer." I think the authors should instead control the # of model parameters, especially since you are running only on small tasks with small sized models. Another case is Equation (7): you have  FeedForward(mt) , but  mt  is not on the right hand side of the equation at all. How does R Transformer compare to Transformers in terms of speed and memory? But at the same time, I feel that the experiments can be a lot stronger, and the paper has limited novelty when compared to prior works. 3.Regarding the motivation to insert a local RNN at the start of each layer. that the positional embedding (PE) have only limited effect.<BRK>In this paper, the authors propose a novel transformer model called R Transformer. Based on the observation that positional embeddings require a a lot of design efforts in vanilla Transformer, the authors propose to use local RNNs toencode local information in replace of positional embeddings. In the R Transformer, however, the multi head attention layer will not be able to obtain positional information from local RNN outputs. The empirical study shows that R Transformer can outperform vanilla Transformers and recurrent architectures, which is promising. How much overhead does the local RNN introduce?
Reject. rating score: 3. rating score: 3. rating score: 3. <BRK>This paper proposes a differentially private version of the lottery ticket mechanism using the exponential mechanism, thus improving the utility of DPSGD by reducing the number of parameters. Thanks for the experiments! Though it is more of a simple combination of the non private lottery ticket mechanism and the exponential mechanism, improving utility for DPSGD is a very important topic in differentially private machine learning. I believe the sensitivity should be something like max(\nu, 1).<BRK>This paper is a mash up of recent work on differentially private stochastic gradient descent (DPSGD) and the lottery ticket hypothesis. Could you not compensate for this by scaling the norm by the number of parameters, or does that mess up the differential privacy calculation? I understand that they use the exponential mechanism, but I would have assumed that the dataset needs to be varied in addition to the model.<BRK>To mitigate such utility loss, the authors proposed to use the lottery ticket mechanism to train a smaller network with similar utility and design an end to end differentially private mechanism for the whole process, including the selection of sub network and the re training of the selected sub network. Note that since the noise is added to the accumulation gradient, so the scale of the noise should be roughly 1/B of the accumulated gradient. Could the authors have a comment on this?
Reject. rating score: 1. rating score: 3. rating score: 3. <BRK>The paper proposes methods for improving the performance of differentially private models when additional related but public data is available. The authors propose to fine tune the private model with related but public data. I vote for rejecting the paper. It neither makes core contributions to differential privacy nor to active learning.<BRK> SUMMARY This paper gives two methods for improving an existing differentially private model. The authors seem to want to claim that active learning is a useful private approach because it gets data independent privacy guarantees and performance similar to algorithms with data dependent privacy guarantees. But overall, I am most interested in the PATE comparison. However, the paper makes a confusing omission: it does not mention PATE [1, 2]. Add discussion of PATE.<BRK>This paper considers the problem of how to improve the performance of an existing DP classifier, with the help of the labelled public data. I do not think this paper has made a lot of contribution to either differential privacy or active learning. There is almost no theoretical contribution made by this paper. The paper uses private argmax algorithm and claims that it satisfies $eps_{support}$ DP. Thanks for the authors  classification.
Accept (Talk). rating score: 8. rating score: 8. rating score: 6. <BRK>The input to the generator are linguistic and pitch signals   extracted externally, and noise. As the comment below notes, it is a sort of patch GAN discriminator (See pix2pix, and this comment from Philip Isola   https://github.com/junyanz/pytorch CycleGAN and pix2pix/issues/39) and that is could be quite significant in that it classifies at different scales. Likewise, perhaps we can imagine something similar in the case of audio at varying scales   in fact, audio dependencies are even more long range. I found the speech sample presented very convincing. It is also promising that producing good speech could be achieved by a non autoregressive or attention based architecture. The authors mention that they hardly encounter any issues with training stability and mode collapse. Is that because of the design of the multiple discriminator architecture?<BRK>With all due respect, previous GAN trials for audio synthesis are inspiring, but their audio qualities are far away from the state of the art results. Overall, this is a very good paper with significant contributions to the filed. 3, The notations in Eq.(1) and (2) are messy. Although I can figure their meaning from the context, one may clarify certain notations if they appear at the first time. Could the authors shed some light on the potential reason? 4,  It would be very interesting to see an analysis of model stability with smaller batch sizes.<BRK>This paper proposes to enable GAN based TTS in the time domain with the careful designs of the (non autoregressive) generator and discriminator. The method also proposes new objective measures inspired by the image recognition network based on the high level features generated by end to end ASR, which is also another important contribution of this paper. Although I really appreciate the authors  efforts on providing implementational details in the appendix, the code and data do not seem to be publicly available, and I m expecting that the implementation of this technique is relatively hard due to their complex designs of the generator and discriminator.
Reject. rating score: 3. rating score: 3. rating score: 6. <BRK>Unfortunately, neither seems to be demonstrated in this paper.<BRK>Minor comments  I would recommend that the authors proofread for English grammar and style in updated versions of the paper. I would vote to weakly reject this paper for two key reasons   first, I think the writing can be improved and explanations can be clarified, especially for people less familiar with the field like myself.<BRK>The paper is out of my research area. I have tried to read this paper, but it was rather tedious with heavy notations. It would be more friendly to represent the models in visible way for example using diagrams as I can see that the model is a sequence matrix operators with non linear transformations after that.
Reject. rating score: 3. rating score: 6. rating score: 6. <BRK>Are you backpropagating through algorithm 3 toward embeddings or just toward thres as in algorithm 4? Given such embedding one can perform faster approximate nearest neighbor search in compare to calculating pairwise edit distances. Finally they modify the embeddings with a stochastic, differentiable algorithm such that they can get guarantees for generalization. Unfortunately, the manuscript is not well written. This paper has effectively memorized the edit distances of some sequences. It seems that only phase 3 (cgk ) is designed to have any accuracy on unseen sequences, and experiments show that it underperforms the original cgk. If this is not true and indeed they are training for example on one half of the corpus and the 100 query + base are unseen during training I am willing to increase my score. Given the added clarification in the paper. SGD I assume? Is there any validation set? The text says "we calculate the absolute loss as in Phase 1 to optimize our embedding network f3" but eq 7 is on f (3).<BRK>In this paper, the authors propose an approach for learning embeddings for strings using a three phase approach. However, it is not clear to me how these theoretical properties are used by the neural network. In particular, Equations 5   8 all suggest that Hamming loss is defined on the outputs of the various neural networks (f_1, f_2, and f_3). The paper also refers to bits in the output of f_3. It seems that Theorem 1 suggests that the binary outputs of CGK’ can be decoded, but I cannot tell whether that extends to the embeddings. It seems as though the three phases could be rolled into a single multi task learning problem in which the network is trained during a single phase. The references are not consistently formatted.<BRK>The paper proposes a scheme to learn representations for sequencesusing neural networks such that the Hamming distance in the embeddedspace is close to the edit distance in the original representation ofthe sequences. This is achieved through a 3 phase algorithm which areclearly motivated and technically sound. Given the clarity, technical soundness and the improved empiricalperformance, I am leaning towards an accept. However, there are acouple of open questions that, if addressed, would help me betterunderstand the contributions:  The main concern is the need for the separated out phases instead  of directly minimizing some combinations of these different  terms. The text is Sec.5.4 does  not appear to match up with the Table 5. Moreover, if there is a cost to Phase 3, can this be properly  explained? If there is a cost to it, could there be a way to decide  if/when we require the phase 3? Minor:  It would also be important to understand the effect of the network  architecture on the downstream performance.
Reject. rating score: 3. rating score: 3. rating score: 6. <BRK>This paper studies the performance of the mirror gradient method when applied to the overparameterized network. The authors claim that the SMD method could find the regularized global minimize for different potential functions, in terms of minimal Bregman distance. 2) The authors  assumption in Assumption 3.1 is too fuzzy: instead of detailed analysis in the papers related to the overparameterized network, the author does not give ANY relationship between the $\epsilon$ and these three important parameters: a) network width, b) probability introduced by random initialization, c) the number of input data. 3) Therefore, according to the too strong and fuzzy assumption mentioned in the last point, Assumption 3.1 simply makes the network equals to the linear model, which leads to minor contributions according to the study of the overparameterized network. 4) The `over parameterized network  discussed now, including the work of (Li & Liang, 2018; Du et al., 2018; Azizan & Hassibi, 2019; Allen Zhu et al., 2019; Cao & Gu, 2019), are mainly focused on the `network of infinite width , however, in the authors  experiment, the network architecture, e.g.ResNet18, is more to be a `very deep network  rather than a `very wide network . Therefore, the authors  theory is built on Assumption 3.1, which is based on `network of infinite width , while the experiment is built on the `very deep network . In conclusion, I am convinced that the authors  work is over claimed in this paper.<BRK>In this paper, the authors study the behavior of stochastic mirror descent on overparameterized nonlinear models. Especially, the authors show that for appropriate initialization, SMO converges to a global minimum with a minimal distance to the initialization. The implicit regularization of SGD/SMD is widely studied in the literature. Although it is claimed that the paper improved existing results by either considering nonlinear models or identify more precise implication regularization, the contribution is a bit minor. I have read the authors  rebuttal and would not like to change my score. The fundamental identity in Lemma 6 was already derived in Azizan & Hassibi (2018). This paper identifies some assumptions to show the implicit regularization studied in Azizan & Hassibi (2018) with heuristic argument. Indeed, as stated in Azizan & Hassibi (2018), the convergence in linear case depends on the non negativity of $D_{L_i}(w,w_{i 1})$. In this paper, the authors just assume that this non negativity holds locally in Assumption 1.<BRK>This paper studies the optimization behaviour of stochastic mirror descent (SMD) on over parametrized nonlinear models. It shows, rigorously and empirically, that SMD finds global minimizer that is approximately the closet global minimizer to the initial point, under the same Bregman divergence. The paper is well written and easy to follow. Pros:1.The main contribution of this paper is extending the previous results on linear case into nonlinear setting, which is much more general. The results are non trivial and interesting. The main concern I have is about assumption 1 in the paper. I am not sure if it is still a high probability, when \eps needs to be small enough so that the intuition behind Figure 2 still holds. b.Is this assumption also made in the referred papers on the top of page 6? For example, one can also assume the PL inequality locally true for over parametrized function, and develop a convergence result.
Reject. rating score: 3. rating score: 6. rating score: 6. <BRK>Overall, this work empirically evaluates different techniques used in privacy learning and suggest useful methods to stabilize or improve performance. Weakness:As far as empirical research, the compared techniques are too few. What if we use those less popular techniques, for example, RMSprop optimization method? The model capacity of neural networks, especially deep networks, has some non trivial relation to the number of filters or the number parameters. It is important to quantify such relation. The baselines are not enough. It has been further developed by the following researchers.<BRK>This paper presents experimental evidence that learning with privacy requires approaches that are not identical to those used when learning without privacy. The specific technical details on some experiments were either difficult to find or were lacking. Moreover, in addition to the experimental evidence alone, most of the components considered in the paper were accompanied by reasonable justification/hypotheses for why the choices enable such improvements.<BRK>The main statement of the paper is quite simple: optimize hyperparameters for the model that you re training (DP SGD) rather than the model it is inspired by. Yet, the findings an recommendations may be useful for practitioners. The results would be much more convincing if these or other models widely used in practice were used as running examples. Section 3.2 reports some numbers for test accuracy but the uncertainty of these numbers with respect to the test set changes (cross validation) is not reported and the numbers are quite close to each other. Would not this problem be detrimental for SGD optimization affecting the privacy budget?
Reject. rating score: 1. rating score: 3. rating score: 3. <BRK>However, some concerns are raised after checking the draft, and I believe the paper could be improved if some of the questions are addressed:* The current experiment could be an interesting demonstrative part to show how the algorithm works.<BRK>It would be good to situate this work within these related works. There are many typos and the paper is difficult to read at times. I would recommend not focusing on the privacy of human policies. The authors mention that given an expressive enough policy, it should be possible to imitate any policy and thus the worst case experts cannot prevent cloning.<BRK>In its current state the paper is hard to follow and decipher this sequence. The paper was a bit hard to follow and I found the experiments not robust enough to fully characterize the method at this point. it d be nice to have it earlier to set the context of the work.
Reject. rating score: 1. rating score: 3. rating score: 3. <BRK>My concern is that the computational efficiency of the conventional NLP model is well known to NLP researchers. It would be nice if the authors provide a more persuasive explanation for the importance of this research question.<BRK>As it currently stands, the lack of strong baselines and the incremental nature of the contribution lead me to believe that this paper does not represent a sufficient advance to warrant publication.<BRK>2.Lack of proper baselines for comparison. Furthermore, the simple bag of word/TF IDF should be included as baselines as well. The approach is useful in online production systems, and it is eco friendly.
Reject. rating score: 3. rating score: 3. rating score: 6. <BRK>This paper proposes as solution to manage the case where gradients are conflicting in gradient based Multi Task Learning (MTL), pointing to different directions. The paper is well written and easy to follow. My point is to show that the whole paper is based on assumptions that are not verified. I would need to be convinced that we are tackling a real problem, not such an idea of something that may happen but in practice is quite rare or not necessarily that harmful. A conjecture I have on the performance of the approach is that it may dampen the gradient for the loss of the different tasks, and their amplitude, to get to an effect similar to what GradNorm is doing. Comparison with other methods to handle MTL is required in my opinion, in particular with GradNorm, which may have similar effects than the current one.<BRK>The paper proposes a simple rule to ignore the so called conflicting gradients in addressing multi task learning problems. The underlying idea is very straight forward if two gradients are contradictory (the angle between them is > \pi) then one should not be considered. I have some questions here. One imagines that if it can work with more than 2 gradients, maybe a better and more robust algorithm can be achieved. Based on the above, I believe that the paper in its current form has not completely studied the problem and hence I am giving the paper a weak reject score at this stage.<BRK>The paper presents a method to boost multi task learning performance by editing gradient to remove conflicts between tasks. In summary, I think this is a good paper, presenting a straightforward and useful idea for multi task learning. I definitely value the way the method is straightforwardly presented: the underlying idea is simple yet strong. In particular, I do not agree with the statement that "[this] work, in contrast to many of these optimization schemes [incl.Sener and Koltun], suggests that the challenge in multi task learning can be attributed to the problem of gradient trashing, which we address directly by introducing a practical algorithm that de conflicts gradients from different tasks." Sure, the wording is not the same, but the idea is there nonetheless. Second, the experiments make it difficult to see the performances of PCGrad alone. Finally, a more generic comment: the naming of the method should remain the same through the paper.
Reject. rating score: 3. rating score: 3. rating score: 3. <BRK>This paper tackles the multi task setting by using modulation connections between three network pipelines, i.e., one bottom up network to contextualize the problem, one top bottom network that is conditioned on the tasks, on a last bottom up network that solves the task. The key contribution of the paper is to introduce a feature wise and spatial wise tensor to modulate the different neural pipelines better. The paper should be as much self content as possible. I also acknowledge the diversity of tasks that are studied. Later in the text, W is defined over (ch, t), but it is also mentioned that t is an input. In the current paper state, I would not be able to reproduce the experiments. The authors used an auxiliary loss on top of TD to help to visualize the network decision.<BRK>This paper introduces a new light weight framework for multi task learning. In this method, the combination of extracted image features and task are fed into a top down network which is responsible for generating a task specific weight matrix. The weights are next convolved with the input image as an input to the task agnostic bottom up network that generates the labels. * Are LL and RU used in Table 1 defined in the text? I would suggest providing confidence intervals for making such kind of arguments.<BRK>> What is the specific question/problem tackled by the paper? The main contribution is a new architecture for training a task conditional model. Modulation as a technique has been successfully applied in these architectures as well. The assumption that task ID is known is fairly severe. in the introduction, it would be good to define multi task learning with the assumption made clear. why is this not enough?
Accept (Poster). rating score: 8. rating score: 6. rating score: 6. <BRK>The paper has significant value given the extensive experimental results and I believe could provide insight to other researchers in the field. Maybe the authors can correct for this. Summary:The authors provide a framework to incorporate robustness against perturbed dynamics to RL agents. Decision:I am happy with the experiments conducted on this paper and I think they would be a nice contribution to the community.<BRK>The performance shows that the robust MPO outperforms vanilla MPO and MPO + Domain randomization. Open Questions:  I am not convinced by the domain randomization performance as the performance increases only marginally over the vanilla MPO and DR has usually performed quite well for robust policies. Could the authors please include a qualitative evaluation of the learned controllers? Conclusion: Currently, I would rate the paper as weak accept, as the derivation is interesting and the approach seems to work quite well in  simulation. Typo in the Background section  Kullback Liebler  (KL)  (I can try to extend my review if the other reviewers disagree)<BRK>It would be interesting to see if domain randomization still performs poorly when applied in this way. Small: typo in Sec.7.: you say "to lengths of 2.0, 2.1 and 2.2", but according to Table 3, these should be "2.0, 2.2 and 2.3"# AFTER REBUTTALThe authors incorporated the feedback of the reviewers and *significantly* rewrote the paper. The fact that robust works better than non robust is obvious even without any experiments. I give only a few.
Reject. rating score: 3. rating score: 3. rating score: 6. <BRK>This paper properly applied several technique from RNN and graph neural networks to model dynamically evolving, multi relational graph data. The contribution on RNN part is design the loss and parameterizes the tuple of the graph. The paper is well written. Although the two component is carefully designed, the are more like direct application.<BRK>The paper proposes a recurrent and autorgressive architecture to model temporal knowledge graphs and perform multi time step inference in the form of future link prediction.<BRK>The generation of TKG is motivated via a joint distribution problem which is then parametrized by the usage of a recurrent event encoder. In addition, by applying a softmax classifier your model does not seem to be able to scale to large knowledge graphs? However, the current formulation could confuse readers (including me). It would be helpful to interpret and clarify the results in more detail. For example, it is not clear what $|E|$ means (I guess the maximum number of triples in a time step?).
Reject. rating score: 1. rating score: 1. rating score: 3. <BRK>This paper proposes to reshape the weights of the layers of deep neural networks and parametrize them with a low rank matrix decomposition (SVD). Main comments* Applying SVD to the matricized weights of deep neural networks is not new. * The related word needs to be reviewed. Lebedev et al provide a method that works both for end to end training or post hoc, by applying tensor decomposition to the trained weights. * Does the proposed loss have an effect on the selected rank?<BRK>Not very appealing as a contribution. The paper focuses on CNN while these numbers and the figure is for FC. I missed having [1] and other similar approaches in the related work and how the proposed method compares to those directly promoting low rank solutions. How would this translate in practice when the latency is not directly represented by the FLOPS (given the parallelization). the compression is limited as the factorization does not promote sparsity when combining the basis (Sparsity on V).<BRK>This paper propose to compress deep neural network with SVD decomposition, which however has been published 6 years ago in this paper to decompose FC layers:Xue, Jian, Jinyu Li, and Yifan Gong. 2013.For convolutional layers, Tucker decomposition, which is high order SVD, is apparently a better choice. (Kim et al.(2016)) They should at least compare their method with this one. In their deduction of full rank low rank model joint training,  W_r^{(l)}   U_r^{(l)}{U_r^{(l)}}^T W^{(l)} is used without any explanation. A larger dataset would be better. The writing of this paper is not good.
Accept (Poster). rating score: 6. rating score: 6. rating score: 6. <BRK>This paper studies the recent application of attention based Transformer networks for image classification tasks and asks the question as to the similarity of functions learned by these attention networks with the standard convolutional networks. The paper shows that the multihead attention units in the lower layers learn to attend on grid like structures on pixels, similar to a Conv filter. The paper claims that the results can be extended to other forms of positional encodings.<BRK>The paper shows both theoretically and in practice that self attention can learn to act as convolutions. The message of the paper deserves a larger audience and I therefore lean to accept despite some shortcomings. Experiments show improvements over learned relative attention, however, experiments are merely conducted on Cifar.<BRK>The paper claims that 1. multi head self attention(MHSA) is at least as powerful as convolutions by showing that a CONV can be cast as a special case of MHSA and 2. that in practice, MHSA often mimic convolutional layers. Note that the experiments using the learned relative positional encoding have “attention scores (are) computed using only the relative positions of the pixels and not the data” (I’m guessing this means W_qry W_key 0 again). 1.The theory shows an arguably contrived link between self attention and convolution.
Accept (Poster). rating score: 6. rating score: 6. <BRK>This work introduces fixed grouping layers to deep learning models. Unlike convolutional layers, the fixed grouping layer only allows the output to be impacted by the specific inputs associated to it by its group. The paper stays firmly in the area of brain scans with the authors hoping to generalize to other applications later. I did find Figure S2A helpful to quickly understand some of the simulated data approach, maybe it should be in the paper and not supplementary. I would have liked clearer description of other areas that might benefit in the last sentence.<BRK>The paper clearly discusses the proposed layer, introduces the necessary formulation, and demonstrates the effectiveness of this layer in a suite of practical applications in fMRI analysis. What is the sensitivity of FGL to non optimal group proposals for A? Addressing some or all of these in a section or two of the text body would potentially raise my score.
Reject. rating score: 3. rating score: 3. rating score: 6. <BRK>This paper applies metric learning to reduce catastrophic forgetting on neural networks. By improving the expressiveness of the final layer, the authors claim that lower layers do not change weights as much, leading to better results in continual learning. I like the idea that the authors propose and the intuition for why it works, and the paper is well written. However, I have some concerns and questions. My main concern is that experiments are only performed in the two task setting, which is highly restrictive. I do not understand the reason why the output embeddings need to be normalised (Section 3.3)? Going from the RWalk paper, this improves results for the baselines considerably, but this may depend on number of samples etc.<BRK>Furthermore, the authors should highlight all this in the experiment text, e.g.noting EWC does poorly but this is because we use a different protocol than this and this paper etc. I do however still have issues regarding the memory usage of the method, specifically which data needs to be stored from previous tasks. Is there anything specific to continual learning done or is the paper essentially pointing out this existing method (metric learning + nearest neighbor) is surprisingly effective for forgetting. It is  very surprising that LwF does so poorly, do the authors have some explanation for this. (minor) first/2nd paragraph of 3.1 seems a bit redundant making it hard to followOverall I think the idea to consider metric learning and local adaptation for continual learning is interesting, however the current work is currently lacking in both experimental evidence (appropriate comparisons) and clear motivation/difference to existing work  for its particular instantiation of this idea. The clarification about “task agnostic” for the experiments does make them look more relevant than I had previously assessed.<BRK>This paper presents a possible way to mitigate catastrophic forgetting by using a k nearest neighbor (kNN) classifier as the last layer of a neural network as opposed to a SoftMax classifier. I think this an interesting and possibly novel use of a kNN layer (I haven t seen similar uses although I m not that familiar with the specific research area). One weakness of this is paper is that I think there are other baselines that should be compared against in Table 1 such as something as basic as SGD with dropout (some of the baselines that are compared against in Table 1 were compared against SGD with dropout in their citations). There are a number of additional approaches outlined in https://www.cs.uic.edu/~liub/lifelong learning/continual learning.pdf. The lack of some more baselines such as SGD with dropout, and not reporting the performance of the same baselines from Table 1 in Table 2, cause me to be very borderline on this paper.
Accept (Spotlight). rating score: 8. rating score: 8. rating score: 6. <BRK>The paper studies why graph NNs lose the expressive power as additional layers are added. A dynamical system perspective is adopted and used to show that under certain conditions on the weights, the expressiveness of the network deteriorates. A case study of Erdos Renyi graph is provided, showing that for dense graphs information loss indeed occurs. 3) In relation to generalization error, mentioned in the discussion section.<BRK>In the paper, the authors carry out theoretically analysis on the expressive power for GNN. The analysis focused on the limiting case when the depth of layers goes to infinite. The authors prove that if the weights of the GNN satisfy certain condition based on the graph Laplacian, then the transformed features contain only degree and connected component information. The authors study a very important problem. It has long been observed that additional depth does not help GNN. Moreover, the technique used in the paper could potentially benefit other theoretical analysis for NN. The analysis is mostly for dense graphs.<BRK>This paper proposes that the outputs for distinguish nodes exponentially approach signals that only carry topological information of the graph, when the weights in GCN satisfy conditions determined by spectra of the augmented normalized Laplacian, by generalizing the forward propagation of GCN to a specific dynamical system. Overall, this paper could be a considerable theoretical contribution. It also points out a useful technique, weights normalization, in training a GCN. As mentioned in the paper, scaling the weights is a trade off between information loss and generalization error, which is an interesting topic worthing further exploration.
Accept (Poster). rating score: 8. rating score: 8. rating score: 6. <BRK>Is this really necessary, when RGN by itself propagates the signal to neighbors that are further away? Strengths:  This research is very relevant for physics inspired machine learning, since many physical systems are governed by underlying differential equations. Additional comments:  2.1 and 2.2: On first reading, it s a bit confusing why there are two different equations for (∆f)i.<BRK>The authors propose new architectures that simulate difference operators. 3.How sparse the data is? I would suggest improving this research on these aspects: 1. I would encourage releasing the code and the simulated dataset upon acceptance.<BRK>The paper concats the original graph feature and the output of the differential operators, and inputs them to a recurrent GNN to obtain the final prediction. Incorporating modulated derivative and Laplacian operators into physical simulators is novel and well justified.
Reject. rating score: 3. rating score: 3. rating score: 3. rating score: 6. <BRK>This paper introduces a factorization method for learning to share effectively in deep multitask learning (DMTL). The non synthetic experiments in the paper are only performed on tasks that are closely related. Could there be an advantage in in this difference? In particular, the formal definition (i.e., without nonlinear activation between M and W) of Mint appears to be a special case of the more general factorizations in [1]; an experimental  comparison [1] would make the conclusions more convincing, e.g., that universality is important.<BRK>The paper propose a single network approach to multi task learning by adding a task specific linear transformation layer after each fully connected layer. Is it better because there is more overfitting for the competitor s approaches given more parameters? It is better to remove some of those and use the space for more solid results, such as clarifying the notations. In the flow presented by the authors, it is strongly suggested to introduce FiLM in more detail and compare it with the proposed approach more clearly in design, theory and experiments.<BRK>This paper proposes a matrix interleaving (Mint) based on neural networks for multi task learning. In this case, does Theorem 1 still hold? Authors claim that the proposed Mint have the ability to represent both extremes: joint training and independent training. This requirement may not be satisfied in many neural networks.<BRK>In this paper, the authors propose a simple but effective matrix interleaving method (mint) for multi task learning, which aims to represent both joint training and independent training. Though the model resembles FiLM(Perez et al., 2018), it outperforms FiLM by a larger margin in three dataset. It would be better for authors to give more detailed comparisons with models that work on combining both joint training and independent training. I would like to see the paper to be accepted for its simplicity and effectiveness.
Reject. rating score: 1. rating score: 3. rating score: 3. <BRK>The authors compared a bidirectional LSTM to traditional machine learning models for genomic contact prediction. What has the choice of a biLSTM to do with the fact that DNA is double stranded? As pointed out by Maria Anna Rapsomaniki, the paper is similar to previously published papers, which are not cited.<BRK>The authors use a bidirectional LSTM RNN further emphasising that memory of the DNA states contributes to chromatin folding structures. a) There is only one equation in the paper and this is also not given clearly.<BRK>The paper predicts DNA folding using different machine learning methods. The authors show that LSTM out performs other methods. They attribute this to the memory of sequential DNA and LSTM model structure. They also propose a weighted mean squared error that improves the performance of the proposed model.
Accept (Poster). rating score: 6. rating score: 6. rating score: 6. <BRK>The authors propose two modifications to an algorithm from [Rafique et al 2018] for optimizing AUC under a min max formulation, prove bounds for the two modifications, and experimentally compare the modifications against SGD and the original algorithm by varying class ratios of four datasets. The proposal builds on [Rafique et al 2018], so it may be considered incremental. The experimental analysis is fairly minimal, with the proposed modifications performing similarly to the original algorithm from [Rafique et al 2018].<BRK>Summary: The authors propose stochastic algorithms for AUC maximization using a deep neural network. This is absent in the descriptions for experiments.<BRK>This paper proposes two algorithms for the non convex concave AUC maximization problem, along with theoretical analysis. I like to see some experimental results on this setting. The paper provides theoretical analysis on the proposed methods, based on Assumption 1, and inspired by the PL condition.
Reject. rating score: 3. rating score: 6. rating score: 8. <BRK>The authors propose TinyBERT, a smaller version of BERT that is trained with knowledge distillation. Overall, I find the direction of this work exciting and making these large models smaller for practical use is an important research area. But I would expect the authors to be much more detailed in their experimental description and make it clear in the paper that the comparative baselines are fair and well tuned. as well as the finetuning parameters that were cross validated for GLUE? The current paper doesn t seem like it could be reproduced. Are those results on the test set (e.g.evaluated on the official GLUE benchmark), or on the dev set?<BRK>Knowledge distillation of BERTWhat has been done before? What are the main contributions of the paper? Novel Transformer distillation method that is specially designed for knowledge distillation of the Transformer based models. What are the key techniques used to tackle this task? Resulting TinyBERT being 7.5x smaller and 9.4x faster on inference and significantly outperforms other state of the art baselines on BERT distillation with only ∼28% parameters and ∼31% inference time of them. Weaknessesexperimental results were not easily comparable to prior work so it is hard to say if claims are well supported experimental resultsQuestionsDid authors try other values of lambda<BRK>This paper proposes a new knowledge distillation method for BERT models. After Author Rebuttal: authors have addressed all of my concerns quite clearly. Additional experiments which targeted a specific design choice at a time made me much more convinced that the techniques proposed in this paper are useful not only for this particular context but also more broadly applicable. Second, authors propose run knowledge distillation twice, once with the original pre trained BERT model as teacher, and then again with task specific fine tuned BERT as a new teacher.
Reject. rating score: 1. rating score: 1. <BRK>It is implemented by a deep autoencoder with biophysically motivated loss functions. The encoder is the main module which conducts the spike sorting task while the decoder is only used for training the model in an end to end fashion. This paper should be rejected due to the following arguments:	  The paper lacks a section on literature survey, to let the reader know how/where the proposed method fills the gap in the current state of the art. It is unclear what this has to do with forcing to explain different phenomena. Things to improve the paper that did not impact the score:  The method (and the paper) is named deep spike “decoder” (DSD) while in fact the “encoder” part of the learned deep autoencoder actually conducts the spike sorting task. 2, line  2: The word “replicate” is repeated.<BRK>This paper describes a machine learning model for learning spiking representations from EMG (electromyography) like data. The basic model is an autoencoder, where the bottleneck layer is designed to project the waveform inputs into "spike" representations. In the end, what does the deep spiker model try to achieve? The bottleneck seems to be implemented by a linear layer binarized with a Gumbel Softmax activation. Some design choices are quite unclear. A Gumbel Softmax has proven its validity and is a logical fit to the problem setup. Gumbel Softmax is a continuous function and binarization makes sense when sampling, that is when assuming a generative process. However, the rest of the paper seems not to explain a probabilistic or generative model.
Reject. rating score: 3. rating score: 6. rating score: 6. <BRK>The paper proposes a transformer based approach to address a number of problems for machine learning from code. First, there are no good baselines. Right now, I even have reduced confidence that the authors do not have bugs in the tasks or the reported numbers. Five tasks are evaluated, which is impressive. This is one reason I want to see the results published. The problem is also quite important. The experiments that show the effect on reduced labelled data are quite important and interesting   in fact, for many tasks, we can start curating datasets and having model working on small data will be crucial.<BRK>This paper describes a BERT based pre training for source code related task. The authors make a series of ablation studies showing that pre training is indeed useful. Overall, I find this work relevant and interesting, albeit somewhat unsurprising. To make my "weak accept" to a "strong accept" I would like to see experiments on more tasks, preferably more complex tasks. There are a few things that need to be clarified:   * How long is each code snippet? One would expect that the longer the code snippet the harder the task. * What is the proportion of positive/negative examples in each task?<BRK># SummaryThis paper presents a BERT inspired pretraining/finetuning setup for source code tasks. Programs are notnatural language. The argument that source code analysis would "pass on the burden ... to downstream tasks" (Page 3) is odd. This work obtains (and nicely analyzes) impressive results obtained by applying CuBERT. However, this still needs to be shown empirically. I wonder, can the authors experiment with pretraining CuBERT only with theMasked Language Model task? Will it worsen the results substantially or at all? What is the fraction of positive/negative examples in the constructed finetuning datasets?
Reject. rating score: 1. rating score: 6. rating score: 6. <BRK>This paper proposes an approach for improving adversarial imitation learning, by combining it with support estimation based imitation learning. In particular, the paper explores a combination of GAIL (Ho and Ermon, 2016) and RED (Wang et.al., 2019), where the reward for the policy gradient is a product of the rewards obtained from them separately. While the motivation and intuition are clear to me, I have reservations about the claims made in the abstract and the experimental sections:1. SAIL is an effective method for solving the reward bias in AIL. 3.The authors claim that SAIL has better training stability, leading to more robust policies. This doesn’t appear to be the case.<BRK>The key idea of SAIL is to construct a reward function by multiplying reward functions learned by GAIL and RED. I thank the authors for including additional experiments. The authors show that SAIL is at least as fast as than GAIL in terms of the sample complexity. The proposed method is well motivated and performs well on benchmarks. Please address and clarify this point. In the theoretical analysis, the paper assumes a rate of GAIL for support estimation.<BRK>The paper proposes an imitation learning algorithm that combines support estimation with adversarial training. The new reward combines the best of both methods. Like the RED reward, the new reward avoids survival bias and is more stable than the adversarial reward. It would be nice to establish a rough upper bound on the performance of the imitation methods.
Reject. rating score: 1. rating score: 6. rating score: 6. rating score: 6. <BRK>This paper proposes a new gradient descent methods for training deep neural network which can take the adaptive step size with only one hyper parameter to tune   the maximum learning rate   and achieve comparable results to stochastic gradient descent (SGD) on various tasks and models. In order to achieve that, they develop a stochastic extension of the Polyak step size for the non convex setting, namely the adaptive learning rates for interpolation with gradients (ALI G), in which the minimal value of the objective loss is set to 0 due to interpolation in neural networks and the learning rates are clipped by a chosen maximal value. After rebuttal:I noticed that the authors:1. did not compare with SGD or Adam with good hyperparameters (https://arxiv.org/abs/1907.08610)2. did not test on the large scale datasets, e.g., imagenet3. the proposed algorithm is not as good as SGD on most numerical experiments. Another main contribution of the paper is to provide the convergence guarantees for ALI G in the convex setting where the objective loss is Lipschitz continuous (Theorem 1 in the paper). I weakly reject this paper, but given these clarifications in an author response, I would be willing to increase the score. The difference between the two algorithms are when regularization is used. Things to improve the paper that did not impact the score:1.<BRK>Thanks for the responses and my concerns seem to be addressed. This work designs a new optimization SGD algorithm named ALI G for deep neural network with interpolation property. The authors provide the convergence guarantees of ALI G in the stochastic convex setting as well as the experiment results on four tasks.This paper shows state of the art results but I still have have two concerns. My main concern is that the performances of other SGD algorithms may be potentially better than the results showed in section 5 because it is not easy to tune the parameter. Another concern is that the authors only give the convergence rate of ALI G in section 3 but haven’t make any comparisons.<BRK>This paper addresses designing and analyzing an optimization algorithm. This work uses the interpolation property (that the empirical loss can be driven to near zero on all samples simultaneously in a neural network) to compute an adaptive learning rate in closed form at each optimization iteration, and results show that this method produces state of the art results among adaptive methods. Some concerns are listed as follows:1 	Convergence was only discussed in the stochastic convex setting, which seems limiting because we rarely deal with convex problems in problems requiring neural networks. After reading the author response:  I m fine with points 4 and 3. My feelings about 1 are still the same.<BRK>This paper proposes a new adaptive learning rate method which is tailored to the optimization of deep neural networks. A number of proofs for convergence in various convex settings are provided, and empirical evaluation on several benchmarks demonstrates (a) ability to optimize complex architectures, (b) performance improvements over, and (c) performance close to manually tuned SGD learning rates. I vote for accepting this paper. The approach is well motivated, the method is described clearly and detail, and the experiments support the paper s claims well. The results in section 5.2. are all very close to each other, and it would be helpful to have a sense of the variability of the different methods.
Reject. rating score: 3. rating score: 6. rating score: 6. <BRK>The main contribution of this work is to introduce a novel metric between MDPs. Pros of this paper:  The idea of using the Lipschitz continuity of the value seems interesting   The paper seems to solve an important problemCons of this work  I think this paper is not yet ready for publication.<BRK>(This restriction is in addition to the restriction imposed by the assumption of Lipschitz continuity). I think one of the experiments that would have been interesting is what happens when the tasks are not Lipschitz continuous. Summary: The paper presents a method for lifelong reinforcement learning problems. However, I find the paper hard to read and I think that the connection of what paper presents to a real life problem is missing. Is there a way to verify this claim.<BRK>* DecisionThe main contribution of this paper is hard to discern, but the ideas presented are interesting.
Accept (Spotlight). rating score: 8. rating score: 8. rating score: 6. <BRK>The authors use a Transformer neural network, originally architected for the purpose of language translation, to solve nontrivial mathematical equations, specifically integrals, first order differential equations, and second order differential equations.<BRK>In this paper, the authors propose a method for generating two types of symbolic mathematics problems, integration and differential equations, and their solutions. The paper is overall clearly written.<BRK>The paper makes a valuable contribution. I will start with discussing this. I will leave it to the authors to come up with a more informative title, but something like deep learning or transformers for symbolic (1d) integration and simple ODEs with be far more accurate. Sentences such as "This suggest (sic) that some deeper understanding of mathematics has been achieved by the model." I look forward to these being incorporated in the revised pdf.
Reject. rating score: 3. rating score: 3. rating score: 6. <BRK>This manuscript proposed to represent the embedding matrix as a small set of anchor embedding and sparse transformation. This paper is not well motivated at all. The authors propose to find anchor embedding by several methods, such as frequency, clustering, or random sampling.<BRK>This paper proposes a general embedding method, Anchor & Transform (ANT), that learns sparse representations of discrete objects by jointly learning a small set of anchor embeddings and a sparse transformation from anchor objects to all the others. The paper is well written and easy to be followed.<BRK>The paper describes a "layer" that aims at producing embeddings for discrete objects by using fewer parameters than classical embeddings layers. Discussions have to be added to discuss the relevance of the approach since it still needs a large number of parameters at train time, and the role of each component could be studied more in depth. First of all, the paper is well written, and the description is very detailed and understandable. The fact that the method would allow incorporating knowledge is certainly the most interesting point.
Reject. rating score: 1. rating score: 3. rating score: 6. <BRK>This paper connects graph convolutional networks with label propagation. There are a numbder of issues that need to be solved before possible publication. The authors shuold also define clearly y_hat in (5).<BRK>The other limitation is that we have transductive training, but the authors are well aware about this. Why “y” has to be reset at each iteration? It is shown how it is possible to infer the relationship between LPA and GCN in terms of label or feature smoothing (how label/feature does propagate over the neighbors) and label or feature influence over the other nodes. Is it possible to add a graphical explanation or idea of the proof?<BRK>This paper introduces a unified model which combines label propagation algorithm (LPA) and graph convolutional networks (GCNs) for node classification. Overall, the idea of unifying GCNs and LAP in an end to end fashion is interesting.
Accept (Poster). rating score: 6. rating score: 6. rating score: 6. <BRK>The authors start with the valid observation that for controlled systems the threat model of explicitly flipping pixels or otherwise changing the inputs directly is less relevant than the case of identifying bad inputs in the environments (which the authors rename "natural observations") that cause the controller under attack to leave its stable region and end up in bad states for itself, unable to recover sometimes. Verifying controllers against adversarial examples with Bayesian optimization. I think these are timely issues. So, the more satisfying result would be the positive one that shows how to train DRL to be (more) robust under such attacks.<BRK>It will be even better if the input space is image and the output space is discrete. Adversarial Policies: Attacking Deep Reinforcement LearningIn this paper, the authors tackle the problem of adversarial attack by controlling the adversarial policy in reinforcement learning. It is also a very important and natural extension to the paper to consider the malicious attack in cooperative games, which can be equally common in real life. The visualization provides useful insight for us to deepen understanding of the policy attack.<BRK>The authors demonstrate this by showing that the victim has a much higher success rate when the observation is replaced with a ‘normal’ opponent. I am kind of on the borderline, but still lean to accept paper. Strengths  This paper presents some experiments on adversarial learning when the policy of the victim is fixed and black box with interesting findings, demonstrates that the adversary can successfully figure out the weakness of the victim. Thank the authors for the response.
Reject. rating score: 3. rating score: 3. rating score: 6. <BRK>However, PS also deteriorates the performance of learning models on the validation/testing set. This paper first changes the search space from a DAG (micro+marco) in e.g., DARTS to a stacked one based on MBConv; and then, propose to use several tricks to train the super net well. Overall, the paper is too experimental. Finally, why the proposed method can be better than others is not well explained and clarified. Please see the questions below:Q1. Is NAS a method only for ImageNet? So, with so many tricks proposed in the paper, I wish authors can carry on experiments on other data sets as well, e.g., CIFAR and MNIST, which can still be preferred. Q8.Could the authors add an ablation study on this point?<BRK>The authors propose a search for neural architectures with different resource requirements by training a single model only. Furthermore, models found at the end of the search require no additional post processing and are ready for deployment. The work seems to be an incremental extension of Yu & Huang (2019b) and phrased as a NAS algorithm. Besides this, the authors did a very good job. The paper is well written, references are given wherever needed and all the closest related work is covered sufficiently. The experimental part conducts several ablation studies which supports their various decisions.<BRK>This paper presents a method for architecture search in deep neural networks in order to identify scaled down networks that can operate on resource limited hardware. The approach taken in this paper is different from other approaches, which train a single big model then fine tune smaller models for specific hardware, or use distillation to train progressively smaller models. Overall, this was a well written paper, and the results appear convincing. As someone knowledgeable about deep ANNs, but not an expert in NAS for efficiency, I was not very clear on a couple of items.
Reject. rating score: 3. rating score: 3. rating score: 3. <BRK>This work explores hypergraph based SSL of histograms. The idea of treating hyperedges as vertexes of another graph is novel and the theoretical analysis is sound. However, the paper has the following issues:1) The writing is poor, especially about the reference, and the description of how and why DHN works (section 3.3). 3) Though the experiments on Cora and DBLP have revealed the superiority of DHN, the authors still need a more thorough empirical evaluation on some challenging benchmarks to draw the conclusion. I m willing to increase my score if the concerns are addressed.<BRK>This paper proposes a semi supervised graph neural network method for hypergraphs. Another weakness is the paper writing. The algorithmic contribution of this paper is clear: it proposes a new network architecture that (1) initialize latent features H^{(0)}_E for hyperedges from a "hyperedge graph" and (2) learn latent features for each node and hyperedge using a GCN type network. However, I find the design of the hyperedge GNN to be fragile regarding the number of layers from the result in Table 4.<BRK>In this paper, the authors propose a soft semi supervised learning approach on a hypergraph. On the one hand, the vertex labels should be not only numerical or categorical variables but also probability distributions. The appendix offers plenty of details that are helpful for the reader to understand both the theoretical and practical perspectives of this paper. I have some questions and suggestions for the authors:1. Besides, such conversion could be one of the most significant technical novelty in this paper, which makes me worry about the methodology contribution of this submission. But such a summary could complete the paper in a more self contained way. However, in practice, we could not only use the one layer network for graph classification, even for a small graph.
Reject. rating score: 3. rating score: 6. rating score: 6. <BRK>2 final paragraph, some  norm is used for the distortion, but this is not defined (either on pg.2 or in App. Some smaller concerns:  Pg. Two main things of concern:  This area (communications, compression, coding) is highly developed, yet there are no comparisons to practical, state of the art techniques in this paper. Is this comparing apples and oranges?<BRK>This paper focuses on transmitting messages reliably by learning joint coding with the bandwidth limited channel. However, I don t see any difference or major items to justify this kind of benefits. 3.This paper has been verified in both Gaussian channel and bandwidth limited channel.<BRK>The paper provides experimental results on the designed model for bandwidth limited channel. The paper proposed variational model for this problem and it seems to me the paper employs the popular models in neural networks for example VAE, etc. Technically, what s new of this paper?
Reject. rating score: 3. rating score: 3. rating score: 6. <BRK>This paper performs a general analysis of sign based methods for non convex optimization. Using this new norm like function and under an assumption, they prove exponentially variance reduction properties in both directions and small mini batch sizes. It assumes that success probabilities are always large or equal to 1/2. I suggest the authors provide some real learning examples, under which it will satisfy the condition.<BRK>This paper focuses on signSGD with the aim of improving theoretical understanding of the method. The main contribution of the paper is to identify a condition SPB (success probability bounds), which is necessary for convergence of signSGD and study its connections with the other conditions known in the literature for signSGD analysis. Karimireddy et. al.2019 both in theory and practice. However, I have some concerns on the generality of SPB that I will detail below. I would be happy if the authors are more exact about this, such as when signSGD is equivalent to Adam etc. al.2019, I see that these are different cases. al.2019, uses mini batch size 1 under unimodal symmetric noise assumption. So, I would suggest the authors to be more exact on this comparison because it is confusing. al.2019, the authors also identify SPB as it is implied by unimodal symmetric noise assumption.<BRK>The paper presents an improved analysis of the signSGD gradient estimator. The authors also show this condition to be necessary by a small counterexample. The only real conclusion drawn is that larger batch sizes improve convergence. They need to show covergence in a case where the conditions in Berstein (2019) do not hold.
Accept (Poster). rating score: 6. rating score: 6. rating score: 6. <BRK>Quality The paper is self content and well written. Novelty The idea of integrating a family of loss functions with model conditioning has also been proposed by Brault et al.[1], in the context of multi task kernel learning.<BRK>This paper assumes that the loss function consists of multiple weighted terms and proposes the method of finding the optimal neural network for each set of parameters by only training it once. The proposed method consists of two aspects: the conditioning of the neural network and the sampling of the loss functions  weights. My decision is a weak accept.<BRK>Overall, the paper proposes an interesting technique, that surprisingly, can work for a range of hyperparameters, and potentially have a high practical impact. The paper is clearly written. I recommend to reject the paper, however, I will appreciate discussions with authors and other reviewers, and will consider changing my score in case of reasonable argumentation.
Accept (Poster). rating score: 8. rating score: 6. rating score: 6. <BRK>This paper does not really touch on this aspect of the work. The paper also introduces a new dataset, ART, to support training and evaluating models for the introduced tasks. The paper, and the dataset specifically, represent an important contribution to the area of natural language commonsense reasoning. It convincingly demonstrates that the proposed tasks, while highly related to natural language entailment, are not trivially addressed by existing state of the art models. Below are a set of more specific observations about the paper. There appears to be over 5 7% accuracy (absolute) improvements from 10k to 100k examples. However, Table 3 is a bit disappointing in that ~26% of the sampled examples fit into one of the categories. While this is fine to do in the test set, I think if the full set of annotated/generated hypotheses are released, model designers can experiment with combining pairs of hypothesis in different ways.<BRK>This paper proposes a new task/dataset for language based abductive reasoning in narrative texts. The paper contributes a dataset (20,000 commonsense narratives and 200,000 explanatory hypotheses). The paper established many reasonable baselines. The paper is well structured and easy to follow. Cons/comments:  	While this is a new and interesting task, the contribution (as discussed above in “pros” above) is somewhat limited. I also suggest the paper discusses e SNLI a bit more. I can see this helps collect and annotate data, but also limit the form of abductive reasoning and how models should be developed.<BRK>Summary: the paper purposes a dataset of abductive language inference and generation. The paper experiments the popular deep learning models on the dataset and observe shortcoming of deep learning on this task. This dataset seems valuable. 1.The dataset is adversarially filtered using BERT and GPT, which gives deep learning model a huge disadvantage. 2.The human score of 91.4% is based on majority vote, which should be compared with an ensemble of deep learning prediction. On a high level, the main difficulty of abduction is to search in the exponentially large space of hypothesis. Formulating the abduction task as a (binary) classification problem is less interesting. The generative task is a better option. I vote for weak accept.
Reject. rating score: 3. rating score: 3. rating score: 6. <BRK>The techniques used rely on GANs since it can be shown that finding the best (random) algorithm and the worst (deterministic) instance is equivalent to finding the worst random instance against the best deterministic algorithm. This is the major disappointment in the paper.<BRK>We only know that the proposed approach finds near optimal solutions with a difference of 0.01 competitive ratio. The paper mentions the MSVV algorithm twice but no reference or explanation is provided. However, hyperparameter searching is not extending any ML technique, it is just an approach to find a good training configuration and show robustness in different hyperparameters settings.<BRK>These two networks are trained jointly from scratch and the underlying algorithm for the training is provided. The only input required is the way to calculate the objective function of the actual problem. However, they state they loosely follow this approach.
Reject. rating score: 3. rating score: 3. rating score: 8. <BRK>Summary of the paper:This paper proposes to meta learn a parametric divergence measure for variational inference (VI). The paper is now greatly improved. The meta learned divergence is applied to approximate a mixture of two Gaussians, regress sinusoid function with Bayesian neural networks, and learn recommender systems with partial VAE. Summary of my opinion:I am leaning towards reject this paper because 1) I m not convinced by their motivation   "this line of work has also shown that the optimal divergence can vary depending on tasks". For unsupervised learning, people use GAN or VQ VAE to obtain realistic samples, but which is not attributed to better divergence measures. Another unsatisfactory point of this paper in my opinion is that it only demonstrated results on toy datasets and MNIST. Although the proposed method is valid for alpha divergence and f divergence, I suggest the authors to find a real application which demonstrates the merit of the idea in practice. What you need to do is to prepare a validation set and select the best hyper parameters based on the performance there. 3.Do you have a validation set for the Bayesian optimization (BO) baseline or do you use cross validation? The details of this baseline should be elaborated. In fact, it is possible to choose the best alpha for each task using BO.<BRK>This paper proposes to use a meta learning approach to learn the divergence used in variational inference and initial variational parameters. For the regression and recommendation system examples, it compares with a VB baseline and a p VAE baseline and shows better results. As for the significance of the paper, I have the following issues:1. For instance, the first two examples are synthesized by hand. 2.To apply the proposed method, we need to have some knowledge about our preference in the evaluation and encode that knowledge into the meta loss. Based on these issues, I think the contribution of the paper is not sufficient and I tend to reject the paper. Also, note that the paper length extends 8 pages. The motivation should be strengthened and the experiments should be more precise to improve the paper.<BRK>SUMMARY OF THE PAPER:This paper proposes a way to automatically select a divergence to use in variational inference (VI) given a set of datasets (tasks). The proposed algorithm works as follows: do a few gradient descent steps on the variational parameters given a fixed divergence parameter, then update the divergence parameter by taking the gradient with respect to a meta loss which is a task specific measure of goodness of the variational distribution (like test log marginal likelihood). The second proposed algorithm does the same, but also learns a good initialization for the variational parameters. STRUCTURE:The paper is well written and easy to understand. NOVELTY:As far as I know, learning a divergence for VI using meta learning is new. The related work is discussed well in section 5. In the simpler experiments, careful ablation studies are done. The results generally show that the proposed method is preferable to alternatives.
Accept (Talk). rating score: 8. rating score: 8. rating score: 8. <BRK>The authors provide the definition of a (L_0, L_1) smooth function. The motivation for using (L_0, L_1) smoothness and gradient clipping is alsoshown to be relevant in practical experiments with language and imageclassification models. I recommend to accept the paper.<BRK>The paper is intuitive and well written. Could you provide a toy example, for example x^3, to show the advantage of the proposed assumption and the convergence under the assumption?<BRK>The authors also generalize their results to SGD. I think it’s not the f^* in Assumption 1? If f^* is the stationary point the algorithm find, then the Assumption 1 is a little weird. It seems that in the proof of Lemma 9, we need the Assumption 3 holds for the set \|x^+ x\| \leq min{1/L_1, 1}. I find the whole paper interesting and match the intuition from the practice. However, I think it can be polished to make the whole idea more clear and more easy to accept by the potential theorists and practitioner audience.
Reject. rating score: 3. rating score: 3. rating score: 6. <BRK>The paper proposes an extension to TRPO that makes use of hindsight experience replay. I believe this can have a great effect on the bias and variance of the proposed method.<BRK>Experimental results are demonstrated for few sparse reward benchmarks, comparing to the standard HPG, TRPO and several variants of the proposed HTRPO approach with weighted IS and with the KL constraint. The key contribution of the paper is based on deriving an on policy adaptation of hindsight based TRPO, that can be useful for sparse reward environments. The authors mention about the state occupancy measure instead of considering the state distribution for the KL term. What was the significance of mentioning it, or why should it be considered even?<BRK>to the TRPO setting. Experimental results show that the proposed changes bring significant improvements over baselines on sparse reward settings. Strengths:+ The paper appears well formulated, and well motivated+ The experimental results appear quite strong. + Description of the experimental details is quite clear.
Accept (Poster). rating score: 6. rating score: 6. rating score: 6. <BRK>This paper is focused on identifying/discovering feature interactions in blackbox models (with a focus on recommendations). The empirical section is quite interesting, with some nice mix of qualitative and quantitative results. Overall the paper makes for an interesting read. I would have wanted to see some deeper discussion on this topic. This would be good to elaborate on to make sure that the models were all compared fairly from a computational perspective.<BRK>This paper proposed a model for extracting global feature interactions from the source model which was later being encoded in the target model to enhance its prediction performance. 2.The descriptions on the feature dimensions of the original data and the generated binary representation data x’ are rather confusing and inconsistent throughout the paper. The theoretical innovation of this paper is trivial.<BRK>The paper proposes a method to detect which features in the input of recommender systems are interacted each other, i.e., combining them behaves useful information, and examines to feed extracted interactions directly into the recommender systems to measure effects on actual recommendation. As long as I heard how the NID work to detect interaction from this paper, it is sensitive not only the true interaction between features but also set of features holding similar signals (e.g., if a hidden unit behaves as a feature A, it may be natural to aggregate all features that implies A by itself, regardless of the meaning of their interaction). This is not a desired case as long as the paper specified the interactions in section 3.
Accept (Poster). rating score: 6. rating score: 6. rating score: 3. <BRK>* Summary:The paper presents a relational state space model that simulates the joint state transitions of correlated objects which are hierarchically coordinated in a graph structure. Graph normalizing flow is further incorporated into the framework to make the joint state transition density more expressive. The proposed R SSM shows performance gains over state of the arts in three benchmarks. * Comments:The paper is generally well written and technically sound. It is also helpful that the authors included preliminaries of the literature. 4.The model keeps track of a global state z^g, but it is not analyzed in experiments. Please clarify what is X_{t h} in Table 3?<BRK>The experimental results provide evidence that 1) the model can flexibly incorporate relation information and some evidence that 2) the model outperforms some multi object time series forecasting approaches from the literature. From my understanding, the main claims are that the model is a flexible way to incorporate relational information, and that the resulting model outperforms other multi object time series forecasting approaches. Essentially, the paper is not very self contained. More motivation for the comparison is needed. The model appears to be restricted to lower dimensional settings. Does it advance the state of the art in a demonstrable way? The paper (PRECOG, arXiv:1905.01296) appears related as it also constructs a latent variable graphical model of stochastically transitioning multiple interacting objects with normalizing flows. >Quality:Is the submission technically sound?<BRK>This manuscript proposes a novel approach to dynamic modeling of time series data based on a state space model that incorporates a Graph Neural Network to model the relationship between the dynamics of different objects. The (complex) architecture is described in details and to some extent justified before its predictive performance is demonstrated on several simulated and real datasets. Overall, while the authors provide evidence of a better predictive performance with respect to several baselines, I am left a bit unconvinced by the study for the following reasons:1. For example, the ensemble Kalman filter has proved accurate for weather forecasting. 2.Lack of interpretability: given the proposed approach relies on an intuitive representation of the model as representing the dynamics of several object tied by an interaction graph, the purely predictive results are not really matching the expectation of the reader to “see” how well these interactions are captured by the model. Can we check in some way that the latent graphical model is learnt properly, even in a toy dataset?
Accept (Poster). rating score: 6. rating score: 6. rating score: 6. <BRK>This paper builds on recent works giving generalization bounds on RNNs. The primary contribution is that they are able to bound the generalization error for RNNs without a dependence on the network size parameters d and m.Their proof is roughly as follows:1. 2.The linear network term can be bounded directly with their Fisher Rao norm. For the simpler terms, their Rademacher complexity can be bounded independently using matrix 1 norm. Their second contribution attempts to address these downsides. They prove another generalization bound for RNNs when training with random noise, which has the effect of increasing the term containing the smallest eigenvalue of the input covariance matrix. They remark on several empirical phenomena that are consistent with their results:  Correlation of features in the input data makes it harder for RNNs to generalize  Weight decay could help by decreasing the relevant gradient terms in their bounds  Gradient clipping could help when the smallest eigenvalue of the input covariance is very smallTheir third contribution is a single experiment.<BRK>In this paper, the authors investigate the topic of theoretical generalizability in recurrent networks. Specifically, they extend a generalization bound using matrix 1 norm and the Fisher Rao norm, proving the effectiveness of adding noise to input data for generalizability. These bounds have no dependence on the size of networks being investigated. The authors claim this is a representation that can be extended to other neural network architectures such as convolutional networks.<BRK>This paper proposes a new generalization bound for vanilla RNN with ReLU activation in terms of matrix 1 norm and Fisher Rao norm. I am actually not familiar with the generalization theorem on RNN. Nevertheless, according to the demonstration of the authors, I can understand the results of the theorems in this paper. However, I still have some concerns as follows. Thus, I understand why the authors claim that the bound they provide has no “explicit” dependence on the size of networks. What is the formulation of the high frequency components of the input signal?
Accept (Spotlight). rating score: 8. rating score: 8. rating score: 8. <BRK>This paper alleviates those constraints and thus brings important algorithm classes to their fold. They train all possible architectures (15625) in this search space on cifar10/100/Imagenet 16 120 (a reduced version of ImageNet with 120 classes). I think the answer is yes, because of the  zeroise  operation but wanted to get the authors  answer.<BRK>I revise my rating to Accept. Summary:This paper proposes another benchmark dataset for neural architecture search. 1.With proper adaptation, both NASBench 101 and the one in this paper are "algorithm agnostic".<BRK>I m not sure "fairness" as in the abstract is the exact core problem; I would call this comparability. This paper describes a publicly available benchmark on which most recent types of NAS algorithms can be evaluated. It is clearly a limitation that hyperparameter search is infeasible to conduct in parallel with architecture search, as pointed out sec 6.
Reject. rating score: 1. rating score: 3. rating score: 6. rating score: 6. <BRK>However, I have some issues with the submission. I believe that the manuscript has a limited impact. This should have been done in a proper cross validation. This paper proposes Amortization Hashing (AHash) which reduces the occurrence of empty bins and thus leads to better similarity estimates.<BRK>The paper proposes a bin split strategy for one permutation hashing which reduces the expected number of empty bins. The theory (and its correctness) is difficult to be justified.<BRK>This paper proposes a new hashing method based on MinHash. It finally reassigns nearby hash values to empty bins. Finally the paper did some experiments on several applications including SVM and nearest neighbor search. I have a few questions. However, I found it is hard to follow.<BRK>Specifically, in AHash, two steps, called Insertion and Amortization, are designed to balance the load in order to reduce the empty bins. The experimental results show the effectiveness of the proposed hashing method. 1.This paper is well motivated and structured, which may serve as a guidance for other works. 3.It seems that the effectiveness of the proposed method is OK but not surprising. Overall, the proposed AHash in this work is interesting and seems effective when dealing with the problem of the unbalanced load. However, there still exist quite a few issues in the current version, which need more explanations and clarifications.
Reject. rating score: 3. rating score: 3. rating score: 3. <BRK>[Original reviews]This paper proposed an unsupervised generative adversarial network for underwater generating realistic underwater images and haze removal, which can simultaneously deal with the color restoration and haze in the realistic underwater environment. However, many existing works used the physical model to represent the imaging principles and using deep network to learn prior knowledge. Thus, I think the proposed idea is a little bit incremental. For the experimental part, the experimental results fully demonstrate the effectiveness of the proposed method in comparison with state of the art methods. Also, I suggest the authors demonstrate the proposed method on not only low level, but also high level vision tasks, e.g., underwater image target detection.<BRK>In this article, the authors propose a generative adversarial network named UWGAN to generate realistic underwater images from the pairs of in air images and depth images. The authors should consult professional proofreading services. As a courtesy towards referees, the quality of writing needs meticulous attention before a scientific paper should be submitted. Moreover, the statement of section 2.2 is not clear. 6.High resolution figures should be given in the manuscript.<BRK>This paper uses U net for underwater image restoration and enhancement. But, it is difficult to obtain realistic underwater images, thus this paper introduces a GAN based method to generate realistic underwater images from in air image and depth map pairs. To evaluate the effectiveness of the proposed method, the figures are important, thus, it would be better to make them clear. The technical contribution of the proposed method is not clear. The proposed method seems to be just using the existing techniques.
Reject. rating score: 3. rating score: 3. rating score: 6. <BRK>4.The paper provided results from multiple molecular optimization tasks. Can the method be used for other types of graph generation? 3.The main claim : " ... our graph decoder is fully autoregressive.."  why is this a merit?<BRK>The set of attaching atoms is limited to 2 right? Sec 3.1 Substructure Tree: Since tree decompositions are not unique, does this work use the different tree decompositions and DFS traversals as data augmentations?<BRK>There are some typos in the paper. Why do you use depth first order not any other orders? Overall, this is a good paper considering its technical contribution and writing.
Accept (Poster). rating score: 8. rating score: 6. rating score: 6. <BRK>SummaryThis work provides a novel model based reinforcement learning algorithm for continuous domains (Mujoco) dubbed POPLIN. QualityThe quality of the paper is high. This is an experimental study and the number of benchmarks and baselines is far above average compared to other papers in that field. There are some plots that do not mention the name of the environment, e.g.in Figure (4), but also some in the appendix. SignificanceThe experiments and the empirical results make the paper quite significant.<BRK>This paper presents POPLIN, a novel model based reinforcement learning algorithm, which trains a policy network to improve model prediction control. The paper studies extensively how to utilize the policy, by planning in action space or planning in parameter space and how to train the policy, by behavioral cloning, by GAN or by averaging the results of CEM. Overall, the paper is well written and the method is novel. It s really interesting to see that CEM works for such a high dimensional space.<BRK>ContributionsThis paper proposes a MBRL algorithm for continuous action space domains that relies on a policy network to propose an initial trajectory, which is then refined, either in the action space or in the policy space, using standard CEM. 3 options are evaluated to update the policy network: a GAN approach, a regression approach (behaviour cloning) and a direct modifications of the policy based on the perturbation found by CEM. Overall, the SOTA performance in MuJoCo is held by MFRL methods, but typically require more training (up to 1M) ReviewThe presentation of the method is thorough, as well as the motivations and the experiments.
Reject. rating score: 3. rating score: 3. rating score: 6. <BRK>The authors investigate the role of entropy maximization in SAC and show that entropy regularization does not do what is usually thought: in the examples they investigate, where the output of the policy network needs to be squashed to fit in the action space domain, squashing would result in having only action at the boundaries, but entropy regularization maintains some intermediate values, hence exploration. From this insight, the authors replace entropy regularization by a simpler normalization process and show equivalent performance with their simpler Streamlined Off Policy (SOP) algorithm. Then they introduce a second "Emphasizing Recent Experience" mechanism and show that SOP+ERE performs better than SAC. This should be explained properly. If this is possible, studying the performance of SAC + ERE would disambiguate the relative contribution of both mechanisms.<BRK>The main reason behind the success of SAC would thus be its ability to keep exploring throughout training, by avoiding saturation. A second contribution is a new experience replay sampling scheme, named Emphasizing Recent Experience (ERE), based on the idea that most recently added transitions should be given higher weights when sampling mini batches from the replay bufffer. Combining both ideas leads to the SOP (Streamlined Off Policy)+ERE algorithm, which is shown to consistently outperform SAC on Mujoco tasks. Although this paper presents interesting insights and very good empirical results, I am currently leaning towards rejection mostly due to missing some important empirical comparisons, which hopefully can be added in a revised version. •	Below eq.1: « the optimal policy is deterministic »  > should be replaced with « there exists an optimal policy that is deterministic »•	« principle contribution »  > principal•	The normalization scheme does not appear in Alg.<BRK>The authors notice that SAC alleviates this problem by means of entropy regularization. Surprisingly, with this improvement, even several parts from TD3 can be removed, such as delayed policy updates and the target policy parameters. The proposed Streamlined Off Policy (SOP) algorithm is thoroughly evaluated, ablation studies performed, code made available. # Suggestions1) It is said that entropy regularization leads to action not being saturated in SAC. 2) The Emphasizing Recent Experience (ERE) replay scheme seems reminiscent of sampling according to a distribution exponentially decaying into the past. Therefore, the authors should tone down the claims of outperforming SAC. Nevertheless, I still find the contribution of the paper valuable and think that it should be accepted, albeit with the aforementioned modifications in the camera ready version.
Accept (Poster). rating score: 6. rating score: 6. rating score: 6. <BRK>This paper tackles the problem of learning with noisy labels and proposes a novel cost function that integrates the idea of curriculum learning with the robustness of 0/1 loss.<BRK>I like the idea of the paper. This paper outperformed [7] and I think it is clearer and better to build a story along this line.<BRK>Summary:To handle noisy labels, this paper proposed a curriculum loss that corresponds to the upper bound of 0 1 loss. However, experiments are a little weak due to weak baselines and experimental setups (see suggestions for more details).
Reject. rating score: 1. rating score: 1. rating score: 1. <BRK>The paper describes a method for word normalization of Amharic text using a word classification system followed by a character based GRU attentive encoder decoder model. The paper is very short and lacks many important details, such as where the data is collected from, how it is processed and split into training and evaluation sets, and how the initial token classification is performed. The paper also doesn t adhere to the conference paper template, which is grounds for desk rejection.<BRK>Model: Bidirectional GRU with the size of 250 hidden units both are used for encoding and decoding layers. This paper is not ready to publish. Please consider to complete the project, polish the writing, and submit to a different venue.<BRK>However, this paper does not use the official template and the content is too short to be a conference paper. I suggested resubmitting to another (NLP) conference after extending the content with detailed description for the model and the method, and conducting more experiments on public acceptable benchmarks.
Accept (Poster). rating score: 8. rating score: 6. rating score: 6. <BRK>This paper proposes to use a semi supervised VAE based text to speech (TTS) for expressive speech synthesis. with a lot of audio examples on the demo page. The paper uses the in house data to perform their experiments and the code does not seem to be publically available.<BRK>Overview:This paper proposes to use semi supervised learning to enforce interpretability on latent variables corresponding to properties like affect and speaking rate for text to speech synthesis. Nevertheless, I think a model that uses only the small amount of labelled data (i.e.without semi supervised learning incorporating unlabelled data) should also be considered.<BRK>The authors propose to do neural text to speech, conditioned on attributes such as valence and arousal and speech rate. It seems that what the authors effectively do is condition on discrete emotion classes, not valence and arousal, which are continuous measures of affect. The idea is interesting and new. Will the data be released?
Reject. rating score: 1. rating score: 1. rating score: 3. <BRK>This paper presents a VAE architecture that separates a fixed content representation and time varying features of an image, that can be used to learn a representation of image transformations in a temporal setting. The paper is well written and the ideas presented are interesting, but in my opinion not novel enough or thoroughly demonstrated to justify acceptance:  there is a very relevant work that is not mentioned by the authors and that can be seen as a generalization of the model presented in this paper: "Disentangled Sequential Autoencoder" by Li and Mandt (ICML 2018), which introduces a model that is also disentangling a content and a temporal representation of sequential data. This is basically the more general model introduced by the authors of this submission in the beginning of section 2, without all the assumptions made in the rest of section 2.<BRK>This paper presents a VAE model. The authors consider time series data and claim that in this situation it is better to model the transformations in the latents instead of each datum independently. The setup is reasonable and seems novel, but since it stops at image registration, which is a well known existing model I cannot qualify the paper as novel. * The conditions on \dot{z} are interesting and potentially useful and they should be explored in experiments. Ideally in a setup where the data is not trivial. Minor issues:* the authors should number all equations.<BRK>Review of “Unsupervised Learning of time varying features”This work looks at using a conditional VAE based approach to model transformations of sequential data (transformations can be spatial, such as rotation of MNIST, or temporal, i.e.screenshots of a car racing game). Like how our own visual system encodes differences in time and space [1], they show that in generative modelling with VAEs, “there is an advantage of only learning features describing change of state between images, over learning the states of the images at each frame.”Such an encoding allows the model to ignore features that are constant over time, and also makes it easier to represent data in a rotating curved manifold. For MNIST, they compare the latent space learned from transformations (z_dot) and show that this approach can encode image geometric transformations quite well. While this paper is interesting and highlights advantages of modeling transformations of sequential data, I don t think the contributions are currently sufficient for ICLR conference (right now it is a good workshop paper IMHO).
Accept (Poster). rating score: 8. rating score: 6. rating score: 3. <BRK>The method is intuitive and a good parallel to Adversarial projections is made in the paper. Also, based on the inpainting results in Fig 7, it s not really clear if the method generates better results than Ivanov et al.<BRK>This paper discusses an important problem of solving the visual inspection problem limited supervision. It proposes to use VAE to model the anomaly detection. The major concern is how the quality of f_{VAE} is estimated. From the paper it seems f_{VAE} is not updated. Will it be sufficient to rely a fixed f_{VAE} and blindly trust its quality?<BRK>Summary: The paper proposes to use autoencoder for anomaly localization. The results for image inpainting looks promising. The proposed method is evaluated using the anomaly localization dataset (Bergmann et al.CVPR 2019) and qualitatively for the task of image inpainting task on CelebA dataset. The proposed approach leads to better performance over the baseline models; it is not clear what is a suitable baseline model for the problem of anomaly localization is?
Accept (Talk). rating score: 8. rating score: 8. rating score: 6. <BRK>This paper presents convergence rates for straggler aware averaged SGD for non identically but independent distributed data. The paper is well written and motivated with good discussions of the algorithm and the related works. The proof techniques involve bounding how much worse can the algorithm do because of non identical distribution and introduction of stragglers into the standard analysis of SGD like algorithms. The empirical evaluation is adequate and well presented.<BRK>This paper analyzes the convergence of FedAvg, the most popular algorithm for federated learning. Thanks to the authors for their response. It is also shown that with constant learning rate eta, the solution found can be necessarily Omega(eta) away from the optimum (for a specific problem instance), thus justifying the decaying learning rate used in the positive result.<BRK>Can convergence be proved? The authors also provide empirical results supporting their theoretical analysis. In contrast, this paper studies FedAvg on the non iid data and inactive devices setting and shows that, with adequately chosen aggregation schemes and decaying learning rate, FedAvg on strongly convex and smooth functions converges with a rate of O(1/T). Overall, I enjoyed reading this paper and I would like to recommend acceptance.
Accept (Spotlight). rating score: 8. rating score: 8. rating score: 6. <BRK>Summary:The paper generalizes regret analysis results from convex online learning tofunctions that are not Lipschitz but Riemann Lipschitz continuous. The authors also transfer the results from the convex online setting to theconvex stochastic setting and the nonconvex setting. The Riemannian distance $dist(x, x )$ on $X$ that follows from this is definedas integrating over the infinitesimal distances on a curve connecting$x$ and $x $ as measured by the norm defined above. The analysis to me seems both rigorous and useful in practice(at least with regard to the formal definitions of Riemannian metrics andRiemannian Lipschitz condition for singular functions).<BRK>The paper establishes optimal regret bounds of the order O(\sqrt{T}) for Follow The Regularised Leader (FTRL) and Online Mirror Descent (OMD) for convex loss functions and potentials (a.k.a.Riemannian regularizers) that are, respectively, Lipschitz continuous and strongly convex with respect to a given Riemannian metric. The authors include numerical experiments involving a Poisson inverse problem. The paper is well written, with a very clean narrative highlighting the main ideas and results. To the best of my knowledge, the literature review is complete and rightly highlights the fact that most results in the literature on Riemannian mirror descent methods have so far primarily addressed offline deterministic problems with exact oracle gradients. Can the authors be more explicit about the use of this inequality, and what makes the Riemann generalization difficult in general?<BRK>This paper investigates online and stochastic convex optimization problems in which the objective function is not Lipschitz continuous. Specifically, the standard condition of Lipschitz continuity is replaced with a more general condition involving Riemannian distances and called Riemann Lipschitz Continuity (RLC). Based on an appropriate definition of Riemannian regularizer and a generalization of Fenchel coupling to Riemannian geometry, the authors provide $O(\sqrt T)$ regret (resp.risk) bounds for the online (resp.stochastic) mirror descent algorithm, under the Riemann Lipchitz condition. The performance of the algorithm is validated on Poisson inverse problems. Overall, this is a dense, yet interesting, paper.
Reject. rating score: 3. rating score: 6. rating score: 6. <BRK>Overview: This paper presents a new generative modeling approach to transform between data distributions via a technique the authors dub “neuron editing”. Section 2: Three desirable properties for a transformation are listed, but what makes these properties desirable? The idea is interesting, though one wonders if there isn’t any other work where the neurons have been “edited” to accommodate different transformations, given that the idea is itself rather intuitive. A more thorough literature review in that regard would be helpful. The CelebA experiments help lay establish an intuitive understanding of the proposed technique and were helpful.<BRK>This paper tackles the problem of mismatched training/test data by directly modifying the latent representation learned with an auto encoder. The proposed method employs a piece wise linear function to transform the representation of samples from a source distribution to that of samples from the corresponding target distribution. The paper argues that the same transformation can be applied to samples from a distribution that are different from the source/target distributions. The method is empirically justified by three different tasks in computer vision and biology.<BRK>The method relies on an autoencoder, instead of a GAN, to which neuron editing is applied. This leads to better reconstructions than GAN approaches as judged by some visualizations and results. The idea of applying neuron editing to an autoencoder is pretty interesting. The motivation of applying the method to medical data to correct for instrument variability is also very interesting. I think the datasets should be explained better, and examples of the images should be given. Also all of the tables in the paper with classification tasks should have sections in the appendix to explain everything about those tasks. Smaller points:In section 2, source, target and extrapolation distributions are not introduced properly.
Reject. rating score: 3. rating score: 3. rating score: 6. <BRK>This paper presents a bilingual generative model for sentence embedding based variational probabilistic framework. Experimental results show that the proposed model is able to produce sentence embeddings that reach higher correlation scores with human judgments on Semantic Textual Similarity tasks than previous models such as BERT. Strength: 1) the idea of separating common semantics and language specific semantics in the latent space is pretty neat; 2) the writing is very clear and easy to follow; 3) the authors explore four approaches to use the latent vectors and four approaches to merge semantic vectors, makes the final choices reasonable. The authors compare their model with many state of the art models that could produce sentence embeddings. Also, the proposed deep variational model is close to an auto encoder framework. You can also train a bilingual encoder decoder transformer model (perhaps with pre trained BERT parameters) with auto encoder objective using the same parallel data set. However, I expected to see more analysis or experimental results to show why it is better than a monolingual variational sentence embedding framework.<BRK>The paper presents a model that, given parallel bilingual data, separates the common semantics from the language specific semantics on a sentence level. Instead the empirical analysis focuses on sentence length, punctuation and semantics. The analysis of all these three aspects is superficial: for sentence length, it consists of  computing the sentence mean and median; for punctuation,  it consists of masking punctuation; and the last part just computes vectors of nouns only (and states that this is "semantics"). But there is no analysis per se. The extent to which these gains are due to tuning as opposed to the inherent design of the model is not clear.<BRK>This paper addresses the problem of constructing a sentence embedding using a generative transformer model which encodes semantic aspects and language specific aspect separately. The model shows promising results on the first task, but weaker results on the second task, especially when compared against pretty naively built sentence embedding from BERT model. One of my question is, why not test this method in more popular benchmark such as MNLI or other classification tasks? MNLI evaluates how each sentence pair relates to one another, thus would be a good benchmark for sentence embeddings as well. I recommend discussing and clarifying these points. Is it just cosine similarity of two sentence embedding vectors? If I understand correctly, this model is constrained by the amount of bitext — some analysis on this would be interesting. I think this makes the baseline significantly weaker?
Reject. rating score: 6. rating score: 6. rating score: 6. <BRK>The paper makes a significant attempt at solving one of the practical problems in machine learning   learning from many noisy and limited number of clean labels. To be honest, the theoretical contribution of the paper is limited.<BRK>This paper studies the problem of learning from multiple tasks and additional noisy data. How to deal with this issue? The studied problem that learning from few shot data and large scale noisy data is interesting.<BRK>This paper presents a classification method when the data consists of few clean labels and many noisy labels. The authors propose to construct a graph structure within each class and use graph convolutional network to determine the clean/noisy labels of samples in each class. The authors compare with several related methods and show the proposed method has better performance in few shot learning experiments. If there is correlation between different classes, how could the model use such class wise correlation to clean the label?
Reject. rating score: 1. rating score: 1. rating score: 3. <BRK>This paper proposes a hybrid technique for rendering “control variate” and class conditional image in two steps, first by generating an approximate rendering of the image (“Y”) conditional on the control variate and then filling in the details with a conditional GAN dependent on a latent noise variable Z (although I note that the caption of Figure 2 which identifies “Z” as the identity makes this rather confusing). Overall: this paper makes a convincing case that it can be used to generate higher quality images, but not that this improves the quality of the disentangled representations. In the “supervised” setting where the control variates are observed, Y can be learned as a simple regression problem independent of the other parts of the model, and this two stage refinement process is demonstrated (using inception scores) to generate convincing samples, including when C consists of up to 10 control variates. In the unsupervised setting, a beta VAE is used to learn a disentangled representation of X as a proxy for C, but then the data is regenerated using a two step process.<BRK>Summary:The paper proposes the use of a hierarchical model for a generative modeling task. The paper report extensive experimental results to validate the proposed hierarchical model. On the unsupervised disentangled feature learning, the framework provides incremental advancement by using beta VAE in conjunction with GAN to use the best of both the worlds. Even though the proposed approach is similar to StackGAN, the experiments and the results mentioned in the paper are noteworthy. 2.Unsupervised control variable discovery:This part is just the use of existing disentanglement VAEs to extract the control variables. Apart from combining these to existing ideas, what can be considered as an added novelty to improve the quality of the disentangled features?<BRK>This paper proposes a method for learning disentangled representations. The approach is used on both supervised (where the factors to be disentangled are known) and unsupervised settings. However, I found that the contribution of this paper is fairly small. The setup where labeled data (c) also seems a bit unnatural (this also seems to be confirmed by the fact that the authors had to build datasets for the problem). The unsupervised results are more interesting but not very much explored (a single set of sampled faces).
Reject. rating score: 3. rating score: 6. rating score: 6. <BRK>In this paper, authors propose to design an unsupervised learning framework, which can capture pose representation by reconstructing images or videos. The paper is not written well. The novelty is also relatively limited, since a large part of this work has been done in Lorenz et al.(2019).Moreover, the only supervision is the reconstruction loss. I am just curious how the neural network can learn the semantics representation (such as foreground and background), without any guidance in the corresponding modules?<BRK>The paper presents an unsupervised approach for learning landmarks in images or videos with single objects by separating the representation of the image into foreground and background and factorizing the representation of the foreground into pose and appearance. It builds upon previous work [Jakab 2018, Lorenz 2019] who proposed to train by reconstructing the original image from one version of the image with perturbed appearance and another with perturbed pose. It seems a bit odd that the foreground is so focused on the central part of the face. It extends this approach by introducing an additional separation of foreground and background in the image.<BRK>The paper presents an unsupervised method to get disentanglement of pose, appearance, background from both images domain and video domain. 1. what are the details of the color jitter process? how do you know it is foreground and only colorizing this part? 2. why the video prediction only on KTH. H36M is also a video based dataset. as for 2d pose estimation, even we give them unimodal gt, sometimes the prediction is bimodal.
Reject. rating score: 3. rating score: 6. rating score: 8. <BRK>Visual reasoning with multi hop feature modulation. The paper is pretty easy to follow, and the intuition is clearly explained. Remarks:   The bibliography work is incomplete.<BRK>All in all, I weakly recommend to accept the paper. All in all, I believe the ideas and results of this paper are promising but insufficient for publication in its current form. The authors put significant effort into reproducing other GNN baselines.<BRK>Exploring such approaches could make it easier for future work to use FiLM with GNNs. The empirical evaluation seems thorough. If so, it would be interesting to hear these works discussed (in the rebuttal and paper) and the exact connection described.
Reject. rating score: 3. rating score: 3. rating score: 8. <BRK>This paper studies a model compression algorithm, where training consists in alternatingly updating a full precision weight vector using SGD for a number of steps N, and then compressing the weight vector to a quantized or low rank representation. The authors present an empirical study of how the properties of this algorithm change as they change the number N.The contribution is marginal, and the paper is hard to read. Periodically compressing networks weights, rather than at every iteration, is not new. The experiments are small scale and not exhaustive and there is very little comparison to previous work and alternative methods. Questions/comments for the authors:  When you say "regularization" you seem to usually mean "compression"?<BRK>This work investigates compression aware training and introduces a new hyper parameter called "Non regularization period". Basically, the paper proposes to apply weight regularization and compression less frequently so that they can use stronger regularization/compression, hence achieving high compression ratio and model accuracy. Overall, the idea of asynchronous regularization and compression is interesting and worth further investigation. However, I vote for a reject for now because(1) The paper is hard to follow and the writing can be significantly improved especially for the abstract and introduction. I felt rather confused when I first read the abstract and introduction. The authors frequently use the word "weight update", however it is unclear whether it refers to gradient update or the update caused by regularization/compression. There s little discussion about the choice of batch size and I don t really understand how weight regularization is decoupled from batch size after reading the paper. (2) The reason why such a training scheme improves model compression is unclear and needs further investigations. In the paper, the authors first interpret model compression as a way of inducing weight decay (and random weight noise). Particularly, the model accuracy is roughly constant over different value of NR period (as shown in Figure 3 and Figure 4) when weight decay is used. I think the authors should give some explanations for that difference. To sum up, I like the idea of asynchronous regularization/compression, but I m not quite satisfied with current version of paper.<BRK>I think that the paper is very well written, I like it. The authors localized a phenomenon  and demonstrated how to exploit it. I trust the results because I performed exactly the same experiments for CIFAR 10 with longer non regularization periods and found that there is no effect (this is also that the authors show in the paper)  but I didn t test on other datasets and obviously didn t think about potential benefits for compression. I still think that the paper should be accepted.
Reject. rating score: 1. rating score: 3. rating score: 6. <BRK>This paper proposes a VAE based conditional molecular graph generation model. Not compared with SotA graph based molecular graph generation models. The significance figure of the table 1 is unclear. However, it is largely admitted in the molecular graph generation studies that the graph based generative models generally performed better. However, the authors do not provide how to understand the figure.<BRK>The authors introduce a variational autoencoder for conditional generation of molecules. how is validity defined? SMILES syntax or validity as a molecule? It s a bit unsatisfying that the authors only use and experiment with a simple logP as the property to control in the model.<BRK># SummaryThe paper considers the problem of generating molecules with desired properties using a variant of supervised variational auto encoders. The latter refers to finding a molecule that maps to a fixed point in the latent space.
Accept (Poster). rating score: 8. rating score: 6. <BRK>Recently many language GAN papers have been published to overcome the so called exposure bias, and demonstrated improvements  in natural language generation in terms of sample quality, some works propose to assess the generation in terms of diversity, however, quality and diversity are two conflicting measures that are hard to meet. This paper is a groundbreaking work that proposes receiver operating curve or Pareto optimality for quality and diversity measures, and shows that simple temperature sweeping in MLE generates the best quality diversity curves than all language GAN models through comprehensive experiments. It points out a good target that language GANs should aims at.<BRK>This paper concerns the limitation of the quality only evaluation metric for text generation models. Instead, a desirable evaluation metric should not only measure the sample quality, but also the sample diversity, to prevent the mode collapse problem in gan based models generation. The idea of sweeping temperature during beam search decoding is not new in the NLP community, which limits the novelty of this paper. What’s more, some parts of the experiment results is also somehow not new, in the sense that the SBLEU vs Negative BLEU tradeoff curve is also shown in [1,2,3,4]. It is not clear from the paper. In this kind of large vocabulary datasets, I think the RL/GAN based training methods would easily breakdown, and far worse than MLE based training.
Reject. rating score: 3. rating score: 3. rating score: 3. <BRK>+ This paper is well written and easy to follow. Then, the generated data combined with training data is used for knowledge distillation. + The proposed GAN TSC can be combined with standard augmentation to achieve higher performance as shown in the experiments.<BRK>In this manuscript, authors adopt GAN for data augmentation to improve the performance of knowledge distillation. 1.The novelty is limited.<BRK>This paper presents an algorithm to generate images for training a student network through distillation. Pros:  I like the paper and the idea behind being able to improve or even train a student network when the original data is not present.
Reject. rating score: 3. rating score: 6. rating score: 8. <BRK>However, the literature survey of this paper is not satisfactory. The authors forgot to mention tensor decomposition and mixed it with efficient architecture design. As for parameter pruning and quantization,  many important papers are missing. 3.Leveraging back part of teacher model s guidance to improve student performance has been investigated by other researchers on OCR tasks in Ding H, Chen K, Huo Q. Compressing CNN DBLSTM models for OCR with teacher student learning and Tucker decomposition[J]. They combine student s CNN with teacher s DBLSTM to learn better representations.<BRK>The choice of the transferring layer seems to be rather ad hoc, and it is hard to say how much tuning needed to get the empirical benefits. I have to guess that is the first a few layers of student and last a few layers of teacher. The paper has really good empirical results. However, I cannot understand the intuition behind the paper why concatenating teacher to student is better than use l2 for intermediate layers.<BRK>The paper suggests a new method for knowledge transfer from teacher neural network to student: student teacher collaboration (STC). Page 6: while KT has conflicts with KD in some cases  > while FT has conflicts with KD in some cases. In this way, the weights in the student subnetwork are learned from the loss backpropagated from the teacher subnetwork. The idea is elegant and, to the best of my knowledge, has never been suggested in other works. The performance is always better than pure student training (which was not always the case for previous methods) and sometimes the results are even better than teacher performance. For object detection experiment the choice of the border is naturally defined by the architecture of the network in Faster RCNN approach. For classification, it could be interesting to study how this choice influences the results.
Reject. rating score: 3. rating score: 3. rating score: 6. <BRK>The paper presents an approach   Delayed and Temporally Sparse Update (DTS)   to do distributed training on nodes that are very distant from each other in terms of latency. However, the paper does not provide any convergence analysis or convergence guarantees. The paper s approach with delayed updates, i.e., delaying gradient updates to other nodes, sends the updates in a later iteration. However, the novelty and contribution is relatively low.<BRK>This paper tackles the issue of scalability in distributed SGD over a high latency network. · There is no theoretical analysis of the soundness of the proposed algorithm.<BRK>Authors provide a technique to compensate for error introduced by stale or infrequent synchronization updates in distributed deep learning. The three approaches are:1. Combined   1. and 2. combinedAuthors evaluate on ImageNet, showing some improvement, and promise to release their implementation for PyTorch/Horovod. Their technique, combined with a reference implementation in a popular framework stands a good chance of having impact. Given the increase in cloud training workloads, even a small improvement in this setting is significant.
Reject. rating score: 1. rating score: 3. rating score: 6. <BRK>For these reasons, I believe the paper in its current form is not yet ready for publication. I believe these embeddings should be shown at convergence time. 9.In Table 4 and accompanying text, the authors conclude that “LILAC is relatively invariant to the order of the label introduction”. Overall opinion  While the ideas introduced in this paper may have merit, I believe the experimental evidence is quite limited.<BRK>Cons:The motivation behind the ideas of the paper and the design of the procedure was not so clear. 3.Reasons for decision:Pros:The class wise idea used in this paper seems to add a new direction to the area of curriculum learning. However, if you just look at the best mean accuracy, a naive label smoothing is often better than the proposed method.<BRK>The observations and proposed methods are both valuable contributions to the field. I hesitated about this, and in the end decided that the interesting observations are the most valuable part of the paper, not pushing SOTA (while of course it s valuable to report numbers, I think our field should focus less on SOTA numbers in general). 2.Did I misunderstand something about the CIFAR 10 SOTA presentation (is there some reason for the choice of numbers cited here?)
Reject. rating score: 1. rating score: 3. rating score: 3. <BRK>*Paper summary*The authors build an input transformation invariant CNN using the TI Pooling architecture of (Laptev et al., 2016). They make some modifications to this, namely 1) they replace the convolutional filters, with input dependent convolutional filters, and 2) they add a decoder network from the final representation, which reconstructs an input transformation rectified image, to encourage the final representation to be fully transformation invariant. This all said, I think the paper is well written and very clear. The second reason is that the work is not well placed in context with prior works.<BRK>The proposed method in this paper tries to make the CNN robust to the input image transformation by learning to generate convolutional filters. The proposed architecture has two main parts. 1) Filter Generation:  Given an input image, a set of predefined transformations are applied to the image. The idea is that these input dependent filters can compensate all of  transformation in the image. Negative points:The proposed method is not novel. 2) The experimental results are weak, the authors should compare their method on more difficult datasets like ImageNet dataset. 3) The authors should compare their proposed method with the "Spatial transformer networks, NIPS 2015." In conclusion, my recommendation for this paper is "weak reject".<BRK>And an L 2 reconstruction loss (with respect to a chosen class representative) is added to the cross entropy loss for classification. Overall the paper is well written, and it is fairly easy to read. However, I am not totally convinced that the two contributions of the paper are significant to transformation invariant representations, and my reasonings are follows1. Why is a decoder needed in the architecture? 2.Choosing a "class representative" in the CNN seems very restrictive. The input conditioned filter generation seems a little confusing.
Reject. rating score: 3. rating score: 3. rating score: 8. <BRK>Some concerns:1, the theoretical results seem a bit incremental compared with (Xu et al.2018).2. it would be nice to comment on how this will affect cases with nontrivial node features and general node classification tasks. The paper proposes a generalized framework for GNN. On standard datasets/tasks, the baselines are not state of the arts. The discriminative power of a network using L 1 is > WL 1.<BRK>This paper proposes a general class of GNN. The new model class generalizes the aggregation step to multiple levels of neighbors. Theoretically, the paper shows the generalized models enjoy better discriminative power. The paper also conducts experiments to demonstrate the effectiveness of the new model class. The paper claims SOTA results on several datasets. However, this paper does not report recent SOTA results:https://arxiv.org/abs/1809.02670https://arxiv.org/abs/1810.00826https://arxiv.org/abs/1905.131923. As there is no randomness at all.<BRK>The paper proposes minor modifications to Graph Convolutional Networks (GCNs) that as proven by the authors enable learning of local features in networks, namely the aggregation over powers of the adjacency matrix (effectively counting random walks within the neighborhood) and aggregating over connections within nodes in the neighborhood. GCNs are of interest to The paper is well written and clear. My intuition says that this may even be an advantage over adding more layers.
Accept (Poster). rating score: 6. rating score: 6. rating score: 6. <BRK>The paper presents an approach for style transfer with controlable parameters. I would have liked more motivation and intuition behind it.<BRK>The paper addresses this by parametrizing the normalization layers in the style transfer model, and training with a parametrized loss function. It would help to have a more consistent story.<BRK>The paper proposed a generative model for image style transfer in real time. I think the proposed approach is reasonable and and experimental results are convincing. Experimental results appear to be promising.
Reject. rating score: 1. rating score: 3. rating score: 3. <BRK>My major concerns:1) They try to propose a new problem, but their description shows that the problem is exactly the same as what most “few shot learning” works aim to solve: use a pre trained model, train a meta learner on few shot training tasks, and apply it to novel test tasks. 2) The algorithm does not have any important contributions comparing to existing ones: they define a prototype per class based on the pre trained model and apply the nearest neighbor classification. 3) I would not call the weighted average as “attention” because it is not: the weight in attention is computed by a module with learnable parameters, while the weight in this paper is computed by the entropy of a pre defined model’s output prediction. This assumption is too strong since it requires class level (rather than lower level) relationships. They make the paper hard to understand. Update:Thanks for the authors  rebuttal! The proposed method tends to be incremental.<BRK>A new task is suggested, similarly to FSL the test is done in an episodic manner of k shot 5 way, but the number of samples for base classes is also limited. The model is potentially pre trained on a large scale dataset from another domain. I would modify it so instead of discarding mini imagenet classes that are overlapping with Places I would discard the problematic Places classes. This way it will be easier to compare to standard FSL. Also, I don’t understand why for CUB the benchmarks includes k {0,1,5} while for mini imagenet it is k {0,20,50}, obviously k {0,1,5} are more interesting. As for the suggested method, I find it hard to judge since there are no strong baselines to compare against. I think being more careful about the benchmark definition with regards to train/test overlap and comparing to stronger baselines will help improve the paper for future submissions.<BRK>This paper proposed a new realistic setting for few shot learning that we can obtain representations from a pre trained model trained on a large scale dataset, but cannot access its training details. Back to the standard few shot classification problem, they will first adapt the model with base class samples and then adapt to novel classes. The proposed new setting is very meaningful since we already have many powerful pre trained models and why not exploit its usage for few shot learning problems. However, I doubt the novelty and effectiveness of the attention way used in the paper. But there are already some relevant studies in the missing reference Large Scale Long Tailed Recognition in an Open World, CVPR2019. Btw, according to the formatting instructions, the abstract should be limited in one paragraph. After Rebuttal:I thank the author for the response.
Reject. rating score: 3. rating score: 3. rating score: 3. <BRK>The paper studies the problem of training policies which maximize returns under worst case environmental perturbations. The min step relies on sample based estimates of gradients and hessians with respect to environment parameters. The problem of robust control is an important one, and the paper presents interesting results, however it lacks a proper comparison to baselines. This way the policy can learn to do system identification. It might not be robust (i.e.not generalize to unseen perturbations) but this should be verified. These baselines are included in Figure 3 in the Appendix but I find those numbers odd. If I only use samples from the two distributions to estimate it then this is likely limited to low dimensional settings, right? [1] Henderson et al., (2017). Deep Reinforcement Learning that Matters. I still think the paper could use more work and hence keeping my score as is.<BRK>The paper proposes a novel solution to a *restricted* reinforcement learning problem. I have published in RL but my familiarity with Wasserstein and robustness is limited. If there is a high score I ll have a closer look. 2) The explanation in on page 4 about how the wasserstein distance is computed is a bit unclear. What is Monte Carlo in this case ? If a method has access to a simulator and a representation of the relevant dynamics parameter space it seems clear that it will be more robust. Maybe it would have to be a latent variable model so you can do the same inference for internal parameters and still do an adaptation in the same way as now.<BRK>Overall, this paper is of well written with a clear illustration of the methodology and comprehensive experimental results. I still have a few concerns below, that prevent me from giving a direct acceptance: 1. However, the proposed method seems to be somewhat incremental: In Yang (2017), it is clearly stated that it is viable to treat the robust reinforcement learning objective as a min max game (in eq.(1)) with support belonging to a Wasserstein ball (in eq.(2)). The statement in the submission “… a novel min max game with a Wasserstein constraint …” seems overclaimed, namely, eq.(8) in the submission, is a combination of eq.(1) and (2) in Yang (2017), with the constraint in eq.(8) not exactly equal but just a relaxation of eq.(2) in Yang (2017). 2.Furthermore, since a few papers have proposed methods to deal with both transition and reward dynamics, it would be nice and complete to also address (or hint on) the reward ambiguity problem.
Reject. rating score: 3. rating score: 3. rating score: 3. <BRK>Given the above, I would like to increase my score from 1 to 3 (weak reject). Such a distribution is usually represented in a fully factorized fashion (e.g., as a set of multinational distributions as in DARTS). This paper proposes to model the architecture distribution using a VAE instead, where the encoder and decoder are implemented using LSTMs. This is largely due to the fact that the authors are comparing their method against baselines in fundamentally different search spaces. * For ImageNet experiments, the authors are using a ShuffleNet like search space which has fundamentally different building blocks than other architecture search baselines (commonly built on top of inverted bottleneck layers).<BRK>This paper proposes to use the variational auto encoder (VAE) to sample the network architectures. I understand the motivation of VAE + one shot, but I am not very convinced by VAE + gradient based. In the last paragraph in Section 4, the paper claims (1) VAENAS G can increase the diversity of architectures, which can be also achieved by sampling the data set. Also, it claims (2) VAENAS G helps to search for large models, which  I do not see experimental supports. Tables 2 and 3: Why is VAENASNet (table 3) different from VAENAS G and VAENAS OS?<BRK>This paper proposed to use VAE to learn a sampling strategy in neural architecture search. The main idea is to use the currently high performing networks to train a VAE from which the sampled architectures for the next iteration will likely supply both high performing networks and better diversity coverage. Are the numbers in Table 2 and Table 3 swapped? Instead of taking this overhead, wouldn t it be easier to randomly sample more architectures?
Reject. rating score: 1. rating score: 3. rating score: 3. <BRK>This paper proposes a regularizer to encourage the robustness to the random contamination for training deep neural networks. The idea is straightforward and intuitive, but not that exciting. The Gaussian noise is too simple and easy to defend. (2) The proposed moment regularizer is very delicate. (3) The Alexnet was proposed in 2011.<BRK>I am not fully convinced by the robustness result in Figure 3. So it is hard to recommend acceptance. The training method in [1] is Gaussian data augmentation. "Certified adversarial robustness via randomized smoothing."<BRK>The authors validate the model with the proposed regularization technique for its robustness against Gaussian attack and other types of attacks, whose results show that the model is robust. Therefore I temporarily give this paper a weak reject, but may change the rating with more experimental results provided in the rebuttal. Figure 3 doesn’t seem like a very favorable result to the proposed model, since we are generally more concerned with adversarial examples generated with small perturbations, as large perturbations may change the input semantics. Cons  Experimental validation seems highly inadequate due to lack of baselines.
Reject. rating score: 3. rating score: 3. rating score: 6. <BRK>The paper is mostly concerned with multi aspect sentiment classification. Overall, I think these days the standard is quite high for papers that make claims about generating convincing justifications/interpretations/explanations. At the moment the output of the system appears to be attention weights over several aspects, showing which words in a review focus on which aspect. The appendix is detailed and shows many examples.<BRK>This paper proposes a neural model for multi aspect sentiment of reviews. High aspect correlation baseline: the average aspect correlation across reviews in the training set is high (that is, if one aspect is positive, most aspects will be and viceversa). For example, if you were to sort the reviews in the validation set by their aspect correlation, and chart the F1 score as a function of the prefix of this sorted dataset what would we see?<BRK>This work addresses the problem of aspect based sentiment analysis *with* rationale extraction that can serve as an explanation (interpretation) of the decision. Compared to previous work this paper models the problem as multi aspect classification, therefore having one model for all aspects instead of one for each aspect. This is a useful paper, and I can clearly see it used. I found the paper very hard to read, having to re read many parts to understand exactly what is the goal of each section. The row names of the table 1 3 do not correspond to the names in the text which makes the understanding even harder.
Reject. rating score: 1. rating score: 3. rating score: 3. <BRK>Here are my concerns and questions:1). I conjecture the paper would like to bring the evolution in genetics and perhap brain cirecuits as well to define a novel neural net model, called NNE by the authors. The paper is somewhat cumbersome in the introduction that makes the reader (here is myself) can not understand the main idea. What do you mean by fixed probability of 1?<BRK>In this paper the authors propose a method for training neural networks using evolutionary methods. The authors prove that their method converges and with high probability succeeds in learning linear classification problems. In terms of presentation, the paper is generally clear and well written. I was not able to assess the importance of the theoretical contributions of the work as my research is not in this area, so my comments are limited to the other aspects. Based on the above comments, I think the work will benefit from further developments before being ready for publication.<BRK>It does not require to calculate the gradient explicitly. The authors have conducted some experiments on MNIST using ANN with only one hidden layer. In this paper s formulation, W is named as a weight generation matrix, which is choosing to be random and i.i.d.with certain probabilities. This is kind of similar to back propagation. Then the probabilities are updated and thus x is changed as well. I do not see it fundamental different to back propagation.
Accept (Spotlight). rating score: 8. rating score: 8. rating score: 6. <BRK>The authors seek to challenge some presumptions about training deep neural networks, such as the robustness of low rank linear layers and the existence of suboptimal local minima. Nevertheless, I think this paper gives useful insight as to the behavior of deep neural networks that can help advance the field on a foundational level. In addition, they present a norm bias regularizer generalization that consistently increases accuracy.<BRK>In this paper, the authors seek to examine carefully some assumptions investigated in the theory of deep neural networks. Reply to rebuttalWe thank the authors for taking into consideration our previous comments and suggestions, including going beyond MLP and adding experiments on other datasets.<BRK>The authors look at empirical properties of deep neural networks and discuss their connection to past theoretical work on the following issues:* Local minima: they give an example of setting where bad local minima (far from the global minimum) are obtained.
Reject. rating score: 1. rating score: 3. rating score: 6. <BRK>They propose a Thompson sampling strategy as a new hyperparameter optimization warmstarting strategy and an optimization method that leverages transfer learning. How is this novel and why can t your baselines employ the same idea? One interesting work is "Collaborative hyperparameter tuning" by Bardenet et al.which overcomes the problem of different scales by considering the problem as a ranking problem. Another interesting work based on GPs is "Scalable Gaussian Process based Transfer Surrogates for Hyperparameter Optimization". Second, this is not how this method works. In your case k is upper bounded by 10 (number of tasks). This might be true but I have a much simpler explanation: the search space of FCNet is orders of magnitudes larger and it contains many configurations that lead to very high MSE and only few with low. Table 6 contains more datasets than Table 2. The aggregated results in Table 2 are nice but actually we are interested in the outcome after the search after a given budget. I have few suggestions to improve the readability of the paper:That there is a choice of copula estimators is mentioned at the very end of the paper. However, Table 6 is explained first in section 5.2 which makes it hard to follow your argumentation.<BRK>This paper tackles the problem of black box hyperparameter optimization when multiple related optimization tasks are available simultaneously, performing transfer learning between tasks. Different tasks correspond to different datasets and/or metrics. Gaussian copulas are used to synchronize the different scales of the tasks. So why is it included in the cost function? In this case, I don t see the point. Why would that be a good thing? Explain why the EI approach is used for the second model (with the GP), but not for the first model.<BRK>Summary:This paper proposes a new Bayesian optimization (BO) based hyperparameter searching method that can transfer across different datasets and different metrics. The method is to build a regression model based on Gaussian Copula distribution, which maps from hyperparameter to metric quantiles. While I am not an expert in this area, the derivation looks sound to me, and the evaluation results of this paper are comprehensive to show that CGP seems to outperform a number of methods.
Reject. rating score: 3. rating score: 6. rating score: 6. <BRK>But from the description in the paper, most of the challenges seem to also exist in other zero shot settings (long tail distribution; large space of labels). In particular, the paper focuses on the zero shot setting where the test predicates are unseen during training. The proposed method looks correct but is a rather direct application of existing methods. The model is trained so that the visual feature vector and the correct predicate embedding are nearby in the joint embedding space.<BRK>Interestingly, their pipeline introduces a new softmax variant, the unbalanced sampled softmax, which addresses the problem of over predicting common predicates. I generally tend towards accepting this paper. A few comments about things that could be strengthened or addressed further:   There could be more meaningful comparison to other zero shot learning algorithms. Even if they are not fully comparable because they were originally meant for a slightly different zsl set up, it would be nice to have more baselines from external work. If not, can the authors clarify what the overlap was and how this might be affecting performance?<BRK>1.Summarize:This paper proposes a new problem setting in visual relation detection which is called “Predicate Zero shot Learning (PZSL)”. Maybe it is better if it is also mentioned in the paper. I will go for a weak accept for the paper at this stage. Also, (+) the proposed initial solution to this problem is interesting. 4.Comments and feedback. The relationship recognition methods are mainly supervised “that” → “to”.
Accept (Spotlight). rating score: 8. rating score: 8. rating score: 8. <BRK>The paper elaborates on the expressivity of graph neural networks (GNNs).<BRK>I believe that the contributions of this paper are significant. The fact that no actual difference in performance between AC GNNs and ACR GNNs was noticed in the only non synthetic dataset used in the experiment should prompt the author to run experiments with more real life datasets, in order to empirically verify the results, but this is a minor point.<BRK>The style of the manuscript is mixed: the abstract and introduction are quite dense for non experts; a motivating real world example could help here.
Reject. rating score: 1. rating score: 1. rating score: 3. <BRK>The paper proposes to learn a representation space that is useful for planning. While the idea may be an interesting direction to improve sample efficiency of planning, it is not the first work that proposes to combine planning with representation learning, and I do not think the work is clearly presented/motivated/validated sufficiently to deem it acceptable. As presented, there are many details that are unclear or imprecise. Why are the choices in the experiments as made? (4) Figure 6 explanation is very sloppy (description and body). How is a plan generated? (9) There exists literature in RL to combine the 2 step metric learning process to 1 step. [3].(10) What is the action space of these domains? These details details are missing.<BRK>The paper aims at learning a latent representation of images in the setting of sequential decision making. The representation are learned such that there exists a metric that correctly reflects the difference in reachability between points in the neighborhood of the current observation and the goal. What is a  plannable representation ? There are a several citations missing in that direction, going back to at least "Metrics  for  probabilistic geometries" by Tossi et al.in 2014.As it was also pointed out in a public comment, relevant citations related to "Learning for planning" seem to be missing, too. Finally, a wider set of experiments needs to demonstrate the method (again, considering the scope of the as of now not cited papers).<BRK>## Strengths  bootstrapping a learned local distance metric to a global distance metric to reduce test time planning cost is an interesting problem  the paper has nice visualizations / analysis on the toy dataset  the learning procedure for the local distance metric is clearly described  the paper uses a large variety of different visualizations to make concepts and results clearer ## Weaknesses(1) missing links to related work: the author s treatment of related work does not address the connections to some relevant papers (e.g.[1 3]) or is only done in the appendix (especially for [4]). This makes it hard to understand the actual contributions of this paper. The baseline that plans greedily with embeddings based on visual similarity has even less hope of working. The authors added comparison to a model free RL baseline as well as proper comparison to a multi step planning version of SPTM. However, these comparisons were only performed on the most simple environment: the open room environment without any obstacle. These evaluations are not sufficient to prove the merit of the proposed method, especially given that it is sold as an alternative to planning methods. The method needs to be tested against fair baselines on more complicated environments; the current submission only contains baselines that *cannot* work on the more complicated tasks.
Accept (Poster). rating score: 8. rating score: 6. rating score: 6. <BRK>In this paper, the authors introduce a black box adversarial attack based on estimating the sign of the gradient. The inclusion of results on the public blackbox attack challenges is also welcome.<BRK>Summary:This paper proposes a black box adversarial sample generation algorithm, which proceeds by learning the signs of the gradient using an adaptive query scheme, which the authors call _SignHunter_. The authors begin by deriving an upper bound on the query complexity for _SignHunter_. "Hyperparameter optimization: A spectral approach." However, this claim is not justified theoretically:  it would be interesting if the benefits of adaptivity could be quantified in terms of improved worst case query complexity etc.<BRK>This paper proposes a black box adversarial attack method to improve query efficiency and attack success rate. The SignHunter algorithm is proposed to estimate the sign of the gradient by a divide and conquer search. Overall, the proposed method on estimating the sign of the gradient for black box attacks is novel.
Reject. rating score: 1. rating score: 3. rating score: 3. <BRK>Paper Strengths:The paper starts with a nice introduction that embodiment is useful for perception. However, the main content of the paper is very different from the introduction. This has nothing to do with embodiment. The same patterns are observed in non dynamic tasks such as image classification. That is the whole point of training. It is obvious that this happens. Due to the issues mentioned above, I do not think there is anything new in the paper and I vote for rejection.<BRK>This work builds on the embodied cognition literature, hypothesizing that representations learned in embodied agents will be of improved “quality” compared to non embodied models, such as neural networks trained on static supervised datasets. Unfortunately, the work doesn’t provide adequate baselines to properly assess the hypothesis. In other words, an embodied agent with a random policy is not equivalent to a non embodied agent. To assess wither *embodiment* is a critical factor for driving good representations, the authors should compare to a model that learns from a static supervised dataset. Overall, there are some nicely presented ideas but the work is unfortunately incomplete, and the results cannot support the hypothesis laid out.<BRK>Paper summary:This is an empirical study of the representations learned by a reinforcement learning agent. The goal of the paper is to show that these features are due to the embodied nature of the agent. Decision:I suggest to reject this paper. While the topic is interesting and the paper is clearly written, there is a lack of control/comparison experiments, such that the paper’s conclusions are not backed up by the analyses. Comparing to a random network is a sanity check, but not sufficient to support the paper’s claims. Without such comparisons, it is unclear whether representations learned in a supervised way would be any different from those learned by RL. This would suggest that the embodied agent learned better representations.
Reject. rating score: 1. rating score: 3. rating score: 8. <BRK>2) A decoder network that first estimates a 3D density map from the latent vector using upsampling and convolutions and then classifies the atom type (atomic number) per voxel using a 3D segmentation. It is also not clear how the training routine (conditioning) and the output post processing (connected components + majority voting) is modified. It would be more appropriate to submit this paper to a domain specific venue rather than to ICLR. Previous work on generative models for molecules have concentrated on molecules with 1D structure (which can be represented as strings) or 2D structure (which can be represented as planar graphs). 3.Joint VAE + UNet training: The joint training of the encoder decoder VAE and the 3D segmentation network results in a decoder that can reconstruct atom locations and atom types, being robust to mistakes in the density map reconstruction. Also, there is no justification of the advantages of using a voxel based density map representation along with 3D convolutions vs, for instance, a graph representation along with graph convolutions (Xie & Grossman, [3]) or a point cloud GAN (Achlioptas). The only such evaluation presented in the paper is related to the distribution of distances between atoms which seems to be close to their actual distribution in nature.<BRK>An encoder decoder architecture is used to create a representation of a molecule and to reconstruct the molecule from its representation. Then a second Neural Network segments the output and assigns an atomic number. Unfortunately, the paper does not compare their work to prior work on 3D structure representations (e.g Gebaur et al 2019). Also, it is not clear whether the 3D representation is better than 1D or 2D representations especially since there have been many new 1D models that perform very well for tasks like molecular property prediction (For example All SMILES VAE https://arxiv.org/abs/1905.13343 ). I think this is a main drawback of this work.<BRK>The paper deals with accurately encoding and decoding 3D atomic positions and the crystal’s species using 2 sets of neural networks a) a VAE that builds a compressed latent space representation of a crystal and b) a UNET for segmenting the latent space into atoms and assigns each atom to its atomic number. Experiments were conducted on over 120K 3D samples of crystals and the results seem to be promising. The paper is neatly written and well organized.
Reject. rating score: 3. rating score: 3. rating score: 6. <BRK>The authors propose to construct reinforcement learning policies with very few parameters. The values of all parameters are learned with a gradient based method. The experimental section sheds light on some important aspects. Why did you decide to use only a subset and why these tasks? For the experiments you chose a FFNN with one hidden layer. Would fixing the architecture to one hidden layer work in general? As you mentioned, in NAS random search is a strong competitor. Concluding, this is an okay paper with limited innovation. The comparison to baseline need to be improved or at least justified.<BRK>Summary: This paper focuses on neural architecture search for constructing compact RL policies. Experiments on continuous control benchmarks show that good performance can be obtained using a very small number of parameters. Favourable reward compression outcomes can be achieved compared to some baseline alternatives. (Although not dramatically novel given cited work by Salimans, Gaier, etc)+ Searching both the partitioning + weight values with ENAS and ES provides a nice way to do learning & compact architecture search simultaneously. The paper claims that RL can be high dimensional with millions of parameters, and so there is a need to apply NAS to RL. But experiments are conducted in low dimensional environments with only 100s of parameters. The paper makes a claim about embedded devices, but this is unconvincing, as for these proprioceptive control tasks, the uncompressed networks are already small enough to run on most embedded devices.<BRK>This paper compresses policy networks using approaches inspired by neural architecture search. Both the partitioning and weights are trained simultaneously, inspired by ENAS. The experiments carried out include comparing to existing works on policy network compression, ablating against random partitioning, as well as a few experiments meant to increase understanding of the learned partitions. The approach is straightforward and has applications in compressing RL policies. Is the current method already highly scalable, or is scalable still a potential that has yet to be reached? For the gradient estimation in eq 3, what exactly is this unbiased with respect to? (How exactly were the number of partitions chosen, for Tables 1 and 2?)
Accept (Poster). rating score: 6. rating score: 6. rating score: 6. <BRK>This paper addresses the issue of meta learning in a transductive learning setting. It will be helpful to clearly indicate for each method in comparison whether/how it also utilises the query set. Overall, this is a well organised and nicely presented work.<BRK>Adjusting and clarifying the notation would improve the paper. Most of my points have been addressed satisfactorily. Thanks to the authors for a detailed response.<BRK>Considering the authors argue specifically for the importance of transduction in the zero shot learning regime, I think it would be reasonable to expect experiments substantiating this, and the strength of their method in this regard, on non synthetic datasets.
Reject. rating score: 3. rating score: 3. rating score: 3. <BRK>This paper suggests a method for semi supervised few shot learning. This regularisation term is adapted from Haeusser et al.(2017) and is encouraging a random walk on a graph (where nodes are prototypes and unlabelled samples) that begins at a certain prototype to end at the same prototype. Also, the choice of using prototypes (unlike Haeusser et al.)<BRK>This paper develops a random walk based method on top of prototypical networks to address the semi supervised few shot learning, i.e.when each classification task can access only a few shot labeled data but many unlabeled data. This paper defines a specific random walk: it walks from a prototype to an unlabeled sample, then walks for several steps between unlabeled samples, and then walk back to some prototype. My major concerns are the motivation and analysis of the proposed random walk and the training algorithm, whose details cannot be found in the paper.<BRK>This submission introduces a version of the random walk regularizer introduced in Haeusser et al.2017, in the setting of a Prototypical Network for semi supervised few shot learning. The authors show that using this regularizer, SOTA results can be obtained, notably on the popular miniImageNet. I could be convinced to accept, due to the impressive effectiveness of the method despite its simplicity. However, the submission is not without faults:1.
Reject. rating score: 1. rating score: 1. rating score: 1. <BRK>Unfortunately, the lack of clarity in the paper and poor writing prevents me from writing a thorough review.<BRK>However, there are several disadvantages of the current status of the paper. The description is very redundant and the texts are very hard to read.<BRK>Second, the methods used in the paper are conventional and there is no novelty from the algorithmic perspective.
Reject. rating score: 3. rating score: 6. rating score: 8. <BRK>The training behavior is then well approximated by a linear model (Taylor expansion at the initialization). * rich regime: The kernel regime is turned into a rich regime when the assumptions of kernel regimes aren t met. Specifically, the paper emphasizes how the scale of initialization controls the transition between the two regimes, which was first pointed out by Chizat & Bach (2018). The contributions are not clearly stated and I can only see the execution of ideas from Chizat & Bach (2018) and applying them to more concrete examples, which leads to analytical results (for linear networks) in Theorem 1/2. How does this change with network widths and different architectures?<BRK>This paper analyzes an inductive bias of the gradient flow for diagonal two or higher homogeneous models and characterizes a limit point depending on the initialization scale of parameters. Significance:To explain the generalization ability of powerful machine learning models that can perfectly learn a training dataset, the implicit bias of the optimization methods and models play key roles when explicit regularization is not adopted. For instance, deep neural networks fall into this scenario. Although, homogeneous models treated in this study is restricted (essentially linear models) and a theory is limited to the continuous gradient flow, these settings are rather common in this context. Toward a better explanation of the generalization performance of deep learning, understanding of the inductive bias of the early stopping before convergence is more important. A provided theory is limited to linear models essentially. Update:I thank the authors for the response. I am convinced of the difference from [Gunasekar+(2017)] and my review stands. I  would like to keep my score.<BRK>I really appreciated this paper. It discusses a very complex question ("Are we learning in a kernel regime, or in a rich regime where features are identified") by looking at perhaps the simplest model the authors could think of, and then study in detail the model. And how simple it turns out to be: just a linear regression with a twist. It is also the simplest model where one can observe  a non trivial inductive bias and  implicit regularisation I do not have much to say on the paper, except that I fully support publication.
Reject. rating score: 1. rating score: 3. rating score: 3. <BRK> This paper tackles an interesting problem, one class classification or anomaly detection, using a meta learning approach. I suggest the authors perform more rigorous experimentation and focus the paper to be a paper about an understudied problem with rigorous experiments/findings, or improve their method beond the small modification made. Due to these weaknesses, I vote for rejection at this time. Strengths     The problem is interesting and under studied in the context of deep learning and transferable methods from similar ML problems (e.g.few shot learning)    The method is simple and adapts a state of art in few shot learning (meta learning, and specifically MAML)Weaknesses    While I enjoyed reading the paper since it tackles an under explored problem, it is hard to justify publishing the method/approach at a top machine learning conference. What are the characteristics of the improved initialization?<BRK>The proposed method builds upon the model agnostic meta learning (MAML) algorithm. Although the authors have pointed out some real applications, I think they have been introduced separated. In other words, since this setting is the combination of two previous areas, i.e., one class classification and few shot learning, I fell that the authors have introduced it by just a combination. What are the unique challenges of this problem? I think these problems should be clarified at first. In summary, I think this paper likes a technical report, not a research paper. I suggested the authors to recognize the presentation.<BRK>One class classification requires only a set of positive examples to discriminate negative examples from positive examples. The current paper addresses a method of meta training one class classifiers in the MAML framework when only a handful of positive examples are available. Meta training one class classifiers in the MAML framework seems to be sound. Regarding episodic training, in contrast to few shot classification problems, support sets in episodes have similar positive examples. Please compare it with the fine tuning method.
Reject. rating score: 1. rating score: 3. rating score: 6. <BRK>Overall I feel that more work is needed to convincingly demonstrate the importante of the proposed approach. The proposed approach is evaluated on a variety of datasets. The paper deals with an important problem and the exposition is clear.<BRK>This paper proposes an approach for local post hoc explanation with introduction of a new regularization that helps regulate the "fidelity" and "stability" of the output explanation (in the style of LIME).<BRK>The paper proposes a new type of regularizer to improve explainability in neural networks. I would recommend for accept, as the paper shows in its experiments that the explainability of neural networks can be improved with the two proposed regularizer, which outperforms simple baselines of l1 and l2 regularizers.
Reject. rating score: 3. rating score: 3. rating score: 6. <BRK>Based on the study, the authors propose some empirical suggestions to fix the generalization gap for acceleration methods. The significance of the paper is not enough. "We re examine this belief both theoretically and experimentally". Q3."we synthesize a user’s guide to adaptive optimizers". Since the paper mainly does empirical study, this question is important. Q4.Why not SGD in Figure 3?<BRK>This paper revisited the a common belief that adaptive gradient methods hurts generalization performances. The authors re examine this in more depth and provide a new set of experiments in larger scale, state of the art settings. ICLR 2019. This makes the contribution of this paper look like combining all the tricks together.<BRK>This paper revisits the question of whether adaptive methods deliver solutions with larger generalization errors than those of solutions found using SGD. There is a missing figure reference at bottom of page 6 The takeaway of this paper seems to be that adaptive methods and non adaptive methods both need careful parameter tuning.
Reject. rating score: 3. rating score: 3. rating score: 3. <BRK>The authors should explain this. Experimental results on a few data sets prove the effectiveness of the proposed approach. Overall, the paper is well written and easy to follow.<BRK>Significance of the contribution is a bit marginal. However, the experiment design and the reporting of results are doubtful.<BRK>The major contribution of this paper is to utilize multiple convolution/pooling operations and merge them with neural networks.
Reject. rating score: 3. rating score: 3. rating score: 3. <BRK>Overall, this paper is not easy to understand in detail. An overview diagram or a toy example would greatly improve the readability of the paper.<BRK>The focus of the paper, i.e., to develop improved models by combining TPPs and representation learning, is a promising approach to this task and fits well into ICLR.<BRK>The authors perform ablation studies by switching off each component as a whole but considering the way this architecture is built, this is not a very useful exercise except knowing that each component contributes to the performance. All baselines produce embeddings and the authors mention that classification for this paper is independent of marker history.
Reject. rating score: 3. rating score: 3. rating score: 6. <BRK>The paper describes a new way of learning context dependent entity representations that are capable of encoding fine grained entity types.<BRK>It also shows that using these representations is very effective for a wide variety of down stream entity centric tasks such as entity typing, linking, and answering entity centric trivia questions. After obtaining the representation, they train the entity embedding (present in the sentence) to be similar to the context embedding. The biggest weakness of the paper is wrt novelty. They try the entity representations on a wide variety of entity centric tasks and get reasonable results.<BRK>This paper aims to learn entity representations by aggregating all the contexts that an entity appears in based on English Wikipedia. The idea is very simple, basically it represents each entity as a vector, and also represents each context as a vector, and maximizes the cosine similarity between the two vectors using a negative sampling training objective. I am a bit concerned about the novelty of the approach. To me, this paper may be better for the NLP community but it should be fine to the ICLR community too. The entity typing results are very strong.
Reject. rating score: 3. rating score: 6. rating score: 6. <BRK>In this paper, the authors try to resolve the problem of estimating mutual information between high dimensional layers in neural network. In general, I think the introduction of the tensor kernels for mutual information estimation is the key contribution of this paper. However, I think this contribution is a little bit incremental, compared to the multivariate matrix based version introduced by Yu et.al. This approach does not capture the special structure of tensor and seems incremental. Also, the phenomenon of "turning point" on information plane for neural network has been challenged since its first publication. So an incremental change on computing such a questionable phenomenon make the contribution of this paper not very strong. So I would like to give a weak reject.<BRK>  SUMMARY  The paper...  suggests an estimation method of mutual information that is less susceptible to numerical problems when many neurons are involved  suggests an estimation method tailored to convolutional neural networks. Computing Renyi’s Entropy however necessitates to get a high dimensional density estimation over the data distribution  Giraldo et. An estimation for the mutual information can also be found in said paper  Yu et al.suggested a generalization of b) that can handle C variables instead of only two. A derived matrix A is defined similarly to Giraldo et al.This way, C individual kernels are defined, which are combined according to Yu et al.EXPERIMENTS  Comparison to Previous Approaches  Very detailed and clear structure of experiment. The authors conjecture that the compression phase is related to overfitting   it would be interesting to see some evidence to support this.<BRK>This paper concerns the "information plane" view of visualizing and understanding neural net training. (It also highlights that the method is easy to implement in practice). In general I am very skeptical that reasonable entropy like quantities can be estimated reliably for high dimensional data using anything along the lines of kernel density estimation, especially based on a single minibatch or small collection of minibatches! The authors provide no experimental evidence that these estimates are even close to being accurate (for example on synthetic datasets where the true entropy / mutual information is known). Clearly the estimated quantities evolve during training, and that may be interesting in itself, but calling the estimated quantities "mutual information" seems like a leap that s not justified in the paper. I think "marginal distribution" is incorrect. Which marginal would it be in any case?
Reject. rating score: 3. rating score: 3. rating score: 3. <BRK>This is a reasonable question to ask, and the authors performed a very large number of experiments attempting to answer this question. Looking at the results tables, I come to a different conclusion from the authors: there does not appear to be a significant difference between using a single head or using multiple heads (this is still an interesting conclusion). My second major concern is with the experimental set up. I think it would be very challenging to extend this approach to a broader set of tasks, however, as the authors suggest towards the end of the paper. Minor issues:I would be careful about claiming that you ve successfully captured regression in a span extraction format.<BRK>With this formulation, one can train a BERT based span extraction model (SpEx BERT) on classification, regression, and QA tasks without introducing any task specific parameters. Strengths:  Extensive finetuning/intermediate finetuning experiments on a range of NLP tasks. The paper is mostly well written and easy to follow. Weaknesses:  This paper presents a lot of experiments. The idea of expressing various NLP tasks (including textual entailment and text classification) as question answer has been well explored in decaNLP (McCann et al., 2018). It would be nice if the authors could elaborate more on how the proposed method differs from theirs.<BRK>This submission proposes a span extraction approach to unify QA, text classification and regression tasks. Overall, I find that the idea of unifying QA, text classification and regression is interesting by itself, but the experiments cannot justify their claims well mainly due to the mixed results. 1.The results are mixed.
Accept (Poster). rating score: 6. rating score: 3. rating score: 3. <BRK>The authors propose a simple but effective strategy that aims to alleviate not only overfitting, but also feature degradation (oversmoothing) in deep graph convolutional networks (GCNs). Inspired by dropout in traditional MLPs and convnets, the authors clearly motivate their contribution in terms of alleviating both overfitting and oversmoothing, which are problems established both in previous literature as well as validated empirically by the authors.<BRK>I also like the discussion in sec 4.3 where the authors explicitly clarify what are the difference between DropEdge, Dropout and DropNode, as the other two are the methods that will pop up during reading this paper. This paper is clearly well written and the authors conducted a comprehensive study on deep GCNs. Are there any reasons that we would like to use deeper network as opposed to shallower networks?<BRK>The authors propose a simple and interesting strategy, DropEdge, to alleviate the over fitting and over smoothing in GCN. The logic is simple and clear and the paper is well written. 7.In Table 2, why there are the DropEdge versions for some methods while not for some other methods (e.g., FastGCN, ASGCN)? Why there is no result of GAT?
Reject. rating score: 3. rating score: 3. rating score: 3. rating score: 6. <BRK>This papers studies how to explore, in order to generate experience for faster learning of policies in context of RL. RL methods typically employ simple hand tuned exploration schedules (such as epsilon greedy exploration, and changing the epsilon as training proceeds). The paper does this by modeling this as a non stationary multi arm bandit problem. Arm (exploration strategy and hyper parameter) is picked according to the return. Why doesn t the proposed bandit algorithm not pick out the best hyper parameter? 4.Comparison with past works. I believe there are other existing works that should be cited and compared to.<BRK>The arms of the bandit are parameters of the policy such as exploration noise, per action biases etc. I think this paper is just below the acceptance threshold. 1.While I see the value in designing an automatic exploration mechanism, the complexity of the underlying approach makes the contribution of the bandit based algorithm difficult to discern from the large number of other bells and whistles in the experiments. For instance, the authors use Rainbow as  the base algorithm upon which they add on the exploration. The curated bandit does not seem to be doing any better than a fixed arm. I have a few more questions that I would like the authors to address in their rebuttal or the paper. 2.Can you elaborate more on the metric for measuring the learning progress LP? In this aspect, the auto tuner for exploration is a plug and play procedure in other RL algorithms.<BRK>The paper proposes an adaptive behaviour in order to shape the data generation process for effective learning. The author frame the modulations search into a non stationary multi armed bandit problem and proposes to adapt the modulations according to a proxy to the learning progress. The proposed proxy is simply the empirical episodic return. In other terms, how this proxy help to tradeoff between exploration and exploitation ? The modulation adaptation problem is framed into non stationary multi armed bandit problem but the authors present a heuristic to solve it instead of using provably efficient bandit algorithm such as exponential weight methods (Besbes et al 2014) or Thompson sampling (Raj & Kalyani 2017) cited in the paper. This introduces a biases in the estimate. I appreciate the thorough empirical results and ablation studies in the main paper and the appendix. They are really interesting. It is not explained in the main paper.<BRK>This goal is achieve by formulating the adapting scheme as a multi arm bandit problem with the actual "learning progress" as a feedback signal. The paper is well written and easy to be understood. The strength of this paper is that 1) the proposed method is new in the sense that it invents an automatic way for exploration. More explanations are needed.
Reject. rating score: 3. rating score: 3. rating score: 6. <BRK>A spectral domain based approach uses a eigenvalue decomposition form of a graph Laplacian (a symmetric matrix) and learning a filter that acts on the eigenvalue of a graph Laplacian while preserving eigenvectors of a graph Laplacian. Overall, I think the paper is well written, but I would suggest to present more meaningful justification why and when the octave GCN is better than the GCN.<BRK>2.Having marginal improvement on the three benchmarks is not that interesting. Overall the paper is written in a coherent and self contained way, where the paper clearly states the related work and the contribution of this newly proposed work.<BRK>Despite reading the paper multiple times,  I am not sure I have the background to know whether what is written is significant or not. I m aware of work on general semi supervised learning and see the much better performance of this approach compared to things like label propagation, but cannot say for sure whether the idea is novel/significant.
Reject. rating score: 1. rating score: 1. rating score: 6. <BRK>This paper presented an image data that are generated from two variables using some physics law. The method, in general, utilize the general idea that the causal direction is easier for the model to describe than the anti causal direction. 3.The method cannot be scaled to more than two variables even with all components given as it requires exponentially many trials of the method. This setting is not so interesting anymore with image input. 4.There is much related work with causality and representation learning also causality with NN or VAE. The math is not very rigorous in general.<BRK>The paper contains some potentially interesting ideas, but the presentation quality is not sufficient for publication. Strengths:  Causality is an important and established research area, and papers on this topic would be timely. Paper contains some interesting ideas to integrate causality into an auto encoder (but see weaknesses below)  Paper proposes a new dataset for evaluating causal mechanisms (but the approach is not evaluated)Weaknesses:  The quality of the writing is inappropriate for a scientific venue.<BRK>The paper also discusses intervening in this graph. Quantitative results would also improve the paper. Visual inspection of the generated images is also used for assessing the quality of the models. The qualitative results provide support for the contributions that could be strengthened. The dataset is also an interesting contribution and it is a good idea to give it visibility.
Reject. rating score: 1. rating score: 3. rating score: 6. <BRK>This work addresses the important problem of generation bias and a lack of diversity in generative models, which is often called model collapse. It proposed a new metric to measure the diversity of the generative model s "worst" outputs based on the sample clustering patterns. Furthermore, it proposed two blackbox approaches to increasing the model diversity through resampling the latent z. Unlike most existing works that address the model collapse problem, a blackbox approach does not make assumptions about having access to model weights or the artifacts produced during model training, making it more widely applicable than the white box approaches.<BRK>Rather than using the labels, the authors use an off the shelf model (on faces) to provide a space on which to measure distances between generated images. The paper could benefit from more clarity. In addition, some explanations in this work are very hard to parse, e.g., the first paragraph of the methods section. I m not sure why ignore white box methods: at least it would be good verification that this method works (e.g., across methods, common measures of collapse used in those settings, etc)page 3The first paragraph of Section 3 is very difficult to understand. (this is not explained, and should be)The main link missing in the test proposed is that to mode dropping. The problem is that this measure wont detect mode dropping if there s aren t samples from those modes to measure anything against.<BRK>This paper presents a set of statistical tools, that are applicable to quantitatively measuring the mode collapse of GANs. The authors consistently observe strong mode collapse on several state of the art GANs using the proposed toolset. The authors analyze possible causes, and for the first time present two simple yet effective “black box” methods to calibrate the GAN learned distribution, without accessing either model parameters or the original training data. The writing and presentation are good. 2) The main contributions of this paper are not quite clear to me.
Accept (Poster). rating score: 6. rating score: 6. rating score: 6. rating score: 6. <BRK>C. ArgumentationThe paper clearly states the problem and what are the contributions. This is simpler and more efficient than existing alternatives. 3. they propose to train the real to binary model using a multi stage teacher student procedure.<BRK>This paper studies the problem of training binary neural networks. A workflow/framework/algorithm description is helpful to better understand the whole framework, and the methodology part in Section 4 requires more details.<BRK>This paper is on building binary network. All these components together makes a strong binary network. The outcome of the model is quite impressive  5% improvement over the best binary model.<BRK>This paper greatly reduces the gap between binarized and real valued imagenet, using a variety of techniques. text is shifting back and forth between using % and fractional gap to describe benefits.
Accept (Poster). rating score: 8. rating score: 8. rating score: 3. <BRK>4) Can we quantify modularity or the extent of disentanglement of internal representations under such a framework? The paper is extremely well written with a good balance between mathematical notation and intuitive explanations. I think this paper should certainly be accepted as it provides an interesting and rigorous tool to understand the behavior and properties of deep generative. Does it vary for GANs versus beta vae like models?<BRK>I highly recommend this paper for acceptance. There is an additional result relating modularity of a subset of neurons to disentanglement. This is very important (and motivates them to group neurons in the model that are statistically dependent) and I cannot agree more with the authors on this point. This paper has many original ideas and substantial novelty.<BRK>Whereas the theory presented has a lot of potential, it seems that the clarity of the paper could be greatly improved, in particular I would have liked more of the formal theory to be included in the body of the paper instead of relying only on the appendix. It would also be interesting to contrast with the following paper: Robustly Disentangled Causal Mechanisms: Validating Deep Representations for Interventional Robustness by Bauer et al.which (whilst doing something pretty different) also treats of causality and disentanglement. It should be made clearer from the start that disentanglement is here only a property of the transformations and that the authors are not trying to define a disentangled internal representation.
Reject. rating score: 1. rating score: 1. rating score: 3. <BRK>This paper presents an approach to learning exploration strategies for contextual bandits by meta learning the exploration strategy across synthetic tasks. The main issue with this paper is that the presentation of the algorithm is vague and it is difficult to determine what the algorithm actually does. How are they related to the other tasks you evaluate on? It seems like the choice of synthetic tasks would be important to what the algorithm learns, so it s important to be clear on what these tasks were and how closely they match the target tasks. However, F is not really defined. The algorithm is dependent on a function class F and PolOpt which finds an f in F with low expected regret. In Section 5.1, the policy is using a probability distribution over actions from f, the entropy of the predicted probability distribution, and a one hot encoding for the predicted action f(a). What do you do with the other data? 2.Section 3.1 describes the test time behavior of MELEE. However, in the equations, pi is a function of x_t. Is this policy taking the action with the highest reward at each state?<BRK>This paper proposes a meta learning algorithm to solve the problem of exploration in a contextual bandit task using prior knowledge. The training step for the exploration policy builds upon the AggreVaTe algorithm, where policy optimization is performed on the history augmented data by using a separate roll out policy to estimate the advantage of a particular action for a particular context, from the point of view of regret minimization. Some of my questions are:1. The method seems a bit hacky to me, for example, it requires calibration of f, requires access to test time examples, and it is unclear why this should be needed if an algorithm were provably good. The theory section says " In particular, we first relate theregret of the learner in line 16 to the overall regret of $pi$", but line 16 in the algorithm refers to "end for". I feel that the paper tries to theoretically analyse an interesting problem, however, at this point the paper is very hard to follow and not complete. Also, it would be good if the authors can explain the significance of the results at the cost of added complexity.<BRK>•	Summary    This paper introduced a meta learning algorithm for the contextual bandit problem, MELEE, which learns an exploration policy based on simulated and synthetic contextual bandit tasks. The algorithm takes the action in an  greedy fashion, i.e.with probability  it will follow the suggestion and with probability  it will sample it uniformly at random. The procedure in step one is proposed to be done in  rounds. In step two, the training set is used for training an exploration policy  . The algorithm suggests the action to explore in an  greedy fashion. 2) the experiments are difficult to understand with only citation to publications. Detailed information of the datasets, tasks, procedures is missing. It refers to the imitation algorithm, AggreVate, which is an instantiate of meta learning for contextual bandits. These results may require deeper investigations. I had a hard time to understand what the function  means.
Accept (Spotlight). rating score: 8. rating score: 8. rating score: 6. <BRK>~The authors build a new method to recapitulate the 3D structure of a biomolecule from cryo EM images that allows for flexibility in the reconstructed volume.~I thought this paper is very well written and tackles a difficult project.<BRK>The authors introduce cryoDRGN, a VAE neural network architecture to reconstruct 3D protein structure from 2D cryo EM images. The paper offers for a good read and diagrams are informative.<BRK>  The authors proposed a novel method for cryo EM reconstruction that extends naturally to modeling continuous generative factors of structural heterogeneity. 4) How is the proposed method generalizable? The problem and the approach are well motivated.
Reject. rating score: 3. rating score: 3. rating score: 6. <BRK>Top k algorithm is a gradient sparsification method which gains its popularity to train deep networks due to its high compression ratio. However due to its computation overhead, it is not efficient on GPUs. This paper performs empirical study on the distribution of the gradients when training various deep networks, and provides a Gaussian approximation analysis to improve the convergence of the top k algorithm. The numerical results are specific with k   0.001d, which makes it hard to see if the Gaussian k algorithm would still work using different k/d ratio. It is worth further investigation of the robustness of this algorithm as a replacement of the top k algorithm.<BRK>This paper makes two contributions to gradient sparsification to reduce the communication bottleneck in distributed SGD. Table 1 "experimental settings": how these values were chosen. 2) The authors note that the top k cannot benefit from the highly parallel architectures popular in ML, and propose an approximate top k sparsification operator. Figure 1: which k was used in these plots? I would also urge the authors to make it more clear that their theoretical results are based on a strong assumption on the distribution of gradients. SOTA validation accuracy for the Imagenet benchmark with Resnet50 is around 76%. 13) Figure 6: Imagenet training scheme is not standard. I would urge the authors to make this very clear.<BRK>This paper  empirically investigates the distribution of the gradient magnitude in the training of DNNs, based on which a tighter bound is derived over the conventional bound on top K gradient sparsification. The authors also propose a so call GaussianK SGD to approximate the top K selection which is shown to be more efficient on GPUs. Experiments are carried out on various datasets with various network architectures and the results seem to be supportive. Overall, I find the work interesting and may have real value for communication efficient distributed training. My sense is that it can be more rigorous than its current form. It is not clear to me why the experiments are not conducted using a consistent distributed setting.
Reject. rating score: 1. rating score: 6. rating score: 6. <BRK>This paper proposes to learn a kernel for training MMD GAN by optimizing over the probability distribution that defines the kernel by means of random features. It is easy to adaptively learn the bandwidth as well:  in this case, it will be just an additional parameter of a discriminator network. This was done in [Arbel2018] where a single gaussian kernel is used and a regularization of the critic allows to learn the bandwidth without manual tuning. Does the proposed method offer an additional advantage compared to those? Mnist and Cifar10 are somehow very simple, what would happen on more complicated datasets (CelebA or imagenet)?<BRK>The paper addresses the problem of kernel learning in MMD GAN using particle stochastic gradient descent to solve an approximation of the intractable distributional optimization problem for random features. 2.For a more detailed analysis of performance, it would be helpful to see the benefits of the kernel learned with the proposed method on synthetic data and its performance on supervised learning tasks compared with other kernel learning methods on supervised tasks.<BRK>This paper aims to improve the kernel selection issue of the MMD based generative models. The author formulates the kernels via inverse Fourier transform and the goal is to learn the optimal N finite random Fourier features (RFF). Does this guarantee the learned spectral distribution lying in the constraint set P as specify in Eq (8)? Experiment results on the IS score and FID on CIFAR 10 show improvement of the proposed methods over MMD GAN baselines, while the results are not comparable to the original MMD GAN due to unknown results.
Reject. rating score: 1. rating score: 3. rating score: 6. <BRK>SummaryThis paper proposes using Wasserstein Distances to measure the difference between higher level functions of policies, which this paper terms as "behaviors". I think the theoretical underpinnings of these paper are extremely motivating and interesting. However, the current state of the empirical section of the paper leaves too much open for me to be able to recommend an accept at this time. I think the connections between the proposed family of algorithms and WD based TRPO and distributional RL is highly motivating. The results show only behavioral studies of the algorithm, not investigating the effects of the WD based regularizer or the effects of the choice of behavioral map. It does seem like a minor disadvantage that distributional RL is well defined in both the on policy and off policy cases, but the proposed family of methods is not. One of the key contributions of this paper is the ability to define regularizers based on the definition of the behavior map.<BRK>This work explores two uses of Wasserstein distances (WD) within reinforcement learning: the first is a variant of policy gradient, where WD is used to guide the policy search (instead of alternative such as Trust region used in TRPO); the second is a variant of evolutionary search where WD is used again to guide the policy updates. In this paper, the behavioral embeddings are assumed to be given; it would be interesting to discuss/explore learning these embeddings. It would be helpful to have a discussion of the complexity (both data & compute) of both algorithms. This is much harder to follow for an RL researcher, and would be improved by adding some intuition relating the material presented to the concepts of Sec.3.<BRK>Summary: this paper proposes a new regularized policy optimization (PO) method which is based on Wasserstein distances. Its idea is to use SGD to optimize the dual form of the WD, then used in two different policy search approaches TRPO and Evolution Strategies. Overall, the paper studies an interesting problem in RL. I have some following concerns about it. The idea of using behavior embedding is new in PO. Besides, they seem not to reappear in other places in the paper. In addition, how BGPG is compared with other related work that also uses skill/policy embedding, instead of a flat PO approach like TRPO. Similar questions are also applied to the experiments for BGES.
Accept (Talk). rating score: 8. rating score: 8. rating score: 8. <BRK>They lay out the details of their framework and motivate the need for leveraging monolingual data in both source and target directions. Finally, they perform experiments on low and high resource tasks. This additional complexity might outweigh the gains obtained in some cases. Hence, I want to see this paper accepted. al who also share al parameters and vocab in a single model.<BRK>This work proposes a new translation model that combines translation models in two directions and language modelss in two languages by sharing a latent semantic representation. The basic idea to joint modeling of translations conditioning on the latent representations and the parameters are learned by generating pseudo translations in two directions. It is an interesting work on proposing a unified framework to translation by conditioning on a shared latent space in four models. It is not only rivaling heuristic methods to generate pseudo data, but surpassing competitive Transformer baselines.<BRK>This paper proposes an approach to neural MT in which the joint (source, target) distribution is modeled as an average over two different factorizations: target given source and source given target. The model is trained on parallel data using a standard VAE approach. I think the paper should be accepted. Although the gains over the baselines are not overly compelling, they are quite systematic, indicating that the advantage is probably real, albeit slight. Please add implementation details for the Transformer+{BT,JBT,Dual} baselines.
Reject. rating score: 1. rating score: 8. rating score: 8. <BRK>The work contributes a library emulating Atari games in GPU in parallel and allowing to speed up the execution of reinforcement learning algorithms. I tend to think that this work will not very much boost the research for new RL methods.<BRK>They show this hardware scaling can be taken advantage of across a variety of state of the art reinforcement learning algorithms and indeed create new batching strategies to utilize their framework and the GPU better. The paper is well written and goes into some detail describing the implementation of CuLE as well as various design decisions taken, as with splitting the emulation process across several kernels. Finally, the paper is very explicit about a number of optimizations that are not being exploited by the new framework and serve as markers for future work.<BRK>My first reaction to this paper was, "So what? It was done very thoroughly and it provides deep insight in the challenges that RL faces for both learning and inference in a variety of settings. I especially liked the analysis of the advantages and limitations of GPU emulation. The paper would be better if:1) The figure fonts were larger throughout the paper.
Reject. rating score: 1. rating score: 3. rating score: 3. <BRK>However, arguing for the importance of this special case would require focused experimental comparison and analysis, which is not present in the current version of the paper. Novelty is clearly limited in light of this overlooked prior work. This paper proposes to modify a standard CNN by requiring all of its layers to share the same filter set, essentially allowing it to be expressed as an iterative (or recurrent) network.<BRK>in multiple places in the text, you refer to the number of "independent" parameters. The idea is motivated by wavelet decompositions and related work. If the authors can show that the same filter applied L times achieves about the same performance, why not also experiment with different L?<BRK>This paper presents an approach to reduce the number of a neural network by sharing the convolutional weights among layers. However I could not find the performance of that interesting baseline in the results. In this paper the setting are slightly different, authors add also a variant with additional 1x1 convolutions and show also results with additional compression.
Reject. rating score: 6. rating score: 6. <BRK>However, there are some minor concerns:[1] The consistency of a sample is measured based on the perturbed samples. This paper proposes a semi supervised active learning method to reduce the labeling cost. In the paper, it said that these samples are generated by standard augmentation operations (e.g.random crops and horizontal flips for image data). This representation is hard to follow in the experiments.<BRK>This paper proposes a new combination method for active learning and semi supervised learning, where the objective is to make predictions that are robust to perturbations (for SSL) and select points for labeling with labels that differ under perturbations. The authors state that they lose 1.26% accuracy to the fully supervised model. However, this is very much not within the margin of measurement error and 1.26% accuracy is rather significant for accuracies around 95%.
Reject. rating score: 3. rating score: 3. rating score: 6. <BRK>This work proposes an attention based prototype learning algorithm, which introduces an attention operation to assign different weights to the prototypes. Comprehensive experiments demonstrate that the proposed method is efficient and effective in various tasks. The authors clarified that ProtoAttend is an inherently interpretable algorithm. However, the interpretability is proved by naïve Prototype learning only. I would be appreciated if the authors provide the pseudo code to show the training procedure of ProtoAttend.<BRK>This paper presents a sample based self explaining method for image classification. For example, some crowdsourcing experiments to check if the provided prototypes can help human users correctly guess the model prediction. The classification decision is based on the label consistency between the identified prototypes (with the relation score in attention mechanism as the weight of different prototypes in determining the  label agreement)The proposed model is intrinsically interpretable since the prototypes with higher weights can play as the decision explanation. But I have several concerns regarding the choice of prototypes and the evaluation of the interpretation:1) According to Eq.(2), it seems that all training samples are used as the prototypes (but with different weights).<BRK>The aim of this work is to make deep learning classifiers more interpretable by "projecting" each input sample into a small collection of prototype examples (with some weighting over those) and then basing the decision on a combination of the latent representations of the chosen prototypes. In this way, the chosen category can be justified as the input being similar to the selected prototypes. The choice fo the encoders for the prototypes and the examples is asymmetric (the first using keys and the second queries). This is not justified. The decisions made to accomplish these are reasonable although somewhat arbitrary. In fact, several ways to encode the desiderata in the loss function are listed in Table 1.
Accept (Poster). rating score: 8. rating score: 6. rating score: 3. <BRK>This paper proposes a new visualization tool in order to understand the behavior of agents trained using deep RL. Specifically, they train a generative model of game states, and then optimize an energy based distribution over state embeddings according to some target function, and then by sampling from the resulting distribution they create a diverse set of realistic states that score highly according to the target function. They propose a few target cost functions, which allow them to optimize for states in which the agent takes a particular action, states which are high reward (worst Q value is large), states which are low reward (best Q value is small), and critical states. They demonstrate results on Atari games as well as a simulated driving environment. It would be both more compelling and more interesting to use the results to *fix* problems detected in the agents. For example, Section 4.4 suggests that the Seaquest agent has not learned that it should surface when the oxygen is low. Having done this analysis, can we then fix the problem?<BRK>The authors propose learning a generative model of states to visualize the behavior of different RL agents. This energy based model aims to sample regions of state space that optimize some target function T. For example, the target function may be "Q value of moving left", in which case the model should generate states s where moving left is expected to be high value. They examine a number of target functions, for maximizing / minimizing the Q value of different actions, maximizing the spread of Q values (max_a Q(s,a)   min_a Q(s,a)), and other saliency approaches. Experiments on Atari and a 3D driving simulator demonstrate that visualized images are qualitatively reasonable, and the states generated aren t just doing nearest neighbor over states in the training set. The approach seems reasonable and the experiments seem reasonable as well. However, I contest the claim that this is one of the first works visualizing and diagnosing RL agents. * Why is the gradient saliency measured in L1 rather than L2 distance? * I buy the results showing the VAE learns to generate novel states. * At a style level, I would not describe this paper as specifically a visualizing weaknesses paper   instead it is more like a framework to learn to generate states that satisfy some predicate of the Q function (as noted by experiments that try to identify critical states, especially positive states, etc.), and I would consider renaming the paper accordingly. If the environment is truly as complicated as stated, then these baselines should be very clearly bad and make a better case for the proposed contribution.<BRK>SummaryThis paper proposes a generative technique to sample "interesting" states useful for analyzing the behavior of deep reinforcement learning agents. In this context, the concept of "interesting" is defined via user specific target functions, e.g.states that arise as a consequence of taking specific actions (such as actions associated with high or low Q values for example). The approach is evaluated in the Atari domain and in an autonomous driving simulator. You talk about optimization objectives, then please specify what the optimization arguments are this is not clear from the description given. The optimization objectives chosen by the authors seem very ad hoc to me and how the motivation relates to the objectives is hard to comprehend (see my Clarity section). The experimental results have very low quality as well results are mainly depicted as images with a verbal explanation. Some pseudocode would be really helpful here. The proposed modification puts emphasis on states where the norm of the policy gradient is high, which is different from putting emphasis on specific regions in the image. OriginalityThe idea of visualizing states that reveal interesting insights about an agent s behavior based on a user defined target function sounds interesting.
Reject. rating score: 3. rating score: 3. rating score: 6. <BRK>SummaryThis paper presents Gradient Rescaling (GR) for robust learning to combat label noise. 3.Can GR be used simultaneously with other noise robust learning methods to further boost the performance? The authors claim that the performance of GR exceeds various baselines. The overall flow of the paper is a bit fuzzy   exhibiting a stream of consciousness style flow. 2.No justifications (both theoretical and experimental) are provided on the claim that controlling emphasis focus/spread will result in more robust learning. 3.This algorithm introduces 2 additional hyperparameters that are correlated with each other. 8.Additional benchmarks of most recent noise robustness algos such as <Lee et al.2019 ICML> are required.<BRK>Summary:The authors first analyze and answer the question: What training examples should be focused and how large the emphasis spread should be? Then, they proposed the gradient rescaling framework serving as emphasis regularization. The paper is well organized except the reference citation (read difficultly)2. The proposed method is very simple and effective. The experiments lack the recent important baseline "symmetric cross entropy for robust learning with noisy labels, ICCV2019", which are the current SOTA. Maybe the author should check the above paper and show the results. The author should conduct at least some experiments on asymmetric noise.<BRK>Summary:The paper proposes a method for noise robustness based on scaling gradients of examples. By choosing the proper scaling parameters (alpha and beta), the method recovers standard losses such as CCE, MAE, and GCE, while also recovering other losses. * It would be nice if the CIFAR 10 Table 4 results are performed using the exact same setting to prior work to make sure the comparison is fair. Comments:  The main drawback of the proposed approach is that there is no clear way of choosing alpha and beta, other than a hyper parameter search, which is not very practical and can lead to overfitting the test set.
Reject. rating score: 3. rating score: 3. rating score: 3. <BRK>In such attacks, an adversary can alter/flip the labels of some of the training examples. The paper proposed a new approach towards certified robustness against this type of attack. The authors use randomized smoothing on a binary linear classifier with logistic loss and deduce a radius of certification that is a function of the probability of flipping labels in the training data and the probabilistic separation p.Major concerns. However, the proposed certification is on the flip rate of the labels in the training data. The only potential relevance of such a problem is upon training models in a federated approach (online learning) even then, one can argue that the portion of the data that the adversary has access to is a small portion to the complete dataset. If I understand Figures 1,2,3 correctly, then what the authors do is that they train networks over different number of label flipping (shown as a percentage on the top of the figure). Then for each test example, they compute the maximum radius given in Eq 10 for multiple qs. Will the certified accuracy at a given percentage of the data be the highest for q that match the flip rate?<BRK>An adversary in such a setting is permitted to flip any r labels from a dataset of size n. The smoothing procedure (stated roughly) is to train on a dataset with "noisy" or "smoothed" labels, obtained by flipping each label with some probability q. The authors obtain a lower bound on r in terms of q. Directly using this technique requires training multiple classifiers on multiple noisy datasets. To show that this method is useful, the authors study the effectiveness of this model against a classifier that performs linear regression on a pre trained feature extractor. The novelty in this paper is that it considers randomized smoothing defenses for data poisoning (label flipping) attacks, as opposed to perturbation based attacks. While the paper was an enjoyable read, I recommend rejecting the paper due to the following shortcomings that:(1) The paper is essentially studying (a variant of) linear regression.<BRK>This paper leverages the randomized smoothing technique, on labels of images during training, to counter the adversarial label flipping attack. The authors proposed a strategy to build classifiers that are certifiably robust against a strong variant of label flipping attack that can target each test example independently. The resulting classifier can make a prediction and includes certification for each test point. I have the following questions about this work:1. ResNets Ensemble via the Feynman Kac Formalism to Improve Natural and Robust Accuracies, arXiv:1811.10745, NeurIPS, 2019Overall, this paper studies an interesting and important certified adversarial defense against label flipping attack problems with a focus on certification on each test data, but more experimental verification is needed. Please address the above questions in rebuttal.
Reject. rating score: 1. rating score: 3. rating score: 3. <BRK>This work proposes a new architecture for abstract visual reasoning, based on Transformer style soft attention and relation networks. The authors test their network on the PGM dataset, and demonstrate a non trivial improvement over previously reported baselines. The paper is reasonably well put together, and I have no reason to question the various technical aspects of the work.<BRK>The model is based on the transformer network, which performs relational reasoning through its self attention mechanisms. The technical novelty of the proposed approach is unclear. This result suggests that the proposed ARNe model does not work well when training with weaker supervision without meta targets. The results could be a lot stronger if the authors show ARNe outperforms the prior work when beta is set to 0. Ablation studies:This model is only tested in the neutral PGM dataset.<BRK>This paper describes a somewhat novel approach to abstract visual reasoning using transformers in the so called "Attention Relation Network" (ARNe), which the authors show to improve on the "Wild Relation Network" (WReN). The paper is well written and makes an interesting contribution, but I feel the results are not quite yet ready for publication.
Reject. rating score: 3. rating score: 6. rating score: 8. <BRK>The paper considers the meta learning in the task un segmented setting and apply bayesian online change point detection with meta learning. The task un segmented is claimed to exist in real applications and the paper explains the idea in a clear way. 2) The part I feel most confused are the experiments. 3) The Figure 1 is hard to read. The hazard rate is not defined before the experiment which makes Figure 3 also hard to understand. It claims MOCA can be used with any meta learning algorithms and how does it show in experiments? And if the problem has task segmentation, why not using traditional meta learning methods, as shown as oracle methods with better performance in paper? Since the major contribution of the paper is providing a meta learning method to work in the problems where task segmentation is unavailable, not having an experiment in this setting (withholding segmentation information does not exactly fall in this setting because it adds a condition that the task segmentation information is originally accessible) is a major reason for my current evaluation.<BRK>This paper pushes meta learning towards task unsegmented settings. Different from the traditional offline meta learning phase with explicit task segmentation, MOCA adopts a Bayesian changepoint estimation scheme for task change detection. The setting is novel and deserves research in depth, and the idea is easy to understand. Are experiment results sensitive to it? Lastly, I think [1] should be cited as related work about continual learning for proposing task free continual learning, which is very similar to the setting in this paper. [1] Rahaf Aljundi, Klaas Kelchtermans, Tinne Tuytelaars. Task Free Continual Learning.<BRK>The paper proposes a solution for using meta learning methods without the need of determining the task segmentation a priori. Authors identify that the task segmented setting is very similar to the problem of change point detection (CPD) and particularly, they connect the generative model of the meta learning approach to the Bayesian recursive method of Adams and MacKay 2007. There is a noticeable effort of describing the solution for both regression and classification problems and the empirical results conclude a positive performance of MOCA. Overall, I consider that the paper is well written with thorough explanations. Of course, there are some details that could be improved (I will comment this later). The contribution of the paper is significant to meta learning models and I think it will help to spread the Adams’ model to other type of problems. There is an important contribution in the paper that I have to mention and it is also relevant for the future application of the Adams model. If one reads the original BOCPD paper, in particular Eq (1), where the predictive posterior p(x_t+1|x_{1:t}) is defined from a marginalisation over the run length values, it is noticeable that this equation is not used in the final recursion of the CPD method. Surprisingly, I find that the authors have find a practical use of this equation (Eq.(7)) and it is in the main core of the meta learning algorithm, this is fantastic. At least specify that is a toy variable for the explanation. Why not reusing one of them or say the change from one term to another?
Reject. rating score: 3. rating score: 3. rating score: 8. <BRK>To this end, it describes a GAN type model for generating realistic images, where the generator disentangles shape, texture, and background. The approach is evaluated only on a single dataset and is not compared to any baselines. There are no quantitative results in the paper, and it is difficult for me to judge how good the method is given only qualitative examples from a single dataset. [1] Rezende et. al., "Unsupervised Learning of 3D Structure from Images", NIPS 2016. al., "HoloGAN: Unsupervised learning of 3D representations from natural images", ICCV 2019. However, I still think that the exposition could be significantly improved, as eg.<BRK>2) A missing very relevant citation of HoloGAN by Nguyen Phuoc et al.[1].It is not yet published, but has been on arXiv for some time. I hope the authors are right that the method will work on other classes after some tuning, but this is not demonstrated in the paper. [1] HoloGAN: Unsupervised learning of 3D representations from natural images. I think the paper looks promising and after further improving the experimental evaluation it can become a great publication. The paper proposes an approach to learning the 3D structure of images without explicit supervision. The proposed model is a Generative Adversarial Network (GAN) with an appropriate task specific structure: instead of generating an image directly with a deep network, three intermediate outputs are generated first and then processed by a differentiable renderer. The method is applied to the FFHQ dataset of face images, where it produces qualitatively reasonable results. cons:1) The experiments are limited.<BRK>Hence, it might not be good to claim  in the abstract: "the first method to learn a generative model of 3D shapes from natural images in a fully unsupervised way",  in introduction: "For the first time, to the best of our knowledge, we provide a procedure to build a generative model that learns explicit 3D representations in an unsupervised way from natural images",  and similar claims in other places. DECISION: This paper has very promising results. Although it is limited to faces, which the community knows is something GANs are good at modeling because of the inherent structure, it is nevertheless a relevant piece of work in modelling 3D scenes in a graphics way and then training using adversarial learning. However, it is more pertinent to check how the model performs objects more complicated than faces. A very simple experiment is to try this on ImageNet images, which are also centered and aligned.
Accept (Poster). rating score: 6. rating score: 6. rating score: 3. <BRK>Under this framework, the author argues that the ranking loss could outperform the state of the art on the Atari benchmark. The overall idea is intuitive yet interesting, and the empirical result is quite impressive. Some questions which I think the paper could discuss more:  In the paper, the near optimal policy is defined with an absolute threshold, which could be task/environment specific. I am wondering whether the author tried to set the `near optimal policy` as a relative value (in the current replay buffer).<BRK>I think this is an interesting paper and approach, however some issues remain. Also I m not sure it would make sense in a sparse reward environment, or an environment with small reward leading up to a single large reward, since the trajectories that get small reward might be discarded erroneously. The authors need to really demonstrate that the benefit is coming from their algorithm and *not* just the re use of replay data that the baselines don t get to see as much of. Does the policy in (2) sum to 1? Probably worth mentioning it here too.<BRK>The end goal in policy learning is to achieve the right ranking of actions at a state (in the case when deterministic policies are optimal), and the paper proposes a method of doing this inspired from the work on learning to rank. They further argue that in the case with stochastic optimal policies, REINFORCE with softmax policies is rank wise optimal, which is not surprising, but at the same time interesting as well. This appears already in past works (which the paper cites in the appendix) and the assumption of the existence of UNOP seems strong. Also, can the same Q learning based method perform better if one controls for the number of gradient updates? I would also appreciate some more clarity in terms of writing and presentation.
Accept (Spotlight). rating score: 8. rating score: 8. rating score: 6. <BRK>The paper proposes a measure of classes of algorithmic alignment that measure how "close" neural networks are to known algorithms, e.g.dynamic programming (DP). I recommend this paper be accepted, since I think it s an important direction of research, and it formalizes a lot of intuition about neural network architectures. It would be very interesting if the authors could actually compute the number of samples, M, for different NN architectures on the toy datasets, and show how it matches empirical findings.<BRK>This paper presents a framework, dubbed algorithmic alignment, based on PAC learning and sample complexity, with the aim to explain generalization on reasoning tasks for different neural architectures. The framework roughly states that in order for the model to be able to learn and successfully generalize on a reasoning task, it needs to be able to easily learn (to approximate) steps of the reasoning tasks. The paper, though dense, is well well written, and carries an interesting conclusion that better algorithmic alignment brings the sample complexity down, i.e.models with better algorithmic alignment to the task (function they want to approximate) should generalize better.<BRK>Unfortunately I am not well versed in the theoretical literature on this topic, so my assessment of the proofs is limited, and I will need to defer to the other reviewers on these matters. The authors build on a body of research that demonstrates the usefulness of different neural network architectures for different reasoning problems. This work seeks theoretical and empirical proof of the reasoning capacity of neural networks.
Reject. rating score: 3. rating score: 3. rating score: 6. <BRK>This paper uses reinforcement learning for automated theorem proving. The proposed method aims to generalize the short proofs to longer proofs with similar structure. Some claims of the paper are not clearly validated by the reported experimental results. It would be nice if there was a clear explanation of the role of curriculum in the learning algorithm. How does that help in the learning?<BRK>The algorithm casts this training task as a reinforcement learning problem, and employs curriculum learning and the Proximal Policy Optimization algorithm to find appropriate neural network parameters, in particular, those that make the prover good at finding long proofs. The proposed prover is tested against existing theorem provers, and for the authors  dataset, it outperforms those provers. I found it difficult to make up my mind on this paper. Also, I liked a qualitative analysis of the failure of the curriculum learning for tackling hard tasks in the paper. Another thing that demotivated me is that I couldn t find the discussion about the subtleties in using curriculum learning and PPO for the theorem proving task in the paper. When I read the experimental result section, I couldn t quite get this sense of huge improvement of the proposed approach over the existing provers. * p2: The related work section is great.<BRK>1.It s an important problem and not much work is done on it. However, I have no idea if e.g.the baselines used in this paper are reasonable, soI would appreciate someone with more experience on that topic weighing in. Some limitations of the paper:1. It seems like mostly an application of existing RL techniques to an ATP environment. 2.The writing is not particularly clear, and could use substantial editing (but thisis something that could be fixed during the discussion period). 3.The focus on Robinson arithmetic seems kind of limiting.
Accept (Poster). rating score: 6. rating score: 6. rating score: 6. <BRK>However, I have some questions to be clarified by the authors as follows. Given all these conditions, I think I make it clear why I give a rating 6 currently. GANs often converge to an LSSP than an LNE, but still, achieve good results. Generally, this paper is interesting and well written.<BRK>I have listed more detailed feedback below. This should be explicitly stated in the paper. The work uses these techniques to visualize the dynamics of GANs trained on standard datasets. The finding that GAN training methods do not find local Nash equilibria is interesting.<BRK>While simple, I found it interesting to evaluate the eigenvalues of the relevant quantities at convergence. This is an important observation and  may cause some consideration of what points should be sought in GANs.
Reject. rating score: 3. rating score: 3. rating score: 6. <BRK>Specifically, the two methods in the paper are motivated by the idea that the K way D dimensional code can be viewed as a form of product quantization. Also I think the empirical performance of the proposed method is promising. E.g.assume K   2 D   2,C   [1 1;1 2;1 1; 1 2] is full rank (rank   2), but the corresponding B   [1 0 1 0 ; 1 0  0 1; 1 0 1 0 ; 1 0  0 1] is not full rank (rank < 4). This provides an alternative way to use product quantization based approach for embedding with low inference memory, without training together with task models. For improving the paper, the relatively minor comments are as the following:1.<BRK>The authors claim that the proposed differentiable product quantization framework has better compression but similar performance compared to existing KD codes.The authors present two instances of the DPQ framework: DPQ SX using softmax to make it differentiable, and DPQ VQ using centroid based approximation. NoveltyJust extending and making Chen et al., 2018b s distilling method to be differentiable has limited novelty. It is true that the parameters for embedding make up a large part of the overall parameters, but I would like some additional explanation of how important they are to learning. Also, why the distilling in Chen et al., 2018b is a problem? 4.Did you run all experiments just one time?<BRK>In this manuscript, authors improve the work in [1] by simplifying the reverse discretization function. The empirical study demonstrates the effectiveness of the proposed algorithm. I’m not familiar with the area. The differences between this work and [1] should be elaborated more in the related work, since they are closely related. Besides, for the technical part, DPQ SX outperforms DPQ VQ while the softmax approximation seems identical to that developed in [1].
Accept (Poster). rating score: 6. rating score: 6. rating score: 6. <BRK>This paper presents a method for jointly making physical predictions and inferring latent physical parameters (e.g.gravity) in an unsupervised manner from video. The results look very impressive compared to existing models as well. However, my main critique would be that the evaluation domains are somewhat simplistic.<BRK>This paper presents an approach for unsupervised estimation of physical parameters from video, using the physics as inverse graphics approach. This needs to be clarified in the main paper. A major limitation of the approach is the assumption that the equations governing the system are known.<BRK>The approach is fairly obvious at the conceptual level; but in the details poses a number of technical challenges especially for the decoder, which are nicely analysed and resolved. But showing only those gives me the impression that the paper was written too early, just to be the first and to make the deadline. The baselines are sensible and ablation studies are done with care.
Reject. rating score: 3. rating score: 6. rating score: 6. <BRK>First of all, the previous work in video predicted is typically formulated as "conditioned frame prediction" where the prediction of the next frame is conditioned on "a set of context frames" and there is no reason why this set cannot contain the goal frame. Hence, I think the authors should answer these two questions to clear up the motivation:1. Why conditioning on the goal frame is interesting? 2.Where the current conditional models suffer by conditioning on the goal image? Given my point regarding context frames, a more fair experiment would be to compare the proposed method with them when they are conditioned on the goal frame as well. The used metrics are not a good evaluation metric for frame prediction as they both do not give us an objective evaluation in the sense of the semantic quality of predicted frames. On quality of writing, the paper is well written but it can use a figure that demonstrates proposed architecture. The authors provided the code which is always a plus.<BRK>Summary: The following work proposes a model for long range video interpolation   specifically targetting cases where the intermediate content trajectories may be highly non linear. This is referred to as goal conditioned in the paper. They present an autoregressive sequential model, as well as a hierarchical model   each based on a probabilistic framework. Finally, they demonstrate an application in imitation learning by introducing an additional model that maps pairs of observations (frames) to a distribution over actions that predicts how likely each action will map the first observation to the second. Their imitation learning method is able to successfully solve mazes, given just the start and goal observations. Strengths: The extension to visual planning/imitation learning was very interesting Explores differences between sequential and hierarchical prediction modelsWeaknesses/questions/suggestions: In addition to SSIM and PSNR, one might also want to consider the FVD and LPIPS, both which should correlate better with human perception. I think a fairly important unstated limitation is that latent variable based methods tend not to generalize well outside of their trained domain. ** Post Rebuttal:The authors have adequately addressed my concerns regarding clarity and metrics.<BRK>They propose two variations of their method: A sequential and a tree based methods. The tree based method enables efficient frame sampling in a hierarchical way. In experiments, they outperform the used baselines in the task of video prediction. I would be good to compare against them for better assessment of the predicted videos. Bottleneck discovery experiments (Figure 8):The visualizations shown in Figure 8 are very interesting, however, I would like to see if the model is able to generate multiple trajectories from the same frame. Conclusion:This paper proposes a novel latent variable method for goal oriented video prediction which is then used to enable an agent to go from point A to point B. I feel this paper brings nice insights useful for the model based reinforcement learning literature where the end goal can be guided by an image rather than predefined rewards. It would be good if the authors can include the suggested video prediction baseline from Wichers et al., 2018 in their quantitative comparisons.
Reject. rating score: 1. rating score: 1. rating score: 3. <BRK>This seems necessary because it is the only realistic dataset evaluated in this work. * The choice of curriculum learning task is arbitrary, and there are no ablations explaining why this is a reasonable task. Substantial work needs to be devoted to expanding experimental coverage.<BRK>As such, there is a lot more recent net architecture work for semantic segmentation that should be directly applicable, and should perform much better than CaseNet when adapted to the task. As a result, the experiments and the significance of this paper are rather marginal.<BRK>The authors consider a network trained for class specific edge detection (e.g.outlining edges of roads in an image). How much was the network tuned to properly work on it? Since this is a common pattern in many application domains, such as specialized medical image processing where labeled data is scarce, the paper is important.
Accept (Spotlight). rating score: 8. rating score: 6. rating score: 6. <BRK>The paper is concerned with neural ODE based networks, specifically their robustness. I recommend the paper for publication. (1) It studies the robustness of neural ODE, and (2) proposes a more robust variant of neural ODEs. I see that the authors have responded to that point on the openreview website.<BRK>This paper investigates the robustness of Neural Ordinary differential equations (ODEs) against corrupted and adversarial examples. The crux of the analysis is based on the separation property of ODE integral curves. The paper is well motivated and clearly written.<BRK>This paper studied the robustness of neural ODE based networks (ODENets) to various types of perturbations on the input images. 2.The ODENet architecture showed in Figure~1 can be regarded as an augmented CNN. I think the identity map gives a good trade off between robustness and generalization. I think it is very important to add these results. I suggest the authors check the conditions to make it valid.
Reject. rating score: 6. rating score: 6. rating score: 6. <BRK>This paper proposed a low dimensional patch manifold prior perspective for reinterpreting the deep image prior. I was initially quite excited about this paper, but as I drilled into the details of this work. For the manifold modeling, though the authors defined each part of the formulation in equations (1) (5), but it is not clear how to design corresponding efficient structures for different low level problems. For example, if the proposed MMES scheme is used in image deconvolution tasks, how to design the corresponding structure? For the experimental part, the comparison experiments in subsection 4.1(Toy examples) and subsection 4.3 (Color image superresolution) lack comparisons with the latest methods.<BRK>The DGP could be applied to images as well, resulting in a nonconvolutional deep image prior. The paper claims that the proposed method is more interpretable, and it would be nice if they could demonstrate this interpretability and the benefits it brings in solving image reconstruction problems. In the image processing tasks, the performance of the proposed algorithm is on par (sometimes slightly worse, sometimes slightly better) than that of DIP.<BRK>This paper introduces a transformation from the deep image prior (DIP) to an embedding with an autoencoder (MMES). The contributions are summarised as a) providing an interpretable analogue to the convnet, b) demonstration of the proposed method s effectiveness, and c) characterisation of the DIP as a "low dimensional patch manifold prior". I think the MMES approach is interesting and potentially a good analogue to the DIP, and explicitly draws out the locality prior the authors claim is integral to DIP. There is still fundamentally a deep network as in DIP. All up I think this is a useful paper, even though the paper overstates its contributions. I would also like to see the "interpretability" statement either clearly explained or removed (I am not convinced that this method is interpretable).
Reject. rating score: 3. rating score: 3. <BRK>The experiments were done on three different datasets. However, I’m not 100% sure about the usefulness of the method. The authors claimed to provide more insights of neural networks with their method which I did not see when reading the paper. Moreover, one assumption from this paper that networks trained with different initializations for the same subtasks produce the same local solution is wrong. Therefore, I’m not 100% sure whether the results produced from all the experiments are trustable. In sum, I rate this paper as a borderline paper and lean towards rejection due to several aforementioned uncertain points.<BRK>The paper discusses an interesting idea for quantizing the similarity of neural networks, based on weight similarity. Unfortunately, the paper is hard to read. Partly due to overloaded terms, such as “solution” being used for describing multiple different concepts across the paper (solution class, local solution classification, local solution retrieval, none of them particularly well defined in the paper). I highly recommend the authors to describe their findings in a more concrete manner. 3.Can you elaborate more on the goals of the experiments? 4.Can you elaborate what the goal of local solution classification is? It is not clear if this is simply the classification accuracy of a trained model, or what is vaguely described in section 3.3I will make a re evaluation of the paper after the above questions are answered.
Reject. rating score: 1. rating score: 6. rating score: 8. <BRK>This paper builds a new graph convolutional network (GCN) based on hyperbolic representations of the graph nodes: all latent representations of the nodes are points on Poincare disks. There are some general discussions on how to adapt gyrovector space theory into spherical spaces. As the authors themselves remarked, "it can be seen that our models sometimes outperform the two Euclidean GCN". The motivation, based on the writing, is that constant curvature spaces are more general where the computation is easy to handle. (4) The writing quality is not satisfactory. Section 3.4, as the main technical innovation, can be extended and includes some demonstrations.<BRK>In this paper, the authors address representation learning in non Euclidean spaces. The authors point out that an extension of the gyrovector space formalization to spaces of constant positive curvature (spherical) is required, and with the corresponding formalization for hyperbolic spaces, one can arrive at a unified formalism that can interpolate smoothly between all geometries of constant curvature.<BRK>Summary:The authors propose using non Euclidean spaces for GCNs. This is inspired by the recent work into non Euclidean, and especially hyperbolic, embeddings. This has been done in particular in hyperbolic space, but some datasets benefit from more complex spaces. The authors combine the mixed curvature product formalism that uses products of Euclidean, hyperbolic, and spherical spaces for embeddings, but use these for GCN operations. Similarly, Gulcehre et al is a 2019 ICLR paper, and so on.
Reject. rating score: 3. rating score: 6. rating score: 6. <BRK>Contributions:The paper considers the distillation of a Bayesian neural network as presented in [Balan et al.2015]The main contribution of the paper is the extension of [Balan et al.2015] to apply to general posterior expectations instead of being restricted to predictions. Their use should be motivated and the performance of the distillation should be properly evaluated in the experiments. The result regarding architecture search is interesting, but it should be expanded and explained more to be a main contribution. The figures are not legible. Even fully zoomed in, they are difficult to read. ...BALD would be problematic to use with this framework due to computational costs.<BRK>Summary:The paper introduces a general framework for distilling expectations of the Bayesian posterior distribution of a deep neural network, aiming to extend the original Bayesian Dark Knowledge approach [1]. I would like the authors to clarify why cross entropy loss is replaced with l(h, h’) |h h’| in the classification case. Strengths:Overall, the paper is well written and the relationship to previous works is well described.<BRK>The authors consider the problem of distilling expectations with respect to Bayesian neural network (BNN) posteriors. I realize this is hiding in Figure 2 somewhere, but is not obvious. + Figures 1 and 2 are too small and difficult to parse. The paper would likely be an useful resource for practitioners in the area.
Reject. rating score: 3. rating score: 3. rating score: 6. <BRK>This paper tackles the problem of catastrophic forgetting when data is organized in a large number of batches of data (tasks) that are sequentially made available. I like that this paper uses a single global probabilistic model instead of separate discriminative and generative ones. The graphical model or factorization assumptions are not even mentioned until after the loss has been defined. 2) Theoretical inconsistenciesAlthough the system might work overall, two things seem to be technically incorrect:  The decoder and classifier are expected to approximate the distribution of training data according to the authors (for valid generative replay). This is not a sound mechanism to achieve an as faithful as possible (limited by the expressiveness of the encoder decoder architectures) approximation to the training data. I.e., there are two different probabilistic models modeling the same data in inconsistent ways and one or the other is used depending on the part of the system. 3) ExperimentsFinally, the experimental results do not look very compelling, it seems to be overall worse than the baselines in the two image datasets and slightly better in the audio dataset, so it s unclear that this approach is superior.<BRK>This paper combines replay and openMax approach to help continual learning. The results shows robustness on different dataset include image and audio in the continual learning condition, where the new come data has a different distribution but the model still able to maintain reasonable quality for the previously and newly come examples. (1) It s very hard to align the contribution claimed by the paper and previous work in the introduction section. I highly suggest the author re write this part and has a separate section about related work and explicit describe the difference compare to others. (2) The contribution seems over claimed, it said it s a unified framework, but I don t understand what it has been unified. I cannot link "automatic" with the proposed method. Is that doable because of the proposed framework? (3) Why use AudioMNIST which is an unusual task for audio?<BRK>Summary: This paper proposes a unified model for continual learning and aims to address the following problems:Out of train domain dataset recognitionCatastrophic forgettingThe out of domain or open set recognition model is not only used to detect outliers but also for sampling “representative data” of previous tasks for forward (and backward) transfer. 1.Given my limited knowledge in the literature on this topic, I would have appreciated a proper related work section. It’s just that the data is sent through the probabilistic encoder and then classified. Typo: incorrect opening inverted comma for the word background in the introduction section (page 1)OVERALL COMMENT The paper combines the generative, and discriminative models into one framework for multiple important tasks. While the contributions are clear in the introduction, the presentation of the paper is somewhat too complicated at a couple of places.
Reject. rating score: 3. rating score: 6. rating score: 6. <BRK>First, the authors perform an extensive study to understand the source of what they refer to as  environment bias , which manifests itself as a gap in performance between environments used for training and unseen environments used for validation. The authors conclude that of the three sources of information provided to the agent (the natural language instruction, the graph structure of the environment, and the RGB image), the RGB image is the primary source of the overfitting. Right now, they describe only what is shown in the figure. The authors use this rather extensive study to motivate the need for new features (semantic features) to replace the RGB image that their investigation finds is where much of this  environment bias  is located. In my experience, environment bias (or, more generally, dataset bias) usually implies that the training and test sets (or some subset of the data) are distinct in some way, that they are drawn from different distributions. Perhaps the biggest problem with the paper as written is that I am not convinced that the  performance gap  between the seen and unseen data is a metric I should want to optimize. This narrative challenge is the most important reason I cannot recommend this paper in its current state. Though the performance on the unseen data does not change much, it raises some concerns about the generalizability of the learning approach they have used: in an ideal world with infinite training data, the network would perfectly accurately reproduce the ground truth results, and there should be no difference between the two.<BRK>The authors tease apart the contributions of the out of distribution severity of language instructions, navigation graph (environmental structure), and visual features, and conclude that visual differences are the primary form in which unseen environments are out of distribution. Experiments then show that semantic level features dramatically reduce the transfer gap, although at a cost of absolute performance. These results, if shown to hold across a significant number of datasets and tasks, would significantly change the focus of research in this field toward a focus on robust high level visual representations (as opposed to e.g.better spatial awareness or better language understanding). This work represents an important step in this direction. After discussing with the reviewers about the methodological issue of the validation set, I have lowered my score to a weak accept, but I think this paper should still be published.<BRK>Summary: This paper provides a thorough analysis of why vision language navigation (VLN) models fail when transferred to unseen environments. The authors enumerate potential sources of the failure namely, the language, the semantic map, and the visual features and show that the visual features are most clearly to blame for the failures. The question you are trying to express is "bias is attributed to what" not "what is attributed to bias". So heading should be "to what inside the environments is bias attributed" (which is admittedly a clunky title)* another nit: "suggest a surprising conclusion: the environment bias is attributed to low level visual information carried by the ResNet features." > idk that this is that surprising, it was kind of natural given the result that removing visual features entirely doesn t hurt performance and helps generalization.
Accept (Spotlight). rating score: 8. rating score: 8. rating score: 3. <BRK>The paper aims to provide a defense to physically realizable attacks for image classifiers. First, the paper demonstrates that Lp ball robustness (obtained via adversarial training or randomized smoothing) do not necessarily result in robustness to physical attacks such as adversarial stickers. It has clear motivation, is very clearly written, evaluates proper benchmarks from recent literature, and proposes a new method that shows a clear improvement over benchmarks in literature. Overall, I think it is a good contribution to the adversarial examples literature, as it provides robustness against more “real world” attacks.<BRK>This paper argues that threat models such as L inf are limited when considering physically realizable attacks, and provides evidence for this by showing that L inf adversarial training is insufficient to fully confer robustness against physically realizable attacks in the literature such as the adversarial glasses attack. The paper then proposes an alternate threat model based on contiguous rectangular regions, and shows that adversarial training against this model does far better. Minor comments:Please avoid subjective intensifiers: "We then use an extensive experimental evaluation to demonstrate that our proposed approach is far more robust against physical attacks on deep neural networks than adversarial training and randomized smoothing methods that leverage lp based attack models."<BRK>First of all, the two physical attacks evaluated in this paper have similar attacking patterns, i.e., mask based pixel attacks. Actually it has been shown that the framework of adversarial training (AT) will overfit to the attacking patterns used in training. So under the more completed and flexible physical attacks, a defense based on the AT framework like DOA may not be a good choice. So what DOA does is just substituting the PGD module in AT to overfit the new attacking patterns, which is of limited contribution and novelty.
Reject. rating score: 1. rating score: 3. rating score: 6. <BRK>My problem is sadly not with the validity of the claims but with their significance. The main qualm I have about this paper is about the significance of the contributions and the motivation. If that is the case, well the results are evidence of a negative result in this respect, which is okay but given how ad hoc and poorly motivated the method is to that objective it s not much of a contribution. Are they trying to better understand theoretically the loss landscape of neural networks?<BRK>This paper combines neuron alignment with mode connectivity. Specifically, it applies paths neuron alignment to the calculation of mode connecting and empirical results show that alignment helps in finding better curves. Combining neuron alignment with mode connectivity seems to be a good idea, but the message the authors want to convey is somewhat vague. The following are some detailed comments and questions:1. However, for the former, the improvement in the performance is not that much; for the latter, there is hardly any insight conveyed by this paper. 3.Usually sufficient details help in good understandings, but in this paper, some key details are unfortunately missing. In sum, the idea seems to be interesting, but the overall quality of the paper is still yet to be improved, and some key details need to be addressed more clearly before it can be accepted as a qualified submission.<BRK>The paper investigates the connection between symmetries in the neural network architectures and the loss landscape of the corresponding neural networks. To overcome this problem, the authors proposed two step procedure: 1. The authors also proposed PAM algorithm where they iteratively apply path finding algorithm and weights alignment algorithm. Novelty and significance. The idea to combine the symmetrization of NN architectures with path finding algorithms is new to the best of my knowledge. Experimentally, the authors showed that ensembling the midpoint and endpoints of the curve found via path finding  algorithm coupled with the neural alignment algorithm delivers a better result than simple averaging of three independent networks. The paper contains several typos and inaccuracies:1.
Reject. rating score: 3. rating score: 6. rating score: 6. <BRK>The paper proposes a framework (Scoring Aggregating Planning (SAP)) for learning task agnostic priors that allow generalization to new tasks without finetuning. This is achieved by learning a scoring function based on the final reward and a self supervised learned dynamics model. Page 3, 3rd paragraph of Section 3: the paper says that "The proposed formulation requires much less information and thus more realistic and feasible"   I agree that this is more realistic, but is it really more feasible? Additionally, are there other baselines that specifically address the zero shot task in the literature? Model based method ..."   missing an "a" as well?<BRK>I am not from this area and don t know much about reinforcement learning. The paper discusses zero shot generalization (adaptation) into new environments. The authors  then say that algorithm is tested on the test environment E2. I am a bit confused by this setting. so it was easier to understand for people not in the domain and familiar with some of the terminology.<BRK>The paper describes a method that aims to learn task agnostic priors for zero shot generalization. First, the baselines presented in the experiments are relatively weak. Second, essentially the proposed method is trying to solve the zero shot generalization by parameter initialization; a model is pretrained on related tasks and used as initializations for target tasks. The authors claim that it is different from prior work mainly because of the neural architecture that deals with sparse rewards via score aggregation. And apparently, the method will also rely on the similarity between the pretrained task and the target task, and such a scope constraint is not discussed in the paper. In other words, I m not quite sure a better architecture is fundamental progress towards zero shot RL.
Accept (Poster). rating score: 8. rating score: 3. rating score: 3. <BRK>Instead, for a pair of classifiers to be compared, it advocated to sample their "most disagreed" test set from a large corpus of unlabeled images. The level of disagreement was measured by a semantic aware distance derived from WordNet ontology. I feel the approach to implicitly assume that the classifiers to be compared are already "reasonably accurate"; since if not, both classifiers might be easily falsified by certain trivial examples, making the "disagreed examples" not as meaningful. I can see the paper to be of interest to a quite broad audience and can motivate many subsequent works. However, since the authors adopted an affinity aware distance, two incorrect predictions can still be compared based on their semantic tree distances to the true class.<BRK>This paper points out that the traditional way of model selection is flawed due to that the validation/test set is often small. Hence, the authors proposed an alternative approach to comparing different classification models by the notion of inter model discrepancy. The main idea is reasonable, but it requires that the models to compare all perform reasonably well. Another potential issue is that the proposed approach cannot handle training set bias. What is a general guideline for one to choose this number $k$ given a new application scenario? The unlabeled set is not "unlabeled" in essence. If my understanding was correct, it cannot contain open set images which do not belong to any of the classes of interest. Hence, while I agree with the authors that existing approaches to comparing deep neural network classifiers could be improved, I think the proposed solution is not a good alternative yet.<BRK>This paper proposes a new method to compare existing classifiers, which does not use fixed test set and adaptively sample it from an arbitrarily large corpus of unlabeled images. The main idea seems similar to adopting active learning for the test set selection. One of the main advantage is that it can select a sample set from an arbitrarily large unlabeled images. So the experiments in this paper is also not convincing.
Reject. rating score: 1. rating score: 1. rating score: 1. rating score: 6. <BRK>This paper presents a method for learning a “wrapper” model which endows a multiclass predictor with an estimate of model uncertainty. The base model is treated as a black box which emits a categorical distribution, while the wrapper model estimates the parameters of a Dirichlet distribution. This paper could be improved by a more thorough comparison to the literature, and by a clearer motivation for the training procedure used. It seems from the preceding discussion that the main goal of the wrapper model is to estimate \beta, but it is not clear how the sampling procedure (2.2) allows for this.<BRK>This manuscript proposes to train a wrapper to assess the confidence of a black box classifier decision on new samples. The idea, although a bit incremental, is potentially useful to practitioners, as argued in the introduction, and some empirical result tend to suggest that the method can be useful. This is introduced as a cross entropy loss, however, taking the expression of the Dirichlet distribution, it is not obvious to me how to reach this very simple expression. Is it possible that the authors simply mimic the expression of Kendall and Gal (2017, eq.(5)), that was designed for the Gaussian case, for which the cross entropy expression is correct? is a first step to see if the achieved result is not easy to get.<BRK>The resulting model output is a Dirichlet distribution with mean equal to the categorical distribution produced by the black box and concentration parameter specified by a separate auxiliary model. A black box model that yields distributions over classes expresses aleatoric uncertainty via that distribution, and uncertainty due to a shifted data distribution is epistemic as additional data from the new domain would reduce it. More problematic is the study’s lack of robust baselines. The authors only present the predictive entropy baseline, but numerous methods exist for out of distribution detection and selective classification. The paper currently only highlights differences in dataset size.<BRK>In this paper, the authors propose to use a dirichlet prior over the multinomial distribution outputted by blackbox DL models, to quantify uncertainty in predictions. The main contribution is to learn the parameters of the prior and use it as a wrapper over the black box, to adjudicate whether to retain or reject a particular prediction made by the model. The paper is well written, and easy to understand. The main motivation of the paper seems to be to learn what samples to drop, but the authors do not address what can be done about the dropped samples (i.e what happens if we end up having to drop 80% of the samples?) Nonetheless, the method seems to provide impressive results on multiple datasets, and I think this is interesting enough to warrant an accept.
Accept (Poster). rating score: 6. rating score: 6. rating score: 3. <BRK>Specifically, after obtaining a language model on a dataset, the model further uses the dataset to build a lookup table and then a k nearest neighbor is used to searching the closest tokens for a token during inference. [Pros]:Overall I think the paper is well written and presents clearly. Also, using the continuous caches  with KNN LM further improve the performance. These results are also insightful and meaningful for the readers to understand the method. Though this is not absolutely new and relatively simple , the authors successfully demonstrate that it can be applied to improve the generation of language model much.<BRK>Summary:The authors extend a pretrained LM by interpolating its next word distribution with a KNN model. The authors show retrieving nearest neighbor from corpus achieve quite large perplexity decrease in several language modeling benchmarks. Even with some discussions on the related work with cache based LM and the work that use training examples explicitly, I feel it is a simple extension/usage of previous approaches. The proposed idea uses KNN to look up training examples for interpolating the prediction.<BRK>3)	Furthermore, though FAISS is very fast, it is hard to get great results with only a small datastore which makes the retrieving slow. Three of my most concerns:1)	It seems that this approach heavily relies on the similarity of context distribution between the training and test set. This question should be discussed more in this work. 2)	The evaluation is only done for PPL, I notice the LM was trained in a corpus scale as pre trained BERT, though none of real downstream tasks were evaluated like BERT.
Accept (Poster). rating score: 8. rating score: 6. rating score: 6. <BRK>This paper introduces a new regularization technique “mixout” for fine tuning BERT. Mixout technique mixes the parameters of two models — the pretrained model and the dropout model. Empirical results show that mixout can stabilize fine tuning BERT on tasks with small training examples (which has been shown to be difficult). I’d like to accept this paper based on the extensive and detailed experiments and promising results. If so, would it introduce a cliff in Fig 4?<BRK>This paper introduces a new regularization technique refered as “mixout”, motivated by dropout. Mixout stochastically mixes the parameters of two models. Experiments shows the stability of finetuning and the method greatly improve the average accuracy. I really like the proposed idea, and the paper is easy to understand and follow, and the experiments are well designed.<BRK>The authors introduce a new regularization technique for the specific task of finetuning models. It s inspired by dropout and stochastically mixes source and target weights in order to avoid moving the parameters towards 0. The paper is in general well written. I have a few concerns that I would like to see addressed:1a. It would be nice to see results which demonstrate the theory. 1b.In few of the prsented empirical experiments is it the case that the use of mixout by itself is useful. 2.Why are there only 4 GLUE tasks reported? Devlin 2018 reports on all but WNLI.
Accept (Poster). rating score: 8. rating score: 6. rating score: 6. <BRK>Summary:The paper presents a new regularization technique termed consistency regularization for training GANs. The idea is the following: the authors propose to penalize the sensitivity of the last layer of the discriminator to augmented images. The authors tested different augmentation techniques and concluded that simple ones behave better (e.g., shifting and flipping). Pros: The proposed technique is very simple and intuitive; it easy to implement, and it is computationally cheap. Also, the paper would benefit from a clear experiment description on CelebA dataset (e.g., adding the results to Table 1). The paper would benefit from illustrations of generated samples.<BRK>This paper proposes to use Consistency Regularization for training GANs, a technique known to work well in unsupervised learning. The author show that using this technique enables them to improve the performance of a standard GAN significantly on CIFAR10. I think this is worth mentioning in the related work. I think the conclusion about the effect of consistency regularization vs data augmentation is a bit vague since consistency regularization has no sense without data augmentation. Do the author have any intuition why this is the case ? References:[1] Arjovsky and Bottou.<BRK>Such mismatches had not happened in the past. The reviewer doesn t think this paper reached the bar of a good ICLR paper but hesitates to reject. This work proposed a training stabilizer for GANs based on the notion of Consistency Regularization. The authors claimed "We conduct a series of ablation studies to demonstrate that theconsistency regularization is compatible with various GAN architectures and lossfunctions.
Reject. rating score: 1. rating score: 3. rating score: 3. <BRK>The proposed method explicitly models the feature distribution as a Gaussian mixture model in both source and target domains. Because the proposed method uses pseudo labeling for the target domain, it seems that the weights to determine unreliable examples are crucial. This paper should be rejected because (1) the novelty of the main idea is marginal, and (2) the performance gain over the baseline methods is also marginal. It is required to explain why explicit modeling performs better than implicit modeling of prototypes by theory or practice.<BRK><Paper summary>The authors proposed Distribution Matching Prototypical Network (DMPN) for unsupervised domain adaptation. By explicitly modeling the distributions that the features follow, the discrepancy between the distribution of source data and that of target data can be easily evaluated. DMPN is trained by jointly minimizing two kinds of loss, which are classification loss on the source data and domain discrepancy loss that is calculated via the explicit models. + The paper, especially the experiment section, is well written and easy to follow. Since the authors explicitly model the feature distributions by Gaussian mixtures (GMs), it might be possible to calculate a standard divergence between source and target data distributions by using the parameters of GMs. Compared with such a straightforward approach, the proposed method seems to be ad hoc and is not theoretically validated. Is the proposed method sensitive against the change of this threshold? I am concerned about whether the proposed method works well with harder datasets such as Office Home dataset, because each class data are modeled by a simple Gaussian distribution in the proposed method.<BRK>Rethinking feature distribution for loss functions in image classification. With the learned features in this form, the authors ensure domain adaptation by minimizing the discrepancy between the distributions arising from the source and target datasets. Transferrable prototypical networks for unsupervised domain adaptation. The paper is a bit hard to follow, and would be improved by giving a more explicit comparison of the methods used here to past work, especially [1] and [3].
Reject. rating score: 3. rating score: 6. rating score: 6. rating score: 8. <BRK>Authors provide an extension to the invertible generative models, by fashioning them into conditional invertible generative networks. The networks are conditioned at the input with a feed forward network initialized with a VGG based classification network. The model is learned using an MAP objective along with some modifications to the original training procedure (described in sec 3.4). The model is evaluated qualitatively on "style transfer" on MNIST digits and image colorization. The technical contribution of this paper is the somewhat straight forward extension of the cINNs to conditional generative networks. Although the results on colorization are claimed to be good, the baselines they compared to are not very recent (e.g.cGANs).Overall, I believe there is very less novelty, technical sophistication and performance improvements in this paper.<BRK>The paper presents an invertible generative network, for conditional image generation. The model is an extension of Real NVP with a conditioning component. Experiments are performed for image generation on two tasks: class conditional generation on MNIST and image colorization conditioned on a grey scale image (luminance). In the experiments conditioning may be a simple class indicator (MNIST) or a more complex component corresponding to a NN mapping of an initial conditioning image (colorization). The authors claim is that they are the first to propose conditional invertible networks. They make use of several “tricks” that improve a lot on the performance as demonstrated by the ablation study. It looks like these are not details, but requirements to make the whole thing work. The Haar component for example should be better motivated. The baselines are probably not the strongest models to date, and better results could certainly be obtained with other VAE or GAN variants. For example, there have been several works trying to introduce diversity for GANs. This is not redhibitory, but this should be mentioned. This could be also commented. Concerning the interpretation of the axis for the MNIST experiment, it is not clear if they are axis in the original space or PCA axis. I keep my score.<BRK>This paper proposes conditional Invertible Neural Networks (cINN), which introduces conditioning to conventional flow based generative models. It is also nice to see the addition of the LPIPS metric. The performance of the cINN is evaluated empirically on the task of colorization, where it is shown to outperform other techniques in terms of nearness to the true colours, as well as sample diversity. The base method for integrating conditioning into the flow is simple and intuitive, and additional modifications which allow for stable training at higher learning rates, such as soft clamping and Haar wavelet downsampling, appear to be very effective. Conditional models often lend themselves to a wide variety of useful applications, so I think this work could be of interest to many. My primary concerns with this paper are related to the comparison of image colorization methods. Things to improve the paper that did not impact the score:5) I was somewhat disappointed by how little attention was spent on the Haar wavelet downsampling method. It seems like a very neat idea, but it is only briefly explored in the ablation study. [2] Zhu, Jun Yan, et al."Unpaired image to image translation using cycle consistent adversarial networks." 2018.[5] Liu, Rui, et al."Conditional Adversarial Generative Flow for Controllable Image Synthesis."<BRK>The authors propose to use a normalizing flow architecture to tackle the structured output problem of generalization. They propose:  a conditioning architecture: they use a convolutional feature extractor (similar to a U Net architecture), and (on top of the common architectural details of models like Glow   Kingma and Dhariwal, 2018) uses Haar wavelets for downsampling;  they train their architecture stably using the maximum likelihood principle;  they demonstrate interesting properties of their model coming from the bijectivity. This is an interesting application of the architecture to colorization. The diverse and consistent colorization results are compelling (with comparison with previous methods), while clearly showing the failure cases where the model should be improved. The paper is clearly written. I m not able to tell from this figure if there was any segmentation happening in the model. The colorization transfer result gives me the impression that the segmentation is not done properly, e.g.the red color from the red car image seems to spill outside of the confine of the car in the colorization transfer.
Reject. rating score: 1. rating score: 3. rating score: 3. <BRK>I cannot recommend acceptance of this paper because of several reasons:  The idea is not novel enough. The idea of adversarial attacks with spatial distortion is not the innovation of this paper and has been proposed and extensively studied by many previous papers. This paper does not have additional innovation and does not lead to additional insight that can warrant an acceptance at ICLR. There is no mention of previous work on spatial transformation attacks in either the abstract nor the introduction (except at the very last). Theorem 1 is a vacuous statement. The experiments are not convincing.<BRK>The whole architecture is trained by a variant of GAN loss to make the adversarial examples realistic to humans. However, I have some concerns about this paper. Perturbation based adversarial examples, spatial transformation based adversarial examples, generating adversarial examples based on the GAN loss are all studied before. And the proposed method integrates them together to form a new attack. 2.The experiments are only conducted on MNIST and Fashion MNIST.<BRK>The paper is well written in general, the idea is intuitive, and the experiments are well described. However, I have a few concerns that lead to me to give a low score (at least in the first round of reviews). Novelty.Leveraging spatial distortions (or other visually meaningful transformations) to generate adversarial attacks is not a new idea, but the authors seem to have been unaware of this very large body of work.
Reject. rating score: 3. rating score: 6. rating score: 6. <BRK>The results are visualized in Fig 3(c). The claim here is that with the same perturbation of the resulting codes (lambda 2) at the output of different GAN layers, the change in the visualized output demonstrates what kind of, if any, semantic is being captured by different layers of GAN. The claim of the paper is that there is a hierarchical encoding in the layers of the StyleGAN generator with respect to the aforementioned "Variation Factors". The basic GAN architecture used in this work is that of StyleGAN. 2) I would like to see the visualizations of the latent codes at the separation boundaries, just to see how well the binary SVM performs and whether or not, non binary information is lost/unaccounted for. Moreover, Layout variation is just view point variation. So I think it will be appropriate to call it "view Point Variation" rather than "Layout Variation". How generalizable is this approach to other kinds of GANs other than PGGAN and BigGAN (or rather, why is this approach relatable to StyleGAN, PGGAN and BigGAN alone)? In the next step, the authors sample a latent code from the learned distribution and pass it throughevery layer of the GAN generator.<BRK>Updates after author response:I d like to thank the authors for their detailed responses. Finally, I agree that given the popularity of StyleGAN like models, the investigation methodology proposed, and the insights presented might be useful to a broad audience. This paper investigates the aspects encoded by the latent variables input to different layers in StyleGAN (Karras et.al.), and demonstrates that these correspond to encoding different aspects of the scene across layers e.g.initial ones correspond to layout, final ones to lighting. This paper investigates which layer’s latent codes best explain certain variations in scenes. The analysis presented in the work is thorough and results interesting. pitch the paper as being more general than it is, and claim the insights to be more applicable. However, what is actually investigated is the layer wise latent code (NOT ‘representation’ which is typically defined to mean the responses of filters/outputs of each layer). Independent of any other concerns, I would be hesitant to accept the paper with the current writing given the very general nature of assertions made despite experiments in far more specific settings. It would be interesting to know if this adversely affects constancy of some aspects e.g.maybe objects also change in addition to layout. Overall, while the results are interesting, they are only in context of a specific GAN, and using an approach that is applicable to generative models having a multi layer code.<BRK>The paper proposes an approach to analyze the latent space learned by recent GAN approaches into semantically meaningful directions of variation, thus allowing for interpretable manipulation of latent space vectors and subsequent generated images. By forming a decision boundary in the latent space for each of these classifiers, the latent code is then manipulated along the boundary normal direction, and re scored by the classifiers to determine the extent to which the boundary is coupled to the semantic attribute. A user study shows that human judgments of the coupling between layers and semantic attribute being manipulated are consistent with this observation. Another set of experiments demonstrate that the importance of different semantic attribute dimensions for different scene categories varies in an interpretable way, and also that certain attribute dimensions influence each other strongly (e.g."indoor lighting" and "natural lighting"), whereas other ones are decoupled (e.g."layout" and other dimensions).
Reject. rating score: 1. rating score: 6. rating score: 6. <BRK>The paper propose a dual generator approach for learning to generate point clouds. (e.g.G is undefined in Algorithm 1) 1.The problem of the point cloud generation of Achlioptas et al., (2018) is it can t generate arbitrarily many number of points. The two generator approach is nothing new.<BRK>How the module weights can be updated this way is unclear. According to the paper, previous approaches for generating such 3 D point clouds involved autoencoder and GANs used separately. The authors propose a framework combining both autoencoder and GAN in a single network. In paragraph 1 of introduction, disadvantage of 3d point cloud data can be better explained.<BRK>This paper proposed a new dual generation model for learning representation by combining GAN and Autoencoder for 3D point cloud data. The paper is in a good writing, and easy to read.
Accept (Spotlight). rating score: 8. rating score: 6. rating score: 6. rating score: 3. <BRK>There are a few comments and questions I have:  The exact definition and usage of nominal environments and rewards are still unclear to me. I think this work is quite fundamental, impactful to be accepted at the conference and is possibly extended to practical scenarios (like explainable and safety RL and imitation learning) in the future. I want authors to add some comments on that.<BRK>The submission considers estimating the constraints on the state, action and feature in the provided demonstrations, instead of learning rewards. The problem considered is interesting, and the authors provide a straightforward but empirically effective method.<BRK>The paper aims to address a new method for inverse reinforcement learning based on maximum likelihood constrained inference. In general, I find the problem very interesting and the motivation of the work is quite reasonable.<BRK>The paper considers learning of constraints in MDPs in an IRL setting with the goal of maximizing the likelihood of demonstrations (in the constrained MDP). The paper is mainly well written and considers a relevant problem. In that case, the proposed approach will fail to identify some constraints.
Accept (Poster). rating score: 8. rating score: 6. rating score: 6. <BRK>The paper does so by introducing the objective which distinguishes the groundtruth entity and the false entity in the Wikipedia text. For this reason, I think it is possible that negative entities do not help, but the fact that the training objective is more entity centric helps the performance on various different downstream tasks. Some marginal concerns I have is that some settings for downstream tasks are not clear. How do you know that the mention with a hyperlink is always an entity? 3.How often were entities to replace chosen? Is "instance of" used?<BRK>This paper proposed to improve pre training of language models (e.g.BERT) by incorporating information around entities based on English Wikipedia. The model was evaluated on a fact completion task (created by the authors on the 10 sampled Wikidata relations) and several open domain QA datasets and an entity typing dataset FIGER, and achieved significant improvements on the BERT baselines. Overall, I think this is a strong paper. The idea is simple but effective, the experiments are thorough and improvements over the BERT baselines are significant. How were the candidates chosen? I am not sure why you picked the most common entities for predictions. What is the percentage of entities that have been replaced?<BRK>This paper proposes to trained better entity centric text embeddings by switching entities mentioned in the text to some other entities with the same type. The authors do experiments on multiple tasks, and the model shows strong performance on all tasks. The idea is novel and the experiment results suggest that the additional "adversarial" target helps. 2.In section 3.2.1, why do you use a different split for TriviaQA? Do you rerun the baseline models on this new split?
Reject. rating score: 1. rating score: 1. rating score: 3. <BRK>This paper presents an empirical study of using error reduction as a curiosity measure. Major points:* About the claims as stated on page 2: 1) The first claim I don t understand, I think what is meant is on navigation tasks they find "their measure of representation learning" (proposed in kolesnikov 2019) seems to correlate well with reward optimization. In 3.1 to back this claim they claim to test disentanglement but seem to test classification. * auto encoders are a large family of models and it is not clear from the paper which exact model is meant by the authors.<BRK>This paper proposes a framework called curious representation learning (CRL) which uses a better visual representation in RL. I have to recommend rejection for this paper. It appears 1) the idea of using curiosity is not originated from this paper; 2) I do not see what is a "better visual representation"; 3) the comparison with baselines does not show that the new method is consistently better. The paper is also very hard to read. There are many inaccurate languages used in the paper.<BRK>Similarly, we don t know how the method compares to state of the art on other tasks considered in the paper. Curiosity is an important topic in the RL field and this paper is well motivated. 6) The training details are missing, both in terms of hyperparameters as well as optimisation strategy for solving minmax. 3) Continuing with comparisons, it s not clear if this method delivers better performance compared to other curiosity based methods.
Accept (Poster). rating score: 8. rating score: 6. rating score: 3. <BRK>This paper proposed an effective defense against model stealing attacks. Merits:1) In general, this paper is well written and easy to follow. 2)  The details of heuristic solver are unclear. This should be clarified better in Sec.3.It is important to highlight that the defender has no access to F_A, thus problem (4) is a black box optimization problem for defense.<BRK>This paper aims at defending against model stealing attacks by perturbing the posterior prediction of a protected DNN with a balanced goal of maintaining accuracy and maximizing misleading gradient deviation. The heuristic solver toward this objective is shown to be relatively effective in the experiments.<BRK>The paper proposes a new method for defending against stealing attacks. 2) The proposed method is straightforward and well motivated. I’m concerned that this may indicate that the attackers are generally weak and this threat model may not be very serious. Including results on a dataset like ImageNet would be nice. This would motivate the defense more.
Accept (Poster). rating score: 8. rating score: 6. rating score: 6. <BRK>TITLESNODE: Spectral Discretization of Neural ODEs for System IdentificationREVIEW SUMMARYExceptionally clear and well written paper that demonstrates a strong improvement on an important problem. PAPER SUMMARYThe paper presents a new method for estimating parameters in a neural ODE based on a polynomial representation of trajectories and alternating updates of trajectories and neural net parameters. ORIGINALITYTo my knowledge the proposed method is novel. SIGNIFICANCEThe paper demonstrates a strong and practically significant improvement in learning, and I expect the results will be of interest to everybody working in this area. At this point, it is not clear how x(t) is represented.<BRK>This work extends prior work on Neural ODEs. This paper puts forth the following contributions: a compact representation of the state transition function as a combination of Legendre polynomials, and an optimization scheme whose error is tied to the polynomial order and whose structure lends itself easily to parallelization. Given these clarifications in an author response, I would be willing to increase the score. The experiments also did not compare against adjoint methods in the multi agent example, or in the low data regime for the single agent example. While it is nice to have empirical results that showcase this, a more comprehensive comparison against current adjoint methods would be more interesting, especially in the multi agent example.<BRK>This work proposes a new approach for the evolution of Neural ODEs for particle systems. The authors suggest to replace the traditional backpropagation through ODEs or the recent adjoint method for backpropagation and instead solve the problem as an alternating optimization scheme. In particular it is suggested that using spectral methods (Legendre s polynomials) first compute a minimizer of the trajectory (optimizing a trajectory x(t)) given the Loss/Langrangian (that is based on the data times of training). After an initial trajectory is computed, follow an alternating minimization, where in the first step, minimize the discrepancy between the network (that describes the time change of the ODE) and the time derivative of the current trajectory. The network s parameters in the two stages are optimized via SGD and ADAM respectively.
Accept (Spotlight). rating score: 8. rating score: 8. rating score: 6. <BRK>[Summary]This paper performs an extensive empirical evaluation of Neural Tangent Kernel (NTK) classifiers kernel methods that theoretically characterize infinitely wide neural nets on small data tasks.<BRK>This paper conducts very interesting and meaningful study of kernels induced by infinitely wide neural networks on small data tasks.<BRK>This paper evaluates the empirical power of neural tangent kernel (NTK) on small data tasks.
Reject. rating score: 3. rating score: 6. rating score: 6. <BRK>This paper proves a good improvement over the baseline methods compared to. Especially on CIFAR100, which is considered a difficult task in the regime of continual learning. It is quite evident to me that this approach has a big advantage over the other incremental classification methods. In my opinion, the paper could be better presented from the angle of generative modeling / out of distribution (OOD) detection. The good results of this paper may be because of the pretrained feature extractor, using a pretrained feature extractor could be practically useful but has limited insight for future research. Also, using a fixed feature extractor is avoiding the forgetting problem instead of solving it, it wouldn t generalize to more general / realistic continual learning setting. 2.The incremental classification setting in this paper is not a very practical assumption for continual learning. i.e.the task boundary are assumed to be available during training, and within each task the data are i.i.d shuffled. One of the goal of continual learning is to prevent catastrophic interference, training separate network for each task, in my opinion, is avoiding the problem rather than solving it. Although there re many papers in the continual learning regime assumes availability of task boundary, I think this setting doesn t bring too much insight to how we can eventually solve the general case continual learning.<BRK>It could be greatly shortened to make space for more experiments. However, there are.... method may not be applicable." They do experiments on MNIST and CIFAR 100Paper contributions:   Review of methods and evaluation settings for continual learning  Review of invertible neural networks   Experiments comparing the proposed method to several other continual learning methods   Examination of memory cost  Exploration of feature space of trained INNsReview summary & decision: The proposed idea of using invertible networks for continual learning sounds interesting, and I think that this could be a good paper. There are also some less critical, but still important aspects of the paper (related work, clarity of explanations, repetitiveness) which lead me to decide this paper is not currently ready for publication. Right now that section has even more related work in it3. A lot of statements and decisions are made without being explained, or are unclear / innacurate        In CL, the objective is to learn several tasks one after the other" this is not the objective, it s the problem setting. in the abstract you state the definition of continual learning says you can t have access to the data from previous tasks, but then mention that previous works use samples from previous data. (rather than e.g.normal MLPs). 2.What is the training regime for the INNs? The experimental baselines seem inconsistent / not comparable, making it difficult to evaluate what is going on. The only sentence that seems to talk about it is "this way ... network won t be able to have an output similar to the outputs on data from its training set", but this is not clear, not enough explanation, and seems somewhat innaccurate. How many, how are they sampled? 3.The single head setting seems very similar to (or maybe the same as?)<BRK>This paper tackles continual learning problem with stacks of invertible network blocks. Similar as the ensemble idea, the proposed method learns an invertible network for each new object class. During test, for each class, each learnt network outputs a norm and the one with the smallest norm is the predicted class. Despite the linear increase of memory usage over number of tasks as authors pointed out in the conclusion, this is an elegant method for continual learning problems in incremental class tasks. The paper is very well written and easy to follow. It seems that there are sufficient details for reproduction. However, I have the following concerns which, I think, may lower the contribution of the paper, unless authors can help clarify. 1.Since authors propose an invertible neural networks based method for continual learning, instead of only focusing on incremental class, authors should also evaluate the proposed method in other continual learning tasks in object classification, for example, incremental domain and task, as defined in this review paper (https://arxiv.org/pdf/1810.12488.pdf). 3.I do not understand the term "learning type" in Column 4, Table 1. What about prototypical networks? 6.How much training data has been used over multiple classes? Authors can briefly discuss how the proposed method can be applied in these scenarios in the future work section.
Reject. rating score: 1. rating score: 1. rating score: 3. <BRK>This paper provides an overview of methods to use embeddings for texts as common sense knowledge. It mentions many aspects where embeddings can be used as common sense knowledge. However, the paper lacks both novelty and in depth analysis. Most methods proposed are basically computing the cosine distances between language embeddings pairs and find the closest one. It s hard to imagine how to scale up this process in real applications. And most of the analyses are based on cherry picked examples rather than evaluation with quantitative metrics.<BRK>  This paper claims to provide a “alternate” view of pretrained embedding as commonsense repository. But this is not a new view at all. It is well known embedding can encode commonsense knowledge, which is a reason it helps a wide variety of recent commonsense related tasks (and there has been considerable analysis on what kind of coomonsense is learned and helpful). The paper also claims to propose three training criteria, which are not new as well. In fact, much work has been performed to learn representation for different semantic orientation, dimensions and relations (e.g., polarity like full/empty is one special case of contrasting meaning, which has been studied a lot in the distributed representation paradigm for years). The paper has not been carefully written yet; e.g., even in the abstract, there is a typo like “typically trained an evaluated”. I do not think this paper, claimed as a position paper, adds any new positions to the existing literature. I do not recommend it for the conference.<BRK>This is a position paper   it discusses overall about multi word embedding (sentence) level and how it can be leveraged in multiple applications. Also, it discusses some open opportunities on how the embeddings can be used as a base knowledge source and some challenges in themPros:1. The examples and analysis of embeddings involved in the paper is detailed2. The authors have run lots of experiments to position their idea correctly. Cons:1.I find many of their intuitions well established in the literature   the intuition of geometric structures in the embedding space, the intuition of incidence angle. 3.What I would be really interested is, a new training algorithm for learning embeddings, that would be able to answer all linear algebra based questions in the embedding space. In my personal opinion, this is half work done for a very good paper. The use of common sense in this paper does not align with that usage and hence, is misleading for me.
Accept (Poster). rating score: 6. rating score: 6. rating score: 3. <BRK>In this paper, the authors investigate the training dynamics of binary neural networks when using continuous surrogates for training. In particular, they study what properties the network should have at initialisation to best train. The authors provide concrete advice: the mean of the stochastic weights should be close to +/ 1 at initialisation. The presentation should also be improved as we can find the following issues:* Missing letters, repeated words or even missing figures (Fig 5 and 7 in the appendix). This will make the paper more accessible to researchers less familiar with the theory but interested in its practical applications. What are the shaded regions: is that some confidence interval?<BRK>The paper provides an in depth exploration of stochastic binary networks, continuous surrogates, and their training dynamics with some potentially actionable insights on how to initialize weights for best performance. This topic is relevant and the paper would have more impact if its structure, presentation  and formalism could be improved. Overall it lacks clarity in the presentation of the results, the assumptions made are not always clearly stated and the split between previous work and original derivations should be improved. Section 3 should also clearly state the assumptions made.<BRK>The paper addresses a very important and relevant topic of initialisation of weights of neural networks. Make more specific what the added value of the paper is. What the authors propose is an extension of the approach to other settings. In order for this paper to be suitable for publication the reviewer would like to strongly suggest:  Organise the material in a way that would make it clear what is claimed, what is proven etc.
Reject. rating score: 3. rating score: 3. rating score: 6. <BRK>There are several issues in the experiments. The paper did not compare with any of the regularizer based robust models. When comparing with PGD adversarial training (Madry), in table 1, there is a more than 10% drop on robust accuracy for CIFAR 10 when \epsilon 8. I still cannot support paper because(1) the authors emphasize the theoretical contribution and claims the bound are tighter. However, they did not directly compare with any certified robust methods, or previous bounds to support the argument. (2) The empirical results look suboptimal. The authors did not convince me why they sampled 1000 images for test for a small CIFAR 10 dataset. The proposed method is 10% less robust comparing to Madry s in table 1.<BRK>This paper proposes to replace the exact gradient by its discretization via finite difference, which is computationally more efficient comparing to "double backpropagation". Overall, the paper is well written but the contribution is very limited and I find some of the experimental comparisons unfair. Thus I do not support publication of the paper. 1) The gradient regularization is not novelThe main contribution of the paper is to use the finite difference in the gradient regularization in order to improve the training time. The method of gradient regularization is not new, hence the contribution is only the computational effectiveness. This would be fine if one could improve the state of the art method s training time by a lot, but according to the experiments, the performance of adversarial robustness is far from the adversarial training, for example in CIFAR10 epsilon 8/255, there is a 12% drop, which is a huge gap. Moreover, the gradient regularization uses l2 norm instead of l_{1} as in table 1, which makes the comparison unfair. 3) Comment on the motivation/theoryThe theory part is fairly straightforward and it clearly shows that the norm of gradient (w.r.t input) itself is not sufficient to guarantee robustness. As a evidence, even under standard training (for example on MNIST), the gradient norm could be very small, in order of 10^{ 4} but still have adversarial example with very small perturbation. Thus, what we also need is to control how fast this gradient changes (Lipschitz constant or w bound). However, the gradient norm regularization does not take into account how gradient changes.<BRK>Summary:This paper provides new understandings on adversarial robustness from the perspective of input gradient regularization. Taking a step further, this paper proposes to use the finite difference to estimate the input gradients, which not only gives a nice property for reduced modulus of continuity (eg.the w bound), but also makes the regulation scalable to large networks and datasets. I quite like the theoretical connections derived in this paper. Empirical evidences support their claims, and demonstrate indeed comparable robustness of input gradient regularization to adversarial training. The empirical results can be strengthened by including the normal input gradient regularization baseline (using double backpropagation), at least on cifar 10. This is less likely to change the conclusions, but  would be interesting to see the comparisons. Note that, there are already new progresses in adversarial training:[1] Wang, Yisen, et al."On the Convergence and Robustness of Adversarial Training."
Reject. rating score: 3. rating score: 6. rating score: 6. <BRK>The authors perform coreset selection of a large set of images (actually a greedy variant from Sener & Savarese). The selected images are then used for training with small batch size. The coreset selection is applied to the inception activations of the images (or a randomly downsampled version of them). The paper is well motivated and solutions that prevent having to use large batches will have a significant impact on the field. The technical contribution of the paper is rather small, and also the depth of analysis could be improved. However, given the importance of the problem addressed in the paper (and the little existing work) I could be open to increasing my score if my concerns are addressed. The main danger of not randomly sampling from the train distribution is that the trained GAN does no longer generate images according to the train distribution. The proposed metrics do not measure this. Maybe this could simply be solved by adding some final epochs which just sample randomly again? I can see how the BN is different from a large batch but it would still be interesting to see.<BRK>This paper applies core set selection to the training of GANs. The motivation is to limit the minibatch size with suitably sampled sets of datapoints. The proposed technique is relatively reasonable: e.g.extract features from an image, reduce dimensionality by the taking random projections, then run Core Set selection. The Core Set selection part of the method is modular from the rest of the GAN training, and can be applied easily. Generally, I think this is reasonabl work. While the idea itself is not extremely novel, it is interesting to see CoreSets applied to GANs. The paper can be made stronger if there is more discussion about the theory of CoreSets and how good are the heuristics used in this paper.<BRK>Summary:This paper addresses the challenging problem of how to speed up the training of GANs without using large mini batch sizes and causing significant performance drop. To achieve this, the authors propose to use the method of core sets, mainly inspired by recent use of core set selection in active learning. Regarding the experimental evaluation, it is clearly shown that the proposed core set selection greatly improves GAN training in terms of timing and memory usage, and allows significantly reducing mode collapse on a synthetic dataset. I think this would be helpful for readers to better understand how this all works. 3.2,  I find “random low dimensional projections of the Inception Embeddings” is not clear, more technical details should be provided.
Reject. rating score: 1. rating score: 3. rating score: 3. <BRK>What is the justification for the selection of the loss function and log rescaling? This is clearly not enough. Moreover, it seems that we need access to the \nabla f(x) for all x in D(f)? What constraints do you need on f?<BRK>ButIt is not clear that the whole process of dual space preconditioned method with the model of computation of precondition given. [Update after rebuttal period]I have read the response,  my confusion in the original reviews cannot be answered satisfactorily. The authors want to propose a general methodology for learning precondition by supervised learning setting. This paper shows twoThe experimental result which includes the result of power function and the logistic function.<BRK>The authors proposed an interesting approach to the preconditioning in optimization problems. However, the paper is not well written. In Proposition 3: I think that the uniform property of the sampling does not directly mean the optimality in the sense of the learning accuracy. The authors need to investigate the more detailed relationship between the distribution mu and the prediction accuracy of the Fenchel conjugate. The proposed method requires the learning of neural networks, which will be computationally demanding.
Reject. rating score: 3. rating score: 3. rating score: 6. <BRK>The manuscript proposed a new method to model the data generated by multiplexed ion beam imaging by time of flight (MIBI TOF). 3.The data part is not clear.<BRK>This paper applies the SPADE semantic image synthesis technique (with a custom attention mechanism) to MIBI TOF data to examine hypotheses about cell to cell interactions in the context of an immune infiltrated tumor sample. 2) The evaluation section is hard to follow.<BRK>The authors present a GAN for multiplexed imaging (MIBI TOF) data called CCIGAN. They present improved reconstruction of interactions compared to other models in the context of PD 1 and PD L1 interactions. Overall the paper is well written, the application and especially the focus on cell cell interactions is novel.
Accept (Poster). rating score: 8. rating score: 6. rating score: 3. <BRK>The paper describes ANM, active neural mapping, to learn policies for efficiently exploring 3d environments. I think this is a well written "ML systems paper" and I m especially happy that real world aspects of mobile robots are taken into account. I was able to follow the overall idea of the approach as well as the description of the three components.<BRK>The paper describes a method for visual robot navigation in simulated environments.<BRK>This paper proposes a new architecture and policy for coverage maximization (which the authors call exploration). [The] coverage is defined as the total area in the map known to be traversable" appears twice in this manuscript. Is the long term goal trained using the reward signal?
Reject. rating score: 1. rating score: 6. rating score: 6. <BRK>The key idea is that with group of interacting agents, if some agents are replaced with new ones, then the newbies would learn the same language as the group. The paper makes bold claims like "cultural transmission induces compositionality" and "Variations in replacement strategy tend to not affect performance". In my experience, the emergence of compositionality (or lack of) in the setup learned here is very sensitive to various aspects of the learning setup, hence are hard to reproduce. Specifically, they the task and talk paper by Kottur et al may yield different conclusions, if parameters of the original experiments are modified, even slightly. The "evolutionary" part has a similar issue. The paper draws conclusions from three anecdotal rules for replacing the population.<BRK>References:[1]: "Ease of Teaching and Language Structure from Emergent Communication", Funshan Li et al[2]: "Co evolution of language and agents in referential games", Gautier Dagan et al [Updated score based on the rebuttal] The experiments are also extremely toy. I think it is important to clearly point out the novelty of the current work compared to those two previous papers.<BRK>This paper studies whether language composition may emerge by partially re sampling new agents inside a pool of language agents. The paper is well motivated with substantial background literature on the cognitive science and emergent communication side. I think that this paper is just above this threshold by a short margin, and I vouch for weak accept. [1] Bäck, Thomas, and Frank Hoffmeister. Although the paper deals with generational transmissions, there are no experiments that analyze the evolution of language generations after generations. I had some difficulties in understanding Fig4, and the final take away correctly. I may have missed this point, but how many seeds did you use to run your experiments? Last point... but it does not undermine the soundness of the experimental protocol!
Reject. rating score: 3. rating score: 3. <BRK>This work proposes an algorithm for handling the weight sharing neural architecture search problem. The reviewer has several concerns:1) the SBMD and ASCA algorithms are existing generic algorithms. The analysis in this work also looks very generic. There is a sense of disconnection with the considered training problems. 2) The convergence rate improvement brought by using mirror descent has been long known. It is not easy to see what is the contribution of this work.<BRK>I have not worked in the optimization filed and I am only gently followed the NAS field. Based on this analysis, the authors proposed to use  exponentiated gradient to update architecture parameter, which enjoys faster convergence rate than the original results in Dang and Lan (2015). 2) The author provide generalization guarantees for this objective over structured hypothesis spaces associated with a finite set of architectures. My biggest concern is the validity of the proposed exponentiated gradient update, at least empirically. Also, comparing to first order DARTS, search cost is the same and this is hard to justify the better convergence rate for EDARTS. The author proposed ASCA, as an alternative method to SBMD. What is the advantage of ASCA comparing to SBMD?
Reject. rating score: 1. rating score: 3. rating score: 3. <BRK>Tensor regression layer to replace fully connected layers  Wavelet scattering layer to replace the first convolutional layer  Second order optimization (K FAC)All the ideas mentioned in this paper are existing ones (although properly attributed), so the novelty of this work is relatively low. The paper mentions that this particular combination is "novel", but it is not clear is there is any significant synergy between these methods and why it should be considered interesting in this particular setup.<BRK>The paper aims at parsimonious reinforcement learning by employing 3different techniques: using tensor regression layers (Kossaifi et al.,2017b), wavelet scattering (Mallat 2011) and using K FAC (Kingma & Ba,2014) as the optimization method. It would also be interesting tocompare the distributions of the eigenvalues of the tensor layersversus the dense layers in deep RL which may provide insights on theachieved savings and the compression trade off. The performances ofwavelet scattering for the reported tasks are weak (better in only onegame) and the space saving is not clear.<BRK>The second method the authors have attempted is to swap the convolution layer of the deep RL architecture with wavelet scattering. In 2.3, there is a lack of definition for  \Lambda_1 and \Lambda_2. Thirdly, as an approximation of the second order optimization, K FAC does not really concern with the main theme of the paper, which is an investigation of potential weights reduction methods. Also in this section, the authors mentioned Tucker decomposition for the tensor regression.
Reject. rating score: 1. rating score: 3. rating score: 3. <BRK>Again, showing results on non underwater images would help to better understand how the proposed method works. Thus, I do not feel that this work may attract the interest of the ICLR attendees.<BRK>All the tuning of parameters/architectures must be done on the validation, with the test kept hidden until the publication. Remarks/ questions:  The writing should be improved as it is even hard to understand some sentences. I think it is more based on the color of the image, the texture, ect.<BRK>I think authors should compare more existing works to demonstrate the superiority of the proposed one. Besides, the experimental results are also weak.
Reject. rating score: 1. rating score: 1. rating score: 3. <BRK>This paper addresses the super resolution problem. The key is to use pixel co occurrence based loss metric. But the description could be clearer. How does it influence the optimization? How are the loss defined? In experiments, there is no evidence showing the benefit from the pixel Co occurrenceThere is a lack of much details.<BRK>The paper considers the problem of generating a high resolution image from a low resolution one. The paper introduces the Grey Level Co occurrence Matrix Method (GLCM) for evaluating the performance of super resolution techniques and as an auxiliary loss function for training neural networks to perform well for super resolution. Next, the paper shows that when trained with the mean L1 GMCM loss function, SRGAN performs best. In summary, the paper proposes to use the GMCM loss for training and evaluation of the super resolution methods.<BRK>This paper adopts a loss metric called Grey Level Co occurence Matrix (GLCM) as a new measurement of perceptual quality for single image super resolution. Experimental validation is carried on X ray images of rock samples and promising results are achieved. My main concerns are as follows:  Novelty is quite limited. Experiments are not convincing. Moreover, the authors should conduct experiments on generic image SR to demonstrate the effectiveness of GLCM.
Accept (Poster). rating score: 6. rating score: 6. rating score: 6. <BRK>In this paper, the authors proposed an attack scheme to any model that pretrained from a general model. The blackdoor for attack in this work, namely the softmax layer, is novel and interesting to me, at least to my knowledge.<BRK>Overall I believe this paper is okay. It seems that without any attack, it should be possible to try random faces and find one that is incorrectly classified. This is a special hybrid setup, where the feature layer is white box while the final layers are black box.<BRK>This should be discussed. The algorithm uses a brute force approach to iterate through each neuron on the final layer before the softmax. 2.The main issue is that the algorithmic is rather simplistic and seems impractical.
Reject. rating score: 1. rating score: 3. rating score: 8. <BRK>This paper targets to understand some key factors that might influence the training speed of neural networks. The authors fail to propose some interesting applications that could make use of the "gradient confusion" to help the training. However the paper has not discussed this existing work and the contribution on top of it. The paper is clearly written.<BRK>This paper introduces the concept of "gradient confusion" to explain why neural networks train fast with SGD. In all these experiments, explain why the batch size and the step size is not a confounding factor? The gradient confusion parameter \eta should depend on the batch size. Please justify why this is a valid assumption for neural network models. Am I missing something?<BRK>[Summary]This paper introduces gradient confusion, a bound on the negated dot product of gradients at two data points, and studies its effect on the optimization of neural networks with SGD. The experiments corroborate these findings and also show that batch normalization and skip connections can reduce gradient confusion and speed up the training. [Decision]I vote for accepting this paper.
Reject. rating score: 1. rating score: 1. rating score: 3. <BRK>Data point equality shouldn t hold for ReLU networks with non zero bias initialization as well. The experiments show promising results but only on cifar10 and only with the outdated BNN, also as a necessary baseline it would be important to show the effect of the bias initialization on ReLU networks. It also needs to improve the writing. I am not convinced that these points are important, and the paper did nothing to try to persuade me.<BRK>[ ] Weak Motivation: The authors argue  We analyze the behaviour of the full precision neural network with ReLU activation  in the abstract. In experiments, what structure is used for ResNet? Only writing down the backward and forward cannot be called analysis.<BRK>This paper proposes a method to initialize the bias terms in neural network layers, and argues that the proposed method improve the performance of binary neural networks (BNNs). The paper justifies the proposed method by analyzing the geometric properties of the ReLU and the hard tanh (htanh) activation functions, as well as by empirical results on the CIFAR 10 dataset using the (binary variants) of VGG 7 and ResNet. While closing the performance gap between BNNs and their full precision counterparts is an interesting problem of practical importance, this paper has several limitations: (1) the analysis of geometric properties of ReLU/htanh is not sufficiently precise and clear;(2) the paper does not clearly present the connections between the htanh activation function and the straight through estimator employed in back propagating the gradients in training a BNN;(3) the experimental results are too limited on just one dateset, and only error rate on validation set is reported, however, lower error rate on validation set won t guarantee better performance on test set;(4) the presentation is imprecise and unpolished.
Reject. rating score: 1. rating score: 3. rating score: 3. <BRK>SummaryThis paper proposes an algorithm to address the issue of nonlinear optimizationin high dimensions and applies it to convolutional neural networks (VGG models) on CIFAR 10. However, in practice for CIFAR 10, the authors initialize L to be 0.01 andgradually reduces it to 0.005 which is hardly the original intent of thealgorithm. Possibilities include evaluating onCIFAR 100 or ImageNet, using a wider variety of networks including ResNets,evaluating on tasks other than image classification.<BRK>In this paper, the authors propose the Homotopy Training Algorithm (HTA) for neural network optimization problems. 4.The authors make a mistake in the proof of Theorem 3.1. The proof of Theorem 3.1 is to verify Assumptions 4.1 and 4.3 in [1].<BRK>The work proposes to learn neural networks using homotopy based continuation method.
Accept (Poster). rating score: 6. rating score: 6. rating score: 6. <BRK>This paper proposes a new approximate policy iteration algorithm that is based on the previous method, called MPO, which formulates policy optimization (PO) as a probabilistic inference problem. Overall, the paper follows an interesting topic in policy optimization as inference. The experiments show a lot of promising results. I have some concerns as follows. Can the authors comment on this? In addition, V MPO is said to work for both discrete and continuous domains?<BRK>The paper proposes an online variant of MPO, V MPO. I listed some additional experiments that would significantly improve the submission in my opinion. Also, by learning a value function V MPO gets closer to REPS. Soundness:The derivation of V MPO is relatively sound. How does V MPO compare to related on policy methods (e.g.TRPO/PPO) on slightly more computational constrainted settings (eg rllab/mujoco with < 1e7 steps)? It really is difficult for me to even roughly estimate it.<BRK>Summary: This paper presents the V MPO algorithm for on policy reinforcement learning that can handle both continuous/discrete control, single/multi task learning and use both low dimensional states and pixels. ++ Results are achieved in a relatively hyper parameter insensitive manner. + Along with the previous results of MPO, this demonstrates an alternative framework of policy gradient based RL: first construct a nonparametric target behavioural distribution and then move the parametric policy towards this distribution.
Reject. rating score: 3. rating score: 3. rating score: 6. <BRK>***TL;DR: relatively well written (if sometimes confusing) paper that reinvents the inference of latent variables in nonlinear dynamical systems that has been published in the 2000s, and that misses an important chunk of literature (and experiments on dynamical systems such as Lorenz 63) from that time. This paper proposes an approach for learning dynamical systems from partial observations x, by using an augmented state variable z that follows dynamics that can be described by an ordinary differential equation (ODE) with dynamics f. The authors motivate their work by the problem of dynamical system identification when only partial observations are available. Learning a state space model involves both learning parameters and inferring the latent states representation (or, in graphical models, the distribution of these latent states) given the parametric models. While the state space models were hampered by their linearity, several papers in 2000s showed how it is possible to learn nonlinear dynamical models, e.g.[4], [5], [6] and [7] to cite earlier ones. Why not predict from the past of the observations, like is done in many other similar work? Most importantly, the ideas of this paper have already been published in [4] (with architecture constraints on the neural network state space model), in [5] (with any nonlinear neural network state space model), in [6] (using Restricted Boltzmann Machines) and in [7] (using Gaussian Process latent variable models). It seems in particular that the number of training points (around 4000) limits the performance of RNN / LSTM models.<BRK>The paper proposes a new deep learning approach based on Takens’s theorem to identify the dynamics of partially observed chaotic systems. In particular, the method augments the state using the solution of an ODE. + The unification of Taken’s embedding theorem and deep learning provides a novel perspective into dynamical systems+ Impressive experiment results compared with baselines including RNN and latent ODE  The proposed method requires knowledge of the underlying dynamic model to solve the ODE, which is not fair for other methods  The model is trained using data from the same initial conditions, which is essentially overfitting. The authors should provide experiments for dataset from different initial conditions. The writing is not very clear. For example, how to solve the optimization problem in Eqn (7),  as the augmented states u_{t 1} are unknown? How to find the bijective mapping M for general dynamical systems?<BRK>Overall I think that the paper makes an interesting contribution. The empirical results seem sufficient to me, but I am not familiar with relevant baselines (see below). I am personally not familiar with the literature on this problem, so my assessment might be affected by this. I did not find the paper easy to read and the presentation assumes a lot of previous knowledge. The temporal consistency of the unobserved component of the latent space is only loosely enforced with the regularization term in (6). One could train using forecasting with more steps (and only doing inference for the initial y_t of the subsequence), as this is closer to what is used at test time. Do you think this would be helpful for having better accuracy when forecasting more steps? Experimental results are convincing to me, as the model is able to recover the performance of other models that do have access to the full state. The paper should cite the work: Ayed, et al."Learning Dynamical Systems from Partial Observations." Is the training data regularly sampled? The authors evaluate all methods with one and four step forecasting in the last two experiments. I think that it would be informative to show a wider range of number of steps, to show how performance degrades with longer predictions (more than 4). It would be informative to also include results of prior art using this dataset, if possible.
Reject. rating score: 3. rating score: 6. rating score: 8. <BRK>Generally, the paper is well organized, with a good writing, and with sufficient experiments. However, I have major concerns about the technical contribution and the experiment parts. 1.It seems that the authors employed the existing adversarial learning techniques directly to the label noise problem. The technique contribution is quite limited. I think the challenging and interesting part should be about how to learn the metric C. Currently, the authors use the semantic distances between different class labels to set up C. This intuitively makes sense but not convincing. We expect the authors to put more effort to study C, which may depend on the label noise rates or the geometric information of the instances.<BRK> Summary:This paper proposes a new regularization scheme inspired from (virtual) adversarial training to tackle the problem of learning with noisy labels. While based on the adversarial training (AR), it was found that AR does not directly transferable to deal with noisy labels. The author then proposed the Wasserstein version of AR replacing the KL with the Wasserstein distance and its approximate. The advantage of WAR regularization over existing methods is the flexibility to incorporate intra class divergence, making it plausible against asymmetric label noise, which is more common in real world datasets. The authors have done solid work in this paper. Minor suggestions:It would be interesting to see the performance on the other common type of real world noise: open set label noise [1], and may be applied to adversarial training against adversarial examples. The adversarial regularization was also used in a recent adversarial training paper [2].<BRK>I think this paper would generate good discussion, and I recommend acceptance. This paper presents a novel approach for dealing with asymmetric label noise. WAR is an iteration on AR that uses the optimal transport distance between categorical distributions to allow incorporating a cost matrix that encodes class similarities. The evaluation demonstrates that the proposed WAR method achieves state of the art results with the largest gains at high noise levels. The authors also evaluate on Clothing1M and evaluate on a semantic segmentation dataset for which they synthesize structured, realistic label noise. Minor points:Comparing the second and third columns in Figure 1, it looks like lambda has a large effect, so it would be good to know how much tuning lambda requires for the main tasks. This does not break the result.
Accept (Poster). rating score: 6. rating score: 6. rating score: 6. <BRK>Summary : This paper proposes an approach for long horizon off policy evaluation, for the case where the behaviour policy may not be known. Comments and Questions : 	  This is an important and novel contribution where the authors propose an approach for OPE that comes down to solving a fixed point operator. The operator defined as in equation 3 is in terms of the backward view of time   this allows the operator to capture the visitation from the previous s,a to the current s. This is the backward flow operator with which a fixed point equation can be described. The core idea comes from equation 9 which tries to minimize the discrepancy between the empirical distribution and the stationary distribution. Ie, does the proposed method introduce any bias, or has significance in terms of lower variance for the long horizon problem?<BRK>SummaryThis paper proposes using a black box estimation method from Liu and Lee 2017 to estimate the propensity score for off policy reinforcement learning. This reduced variance allows the proposed method to "break the curse of horizon" as termed by Liu 2018.ReviewThe proposed fixed point formulation for learning a parameterized off policy state distribution allows for lower variance off policy corrections decreasing the impact of the curse of horizon. This approach appears both theoretically sound and empirically well supported. I am concerned with the consistency argument made in section 4.3. In the general case, without access to the environment model, it is not possible to obtain two samples of x . It is worth noting that, because the parameter settings were tuned only for 50 trajectories, it is important to primarily assess performance based only on that point.<BRK>This paper proposes a new algorithm to the off policy evaluation problem in reinforcement learning, based on stationary state visitation. The proposed algorithm does not need to know the behavior policy probabilities, which provide a broader application scenario compared with previous work. It is based on directly solving the fixed point problem of the resulting target stationary distribution, instead of solving a fixed point problem about the stationary ratios. However, several unclear places in the current version hurt the presentation of results. 2) Related with the last one, B has an integral. To approximate the integral, we only have one sample from the transition probability actually, and the sample state is not uniformly distributed. These details are important for function approximator as NN.
Accept (Spotlight). rating score: 8. rating score: 8. rating score: 6. <BRK>This paper presents approaches to handle three aspects of real world RL on robotics: (1)learning from raw sensory inputs (2) minimal reward design effort (3) no manual resetting. Overall I think this is a good paper and valuable to the community. Experiments in simulation on the physical robots are performed to demonstrate the effectiveness of these components. Close related work is also used for comparison.<BRK>The paper takes seriously the question of having a robotic system learning continuously without manual reset nor state or reward engineering. The merit of the paper in this respect is to focus on a specific question and provide concrete results on this question, but this work should be positionned with respect to the broader approaches mentioned above. More local points:In the middle of page 5, it is said that the system does not learn properly just because it is stuck at the goal.<BRK>  *Synopsis*:  This paper focuses on current limitations of deploying RL approaches onto real world robotic systems. They focus on three main points: the need to use raw sensory data collected by the robot, the difficulty of handcrafted reward functions without external feedback, the lack of algorithms which are robust outside of episodic learning. Main Contributions:    A discussion of the current limitations of RL on real robotic systems    A framework for doing real world robotic RL without extra instrumentation (outside of the robot). I am not as familiar with the RL for robotics literature, but from some minor snooping around I believe these ideas to be novel and useful for the community. Specifically, giving the evaluation metrics you mentioned in the appendix.
Reject. rating score: 1. rating score: 3. rating score: 6. <BRK>Efficient Inference and Exploration for Reinforcement Learning This paper presents a pure exploration algorithm for reinforcement learning. This approach based on central limit theorem appears to be more insightful and cleaner and I think there is something nice about that! The proposed algorithm is reasonable, for the setting in question, and the experimental results show that it outperforms benchmark exploration algorithms in this setting. [The paper is 24 pages long and we have many reviews]However, there are also some significant places where this paper falls short:  The problem setting that the authors consider is really not typical of the "exploration" problem in RL... Overall I do think this is an interesting paper, with a novel approach to pure exploration in tabular MDPs under specific assumptions.<BRK>This paper studies the asymptotic properties of action value function $Q$ and value function $V$. Based on this argument, they also derived a similar convergence of value function $\widehat V_n$ in distribution. The proof of convergence in distribution is established based on the following idea: $\widehat r_n$ and $\widehat P_n$ converge to $r$ and $P$ by central limit theorem. Therefore, it needs to be justified that the limit point is $Q^*$. Thus Algorithm 1 may be inefficient in practice. I do not think the authors have addressed all my concerns.<BRK>This paper studies the inference problem of reinforcement learning. Experiments were performed to compare this method with previous algorithms, e.g., UCRL. The inference results for the Q function is definitely important and will be useful for the community. More comments:* The assumption on exploration policy is quite restrictive. It essentially says the bound does not hold if the MDP is not communicating. However, the Q function is still well defined. It is my understanding that your algorithm has a much larger regret due to that it requires to sample from every state.
Reject. rating score: 1. rating score: 3. rating score: 6. <BRK>Pros:1.The research problem studied in the paper is important. 2.All the experiments are conducted based on the convolutional graph neural network based methods. It is suggested to evaluate on the other types of graph neural network, e.g.Recurrent Graph Neural Networks, Graph Autoencoder.<BRK>I found the experiments in this section to be less convincing. The contributions of the paper are mainly experimental and show different "surprising" aspects about GNNs. However, I think the presentation may be improved and more information (e.g.about how the experiments were conducted or more plots) should be reported in the main paper or the appendix. Otherwise, it feels that the presented experimental results have been hand picked.<BRK>This paper analyzes the properties of graph neural networks (GNNs). The authors examine performance is a function of graph connectedness, and find a weak correlation. Some concerns: a) The part about attributes and topology is not very clear to me. What are the attributes?
Reject. rating score: 6. rating score: 6. rating score: 6. <BRK>This paper proposes to provide a detailed study on the explainability of link prediction (LP) models by utilizing a recent interpretation of word embeddings. The authors utilize this categorization to provide a better understanding of LP models’ performance through several experiments. This paper reads well and the results appear sound. My concerns are as follows:•    I am wondering about the reason for omitting Max/Avg path for two of the relations in WN18RR? On overall, although I find the proposed study very interesting and enlightening, I believe that the paper needs more experimental results and decisive conclusions.<BRK>Summary:The paper attempts to understand the latent structure underlying knowledge graph embedding methods. The work can be seen as an extension of understanding of PMI based word embedding methods. Through results, they demonstrate that a model’s ability to represent a specific relation type depends on the limitations imposed by the model architecture with respect to satisfying the necessary relation conditions. The results in Tables 3 and 4 demonstrate that MuRE is the most effective method for handling different types of relations but then how come its performance on FB15k 237 (.336 MRR) is significantly lower than other methods like TuckER (.358 MRR). It would be great if authors could provide some more reasoning behind coming with these predictions. 3.In Section 4.2, it is stated that “ranking based metrics like MRR and hits@k are flawed if entities are related to more than k others”.<BRK>There has been a large family of knowledge base models developed in the recent years, aiming to encode both entities and relations in a latent space, so the entities can be “linked” via a relation specific mapping. The paper categorizes all the relations into 3 types (1) related, (2) specialisation, and (3) context shift, and examines some relations in WordNet and NELL, and then empirically evaluates the performance of different types of models and draws the correlation of the results and intuitive understanding of different types of relations. for the important relations in practice. It is true that DistMult works well on the R type relations but it is not consistent between WN and NELL. It d be really great if the paper actually provides some insights on we can further improve these entity embeddings according to this categorization.
Reject. rating score: 3. rating score: 6. rating score: 6. <BRK>This paper uses KNN based retrieval method for extracting relevant information from different sources for the task of generating response in a dialogue task. The method is very straight forward and using nearest neighbors from external information sets as auxiliary information and encoding that information via same neural net to produce the encoded external information. The concatenation of encoded input and auxiliary information is concatenated to produce the output in the dialogue via decoder network. The novelty of the proposed method is incremental over Dinan et al.(2018).The KNN based retrieval module can produce some drift in dialogue from the actual context of the input which can result in irrelevant response in dialogue.<BRK>The idea seems simple and intuitive, and the results show improvements over prior work in dialogue generation and retrieval. The reviewer appreciates the human study and conversation example provided in the paper to qualitatively evaluate their model. If not, the reviewer recommends to provide some experiments with same conditions. This design choice makes the architecture inflexible to the number of data sources. If the authors conducted a quantitative evaluation to test this, it would be valuable to add it to the paper.<BRK>Summary:  The paper proposes augmenting transformer neural networks with KNN based information fetching modules that can access relevant external knowledge, combine knowledge from different sources, and integrate the information into seq to seq architectures. Strengths:  The paper is well written and well motivated. The authors evaluate their approach on 2 publicly available datasets and compare it to existing approaches, showing improvements in terms of F1. The authors conduct a human evaluation to compare their approach against other approaches. Parts of the analysis are rushed (e.g., sections 6.2 and 6.3).
Reject. rating score: 3. rating score: 3. rating score: 6. <BRK>I have read other reviewers  comments and the response. Overall, the contribution of retraining and detection with previously explored kernel density is limited. I am surprised that these standard detection methods were not even mentioned at all. Moreover, the detection performance reported in this paper is not better than the one reported in [2] (ResNet, CIFAR 10, 95.84%) which do not need retraining. RCE is also a baseline? 6.Some of the norms are not properly defined, which can be confusing in adversarial research. ICLR, 2018[2] A simple unified framework for detecting out of distribution samples and adversarial attacks.<BRK>A color bar would help here at the very least. I lean towards rejection of this paper. Would  this still work for a dataset with a large number of classes (e.g., ImageNet)? Optimize this objective using both gradient based and gradient free attacks  As the proposed defense is attack agnostic, I also suggest trying it out on rotation translation attacks, as the worst case attack can always be found by brute force searchOther   The citations for adversarial training in the 2nd paragraph of the intro are unusual.<BRK>Update after author response:I would like to thank the authors for the thoughtful response, and for addressing some of the concerns raised by the reviewers. As neural networks get deployed for increasingly critical applications, the issue of defense against adversarial attacks becomes progressively relevant. The paper is clearly written, and the approach is sensible. The work is somewhat incremental and the novelty mostly lies in pulling a few different methods together that seem to work well in unison. However, since the unified approach is simple yet novel, and the results fairly promising, I am somewhat inclined to accept this paper.
Accept (Poster). rating score: 8. rating score: 6. rating score: 6. <BRK>The authors propose a method to diversify samples generated from a VAE. The method is based on determinantal point processes in the latent space and relies on specifying a kernel in the sample space and a quality metric in the latent space. The experiments are sensible and show clear improvement over a reasonable selection of baselines.<BRK>This paper presents a novel approach for forecasting object trajectories (e.g., predicted paths of vehicles) forcing diversity of outputs. The authors adopt determinantal point processes (DPPs) to capture the diversity and propose a diversity sampling function (DSF) which consists of a neural network. But, since the standard log likelihood function can be singular, they additionally present an objective function for diversity by maximizing the expected cardinality, which admits replicated outputs. 4.It is also possible to apply DPP MAP inference to a set of latent codes generated from the encoder of cVAE. Are these outputs also diverse? or does the diversity of latent space reflect the diversity in the data trajectory space? Overall, this work proposes an approach combined cVAE with a DPP for forecasting diverse trajectories and the empirical results are promising as it outperforms other methods.<BRK>Rather than using the DPPs negative log likelihood as a measure of the trajectory set s diversity, the authors use the DPP s expected sample size as a proxy for a diversity metric. Experimentally, the authors show on two tasks that this approach generates more diverse, relevant trajectories than several competing baselines. I would have liked to see more extensive experiments, in particular (a) other baselines (DPP NLL as a diversity loss, different DPP kernels) and (b) experiments on larger datasets. However, I would have liked to see that intuition verified through an experiment that used the NLL as a diversity loss. The idea of using the expected subset size as a metric for diversity is compelling.
Reject. rating score: 3. rating score: 3. rating score: 6. <BRK>The paper proposes an image generation method with the focus on generating anime faces from various artists. The proposed method which is a combination of conditional GANs and conditional VAEs manages to generate high fidelity anime images with various styles. The goals are clearly stated and the background (which is more of history), as well as related work, is comprehensive. I agree with the authors that quantitative evaluation of generated anime faces is not easy (although it is possible with a carefully designed human study), however, the disentanglement (which is the focus of the paper) is easy to evaluate quantitatively. This demands for more experiments on disentanglement datasets with known generative factors. However, the impact of the paper (in the current form) is not clear. Also releasing the code dataset should increase the impact of the paper.<BRK>This work introduces a Generative Adversarial Disentangling Network based on two stages training the first aims at learning a style independent content encoder and then content and style conditional GANs is used for synthesis. To condition on the style (artist) authors introduce an extra adversarial classifier so the generator tries to generate samples that would be classified as the artist that it is conditionedon. Authors compare the proposed method against the original neural style transfer and StarGAN over various styles within the context of anime illustrations and the NIST Dataset where styles are being represented by artist name. While the work tackles some of the problems by conditioning only on artist names other than style features that might be hard to have annotations for. The proposed modifications are quite incremental.<BRK>This paper proposes a method to learn disentangled style (artist) and content representations. 2.By carefully designing two stage training objectives, the method learns a style independent content encoding E at the first stage and the style encoder S and generator G both from the first and the second stage. I think this paper makes a good contribution to disentangle style and content in anime. My main concern is the complicated learning procedure design may affect the reproducibility of this method. 1.I encourage the authors to release their code when published. Aside from that, are you using the trained G, S from the first stage to initialize G, S in the second stage? 3.There are eight different terms in stage 2, so it worth checking the necessity for those terms.
Accept (Poster). rating score: 8. rating score: 6. rating score: 6. <BRK># SummaryIn this paper, the authors propose a novel approach for learning the structure of Directed Graphical Models (DGMs) from observational data that allows to flexibly model nonlinear relationships between variables using neural networks.<BRK>This work addresses the problem of learning the structure of directed acyclic graphs in the presence of nonlinearities. The proposed approach is an extension of the NOTEARS algorithm which uses a neural network for each node in the graph during structure learning. This adaptation allows for non linear relationships to be easily modeled. In addition to the proposed adaptation, the authors employ a number of heuristics from the causal discovery literature to improve the efficacy of the search. Overall, I found the paper to be well written and sensible. How were these chosen?<BRK>Summary: The authors propose a prediction model for directed acyclic graphs (DAGs) over a fixed set of vertices based on a neural network. Minor comments: “X_\pi_j^G denote the random vector containing the variables corresponding to the parents of j in G.” – this definition is not clear.
Reject. rating score: 1. rating score: 3. rating score: 6. <BRK>They also claim to focus on black box attacks because is it more practical in a real world setting. The paper presents thorough ablation studies on the architectural choices that go into this model. This is a positive quality of the work.<BRK>The paper deals with robustness against adversarial attacks. On the one hand, an easier way to surely get rid of those patterns is simply to blur the images accordingly before feeding them to the network. Almost all pixels will be seen by some non defective filters, so it would seem that the hi res information is implicitly still there. Why?What completely confused me was which networks were actually used to generate the adversarial examples. If defective convolutions indeed become popular, then an attacker would obviously know about that and also use a defective network to generate his adversarial examples.<BRK>* Summary *The paper proposes defective convolutional layers as a measure of defense against adversarial attacks on deep neural networks. In the white box setting (Table 8) a stronger attacker with a larger number of iterations and random restarts should be used in order to ensure that the difference in defense performance is real. The shape vs texture tradeoff is supported by experiments showing that defective CNNs perform worse than normal CNNs on images with permuted patches and that adversarial examples with larger epsilons exhibit more semantic shapes. What happens if this method is combined with adversarial training?
Accept (Poster). rating score: 6. rating score: 6. rating score: 3. <BRK>Summary:This paper introduces and analyzes a training algorithm for neural networks with non smooth regularization and weight constraints (such as sparsity or binarization). The paper is clearly written and easy to follow. This is particularly important in the binary case where additional optimization parameters are added and updated in each iteration. The paper could be improved by more extensively optimizing hyper parameters of all algorithms in the experimental evaluation.<BRK>The paper proposes a new gradient based stochastic optimization algorithm (with gradient averaging) by adapting theory for proximal algorithms (originally developed for convex problems) to the non convex setting.<BRK>[Summary]This paper proposes Prox SGD, a theoretical framework for stochastic optimization algorithms that (1) incorporates momentum and coordinate wise scaling as in Adam, and (2) can handle constraint and (non smooth) regularizers through the proximal operator. With proper choices of hyperparameters, the algorithm is shown to converge asymptotically to stationarity, for smooth non convex loss + convex constraint/regularizer.
Reject. rating score: 6. rating score: 6. rating score: 6. <BRK>This paper proposes the variational hyper RNN (VHRNN), which extends the previous variational RNN (VRNN) by learning the parameters of RNN using a hyper RNN. VRHNN is tested and compared with VRNN on synthetic and real datasets. The performance of VHRNN is promising and certainly better than the previous VRNN for some applications. Isn’t it useless to present results for the VRNN with a latent dimension of 4, at least as a sanity check? While the FIBO is mainly used as the objective in this paper, is the ELBO enough if the authors care the simultaneous low reconstruction error and low KL divergence?<BRK>This paper proposes a variational hyper recurrent neural network which is a combination of the variational RNN and the hypernetwork. The model and learning algorithm are compared to the variational RNN and tested on a variety of synthetic settings where the VHRNN outperforms the VRNN in held out likelihood. On four real world sequential datasets, the paper finds that the model outperforms the VRNN across many configurations and with a fewer number of parameters. Summary: I don t think the model presented here is very novel, in that it is a combination of existing ideas; however, the paper does a good job of studying the model in a variety of different configurations on both synthetic and real world data.<BRK>In this paper the authors propose an architecture based on variational autoencoders and hyper networks. The experimental results show the benefit of the model compared to a similar method without hypernets. In terms of novelty, the combination of auto encoder RNNs and hyper networks is not entirely novel and it has previously been developed (https://www.biorxiv.org/content/10.1101/658252v1). The architecture is not entirely clear from the text. In terms of model architecture, it wasn’t clear for me why the hyper network for \phi is feedforward but the one for \theta is RNN? How many units in each layer? There are four sets of weights in the primary model: weights of RNN, dec, enc, prior. Do they refer to the number of parameters in the hypernetworks only?
Reject. rating score: 3. rating score: 3. rating score: 3. rating score: 6. <BRK>This paper points out that the industrial copyright detection systems are susceptible to adversarial attacks and discuss the reason why these systems, especially the neural network based systems, are vulnerable to attacks. The authors describe a widely used music identification method and propose a simple gradient method to attack this system in this paper. The white box attack on their own model and the experiments on two real world systems (AudioTag and YouTube) validate the effectiveness of the proposed method. Weak points:From my point of view, this paper is more like an application oriented paper than a research paper. The detail of the model used in the experiment is not presented, such as how to train the model.<BRK>While the paper considers a system that was designed by the authors, because they demonstrate transferability to commercial products, this is not a limitation of their work and demonstrates a realistic threat model. The study of this application is well motivated: adversarial examples need not be played in the physical domain (since the audio/video is uploaded directly in a digital format) and copyright detection is a bit more complicated than object classification (because false positives are expensive), small margin between classes. Could you motivate the choice of the L_inf norm as an appropriate metric for the magnitude of the perturbation added to audio samples? The paper is well written and easy to follow.<BRK>[Summary] This paper proposed a black(white) box attacking algorithms to attack industrial copyright detectors. Specifically, a simple un targeted gradient based method can successfully fool commercial system like YouTube and AudioTag. A cognitive study or something like a user study should be conducted. As this paper suggests, these more accurate detectors also come with severe security problems, and such problems are rarely studied before. I am not an expert in acoustic/audio recognition, but this paper shows that a not very complicated approach can work in real world without too much domain/expert knowledge. First, section 4.2 is very similar to Wang et al.(2003).What’s the biggest improvement or novelty here? The biggest advantage of deep learning (end2end trainable as feature extractor) is not utilized. However, this part is missing. The experiments in this paper is more like a “proof of concept” rather than “serious evaluation”. First problem is that the norm is used to evaluate the perturbation.<BRK>The authors of this work bring to light the security vulnerabilities of copyright detection systems to (DL style) adversarial attacks. Following which the authors embark on designing a simple copyright system based on a neural network. Overall the paper brings about an interesting and pressing issue in a timely manner that seems to be of broad interest to the security and ML community. The paper is very well written and I enjoyed reading it. However, I am not as familiar with the literature with adversarial attacks for audio. In particular, it is difficult for me to assess the success of the imperceptibility of the perturbation in the audio domain from l2 and l_infinity norms, so it is hard for me to judge whether such adversarial examples are competitive and useful.
Reject. rating score: 1. rating score: 3. rating score: 3. <BRK>The paper proposes an approach to find a map between two feature spaces to maximize correlation between them and to use the resulting map for inference. First, the approach looks very similar to deep CCA, but the connection is never mentioned. This connection needs to be clarified. The paper severely lacks in relation to relevant related work. The improvement in convergence from RFA for supervised learning is interesting and this aspect deserves more analysis.<BRK>The paper proposes how the correlation between two different types of data can be extracted from learned representations. The proposed metric can also be used as an alternative to cross entropy loss. However there are significant draw becks:1) Similarity and Metric Learning is a booming area in machine learning with several different directions focusing on different problems.<BRK>This is performed by modeling features with neural networks and optimizing an objective function that maximizes a measure of correlation between the features versus learning a generative model such as a CVAE. * The following sentence is a run on. * Is there an interpretation for why both spaces require the same feature dimension, k_0? ***Comments not related to decision:* It is generally good to avoid sweeping statements such as the first sentence of the introduction. * It seems strange to have the supervised learning experiment of 4.1 as the first experimental result of the paper since it is an unintended and unexplained side effect of the approach.
Reject. rating score: 3. rating score: 6. rating score: 6. <BRK>The paper proposes an analysis of CycleGAN methods. The main proposition of the paper is that the task at hand can be efficiently coded through the distance (cost) function of OT. Overall the paper is well written and the proposition is reasonable. 2477 2486In this spirit, I wonder if a comparison with only the vanilla cycleGAN is sufficient to really assess the interest of using the OT formulation of the problem. As a summary:Pros:A nice interpretation of CycleGAN with OTThe paper is fairly well writtenCons: Overall the quantity of novelties is, in the eyes of the reviewer, somehow limited.<BRK>The paper revisits unsupervised domain translation (UDT) in light of optimal transport. Then the paper redefines UDT problems in the optimal transport framework. Last it proposes an approach to solve UDT problems based on the dynamical formulation of optimal transport. The paper helps to clarify some shortcomings of previous approaches and proposes a new solution. The paper is well written. * I think that there should be a paragraph for the computation of the inverse. As said before, the design of the cost function is sensitive.<BRK>Summary: The paper addresses the ill posedness of the unsupervised domain translation (UDT) problem. It provides a more structured and rigorous problem definition than previous works (mainly CycleGAN based), and proposes the theory of optimal transport (OT) as a better framework for solving UDT. The paper provides an interesting link between a dynamical formulation of OT and residual networks, which leads to a practical algorithm for solving OT/UDT. Experiments highlight two main points: 1) CycleGAN are biased towards learning nearly identity mappings, and 2) the OT formulation allows for modelling explicit biases in the learned solution through the design of the cost function. Strengths & Weaknesses:  +  The paper addresses an important problem, which as far as I know, is widely known but not properly or explicitly addressed in prior work. However, the paper needs to emphasize that this is particularly tied to the choice of resnet architectures that is commonly used.
Reject. rating score: 3. rating score: 3. rating score: 6. <BRK>This work introduces GQ Net, a novel technique that trains quantization friendly networks that facilitate for 4 bit weights and activations. This is achieved by introducing a loss function that consists of a linear combination of two components: one that aims to minimize the error of the network on the training labels of the dataset and one that aims to minimize the discrepancy of the model output with respect to the output of the model when the weights and activations are quantized. For the discrepancy metric the authors use the KL divergence from the predictive distribution of the floating point model to the one of the quantized model. This work is well written and in general conveys the main idea in an effective manner. The idea seems on a high level to be interesting and simple; train floating point models that can fit the data well while also encouraging them to be robust to quantization by enforcing the predictive distributions of the fixed and floating point models to be similar in the KL divergence sense. This raises the question whether the boost in performance is due to the several additional steps employed (which in general can be applied to other quantization techniques as well), and not due to the main idea itself.<BRK>The paper propose a new quantization friendly network training algorithm called GQ (or DQ) net. It is a well written paper. I think the paper will benefit from a more in depth discussion and analysis on this regularization issue. The schedule for the loss term blending parameters looks drastic to me. In the ablation studies, it seems that some of the suggested training options are conflicting each other and the clear winner seems to be the multi domain BN.<BRK>Specifically, during training, the proposed method contains a full precision branch supervised by classification loss for accurate prediction and representation learning, as well as a parameterized quantization branch to approximate the full precision branch. Strengths:+ Well written paper with good clarity and technical correctness. + Good and clear ablation study. Weaknesses:  Major performance improvement comes from the combination of different incremental improvements. The proposed framework seems technically correct and effective. However, a major weakness of this work is that most of the performance improvement comes from a combination of add on improvements, except that the authors put them together into a unified framework and explained elegantly. For example, the alternative optimization of W and \theta is similar to alternative re training in network pruning, although a unified loss/optimization framework is applicable in this case. Others such as dynamic scheduling and gradient detach are also heuristic driven. Citations and comparisons to more recent binarized networks other than XNOR Net will be appreciated too.
Reject. rating score: 3. rating score: 3. rating score: 6. rating score: 6. <BRK>This paper changes the distribution of number of filters (called “filter distribution template”, or "template") at each layer in modern deep Conv models (e.g., VGG, Inception, ResNet) and discover that the model with unconventional (e.g., reverse base, quadratic) template sometimes outperform conventional one (when f_{l+1}   2 f_l). It is not justified at all and the empirical result is also mixed (See Table 1). This brings about the question mark of the motivation of this paper from the first place. In contrast, empirically people have observed that a model with more parameters (and/or more FLOPs) within the same architecture family gives better performance. This is not directly related to the total number of filters, which the main topic in the paper. I would strongly suggest the authors to compare the performance between different templates when keeping #parameters and/or FLOPs fixed. This should be easy to do by computing how many filters are needed per layer to reach the desired #parameters/FLOPs, while keeping the desired distribution. Also, to make a strong conclusion, they paper should also report ImageNet results trained with different templates. Post Rebuttal I reread the paper after authors revision and rebuttal. Thanks authors for the hard work. Indeed the authors have compared different templates when the number of parameters remain approximately the same (in both the original version and the new revision). However, after rereading the paper, the conclusion is still not that clear and I didn t see a clear take home message about which filter template is better than the other. In the section "Template Effects with similar resources", it seems that uniform template patterns is the best for many models, which somehow is negative results given the motivation of this paper. In addition, when comparing Fig.3 in the original version versus that (Fig.3) in the revision, some curves have changed their shape drastically (e.g., MobileNet on CIFAR10 and CIFAR100) and uniform template shows stronger dominance. This worries me a bit that the experiments might still be preliminary and the paper is yet not ready for publication. I will keep the score.<BRK>This paper investigates the impact of several predefined filter templates, including uniform template, reverse template, quadratic template, and negative quadratic template, to the performance of different neural networks such as VGG 19, ResNet 50, Inception Network, and MobileNet. However, it is impossible to enumerate all possible templates and hence some good templates may not be included and studied in this paper, which makes the empirical studies in this paper less useful. In experiments, authors need to compare with the results of neural architecture search. Based on such comparison, we can see whether the templates used in this paper are reasonable. In experiments, it seems that different templates may have their own characteristics and their usefulness also depends on the neural network used.<BRK>Apart from reversing the filter distribution, which starts a network with a large amount of filters and then reduces them (given how the filter number is generally increased with depth and reduction in spatial resolution), the authors also propose using the same number of filters per layer ("uniform"), as well as "quadratic" and "negative quadratic" distributions. There does not seem to be any particular motivation for these patterns, but this is at least better than poor justifications. The results include 4 common CNN architectures and experiments on CIFAR 10 and CIFAR 100, which is acceptable, but only single results are shown, which makes it hard to judge the significance of the performance of the redistributed networks, especially given that there seems to be no particular trend in the performance of any of the templates. Unfortunately memory footprint results are mixed, and the default architectures tend to perform much better with respect to inference time. Most of the figures are hard to read and not labelled completely (e.g.relying on the caption to dictate which column corresponds to which architecture), so these should be reworked. Combined with the lack of multiple runs to provide more solid evidence for the empirical findings, I would reject this work.<BRK>This paper presents a simple methodological study on the effect of the distribution of  convolutional filters on the accuracy of deep convolutional networks on the CIFAR 10 and CIFAR 100 data sets. For these distributions, the total number of filters is varied to study the trade off between running time vs. accuracy, memory vs. accuracy and parameter count vs. accuracy. Given that this paper is easy to read and presents interesting insights for the design of convolutional network architectures and challenges mainstream views, I would consider it to be a generally valuable contribution, at least I enjoyed reading it. Despite the intriguing nature of this paper, there are several weaknesses which make me less enthusiastic about the quality of the paper:  The experiments are done only on CIFAR 10 and CIFAR 100. It would be useful to see whether the results also hold for more realistic vision benchmarks. Even if running all the experiments would be costly, I think that at least a small selection should be reproduced on OpenImages or MS Coco or other more realistic benchmarks to validate the findings of this paper. It would be interesting to see whether starting from the best channel distributions, applying MorphNet would end up with different distributions. The paper does not clarify how the channel sizes for Inception were distributed, since proper balancing of the 1x1 and more spread out convolutions is a key part of that architecture. The paper presents itself as a methodology for automatically generating the optimal number of channels, while it is more of a one off experiment and observation than a general purpose method. Another small technical detail regarding the choice of colors in the diagrams: the baseline distribution and constant distribution are very hard to distinguish. This is especially critical because these are the two best distributions on average. If it contained at least partial results on more realistic data sets, I would vote for strong accept, but in its current form, I find it borderline acceptance worthy.
Accept (Poster). rating score: 6. rating score: 6. rating score: 3. <BRK>This paper contributes theoretically to the information bottleneck (IB) principle. The core result is given by theorem 1: the phase transition betas necessarily satisfy an equation, where the LHS is expressed in terms of an optimal perturbation of the encoding function X >Z. The reasons for the weak acceptance instead of a strong acceptance are explained by the following weaknesses. As a theoretical contribution, it is important to have good formulations and clear statements.<BRK>This paper studies the phase transition problem in the information bottleneck (IB) objective and derives a formula for IB phase transitions. Based on the theory developed in the paper, an algorithm is developed to find phase transition points. The interesting observation is phase transition can correspond to learning new class and this paper conjectures in IB for classification, the number of phase transitions is at most C 1, C the number of classes. The theory developed connects the IB objective, dataset and the representation, and thorough proofs are given. The experiments matches the theoretical findings.<BRK>In this paper, the authors studied the phase transition in the information bottleneck, which is defined as the point where the IB loss landscape changes. The authors give a theorem showing the practical condition for IB phase transitions which is related to Fisher information. For example, in definition 1, the authors write "\eta << \beta" and "|\epsilon| << 1", where the quantities \eta and \epsilon are used many times in the proof. The authors should consider rewrite their proof by using limit.
Accept (Poster). rating score: 6. rating score: 6. rating score: 6. <BRK>This paper proposes a novel way to outsource a part of the information processing in a deep learning model to an untrusted remote location while revealing only little information about the input or final output of the computation. In my opinion, this should be a centrepiece of motivation for this work (unless I have overlooked something). Additional distractor signals are encoded through a GAN type approach.<BRK>Abstract:In this paper, the authors propose to hide information in phase of the input features. I think the general idea of the paper is interesting, but overall, the paper is very poorly written. It appears that it is written in a rush. *The abstract and introduction are poorly written.<BRK>Summary:This paper proposed a complex valued neural network to protect the input data from hidden features of DNNs. Detailed comments:The research topic and main idea of this paper (i.e.introducing a complex valued neural network for hiding sensitive input data) are interesting and the authors showed that the proposed idea indeed works well using various neural architectures and datasets. Overall, the paper is well written and the ideas are novel.
Reject. rating score: 3. rating score: 3. rating score: 6. <BRK>Summary:This work proposes a method to perform clustering on a time series data for prediction purposes, unlike the classical clustering where it is done in an unsupervised manner. The effect of Embedding Separation Loss (Eq.5) seems quite limited. Table 3 and Figure 3 show that the proposed method can capture heterogeneous subgroups of the dataset.<BRK>This paper proposes a temporal clustering algorithm for the medical domain. The main advantage of the proposed method is that it uses supervised information for temporal clustering. Perhaps a bigger issue is that there is a general tradeoff between reconstructing the time series (the unsupervised learning portion) vs. predictive performance that is common to predictive clustering problems   it is not addressed here.<BRK>The authors propose a clustering approach for time series that encourages instances with similar time profiles to be clustered together. More so in practice when the number of identified clusters is a function of the model architecture and hyperparameters (\alpha and \beta).
Reject. rating score: 1. rating score: 1. rating score: 3. <BRK>This paper seeks to analyze the global robustness of neural networks, a concept defined in the paper. Second, the paper tries to do too many different things, and as a result does not give enough attention to any particular topic. While I acknowledge that a few prior works exists along these lines, I do not feel that this work provides much new insight into why global robustness is interesting to examine. The authors go on to prove results showing that an empirical estimator of the local robustness will converge to the global robustness. Next, I would suggest that the authors avoid using the word “guarantees” if they are estimating empirical local robustness in an approximate (rather than exact) manner. Guarantees implies strict results, but the authors use a weak attack (FGSM) to approximate empirical local robustness.<BRK>Following that, the paper investigates how to estimate local and global adversarial robustness using an estimator based on evaluating these quantities on the empirical distribution. Comments:The authors  insistence on their contribution being proving measurability does not make sense   of course everything is measurable! Furthermore, the formal definitions or local and global robustness are well known, the bounds in Theorems 1 and 2 are not novel and highly unlikely to be tight. The redeeming aspect of the paper is the experiments, where the authors show that these bounds can actually be (approximately) calculated. However, I feel that merely experimental results with correct but not significant theoretical contributions does not meet the bar for acceptance.<BRK>This paper studies the adversarial robustness of neural networks by giving theoretical guarantees, providing statistical estimators and running experiments. The problem is that a fair bit of it is quite basic: for example the measurability property is very much expected   noone was doubting it, and the proof is more of a formality than a contribution. Similarly with the statistical sampling: the method seems to rely on i.i.d.sampling   has this reviewer missed any important details? If not, then it s only the bounds that are a contribution, but the method is not. We would appreciate more specific description of the main contribution, without it we cannot recommend the acceptance of this paper. I am very grateful to the authors for their response. I would strongly suggest re writing it, possibly into separate papers, to make the things pointed out in the response more clear and self standing.
Accept (Spotlight). rating score: 8. rating score: 8. rating score: 8. <BRK>This paper proposes a straightforward method for training black box solvers of a restricted kind (namely those with inputs in R^n and linear cost functions). I would recommend accepting this paper. It is a well written paper with a novel idea supported by good experimental results.<BRK>  Summary  The authors propose a method for efficiently backpropagating through unmodified blackbox implementations of exact combinatorial solvers with linear objective functions. Overall, I recommend for acceptance. Are there arguments for why this is important besides the experimental results? Recommendation  This paper addresses an important problem and presents a novel approach.<BRK>This paper shows how end to end learning can be done throughcombinatorial solvers by using the derivative ofcontinuous surrogate function in the backward pass. One elegant part of the method is that no modificationor relaxation is done to the combinatorial solver inthe forward pass and that the backward pass just requiresanother call to the blackbox solver. One of my concerns with this work is that the ResNet baseline in theexperimental results seems like too much of a straw man for the tasks. I do not see why they should have the capacity to generalize well.
Accept (Talk). rating score: 8. rating score: 8. rating score: 8. <BRK>This paper studies the causes of why DARTS often results in models that do not generalize to the test set. To address this problem, the paper proposes several different ways to address this, e.g., an early stopping criteria and regularization methods. The final method is fairly simple, running the search with different regularization parameters and keeping the best model, which suggests it could be widely used for DARTS based approaches.<BRK>This paper seeks to understand why Differential Architecture Search (DAS) might fail to find neural net architectures that perform well. The results look promising (though as someone who is not familiar with the datasets, I don t have a sense of the significance of improvements.) In general, this is a strong paper. I enjoyed reading it.<BRK>This paper proposes an interesting systematic study of differentiable approach in NAS. It then proposes an early stop scheme (DARTS ES) to stop the search before this phenomenon occurs. Could the author comment on it? The idea to use more search spaces for comparison is fair and performance increase demonstrates the proposed R DARTS and DARTS ES  are effective. The reason to use one shot is that all the sub paths will have a fair chance to be trained.
Accept (Poster). rating score: 8. rating score: 6. rating score: 6. <BRK>Update: the authors have addressed my issues below and I have raised my score to 8. ***This paper on recurrent neural networks goes back to Rosenblatt s continuous time dynamics model and uses a discretised version of that equation (equation (1) in the paper) to build an incremental version of the RNN, called incremental RNN, where the transition from hidden state h_{k 1} at step k 1 to next step s h_k is done using small incremental updates until the system achieves equilibrium. It claims that it manages to solve the vanishing gradient problem by keeping all gradients \frac{\partial h_k}{\partial h_{k 1}} equal to minus identity matrix.<BRK>In this paper, the authors propose the incremental RNN (iRNN), which is inspired by the continuous time RNN (CTRNN). The main theorem provides a good theoretically guaranteed solution. The motivation of the paper could be improved. The paper is clearly written and well organized.<BRK>The authors present a novel work to address the problem of signal propagation in the recurrent neural networks. This idea is elegant. Overall, I think the paper is an interesting contribution to the community. In practice, the long term gradient problem may still exist in the incremental RNN.
Reject. rating score: 6. rating score: 6. rating score: 8. <BRK>This paper suggests an approach for learning how to sparsify similarity search graphs. The paper suggests a learning framework that uses sample queries in order to determine which edges are more useful for searches, and prune the less useful edges. This is a sensible and potentially useful approach in line with the recent flurry of work on improving algorithms with tool from machine learning. The paper would be significantly helped by showing non negligible improvement on more than one dataset or in more settings.<BRK>It formulates the task of pruning the graph as a problem of learning annealable proximity graph. A hard pruning processes is used after the learning process, and the results shows that the proposed method can reduce 50% of the edges and speed up the search time by 16 41%. The proposed method is mainly based on the comparison with [Malkov 2016], which did not use an extra training set to learn the NPG as proposed in this paper. However, I cannot be convinced it is the state of the art for large scale nearest search unless I see more comparisons in the new version.<BRK>Summary:The authors propose an extension of proximity graphs, Annealable Proximity Graphs (APG) for similarity search. APG augments pre built proximity graph with a learnable weight and a keep probability with each edge in the graph which are updated in an iterative manner. Once the optimization finishes, edge weights are ranked and less important edges are pruned as per the desired pruning ratio. Overall, I find that the proposed method and its results are convincing. The paper is very well written and all steps are rightly justified. In Figure 4, for all three datasets, the performance starts to drop exactly after 50%.
Reject. rating score: 3. rating score: 3. rating score: 6. <BRK>This paper gives a perspective of GANs from control theory, where it is well established in which cases a dynamical system (which can be described analytically as a function of time) is stable or not (by looking at the roots of the denominator of the so called transfer function in control theory). It is interesting that the analysis using this framework on simple examples is in line with known results in the GAN literature (Dirac GAN). In my understanding, the authors present these results to justify the validity of the approach. In other words, although in the present results, the proposed NF SGAN/WGAN outperforms the baseline, the reported performance of the baselines is worse than in related works on CIFAR10.<BRK>Authors use control theory to analyze and stabilize GAN s training. This has not been considered in the analysis. There are a few work in the literature that analyze local stability of GANs (e.g.https://arxiv.org/abs/1706.04156) as well as using some control theory for analyzing global stability of GANs (e.g.https://arxiv.org/abs/1710.10793). The connections of the proposed approach with existing literature should be better explained. In the empirical results, the performance of the proposed method and Reg GAN (from the numerics of GAN paper) are quite similar. Are there instances that the proposed approach significantly improves the stability of practical GAN architectures?<BRK>This paper proposes a novel view for stabilising GANs from the perspective of control theory. This view provides new insights into GAN training and may inspire future research along this direction.
Reject. rating score: 3. rating score: 6. rating score: 8. <BRK>This paper introduces a method for using an ensemble of deep reinforcement learning policies, where members of the ensemble are periodically updated to imitate the most promising member of the ensemble. In that sense, the presented results make sense, but I disagree with the framing of the results and how they are presented here. The authors show that the notion of best is spread across different agents.<BRK>This paper proposes an RL training procedure that maintains an ensemble of k policies and periodically pushes all the policies to be closer to the best performing one. The formulation, experiments and analysis are very clear and show a mild improvement over using the same underlying RL algorithm without the imitation part. In their case, they use SAC. They call this ablation SAC ensemble.<BRK>Summary:This paper proposed an ensemble method (CIKD) that train multiple agents anduse knowledge distillation to transfer knowledge from the current best agent tosub optimal agents periodically. The experimental results are sufficient, and the ablation studies are conducted thoroughly. I recommend the acceptance of this paper. Investigation on the reasons for improvement:Though extensive ablation studies have shown the effectivenessof each component of CIKD. Are there any guidelines to set up this threshold?
Reject. rating score: 3. rating score: 3. rating score: 3. <BRK>Summary:This paper proposes a method that saves memory and computation in the task of video prediction by low rank tensor representations via tensor decomposition. No videos providedThe paper does not provide any videos which is a must for video prediction papers. The method is able to outperform standard convolutional lstm and other methods by using less parameters when testing it in the Moving MNIST and KTH datasets.<BRK>This paper proposed a convolutional tensor train (CTT) format based high order and convolutional LSTM approach for long term video prediction. (2) The justification of high order modeling in long term prediction. How does it compare to TT for high order ConvLSTM? Despite the promising results, this paper is not ready for ICLR yet.<BRK>This paper build a higher order spatio temporal model by means of combining Convolutional Tensor Train Decomposition(CTTD) and ConvLSTM, and utilize the combination method to solve long term video prediction problems. Conclusion:This paper is in some way novel, but not enough for ICLR, and the experiment results seems not enough convincing, so I will give a weak reject. Experiments on Moving MNIST and KTH datasets show that the proposed method achieved better results than standard ConvLSTM, and in some way comparable with SOTA model.
Accept (Spotlight). rating score: 6. rating score: 6. rating score: 6. <BRK>The authors proposed a so called feature wise transformation layer and integrate it into some existing metric based few shot learning methods, which helps learn more diverse feature distributions for better generalization ability. I think this is a pioneer work on few shot learning for domain generalization, which is quite interesting. Is there any chance that another set of (\theta_\gamma, \theta_\beta) may lead to better performance than the learning to learn version? 3.Algorithm 1 should be quite expensive in time. It is not clear how the algorithm will be terminated.<BRK>In this paper, the authors propose a feature wise transformation layer for the cross domain few shot classification task. The reduction of significant shifts in the feature norm could be crucial for successfully transferring the source domain model to the target domain. Remember that the proposed method is designed to minimize the large discrepancy of the feature distribution. I am suggesting the authors include more recent state of the art as baselines and comparisons, which could make such submission much stronger. 3.This paper mainly focuses on metric based few shot frameworks.<BRK>This paper proposes a feature wise transformation layer to augment the image features, which is a new regularization of neural networks and leads to better generalization ability of the features. And the proposed method performs well in the few shot classification problem. Which leads to consistent improvement in the cross domain leave one out setting. Is the initial value sensitive to the performance? * There is no direct comparison to the state of the art methods. Overall, the paper is well written and the figures are well illustrated. The experiments show the effectiveness of the proposed feature wise transformation layers and the learning to learning approach. But the above concerns should be addressed.
Reject. rating score: 3. rating score: 3. rating score: 6. <BRK>Overall an interesting paper. However, I think a few things could be improved and I would be willing to rise the score if the authors could addressed the above points. The experiments suggest that RNNs are not optimal in terms of prediction of the future.<BRK>In general the IBL is not tractable and hence the paper uses approximate computations. I definitely like the underlying question of the paper.<BRK>A significant section is dedicated to an empirical study that shows that classically train MLE RNNs lead to internal representations with a suboptimal mutual information to the past and the future. It then considers stochastic training, adding Gaussian noise to the hidden states during the training of the RNNs to limit past information. I have the feeling that the benefit that is not understood and that intuitions that arise from the manuscript may not be as useful as we would like. The stochastic training does seem useful.
Reject. rating score: 1. rating score: 6. rating score: 8. <BRK>The fundamental claim is thereby unsustainable by current results. The idea is extremely intriguing and very promising, easily leading to supportive enthusiasm; it is my personal belief however that accepting this work in such a premature stage (and with an incorrect claim) could stunt further research in this direction. The "standard neuroevolutionary algorithm" from 2006 presented as baseline has not been state of the art for over a decade.<BRK>Post rebuttal update I am satisfied with the author response and changes to the paper. I have increased my rating accordingly. Since MERL is an ensemble method, is that not a more direct comparison?<BRK>I hope that the paper could have a deeper discussion if it is an important design decision. The response addressed my questions. Thus I will keep my original recommendation of acceptance.
Reject. rating score: 1. rating score: 1. rating score: 3. <BRK>The motivation of the method is to obtain representations that are more robust to input distortions such as blur, noise, over exposure, etc. Figure 2: the provided t SNE projections do not show a clear advantage of the proposed method.<BRK>The authors propose an approach to reduce the sensitivity of neural networks to visual distortions. How was the eps used to train the network chosen? This seems to suggest that the robust model baseline reported in this paper is not accurate/representative.<BRK>This paper proposed a method to represent data with its second order information in the deep network, to improve its representation robustness. Experimental results show that the new representation is more robust against real world distortions. It would be nice if the authors can provide some mathematical derivation or justification of the chose expression, in addition to the empirical evidence.
Reject. rating score: 1. rating score: 1. rating score: 1. <BRK>In particular, the paper is concerned with posterior collapse i.e.posterior remains at the prior, limiting the model’s ability to reconstruct the data. Experiments are performed on a synthetic 2D dataset, ‘Gaussian ovals,’ and on OMNIGLOT. No careful ablation study of this sort is performed.<BRK>The experiments demonstrate a small improvement of SSIM metric on Omniglot. 3.The novelty compared to (Abiri & Ohlsson, 2019) is very small. The text is also not very well written. 4.The text of the paper can be significantly improved.<BRK>The authors listed a few candidates, including the diagonal and full covariance matrices. In general, this paper is confusing and difficult to follow.
Accept (Poster). rating score: 8. rating score: 6. rating score: 3. <BRK>This paper proposed a GNN to improve an existing search based solver for 2 QBF solvers. Justifiably, a reinforcement learning formulation is used. I thought that the paper was very impressive. Limitations of the current approach are also properly discussed. Only comment I have is that using shallow networks with one iteration sounds not enough for a problem like 2 QBF solving. I noticed that you have this exploration in the appendix, would recommend moving it to the main paper. I would also have liked 2 QBF to be mentioned more explicitly in the abstract and early introduction.<BRK>This will be an uncharacteristically short review. The work poses an interesting idea: why not mix heuristics and learning. The elephant in the room is that I have read this paper before. It was not the best decision for ICLR 2019 to have rejected this paper, honestly. I will argue to accept the paper if the references are updated, it deserves a wider audience. The representation also does not encode the quantifiers well but I feel this is a question for future work. Minor comments:  Please update your paper. This is one of the most intriguing question in artificial intelligence today.<BRK>This paper investigates the problem of predicting the truth of quantified boolean formulae using deep reinforcement learning. The introduction is well written and explains the interest of learning new heuristics for QBF problems. So, it is quite difficult to accept the paper in its current state. Well, this is not an algorithmic problem, but a decision problem. Did the author perform some grid search to find those values? I guess that the authors are talking about 2 QBF, for which the prenex is of the form $\forall \exists$. Is it a literal defined on a universally quantified variable? First of all, I am not entirely convinced that an MDP is the right choice for specifying this problem. This should be clarified in the paper.
Accept (Spotlight). rating score: 8. rating score: 6. rating score: 6. <BRK>Summary:This paper introduces a probabilistic generative model forunsupervised style transfer of text. The approach introduced inthe paper does not require paired training data. Anencoder decoder model is trained to transfer text from one styleto another and back. Review:This work is very well written and easy to follow. Great results are shown on 5 text transfer problems. Ishaving the loss of a language model work that much better than the regularentropy term? I would also like if possible if you could share some of the repetitive examplescreated by BT NLL which explain its low PPL.<BRK>In this paper, the authors propose a probabilistic framework for unsupervised text style transfer. Given two non parallel corpora X,Y in different domains, the authors introduce unobserved corpora \bar{X}, \bar{Y}. These are used as latent variables that control the generation of the observed data. The approach is evaluated on five style transfer tasks, as well as unsupervised machine translation. I lean towards the acceptance of the paper because the approach is fairly simple and elegant, while obtaining promising results. In particular, an experimental comparison to stochastic sampling, which should more closely approximate the expectation, would be appreciated.<BRK>The main contribution of this paper is a principled probabilistic framework of unsupervised sequence to sequence transfer (text to text in particular). Overall I think it is a well written paper with a large experimental suite, although I am skeptical of actual connection between probabilistic formulation and whats actually happening in practice. Although the evaluation metrics on text style transfer tasks like sentiment transfer, formality transfer, author imitation are in line with previous work ideally the human evaluation needs to be done to truly see how well each method performs. On unsupervised machine translation, authors show a large improvement on Serbian Bostian translation.
Accept (Poster). rating score: 8. rating score: 3. rating score: 3. <BRK>The authors use tools from algebraic geometry to study the critical points that arise in linear neural networks under a variety of loss functions. This paper provides important insight into the loss landscape of neural networks, and it exhibits nuanced observations even in the linear case. Naturally, there are many important follow up studies that can be done in the case of non linear networks. It is vital to have a thorough understanding before that work can be satisfactorily undertaken. It was challenging for me to identify which of these items were the main results, as summarized on in the first paragraph of the conclusions section. The word "rather" should be clarified.<BRK>This paper studied the landscape of linear neural networks using algebraic geometry tools. They introduced a distinction between "pure" and "spurious" critical points. My concern to this paper is that the landscape of linear neural networks, which is the subject of this paper, has already been analyzed in the literature. The final results of this paper, though derived using a different approach, are not new. However, I don t have the expertise to assess whether this viewpoint was implicitly contained in the proof of previous results. I don t hold a strong opinion, since there could potentially be great ideas inside the algebraic geometry tools.<BRK>This paper studies the critical locus of loss functions of deep linear networks. My main concern is the contribution and importance of this paper as the landscape of the training loss function of deep linear networks has been well studied. This paper indeed provides some new techniques and insights in this research direction, but the authors do not discuss the possible extension to a more complicated and practical case. I would like to raise my score if the authors can provide some reasonable and convincing discussions regarding the extension to the nonlinear case. The properties of critical points of deep linear networks have been studied in many literatures, the authors should provide detailed comparison and discussion between the derived results and these related work in the surrounding text of theorems. Again, it is more interesting to explore the benefit of reparameterization for nonlinear networks.
Reject. rating score: 3. rating score: 3. rating score: 6. <BRK>This paper introduces a better searching strategy in the context of automatic neural architecture search (NAS). Current search strategies for the weight sharing NAS methods either focus on uniformly training all the network paths or selectively train different network paths with different frequency, where both have their own issues like wasting resources for unpromising candidates and unfair comparison among network paths. To this end, this paper proposes a balanced training strategy with “selective drop mechanism”. However, the analysis of the current approach is limited and some relevant papers are missing. More details below. Arguments:1) This paper’s main focus is on developing a better search strategy for NAS based weight sharing methods. 2) The related work doesn’t discuss the latest papers which suggest why and in what ways the current search strategies are based for weight sharing based NAS approaches. As this paper is trying to address this problem, I would naturally assume that this is well discussed, but it’s missing!<BRK>In this paper, the authors proposed a new training strategy in achieving better balance between training efficiency and evaluation accuracy with weight sharing based NAS algorithms. The proposed method achieved the SOTA on IN mobile setting. Overall I found the idea proposed in the paper intuitive and convincing. Especially I appreciate the ablation study that identified one of the option is encouraged too much in the early stage will can lead to worse final performance. From the methodology perspective, I think this is a solid incremental contribution. My main concern is that the authors did not indicate that the code/model will be open sourced, which will help verification as well as reproducibility.<BRK>This paper relates to automatic neural architecture search techniques. The authors propose a model to balance the two issues mentioned above. Their aim is to produced balanced training while trying to reduce conflicts between the different potential network paths. So their algorithm Has two phases, one where it randomly builds a block in the network and another one where it discards vertices form the layer that are below a certain threshold. Section 4.4 says that "only the operators with performance much lower than the average of others will be dropped." Is this approach conservative? There are several minor typos that the authors might want to correct. It s set notation. Would it be better to write "by contrast"?
Reject. rating score: 1. rating score: 3. rating score: 3. rating score: 6. <BRK>The paper presents a way to learn a vectorial representation for items which are only described by triplet similiarity expressions. The paper not only claims  large scale representation learning  but also utilizing the described idea to use neural networks to "directly, approximately solve non convex NP hard optimization problems that arise naturally in unsupervised learning problems." Both claims are not really shown in the paper: (i) The experiments are not large scale and (ii)  it becomes not clear how any substantiate insight with respect to NP hard problems can be gained here apart from the fact that it tackles a ML problem, which many seem to be computationally hard problems. The more interesting question of how to generalize to unseen items (how would that be possible given that items have no representation at all) is not discussed at all and seems not to be realizable, which makes the starting point of such methods (items have no representation) questionable.<BRK>The paper proposes to use the triplet loss as a convex relaxation of the ordinal embedding problem. The experiments presented in the paper include a set of simulation experiments and a real world task. I am giving a score of 3. This work is an interesting application of deep learning, but it gives little insight as to why deep networks are able to solve the problem and how to solve ordinal embedding itself. To elaborate, the problem is known to be NP hard in the worst case, while the data sets used in the paper seem to have certain nice properties.<BRK>In this paper, they aim to learn useful feature representations without meaningful low level input representations, e.g.just an instance ID. Instead, meaningful representations are learned through gathered triplet comparisons of these IDs, e.g.is instance A more similar to instance B or instance C? Interesting technique to take advantage of neural networks to efficiently learn ordinal embeddings from a set of relationships without a low level feature representation, but I believe the experiments could be improved. One of the main advantages of this approach is efficiency, which allows it to be used on large real world datasets. However, since quantitative exploration of large real world datasets may be challenging and expensive to collect, the synthetic experiments could have been more detailed. 2.I think that the claim that the use of neural networks with discrete inputs can approximately solve NP hard optimization problems is an exciting one, which likely necessitates more experiments (or theoretical results), but as it stands I don t think it is a fundamentally different conclusion from the fact that this method provides a great scalable solution for the ordinal embedding problem.<BRK>The paper presents a Neural Network based method for learning ordinal embeddings only from triplet comparisons. Still, there are some issues the authors should address:  for the experiment with Imagenet images, it is not very clear how many pictures are used. While there is no clear line between shallow and deep neural networks, I would argue that a 4 layer NN is rather shallow. For the sake of completeness, more experiments in this area would be nice. I don t see a discussion about the downsides of the method (for example, the large number of triplet comparison examples needed for training; and possible methods to overcome this problem).
Reject. rating score: 1. rating score: 3. rating score: 3. <BRK>This paper introduces two networks that are trained to predict DDS. While one is trained with perfect information, the other one (ISSN) with imperfect information. Monte carlo tree search for games with hidden information and uncertainty. Diss.University of York, 2014.] Please add lot more details about the evaluation. Summary:I think the paper is looking into an interesting problem and is going in the right direction, but the experimental section is at this point no good enough to suggest an acceptance.<BRK>This paper deals imperfect information games, and builds a Bayesian method to model the unknown part of the current state, making use of the past moves (which constrain the game, here Contract Bridge). This results in a counter for each legal move and we treat this as the training target. position : the term is not defined.<BRK>This paper presents an approach to playing imperfect information games, an “Incomplete State Solver Network” (ISSN) within the domain of contact bridge. I believe that the work that the authors did in regards to this paper is valuable. However, I think the paper is not yet ready for publication as a communication of this work.
Reject. rating score: 6. rating score: 6. <BRK>This paper proposes a pretraining technique for question generation, where an answer candidate is chosen beforehand, and the objective is to predict the answer containing sentence given a paragraph excluding this sentence and the target answer candidate. 3) How many synthetic QA pairs are used for Table 3? (This is not a weakness of the paper, but more a subjective opinion about the paper s claim) The paper claims answer containing sentence generation is close to question generation. Consider the first example in Table 5. I believe this paper should discuss previous work in question generation and compare the performance with them. Some of the work on question generation. NAACL 2010. 6) I believe that the most popular approach in question generation literature is selecting named entities & noun phrases using off the shelf tools, and wonder if authors have compared their method with this baseline. Is it correct? Regarding Section 41) I am curious why the improvement in BLEU 4 score (in Table 2) is significant but the improvement in the end task (in Table 3) is marginal.<BRK>The paper in the field of machine reading comprehension. Specifically, the authors propose a method that dynamically generates K answers given a paragraph  in order to generate diverse questions and, secondly, pre training the question generator on answers in a sentence generation task. Overall the paper is written rather well, however,  at times  it is tough  to understand because of the technical writing and heavy use of abbreviations. I have a couple of questions:1. What are the standard errors? How does the number of parameters compare to previous state of the art? 4.In Figure 5a the F1 scores of Bert+NS and Bert+AS seem to converge with increasing  data size. Machines definitely do not surpass humans in reading comprehensions.
Reject. rating score: 3. rating score: 6. rating score: 6. <BRK>This paper presented a denoising adversarial autoencoder for sentence embeddings. The idea is that by introducing perturbations (word omissions, etc) the embeddings are more meaningful and less "memorized". Strengths: I thought the idea is nice, and the results do seem to show improvements in a number of interesting tasks. Weaknesses: I don t really think the explanation, especially in Theorem 1, makes a lot of sense. Qualitatively speaking, it s true that "memorization" in autoencoders (where the latent space has a 1 1 mapping with the input space) is problematic when the autoencoders are too powerful, but it is not always the case, and it is too far to say that the probability in theorem 1 is ALWAYS agnostic to encoding. Here, injecting noise in the exact same fashion proposed in this paper is a well accepted practice. While I think it s interesting that it works well here, I wouldn t really frame it as such a novelty, in that case, and I believe the other works on denoising autoencoders should be compared against in the experiments. There is some wording that can be tightened in the main text, to make more room.<BRK>The paper proposes a simple (rather straightforward) approach based on adversarial learning, with some theoretical guarantees, which obtains good performances for reconstruction and neighborhood preservation. While the authors cite this work, I cannot understand why they did not include it in their experiments. Also, theorem 3 gives an upperbound of the achievable log likelihood, which is "substantially better when examples in the same cluster are mapped to to points in the latent space in a manner that is well separated from encodings of otherclusters". Ok but what does it show for the approach. Wouldn t it be possible to theoretically analyze other baselines? Also, all the theoretical analysis is made based on strong assumptions. Are these verified on considered datasets?<BRK>The paper argues that adding noise to the inputs of an adversarial autoencoder for text improves the geometry of the learned latent space (in terms of mapping similar input sentences to nearby points in the latent space). Overall, the paper addresses an important problem of improving autoencoder based generative models of text, presents a simple solution to do so and mathematically and empirically demonstrates its effectiveness. I have a few questions & comments1) I’m curious about whether the authors have an intuition for why input space noise is better than latent space noise? 2) It would be great to see Forward / Reverse PPL results on bigger datasets like the BookCorpus or WMT similar to [2]. Does this theorem still hold? 7) It would be good to point out that the model presented in this work is far from SOTA on sentiment style transfer benchmarks like Yelp.
Reject. rating score: 1. rating score: 1. rating score: 1. <BRK>This submitted manuscript is exactly the paper (bearing no difference) that was submitted to ICLR 2019 and also rejected.<BRK>I would also appreciate authors to consider the comments from the previous ICLR 19 reviews and improve the paper. In Proceedings of the 27th ACM International Conference on Information and Knowledge Management (pp.2175 2182).<BRK>The organization of the paper can also be improved (see Major comments). In summary, the paper in its current form does not yet reach the ICLR standard due to the lack of clarity in its technical description, and lack of sufficient detail in experiment results to be convincing. The authors tested the proposed method on a synthetic dataset based on UCI electronic power consumption data, and a proprietary dataset for churn prediction.
Accept (Poster). rating score: 6. rating score: 6. rating score: 3. <BRK>The authors proposed a method for code optimization for deploying neural networks. The main idea is to formulate it as a search task over tuning knobs in the code template, and to apply reinforcement learning to optimize the configurations for the tuning knobs with respect to a cost model.<BRK>The paper exploits reinforcement learning to address code compilation, which is novel for me. Post rebuttalThank the other two reviewers and the authors to help me better understand the paper.<BRK>This paper proposes an optimizing compiler  for DNN s based on adaptive sampling and reinforcement learning, to drive the search of optimal code in order to reduce compilation time as well as potentially improve the efficiency of the code produced. The authors mention it is (will be) integrated in the open source code of TVM.
Reject. rating score: 3. rating score: 6. rating score: 6. <BRK>This paper proposes an unsupervised method for learning representations of speech signals using contrastive predictive coding. Considering the fact that such an approach was suggested recently by [1], a detailed comparison with uni directional models is needed.<BRK>This paper investigates an unsupervised learning approach based on bi directional contrasive predictive coding (CPC) to learning speech representations. The reported work is interesting and may have value to the speech community.<BRK>Overview:This work uses contrastive predictive coding (CPC) to learn unsupervised speech representations on large amounts of unlabelled speech data and then uses the resulting features in downstream speech recognition systems. The work in [1] is very related.
Reject. rating score: 1. rating score: 1. rating score: 6. <BRK>The paper describes a new method called Atomic Compression Network for constructing neural networks. The idea is straightforward. First, I don’t think the proposed idea is very innovative.<BRK>This paper proposes a new way to create compact neural net, named Atomic Compression Networks (ACN). I am leaning towards rejecting this paper because the experimental setup is not well justified and a few important details are missing before conclusions can be drawn.<BRK>This paper explores the use of replicating neurons across and within layers to compress fully connected neural networks. The idea is simple, and is evaluated on a number of datasets and compared with fully connected, single layer, and several compression schemes.
Reject. rating score: 1. rating score: 1. rating score: 3. <BRK>### Summary of contributionsThis paper aims to accelerate the training of deep networks using a selective sampling. The central premise of the paper is unclear, the writing/presentation needs improvement, and the experiments are not convincing. Finally, in 2019 CIFAR alone is not longer a sufficient dataset to report experiments on.<BRK>This paper proposes a minimal margin score (MMS) criterion to speed up the training of the deep networks.<BRK>A new approach is proposed to speed up training in deep models. The idea is to select sample batches when back propagating the error based on the distance of the prediction foe the sample from the decision boundary.
Reject. rating score: 1. rating score: 3. rating score: 6. <BRK>This paper presents a RNN RL based method for the symbolic regression problem. My main concern is about the proposed method, where the three RL related equations (not numbered) at page 5 are also direct copy from textbook policy gradient equations without specific adaptation to the new application considered in this paper, which is very strange. The two conditional probability definitions considered at page 3 are not mentioned in later text. These are only fractions of the underlying method and by reading the paper back and forth several times, it is not clear of the basic algorithmic flowchart, let alone more detailed description of the related parameters.<BRK>This paper does a good job at specifying a solution, but never states the problem. This was an example of symbolic regression: discovering a symbolic expression that accurately matches a given data set. For people familiar with policy gradients and RNNs, you need only look at the policy RNN in Figure 1. It would also help with clarity if you could please add the fitting of the parameters of the symbolic expressions using BFGS to the pseudocode. The RL approach is standard and the authors have executed it carefully and conducted the necessary ablations. The experiments indicate that the proposed RL approach works better than genetic programming (GP) for what appears to be a simple benchmark.<BRK>This paper presents deep symbolic regression (DSR), which uses a recurrent neural network to learn a distribution over mathematical expressions and uses policy gradient to train the RNN for generating desired expressions given a set of points. The fitness on the dataset is used as the reward to train the RNN using policy gradient. The experimental results show that it outperforms genetic programming as well as the ablation study shows the usefulness of different extensions. Is it because the RNN never chose the constants in the learnt distribution or the BFGS algorithm prefers constants to be 1? How was the Recovery metric calculated? How would a random search that enumerates over all expressions and uses BFGS to compute constants work?
Reject. rating score: 1. rating score: 3. rating score: 3. <BRK>The authors claim that the new design is able to lower the energy consumption in the matrix multiplication. In the experimental analysis, the authors demonstrate the effectiveness of the proposed method. This paper should be rejected since it proposes exactly the same architecture with the following published work:BitBlade: Area and Energy Efficient Precision Scalable Neural Network Accelerator with Bitwise SummationAlso, the authors do not provide a valid approach for the auto selection of quantization bits, which is more significant in my opinion.<BRK>This paper presents a decomposable MAC unit for low power computation for matrix multiplication operation for neural networks. Although I believe I understood the idea of this technique, I strongly believe this paper should be submitted to an architecture of design automation conference instead of ICLR. I am also not in the position to assess the experiments, which were conducted with synthesis of Synopsis 28nm library.<BRK>It is an interesting work but it is not sure that there is a difference in contribution when compared to previous work. A similar idea is addressed in [1]. 2.In Figure 7,  the performance and energy consumption of systolic arrays of decomposable MAC  is shown. [1]     Sungju Ryu, Hyungjun Kim, Wooseok Yi and Jae Joon Kim. BitBlade: Area and Energy Efficient Precision Scalable Neural Network Accelerator with Bitwise Summation.
Reject. rating score: 1. rating score: 1. rating score: 3. rating score: 6. <BRK>This work sits at the intersection between neuroscience and machine learning. It proposes to use neural recordings from the rodent hippocampus to shed light into how biological agents achieve continual learning. The approach involves (i) analyzing neural data from the rodent hippocampus with the goal of identifying how neurons encode various behavioral variables; (ii) training different RL agents to solve a computer version of the same task, including tabular and DQN agents; (iii) contrasting the performance and representations of the artificial agents with those of animals. The paper has an admirable goal: to find links between neuroscience and machine learning, using tools from one to promote advances in the other. However, I believe that this paper does not deliver on either end due to major methodological and analytical flaws, rendering it innapropriate for ICLR. The interpretations of the dPCA components seems very preliminary and lacks methological rigor. In particular, many alternative interpretations of the components are possible. For instance, the claim that a given dPCA component represents an eligibility trace needs much more evidence than simply showing that this component decays over space when the eligibility trace also decays. animal.Ideally, the results for all animals would be reported (or some statistically sound aggregate of all animals). 5.Finally, the authors compare the performance of animal 3 with the performance of different RL agents. Again, this comparison is incredibly superficial and neglects many confound variables. Therefore, I cannot recommend the paper for publication at ICLR.<BRK>5 & 7 seem to suggest that. What type of recordings were these? The researchers conducted demixed principal components analysis (dPCA) on the data, then compared the activity of the components to aspects of a tabular Q learner. They argue that some of the components match variables used by the Q learner. They find that the rat is better than the Q learners at continually updating from the egocentric to allocentric task, and vice versa. I really like the attempt to link hippocampal activity with RL representations. But, this study is very muddled, and there are very serious problems with the paper that render it inappropriate for acceptance to ICLR. Given the importance of the hippocampus for spatial representations, surely a big component of the variance can be explained by the animal s location in space. Why not include this? The authors  claim, for example, that time component #1 is an eligibility trace is a real stretch, in my opinion (see more below in point 3). What about the hippocampal representations makes the rats better at continual learning? Nothing like that is provided. In addition to these problems, there are several small ones:  Was only one animal included in this analysis?<BRK>This work focuses on the problem of continual learning. It first proposes an analysis of neural recordings from rodents hippocampus that perform a task related to continual learning. The authors then claim to identify similarities in representations of behaviorally relevant variables between biological networks and standard artificial RL systems. I have found the results of the analysis of neural data interesting and well explained. As such I think this manuscript is not ready yet for publication in ICLR. In particular it would be useful to explain in what sense the task considered is related to continual learning (switches between allocentric and egocentric tasks that are not informed by experimenter, but that the rodent has to figure out), because for now it is only briefly stated in the legend of figure 1. Globally these results are well presented. It would seem to me that starting points north s_north and south s_south would have distinct Q(s_north,.) In this section a natural extension could be to use hierarchical RL, or options to model behavioral strategies such as allocentric and egocentric and compare with the hippocampal recordings. This seems rather crucial, because if information about allocentric VS egocentric task is provided, contrary to the behavioral task for rats, then the network is asked to do strategy switches based on some cues, which is a different problem than continual learning. If the DQN receives information about whether it is in a allocentric or egocentric trial, I am very surprised it is not able to perform this simple task.<BRK>This paper describes an analysis of hippocampal neuronal activity in rodents during spatial navigation tasks. Features were extracted from the data using a component analysis technique, and these features were then compared to quantities which arise during training of the DQN reinforcement learning algorithm. Unfortunately I am not well versed enough in this literature to assess the merits of this submission. However, I could not find any glaring issues, unclear sentences or any other obvious sign of incompetence. My apologies for the inadequacy or this review (and/or the paper assignment algorithm).
Accept (Poster). rating score: 6. rating score: 6. rating score: 6. <BRK>Authors claim that compositional languages are easier to be learned and that they allow listeners to more easily understand provided messages. This is not a big surprise, but I like the simple but clever idea of reset that the paper exploits. I am not sure why this should hold. Moreover, authors claim that if Ia is too long, no improvement of the topology can be made. Also, the considered learning scheme is that in the transmitting phase Alice records messages for all objects in D. But is it realistic ?<BRK>This paper studies the emergence of compositional language in neural agents. I do have concerns for this paper around utility and novelty. I think this is a nice contribution, but the main question for me is whether this is enough for an ICLR acceptance. In my mind, the main scientific contribution of this work is the empirical verification of the principle ‘compositional languages are easier to learn’.<BRK>This paper proposed a neural iterated learning algorithm to encourage the dominance of high compositional language in the multi agent communication game. The authors also demonstrated that topological similarity is correlated with zero shot performance. I wonder is the hypothesis still true in this case? I agree with the authors that compositional language can and will lead to better generalization ability. However, from Algorithm1, it seems Bob receives the message only update with its parameters.
Accept (Poster). rating score: 8. rating score: 6. rating score: 6. <BRK>The paper addresses the problem of exploiting human demonstrations in hard exploration (RL) problems. A new set of challenge tasks is introduced that destroys the performance of very strong baseline systems while highlighting the strength of the new system. This reviewer moves to accept this top quality RL paper. The new agent, R2D3, is the primary contribution in combining and outperforming previous SOTA agents. What about SimCity?<BRK>Summary The authors propose R2D3, and algorithm for learning from demonstrations in partially observable environments with sparse rewards. Furthermore, the authors propose a suite of eight challenging tasks on which the proposed method is tested and compared to relevant baselines. Generative adversarial imitation learning.<BRK>In this work, R2D3 (Recurrent Replay Distributed DQN from Demonstration), which combines R2D2 [1] with imitation learning (IL), is proposed. Similar to the existing works on “reinforcement learning (RL) with demonstration” such as DQfD, DDPGfD, policy optimization with demonstration (POfD) [2], hard exploration conditions (sparse reward, partial observability, high variance in initial states) are assumed, which is difficult to achieve good performance with RL without demonstration in general. Eight tasks in such conditions were devised and used to test the performance of R2D3. For example, POfD [2] assumes sparse reward tasks with *imperfect* demonstrations, which is difficult to achieve good performance by using RL or IL.
Accept (Poster). rating score: 8. rating score: 6. rating score: 6. rating score: 3. <BRK>The paper presents a method for rendering of reconstructed objects. The approach leverages two distinct neural networks. Second, CompositionNet, which learns to combine multiple warped view into the final image and to remove the related artifacts. The proposed approach is well described, and does seem to fix a few issues with current methods. One thing that was not discuss was about the effects integrating a neural network into the rendering pipeline have on the output resolution. Classical methods are not really limited on this regard, but neural networks cannot easily provide high resolution outputs.<BRK>This paper studies the problem of re rendering the appearance of an object from novel viewpoints, given reference images of the same object as input. But I think a more thorough comparison with Hedman et al.would be crucial in justifying the design choices. The authors propose an EffectsNet architecture to derive view independent Lambertian reflectance maps and train the network in a self supervised way through re projection. It is nice to see comparison with Hedman et al.(2016 and 2018) but I feel that details are missing.<BRK>From a set of images taken of a static object under constant illumination, the proposed method first selects the four viewpoints nearest to the requested novel viewpoint. The key contribution of the submission over previous work is the handling of view dependent effects such as specular highlights. The proposed method improves the robustness of existing neural rendering methods to materials that are not roughly diffuse. I would have appreciated an ablative experiment where the proposed pipeline is kept as is, but a state of the art intrinsic decomposition technique (as discussed in sec.2) was applied instead of EffectsNet. A colormap representing the percentage of error wrt. EffectsNet is used for both removal and addition of view dependent effects, which makes part of the paper confusing. I suspect this value might be important, as too much regularization might give underestimated view dependent effects and too little might provide strong and incoherent effects.<BRK>To justify the presented method it would be great to see experiments with a more clear difference to Hedman et al.baseline.3) Authors of Pix2Pix (Isola et al., 2016) do not provide experimental results on novel view point synthesis. Although it is completely fine to refer to Pix2Pix as to the closest and simplest neural based baseline I do not find it convincing to compare the proposed method to Pix2Pix without any other baselines as in the figures 5, 11, 12. I only have some concerns about the experiments. 1) The method relies on the depth maps and the main contribution of the paper is related to the rendering of view dependent effects.
Reject. rating score: 3. rating score: 3. rating score: 8. <BRK>Summary of the Paper:  This paper describes a method for training Bayesian neural networks in the context of stream data. The method proposed is based on using a quantization approach with some techniques to estimate the change in probability distributions. For example, the sentence "it has been unable to estimate the uncertainty of the predictions until recently" sounds awkward. The authors have to better explain the features and challenges of stream data. The description of the baselines the authors compare with is not clear. In table 2 the benefits of the proposed approach are not very significant. I believe that this paper needs for work and is not yet suitable for acceptance.<BRK>The idea is to combine online vector quantization with Bayesian neural networks (BNN), so that only incremental computation is needed for a new data point. One of my major concerns is the fairness in terms of comparison to BNN approaches. Could the authors elaborate on this? Also, the authors should have made it clear that they are using MC dropout as a BNN. Even the earlier work, probabilistic BP (PBP), as cited in the current paper, also counts as sampling free. The organization could be improved to make the paper more readable.<BRK>The paper proposes a differentiable Bayesian neural network. Papers, such as [1], have proposed to use a vector quantization process that can be applied online to a stream of inputs. I think the paper might need a bit more explanation about codevector, since it s not a very well acknowledged concept in this field. The main issue for me to understand it is how to get these codevectors. 3.Given the insufficient understanding of codevector, figure 2 is a bit hard to read. 1) (a) (d) are figures for x0 at t 0, which is not time variant. But according to algorithm 2, y_*< T(c_*), would that be a problem? I think (a) (d) are informative but not straightforward to read.
Reject. rating score: 3. rating score: 3. rating score: 3. <BRK>Overall, I don t think the paper makes it over the bar to accept but I hope the authors continue to improve upon the work and get it into shape where it could be accepted at another strong conference. In this case, any benefit appears to be ill explained by the underlying motivation. This is a weird choice. E.g.the seemingly incoherent objective of summing over the quantiles falls flat.<BRK>This paper studies the problem of safe adaptation to avoid catastrophic failure in a new environment. The proposed method (risk averse domain adaptation (RADA)) learns probabilistic model based RL agents from source domains, and uses them to select actions that has the best worst case performance in the target domain. The paper mentions safety critical applications like auto driving.<BRK>As stated in the introduction, “enables fast yet safe adaptation within only a handful of episodes.” Intuitively, we can not expect to be safe and fast at the same time. It would be better to discuss why cautious exploration can ensure fast and safe adaption, which would be more interesting. Additionally, in Figure 3, some fast adaption methods, such as MAML, should be compared to be more persuasive. Method: 1. 2.This work formulated safe adaption as minimizing the risk of catastrophic failure. What’s the relationship between “the generalized action score” and “risk of catastrophic failure”? The “generalized action score” is the main difference with PETs. However, it is a little bit hard to follow the idea from “risk of catastrophic failure” to “the generalized action scores”.
Accept (Poster). rating score: 6. rating score: 6. rating score: 6. <BRK>This paper proposed a certified defense method for adversarial patches. The paper is motivated by the finding that several existing works on adversarial patch defenses are easily "breakable" by in the white box setting. And the contribution of the paper is significant to the field.<BRK>The paper proposes a certified defense for adversarial patch attacks. Overall, the contribution of this paper is novel, and results are promising,but it still has some missing components, especially the idea of combiningmultiple IBP bounds into one, which can be very effective for adversarialpatches, as I will elaborate below.<BRK>This paper attempts to extend the Interval Bound Propagation algorithm from (Gowal et al.2018) to defend against adversarial patch based attacks. I think the technical contribution of this paper is a bit weak in that they mostly followed the original IBP and the only novelties are the random patch training and guided patch training. All the experiments mentioned above would significantly strengthen the experiments section of the paper.
Accept (Talk). rating score: 8. rating score: 8. rating score: 8. <BRK>This paper introduces an unsupervised learning algorithm Dynamics Aware Discovery of Skills (DADS) for learning low level “skills” that can be leveraged for model predictive control. Overall, I feel this is a very well motivated and interesting submission with very thorough experiments.<BRK>The authors try to incorporate intermediate level skills into model based RL, which is an essential problem in the RL field. This procedure is well motivated and the mathematics is easy to follow. (3)their method is compatible with the idea of continuous skill spaces, which seems to give rise to more diverse trajectories and hence offers greater utility. It’s an accept for me. On one hand, using model free unsupervised RL methods to learn intermediate level skills is not a new idea. on the other, approaching this problem via mutual information is, as far as I know, new to this field.<BRK>The proposed approach is convincingly compared in several ways to both previous model based approaches and model free approaches. This is a very interesting approach, and although I can already think of ways to improve, it seems like an exciting step in the right direction to develop more autonomous and sample efficient learning systems. I suggest to rate this submission as  accept .
Accept (Poster). rating score: 8. rating score: 8. rating score: 3. <BRK>In this paper, the authors investigate non autoregressive translation (NAT). The paper is well organised and the experiments are sound and interesting, shedding light on an aspect of NAT models that s been rather dismissed as secondary until now: the impact of varying AT knowledge distillation. First, so as to better please those out there who are more "state of the art" inclined, I suggest the authors to better emphasise their improvements. This could be better stressed in the introduction and it would make the main take away messages from the paper stronger. On a more general note, I would like to know how robust these models are. You probably mean sampling according to the model distribution p(y_t | y_{<t}) for all t, which is not random.<BRK>I am updating my score to 8 given the rebuttal to my and other reviewers  questions] Summary: This paper studies knowledge distillation in the context of non autoregressive translation. In particular, it is well known that in order to make NAT competitive with AT, one needs to train the NAT system on a distilled dataset from the teacher model. Using initial experiments on EN >ES/FR/DE, the authors argue that this necessity arises from the overly multimodal nature of the output distribution, and that the AT teacher model produces a less multimodal distribution that is easier to model with NAT. Based on this, the authors propose two quantities that estimate the complexity (conditional entropy) and faithfulness (cross entropy vs real data), and derive approximations to these based on independence assumptions and an alignment model. The translations from the teacher output are indeed found to be less complex, thereby facilitating easier training for the NAT student model.<BRK>The paper analyses recent distillation techniques for non autoregressive machine translation models (NAT). However, AT models can not be parallelized that easily such as the NAT models. The authors analyze why such distillation is needed and what the effect of the complexity of the training set is and further propose 3 methods to adjust the complexity of the teacher to the complexity of the NAT model. The paper is well written and easy to understand and the experiments are exhaustive and well done. I think the analysis using the synthetic dataset is nice, but I am not sure how surprising the results are. Hence, I think the contribution of the paper is a bit limited. I am, however, not an expert in the field to fully judge the contribution and will therefore put low confidence on my review.
Accept (Poster). rating score: 8. rating score: 6. rating score: 3. <BRK>This work addresses the problem of training softmax classifiers when the number of classes is extreme. The authors improve the negative sampling method which is based on reducing the multi class problem to a binary class problem by introducing randomly chosen labels in the training. Their idea is generating the fake labels nonuniformly from an adversarial model (a decision tree). The work is very technical in nature, but the proposal is presented in detail and in a didactic way with appropriate connections to alternative methods, so that it may be useful for the non expert (as me).<BRK>This paper focuses on efficient and fast training in the extreme classification setting where the number of classes C is very large. The paper proposes a method to sample the negatives in a non uniform manner. In particular, given an example, an adversarial auxiliary model that is tasked with tracking the data distribution samples the hardest (adversarial) negatives for the example. There has been quite a bit of work on non uniformly sampling "hard" negative classes. 2.In experiments, the authors do not include the performance of softmax loss (eq.(1)) due to its large computational cost.<BRK>The paper presents a method for negative sampling for softmax when dealing with classification of data to one from a large number of classes. This is based on building an auxilary model using decision tree from which the adversarial negative classes are sampled, so that the distribution of the negative samples can be close to the positive ones leading to higher SNR while training. The proposed method is compared to other methods for negative samping on two publicly available large scale datasets from the extreme classification with XML XNN features. The proposed approach with adversarial negative sampling using an auxilary model seems interesting 2. This is important to understand what (if at all) is lost by doing approximation as proposed.
Reject. rating score: 3. rating score: 6. rating score: 6. <BRK>The authors compare the convergence for different activations, showing a slower convergence (hence better propagation) for some piecewise smooth activations compared to ReLU. For residual networks, the  edge of caos  behavior is claimed to be in place regardless of the initialization. The detailed characterization of these limiting kernels for different activations and initializations is interesting. some comments:  title: isn t the "mean field behavior" already subsumed in the definition of NTK? after lemma 1: generalization may not provide a strong argument here unless further discussed: the constant kernel is obviously bad for learning anything, but the EOC limiting kernel is also pretty bad for predicting anything outside the training data  section 3: typo "ourselves"  prop 3: specify that phi is the relu  the proof of prop 3 should be more detailed.<BRK>This paper brings those analysis for NTK and discovers few interesting results. The authors show that 1) for fully connected feed forward networks, outside initialization edge of chaos (EOC) the NTK converges exponentially to constant kernel. 3) Certain class of activation function (denoted class S, including ELU/Tanh/Swish) it has even slower convergence(O(log(L)/L)) compare to ReLU (O(1/L)). However the contribution of the paper is interesting and worth the ICLR audience to know about, especially with the current surge of interest in NTK. In this case the connection to very deep behaviour of NTK is not straightforward to actual network training since the training dynamics will be different. In section 5.2, the authors’ claim full batch GD is practically impossible. I also thank the authors for the clarification.<BRK>It also sheds light on the impact of different activation function on NTK by generalizing the results of [2]. The authors show that if deep neural networks are not properly initialized, the NTK can have a large condition number, which leads to the poor performance of training and generalization. However, this paper can be better if the authors can polish their paper and reorganize the appendix part. On the Impact of the Activation function on Deep Neural Networks Training. To analyze generalization with NTK, we need a little more, see [1]. This is not some core issue, but I think the authors should make the description more accurate. 5.I think for clarity the authors should give a formal definition of ANTK, or if it is unnecessary, better directly use the original NTK. I don’t think it directly related to the invertibility of NTK. 6.There are several typos in the appendix. 7.In the proof of Lemma 3 and Lemma 4, I don’t get why |a_l| < l + |a_0|. Overall, this paper is interesting and gives a unified perspective on the recently developed NTK and Edge of Chaos initialization.
Reject. rating score: 3. rating score: 3. rating score: 3. <BRK>This paper proposes an invertible flow based method for the one shot graph generation. 2.The paper demonstrates their method on a molecular graph generation task. [Page 5, Sec 3.3.1] The second paragraph in Sec 3.3.1 is confusing to me.<BRK>This paper presents a new reversible flow based graph generative model wherein the whole graph i.e., representative attributes such as node features and adjacency tensor is modeled using seperate streams of invertible flow model. The paper focusses it applicability for molecular graphs. Few more limitations:1. Although paper claims one shot generation of graphs, in reality it seems otherwise. It is not possible to train model with variable number of nodes.<BRK>In this paper, a GraphNVP framework for molecular graph generation is proposed. I have several concerns with regards to this model and the proposed algorithm:1.
Reject. rating score: 1. rating score: 1. rating score: 3. <BRK>The authors propose a learning strategy to fit predictive models on data separated across nodes, and for which different set of features are available within each node. The problem tackled in this work is interesting, with an important application on medical records from > 100,000 individuals followed  over time. The paper should definitely compare the proposed methodology with respect to this paradigm.<BRK>This paper describes a structure to approach the federated machine learning problem for hospitals. There is no open benchmark that the community can work on. I suggest that the paper focus on the method and not the private dataset used. Typos: "Step 1..Claims"Some of the citations seem to be listing every author of the paper which is very hard to read the paper.<BRK>This paper considers the problem of learning from medical data that is separated both horiontally (across different practices and centers) and vertically (by data type). I also have a question regarding the simulation. Specifically the authors investigate an ML approach to risk stratifying elderly patients with respect their likelihood of falling in the next two years.
Reject. rating score: 3. rating score: 6. rating score: 6. <BRK>Aiming to resolve the question whether and why deep networks are biased towards simple functions, this paper gives a spectral analysis on neural networks  conjugate kernel(CK) and neural tangent kernel(NTK) on boolean cube. The spectral analysis in this paper is heavily due to working on boolean cube. However, the authors failed to show it at least from the following perspectives:1) This paper did not show the trend of eigenvalues, but only the weak version of, for example, $\mu_{2k 2} > \mu_{2k}$. So the weak simplicity bias theorem actually only describes the relation among finite $d$ eigenvalues. Based on the spectral analysis, it is not rigorous enough to claim the networks are biased to simple functions, given that the target function consists of simple multilinear monomial functions. To summarize, this paper definitely contains some rigorous analysis which I appreciate, but it made some claims that are not verified.<BRK>Updates:Thanks for the updates. I find the new theoretical results interesting and potentially useful,  which shows, in the large $d$ setting, spectrums of CKs/NTKs  for boolean cube, sphere and isotropic Gaussian are closed to each other in some sense. The key objects in understanding such networks are the conjugate kernel [1, 2] (CK defined in the paper) and the Neural tangent kernels [3] (NTK). Understanding properties of such kernels, in particular, their spectra distribution and eigenspace, could be potentially an important step towards a finer gained understanding of generalization in neural networks. The main contribution of this paper is the development of the spectral theory of CK and NTK on boolean cube (similar or weaker results on uniform distribution in spheres and Gaussian distribution in R^n). The authors also develop some computation tools to compute the spectra; Lemma 3.2. Most noticeably, the authors show that the observation in [4]  neural network is biased towards simple functions  is NOT universal. Deep Neural Networks as Gaussian Processes.<BRK>This paper examined the spectrum of NNGP and NTK kernels and answer several questions about deep networks using both analytical results and experimental evidence:* Are randomly initialized and trained deep networks biased to simple functions? Although this boolean cube setting is followed from previous works on the same topic, it does limit the scope of the paper. Discussions on how this assumption relates to practical problems are missing from the paper. I also found that many analytical results (e.g., computing eigenvalues of a kernel operator with respect to uniform distributions on a boolean cube) in the paper are highly nontrivial to derive, which adds to the value of the paper. Is this difference intended or it is a mistake? Some minor issues are* The definition of "neural kernels" seems unnecessary and a bit sudden. It would be helpful to include the definition of Phi just after Eq.(2) for CK and NTK.
Accept (Poster). rating score: 6. rating score: 6. rating score: 3. <BRK>This paper proposes a parallel version of the stochastic weight averaging method. The authors comment in the conclusion that "it is possible that regions with bad and good generalization performance are connected through regions of low training loss". Can one check if this is related to the invariances described in Dinh, et al.(2017)?What is the relationship between SWAP and methods on local SGD? What is the reason for this?<BRK>Local SGD also has variants with Polyak momentum and Nesterov momentum [3]. The experiments show good performance. Furthermore, the authors never explicitly explain what exact Update() function is used, which is very unfriendly to the readers. Phase 2 of Algorithm 2 is called local SGD, proposed in [1,2].<BRK>In the paper, the authors propose a novel three stage training strategy for deep learning models, training using large batch, training using small batch locally and then aggregate models. From my point of view, it is better to use it in the large batch training. In the proposed method, it is required to tune the switch point between phase 1 and phase 2.
Accept (Poster). rating score: 8. rating score: 6. rating score: 6. <BRK>There seems to be some unclarity about the optimization algorithm. In short, I suspect that the proposed optimization has some difficulty with its convergence. They further suggest `a variant of the Viterbi algorithm’ for (i) and gradient based algorithms for (ii). I think this needs some explanation. There is a statement on the initialization of the optimization in p.7: “h corresponding to the beam search solution to initialize the g variables”. For instance, will the results change if g’s are initialized by the solution of the greedy method. Additional comments: 	It would be more friendly to the readers to show the definition of the notations like y^n_1. Why do the authors call it `epoch’? It also uses `epochs’ for the horizontal axis title.<BRK>This work introduces a new algorithm to improve decoding in discrete autoregressive models (ARMs). Because exact decoding of ARMs is computationally difficult, the contribution of this paper is to introduce a new approximate solution. However, based on the references cited in this paper, there are other approximate solution for ARMs and I believe the authors need to use those as baselines to show that the proposed approximate solution is useful.<BRK>The paper proposes a decoding algorithm for auto regressive models (ARMs) to improve the decoding accuracy. Then the constraint of the auxiliary variables can be imposed by penalty based continuous based optimization. Overall, the proposed method can be very useful in RNN decoding and structure prediction. I am surprised (if the authors  claim is correct) the RNN community still relies on greedy/beam search for decoding. Don t say ARMs are non Markov model and/or with unbounded Markov order. This is very misleading and over bragging. 2.Is there a way to show the standard deviations/error bars in the test results, say, Table 1 & 2 and the figure?
Reject. rating score: 3. rating score: 3. rating score: 6. <BRK>This paper proposes Recency Bias, an adaptive mini batch selection method for training deep neural networks. To select informative minibatches for training, the proposed method maintains a fixed size sliding window of past model predictions for each data sample. At a given iteration, samples which have highly inconsistent predictions within the sliding window are added to the minibatch. +ves:+ The idea of using a sliding window over a growing window in active batch selection is  interesting. However, unfortunately, I still think more needs to be done to explain the consistency of the results and to study the generalizability of this work across datasets. In other words, an ablation study on the effect of the selection pressure parameter would have been very useful.<BRK>This paper proposes an interesting heuristic of batch construction from samples. Instead of the usual random sampling, the authors to sample based on some measures of the ``uncertainty”. To be specific, the uncertainty is measured as a normalized entropy estimated from a window of historical predictions. I like the idea of designing more sophisticated ways to encourage more exploration over the samples that the model is not good at. It is not even close on MNIST, CIFAR 10 and CIFAR 100. Also those datasets are relatively small one. Can authors add results on larger datasets such as tiny image net? Besides this main concern I also have some worries about the design of the algorithm.<BRK>This paper explores a well motivated but very heuristic idea for selecting the next samples to train on for training deep learning models. This method relies on looking at the uncertainty of predictions of in the recent history of statements and preferring those instances that have a predictive uncertainty over the recent predictions. Annealing the selection bias: as the training goes on the selection becomes more random and less biased. This approach is evaluated in on three simple data sets: MNIST, CIFAR 10 and CIFAR 100. Although this is a very limited subset of models, the results are consistent and statistically significant, although their effect is not really huge. The paper gives very little theoretical justification or analysis of the results but gives only the presented empirical evidence which seems to support the hypothesis on the efficacy of the approach. In the presence of more empirical (or even theoretical) evidence, I would vote for strong accept.
Reject. rating score: 6. rating score: 6. rating score: 6. <BRK>The authors investigate off policy actor critic reinforcement learning where they want to make use of shared experience replay. One was to mix replayed experience with on policy data and the other was to create trust regions that only selects well behaved behavioral distributions for state value estimation. According to the authors the several experiments provide evidence that their algorithm achieves competitive or even state of the art results in data efficiency. They underpin this with some theoretical analysis.<BRK>The authors first analyzed the cause of instability in the prior work, from the perspective of bias and variance. Two remedies were then presented: (i) mixing the experience replay with online learning; (ii) proposing a trust region scheme to select the behavior policies. In my opinion, the empirical results are impressive, and the authors also provided some insights for the motivation. Given the results on Atari games, this paper could be a great contribution on the actor critic methods. The authors also need to address a few confusing statements and missing details. 1.In Proposition 3, the authors claimed that mixing with on policy data can reduce the bias. Also, what is the amount of bias reduced? Is this statement only based on empirical results? 5.What are the hyperparameters for the 9 agents used in Figure 1?<BRK>This paper investigates off policy actor critic (AC) learning with experience replay using V trace. The proposed learning method LASER demonstrates the state of the art data efficiency in Atari among agents trained up until 200M frames. In all, this paper is well motivated and technically sound. The draft can be improved by making it more self contained by providing a sketch of the proof rather than refer everything to the appendix. 2) what does “very off policy learning” mean? 3) In figure 3(left), why “LASER: shared + trust region” performs worse than “LASER: not shared”? Q^w should be explained in the main text.
Reject. rating score: 3. rating score: 3. rating score: 6. <BRK>** SummaryThe paper studies a specific class of Non Markovian Reward Decision Processes (NMRDPs). The authors consider a sub case where the trajectory can be mapped to a specific "task" and the reward function can be formalized as a mapping between the state task pair to the reward. The authors focus on the representation learning problem of the mapping between trajectories to an embedding of the task itself. The resulting LSTM based architecture is then evaluated on a grid world synthetic problem and on GPS trajectories from tourists in the city of Salzburg. ** Overall evaluationMy main concerns about the paper is that the exact objective of the setting and the representation learning is not that clear and the empirical validation is limited to a qualitative assessment of the representation learning with no down stream task. 1  The assumption of task dependent reward functions is very sensible. Does it make solving N simpler or more effective than using a simple encoding for the task? No evaluation is available in this sense. It seems like you are simply using the mapping from the whole trajectory.<BRK>Short summary of paper:The paper investigates representation learning  from trajectories in a framework which generalizes MDPs (NMRDPs), in which rewards are non Markovian and follow a hidden process (this can be seen as a more structured, special case of a POMDP). The authors suggest learning a trajectory embedding by using a triplet loss, justified by a sufficient condition for learning an embedding which corresponds to the true task id. Overall I felt the paper fell a bit short of the standards for ICLR, based on subpar writing, lack of comparison to convincing baselines, and unclear applicability to more complex environments. Lack of baselines:No alternative or baseline seems considered in the paper. The dataset used for experiments is also interesting.<BRK>For this, NMRPDs are transformed into Markovian Decision Processes. The idea is that the reward function can only depend on the last state of the trajectory and the task. A task representation is introduced. The proposed approach is evaluated in several experiments. Detailed comments:A point of criticism is that it seems that the authors do not compare with similar or related methods in the experiments. The paper is well written in general with only some typos: E.g.
Reject. rating score: 1. rating score: 3. rating score: 3. <BRK>The paper claims novelty on using RGSM to improve performance.<BRK>Weaknesses:1 The main contribution of paper is to adapt the RVSM/RGSM algorithms to the AT of DNNs to sparsify the deep model.<BRK>Cons:  The paper has limited novelty.
Accept (Poster). rating score: 6. rating score: 6. rating score: 6. <BRK>This paper proposes a graph convolutional  network based model for joint embedding of nodes and relations in a multi relational graph. The framework comprises of node/relation embedding, nonparametric compositional operation as in knowledge graph embedding, and finally convolution operation with direction specific weight matrices. I have just two minor comments:1. Right now, the paper reads that the main difference is only in employing the basis representation for initial relation features in your work.<BRK>This work introduces a GCN (Graph Convolutional Network) framework for multi relational graphs. Authors write: “limited to embedding only the nodes of the graph” about Weighted Graph Convolutional Network (Shang et al., 2019), however that paper in fact does relation embeddings (see.e.g.figure 1 in that paper). I would propose to discuss this issue and comment on exact differences/similarities with the proposed approach.<BRK>In this paper, the authors developed GCN on multi relational graphs and proposed CompGCN. The authors also compared the proposed CompGCN with other existing GCN variants and summarized the relationships between CompGCN and other models. In the experiments, three tasks including link prediction, node classification and graph classification were performed to evaluate the performance of the proposed method. There are several concerns on this paper. First, this work is somewhat incremental on R GCN ("Modeling relational data with graph convolutional networks"). Second, the experimental setup is unclear. Third, in Table 2, there are some missing values of some compared methods, but it is not clear on their absence.
Reject. rating score: 1. rating score: 3. rating score: 3. <BRK>Contributions: This paper builds an ad hoc connection between the Transformer and the numerical ODE solver (the Lie Trotter splitting scheme and the Euler s method) for a convection diffusion equation in a multi particle dynamic system. Then, the author(s) developed an ad hoc Strang Marchuk splitting style architecture, named Macaron Net. Finally, this paper provides some experiments to verify the performance of the proposed architecture. Theorem 1 in the paper is a known result, and it is irrelevant to the paper, I highly recommend the author to remove it from the main text. From a statistical mechanics point of view, this comparison does not make sense. The author should better position the paper to exist work. After simply checking two existing papers, I found that the author ignored the comparison with BERT large. Moreover, the author should report the parameters used in all their experiments.<BRK>In this work, the authors show that the sequence of self attention and feed forward layers within a Transformer can be interpreted as an approximate numerical solution to a set of coupled ODEs. They then present experimental results that indicate an improved performance of their Macaron Net compared to the Transformer and argue that this is due to the former being a more accurate numerical solution to the underlying set of ODEs. The authors highlight an interesting connection between the Transformer architecture and ODEs. In particular, they derive a set of ODEs that is solved numerically by the Transformer and borrow from the body of literature on numerical ODE solvers to improve the architecture. In particular, the overall accuracy of the numerical solution to the original ODE depends on the accuracy of the operator splitting and the accuracy of the integration scheme used to solve the split ODEs (e.g.Euler’s method). As far as I am aware, this is not commented on in the paper at all. I suspect that such an analysis would support my previous comment and show that the proposed new architecture is not more accurate ODE solver than the original one. It is not clear to me why consistency with Strang splitting requires two different layers rather than applying the same FFN layer twice.<BRK>The paper points out a formal analogy between transformers and an ODE modelling multi particle convection (the feed forward network) and diffusion (the self attention head). The paper then adapts the Strang Marchuk splitting scheme for solving ODEs to construct a slightly different transformer architecture: “FFN of Attention of FFN”, instead of “FFN of Attention”. The new architecture, refered to as a Macaron Net, yields better performance in a variety of experiments. PROs1.The proposed new architecture is fairly simple. As I understand it, the motivation for the splitting is to improve the numerical performance of the update scheme for the ODE. 2.Following on from the above point, the analogy between the multi particle system and the transformer is quite weak. But I recommend the authors dig into the equations and the dynamics to see what it really going on under the hood. Just showing improved performance on a few benchmarks is not enough to convince the connection is solid.
Reject. rating score: 3. rating score: 3. rating score: 3. <BRK>This paper studies an optimistic variant of AMSGrad algorithm, where an estimate of the future gradient is incorporated into the optimization problem. Moreover, the second equation should be inequality.<BRK>Summary:This work proposed a new variant of AMSGrad called Optimistic AMSGrad, which makes use of the ideas from Optimistic Online learning. In the paper the authors showed that by predicting the future gradient using m_t, the regret of Optimistic AMSGrad can be lowered from \sum |g_t| to \sum |g_t   m_t|, which improves AMSGrad directly. The authors should add more explanation in Section 3.<BRK>The authors do a good job of presenting the method (by introducing the background in proper order), the paper seems self contained and cites the relevant literature. estimate and new 2nd moment estimate. In short, the method’s stability and outperformance might rely on the selection of the algorithm for gradient prediction. Thus, I think it would be useful if the authors could provide comparison over *wall clock time* as well as long run comparisons when the compared methods converge (it would be interesting to see if Optimistic AMSGrad obtains better final train/test accuracy?). In this case, after reading the paper, it is not clear to me what is the problem that the proposed method solves (or its advantages).
Reject. rating score: 1. rating score: 1. rating score: 3. <BRK>Strengths:+ The biological motivation is quite clear+ Architecture is simpler than that of previous related work (hGRU)Weaknesses:  Not clear what the objectives/contributions are  No advancement of state of the art in computer vision  No novel insights about brain function  Motivation of the "texturized challenge" is unclear  Performance on BSDS500 is far from state of the art  Value of the qualitative analysis on stylized ImageNet is unclearOverall, I m not sure what the goal of this paper is. The lack of a clear statement about the contributions of the paper seems to confirm this impression – the authors don t seem to know either.<BRK>In practice, the work proposes a variant of convolutional LSTM cells, that incorporates additional convolutions. It seems hard to reproduce the results using the information given in the paper. In particular, the contributions are not clear enough. The experimental setting is not convincing. It seems that a simple state of the art CNN architecture would do better than the proposed approaches.<BRK>4.Necessary comparisons are not made with at least a few state of the art models in CIFAR. The paper is easy to read and understand. The authors proposed a Conv LSTM inspired model called V1Net.
Accept (Poster). rating score: 6. rating score: 6. rating score: 6. rating score: 3. <BRK>This paper studies the problem of unsupervised scene decomposition with a foreground background probabilistic modeling framework. It’s better to clarify this point in the rebuttal.<BRK>Post rebuttal update: I have read the authors  response and I am happy to increase my rating to 6. This makes comparison impossible. I think the paper would benefit from being a bit clearer about this. There are some reasonable results. It would have been nice to have an ablation of some of the components, including the boundary loss.<BRK>In this paper, the authors propose a generative latent variable model, which is named as SPACE, for unsupervised scene decomposition. In sum, I think this paper is not ready to be published. I have read the authors  reply and the updated version. There are several issues need to be addressed:1, The organization of the paper should be improved.<BRK>The paper proposes SPACE: a generative latent variable models for scene decomposition (foreground / background separation and object bounding box prediction). I found the qualitative comparisons to be confusing as they were mostly for different input frames, making it hard to have a direct comparison of the quality between the proposed method and baselines.
Reject. rating score: 3. rating score: 6. rating score: 6. <BRK>This paper proposes a temporal probabilistic asymmetric multi task learning model for sequential data. This paper criticize that the task loss used in previous works may be unreliable as a measure of the knowledge from the task. The organization of Section 3 is not good.<BRK>The submission proposes a probabilistic method for multitask learning for sequence data focusing on the use of uncertainty to control the amount of transfer between tasks. The approach additionally models transfer between features from different time steps. The evaluation shows that the method outperforms recent state of the art models on two medical benchmarks. By introducing probabilistic modelling to control the amount of transfer between tasks the paper provides an interesting perspective and is able to show strong performance.<BRK>In this paper, the authors proposed an asymmetric multi task learning method on temporal data, and applied it to clinical risk prediction. 1.The details on how to construct F_\theta to generate the hyper parameter of a in (10) are missing. In this case, is the proposed method practical?
Reject. rating score: 3. rating score: 3. rating score: 6. <BRK>Due to using different baselines, the numbers are not comparable. Pruning methods are important for real time applications of CNNs on low resourced devices, so the paper addresses a genuine need. Are these rolled in the estimations on the speedup? This paper presents an offline and online pruning method for CNNs where RL is used for tuning the sparsity.<BRK>If a simple combination of existing static and dynamic methods works well, you will need to justify the need for the more complicated DRL approach you propose in this paper. Please improve the writing and summarize the process in pseudocode or illustrate it. Overall, I do like the idea of combining static and dynamic pruning, the DRL approach is reasonable and it seems to do well in practice. However, there are some key issues that must be addressed by the authors.<BRK>Strong results are shown for CIFAR10 and ILSVRC2012. Some of the algorithmic details could benefit from some clarification. It proposes a unified framework to manage the trade off between static pruning to decrease storage requirements and network flexibility for dynamic pruning to decrease runtime costs.
Accept (Poster). rating score: 6. rating score: 6. rating score: 6. <BRK>The paper then uses the "labelled" points for semi supervised learning with a proposed ensemble of models.<BRK>I find this paper interesting and somewhat novel, with the following comments.<BRK>This paper proposes a method for unsupervised clustering. These samples are determined by the pruning of a graph whose edges are determined by the votes of an ensemble of clustering models. I have trouble understanding the experiment settings.
Accept (Poster). rating score: 6. rating score: 6. rating score: 6. <BRK>This paper shows how SNNs can integrate backpropagation with a second accumulation module that discretizes errors into spikes. In other words, the authors show how to translate event based information propagation (used by SNNs) into a backpropagation method, which is the main contribution. I m a bit confused about the computational complexity estimation of the SNN.<BRK>I now increase my score to a weak accept. This paper proposes a new backpropagation algorithm learning algorithm "SpikeGrad" for Spike based neural network paradigm. They show that training a fixed architecture using their method is comparable to other prior work which uses high precision gradients to train them. They also show how to exploit sparsity of the gradient in the back propagation for SNN. Could you please elaborate this in the rebuttal?<BRK>This paper proposes a first framework of large scale spiking neural network that exploits the the sparsity of the gradient during backpropagation, to save training energy. The paper is clearly written.
Reject. rating score: 1. rating score: 1. rating score: 6. <BRK>However, I am still on the same boat with R1 and recommend rejection for this submission. ################################This submission seeks to evaluate generative models in a continual learning setup without storing real samples from the target distribution. I question the scope of this paper, as this is not a topic of general interest to the community. Additionally, the density ratio estimation technique is fairly standard. While continual learning is a trendy topic in the AI community, it s less well received in the context of generative modeling, probably for the lack of real applications. This submission feels more like playing a game with the rules defined by the author(s), not driven by practical considerations.<BRK>The basis of the continual density estimation is based on a recursive relationship between the density ratio at step t and that at t   1. Continually estimating the density ratio without storing the raw data is an interesting topic and could be useful for continual learning. However, I give reject to this paper because of the following reason: The writing of this paper is not easy to follow. The beginning of section 3 (CDRE in continual learning), I found it difficult to understand why the model q needs to be updated (indexed by t) while p(x) is not dependent on t. As far as I know, under the continual learning setting the data distribution p(x) is also conditioned on t. I interpret it as a general introduction on how density ratio could be estimated continually.<BRK>In this paper, authors propose a continual learning for density ratio estimation. The formulation of the CDRE Eq.(5)is quite intuitive, and it makes sense. The paper is clearly written and easy to read. I am pretty new to the continual learning. However, I am still not that certain whether the setup is realistic. More specifically, if it is a privacy issue, we may not be able to use the model trained by the private data as well.
Reject. rating score: 3. rating score: 3. rating score: 3. <BRK>Overall, I like how the paper addresses the weakness of the existing graph Laplacian operators (dominance of the first eigenvector) and proposed a new method with theoretical justifications. However, I also have concerns about the paper that I feel necessary to be resolved. Namely, the meaning of robustness in the neural network (adversarial robustness) and the SBM literature (spectral robustness) are different. I also hope the paper could have done the experiments on more datasets since there exists some evidence on the unreliability of evaluations on citation networks [3]. How is the knowledge distilled between graphs?<BRK>In this paper, the authors study the classic GCN and proposed the new convolution operator with wider spatial scope and robust properties. The proposed models could improve the accuracy in both benign and evasion setting on synthetic, ie., SBM dataset and real world benchmark graphs. In the paper, compared to the classic GCN, the authors replace the adjacent matrix $A$ with the proposed “variable power operator”. However, the proposed models are claimed to defense against evasion (testing) attacks. The efficiency of the proposed might be an issue. "Adversarial attacks on graph neural networks via meta learning."<BRK>It is unclear how these analyses can be generalized to real graphs. There are some concerns that need to be addressed or clarified:A major concern is that the theoretical analyses in this paper are limited to graphs sampled from the SBM model.
Reject. rating score: 1. rating score: 3. rating score: 3. rating score: 3. <BRK>What is specific to panoptic segmentation vs semantic or instance segmentation? What are the benefits of the proposed interactive methods in terms of scalability? The paper proposes to apply two existing algorithms (Nguyen & Brown 2015, Rempfler et 2016) to a new task (interactive panoptic segmentation): what is the claimed novelty? ## Update following the rebuttalThanks to the authors for their replies.<BRK>This paper investigates scribble based interactive semantic and panoptic segmentation. The algorithms described build a graph on superpixels and do not require deep features or labels but rely on “scribbles” for supervision. I’d like for the experiments section to have a proper comparison to prior scribble algorithms (e.g.in section 4.4, comparing to other algorithms with the SOTA approach as baseline) to clearly show the advantage of their approach.<BRK>This paper proposes two graph based deep network feature fusion methods with connection constraints for semantic and panoptic segmentation. I am not completely convinced by the novelty and experiments. (1) First, the idea of smart annotation can be formalized as a weekly supervised segmentation problem where only part of the annotation is available. To approach the panoptic segmentation problem, the authors essentially used scribbles to separate semantic region predictions into individual instances.<BRK>This paper introduces post processing methods for panoptic (a combination of semantic and instance) segmentation, which are capable of using scribble annotations provided interactively by users. Separately, the paper should compare the proposed method for semantic and instance segmentation with other methods that use weak labels such as:* Laradji, I. H., Vazquez, D., & Schmidt, M. (2019). The problem of panoptic segmentation is fundamental to computer vision, and as such, of relevance to the ICLR community.
Accept (Spotlight). rating score: 8. rating score: 6. rating score: 6. <BRK>The ranking results would be much more compelling if they included a broader range of architectures, including more recent models with more branching, e.g., DenseNet. Is there some reason ResNet101 has higher generalization error than 18 and 34? Net.Criticality for ResNets is inversely correlated with the number of layers; is there an explanation for this?<BRK>It will be useful ot have a discussion on this issue. The paper uses a convex combination of the initial weights and the final weights of a layer/module to define an optimization path to traverse. Empirical results on CIFAR10 show that the network s criticality is reflective of the generalization performance.<BRK>Overall, a good paper. This is quite an important problem to study as this helps to develop better understanding of the current architectures and potentially reduce their size without suffering the accuracy drop. This is a great theoretical contribution. The results demonstrate the module criticality to be a good metric for generalization of models.
Reject. rating score: 3. rating score: 3. rating score: 6. <BRK>I thank the authors for their detailed response and appreciate their hard work in bringing us this paper. The bag of words representation is effective but it is not the case for natural language. It is clear and well designed. Cons:(1) It seems that the two main contributions are related to the language.<BRK>However, they addressed two limitations of previous works about visually grounded embodied language learning models. To overcome the problem, a multitask model is introduced. The paper shows that the proposed model outperforms a range of baselines in simulated 3D environments. Some of the main contributions, e.g., "modularity and interpretability" and "transfer to new concepts," are not evaluated quantitatively.<BRK>I would recommend for acceptance, as the experimental results show that the proposed approach successfully transfers knowledge across tasks. First, the paper uses a new environment to evaluate the SGN and EQA task instead of the benchmark environments for these two tasks, making it difficult to compare performance to previous work. The environment in the paper is small (compared to e.g., House3D for EQA) and has a limited variety.
Reject. rating score: 1. rating score: 1. rating score: 3. <BRK>The is mainly concerned with classification tasks for time series. Subsequently, a sparse auto encoder is used that encodes the last layer of the classifier. For training the auto encoder the classifier is fixed and there is a decoding loss as well as a sparsity loss. The sparse encoding of the last layer is supposed to increase the interpretability of the classification as it indicates which features are important for the classification. In general, I think this paper needs to be considerably improved in order to justify a publication. The authors have to improve the motivation part as well as the discussion of the results. Shouldnt it improve performance to also adapt the discriminator to the new representation. At least comparison to such an approach would be needed to justify something more complex. While the comparisons seems to be exhaustive, it is already too many plots and it is very easy to get lost.<BRK>The aim of this work is to improve interpretability in time series prediction. To do so, they propose to use a relatively post hoc procedure which learns a sparse representation informed by gradients of the prediction objective under a trained model. In particular, given a trained next step classifier, they propose to train a sparse autoencoder with a combined objective of reconstruction and classification performance (while keeping the classifier fixed), so as to expose which features are useful for time series prediction. Sparsity, and sparse auto encoders, have been widely used for the end of interpretability. In this sense, the crux of the approach is very well motivated by the literature.<BRK>In this paper, the authors proposed an algorithm for identifying important inputs for the time series data as an explanation of the model s output. Given a fixed model, the authors proposed to put an auto encoder to the input of the model, so that the input data is first transformed through the auto encoder, and the transformed input is then fed to the model. In the proposed algorithm, the auto encoder is trained so that (i) the prediction loss on the model s output to be small, (ii) the reconstruction loss of the auto encoder to be small, and (iii) the transformed input of the auto encoder to be sufficiently sparse (i.e.it has many zeros). Thus, to me, the soundness of the proposed approach is not very clear to me. The authors tried to justify the approach by raising related studies, which is however the mix of the two different approaches.
Accept (Poster). rating score: 6. rating score: 6. rating score: 6. rating score: 3. <BRK>Improved wall clock training time has been achieved through distributed actors and learners, but often at the expense of sample efficiency. IMPACT repurposes successful concepts from deep RL   the target network, importance sampling and a replay buffer to demonstrate improvements on both axes in on three continuous environments and three games from the Atari Learning Environment. PositivesThis was a well written paper proposing to address the sample efficiency of distributed RL algorithms. The diagrams of the algorithm were also well done. Improving the sample efficiency of algorithms is an important objective and the approaches followed here are sensible. Also, the IMPALA baseline should be validated for the continuous control tasks   it s surprising that this once SOTA algorithm flounders in even simple tasks like Hopper v2 or HalfCheetah v2. Update:The authors have addressed my initial concerns carefully through extra experiments and details in the Appendix and I have updated my rating accordingly.<BRK>This paper introduces IMPACT which is a distributed RL algorithm that shortens training time of RL systems while maintaining/ improving the sample efficiency. The authors break down the novel component of their model into three categories: target network, circular buffer, and importance sampling. Overall I think this is an interesting paper which can motivate more work in this area. Updates:I would like to thank the authors for their response.<BRK>The paper studies a novel way for distributed RL training which combines the data reuse of PPO with the asynchronous updates of IMPALA. The main contribution is the observation that using a target network is necessary for achieving stable learning. I think this is an important result which seems to be validated by another ICLR submission (https://openreview.net/forum?id SylOlp4FvH). The experimental section could definitely be improved I was hoping to see more results on Atari or DMLab. Thanks for clarifying and for the extra experiments. I m keeping my score as I still think it s appropriate.<BRK>The paper proposes a new distributed algorithm for reinforcement learning. The paper lists three main contributions: a target network for stabilizing the surrogate objective, a circular buffer, and truncated importance sampling. The introduction of a circular buffer is not very novel. are standard ways to improve performance in distributed training. The evaluation of the proposed algorithm is reasonably well done (considering the page limits), with a suitable set of benchmarks (although relatively few).
Reject. rating score: 1. rating score: 1. rating score: 3. <BRK>In summary, based on the above reasons, I vote for the paper to be strongly rejected.<BRK>4.There are a few typos. The proposed method is clearly presented in the paper. Besides, Eq (1) should be written in a more formal way.<BRK>In the training stage, the instance embedding and its label embedding are forced to be close. Using deep neural networks for multi label problems is also not new, see [3,4].
Accept (Spotlight). rating score: 8. rating score: 8. rating score: 6. <BRK>This is beyond conventional theoretical studies for infinite width networks where depth is kept finite when the width is taken to be infinite. The main object that paper studies is the neural tangent kernel which is of great interest to the theoretical deep learning community as it describes gradient descent dynamics in a tractable way. While standard NTK becomes deterministic in the infinite width limit, when both depth and width are simultaneously taken to infinity this paper shows that NTK is no longer deterministic. There are two main limitations of otherwise significant work. I worry that the method may be too specific to the particular network setting. Still this does not eclipse the strong results it could say for ReLU networks. Second limitation is lack of empirical check.<BRK>This is an important contribution to understand finite depth and width corrections to the NTK. In this paper, the authors show that in the double limit regime, i.e.depth/width   \beta and depth, width  > infinity, the diagonal terms of the NTK, as well as the first gradient step, is NOT deterministic. It will be very helpful to have some experiments to support the main theorems in the paper since the proof is quite involved. Neural tangent kernel: Convergence and gen  ´eralization in neural networks. How to start training: The effect of initialization and architecture.<BRK>This paper studies the finite depth and width corrections to the neural tangent kernel (NTK) in fully connected ReLU networks. NTK has been a popular subject of theoretical study in deep learning, and it s an important question to understand when and to what extent NTK can capture the behavior of real neural networks. This paper makes partial progress by analyzing the diagonal entries of the NTK in fully connected ReLU networks. Of course, it s already implied by the current paper that the NTK is not deterministic and can move a lot when beta is large, but it s unclear whether the reverse is true, i.e., what is the regime when the NTK becomes deterministic and frozen when an entire dataset is involved. "In fact, empirically, networks with finite but large width trained with initially large learning rates often outperform NTK predictions at infinite width." I am updating my rating to weak accept in light of this.
Accept (Poster). rating score: 6. rating score: 6. rating score: 6. <BRK>The main contribution in this paper is the use of flow based models for video prediction, and it is the first work in this direction. The major idea sounds and the paper is clearly written. It would be interesting to see more dynamic scenarios such as driving or human motion scenes.<BRK>The paper "VideoFlow: A Conditional Flow Based Model for Stochastic Video " proposes a new model for video prediction from a starting sequence of conditionning frames. It is based on a state space model that encodes successive frames in a continuous hierarchical state, with contraints on trajectories of the codes in this state.<BRK>This paper presents a stochastic model based on Glow for conditional video generation. The major novelty of this work is to introduce the flow based models to video modeling and learn the video dynamics via the dependencies of the latent variables. — Experiments —b.1) Throughout the experiments, the VideoFlow model is mainly compared with two stochastic video prediction models that were probably proposed by the same research group. Thus, the comparisons in Table 2 might be unfair.
Accept (Poster). rating score: 6. rating score: 6. rating score: 6. <BRK>This paper presents a method for adaptive adversarial example detection. The proposed model is composed of both classifier and adversarial detector, where the classifier makes the classification prediction and the adversarial detector evaluate if the input sample is natural of adversarial. The authors provide extensive experimental results showing the promising performance of the model in detecting various types of adversarial attack. Would this cause unstable training? 2) Maybe I missed it, but it seems that the objective function in Eq.(5) is based on the adversarial detector. How could the classification performance of classifier f be guaranteed in training?<BRK>Review: The paper addresses the adversarial example detection problem. The framework proposed in the paper divides the input space into subspaces based on a classifier’s output and trains detectors on these subspaces to classify a natural sample (classified as that class) from an adversarial one fooling the network. The goal is to use a robust optimization approach to enable detection methods to withstand adaptive/dynamic attacks. Hence, an asymmetrical adversarial training (AAT) regime is employed which presents solving a min max problem. There are three different attacking scenarios and evaluation shows that the combined attack turns out to be most effective (as it fools both the classifier and detectors) against integrated detection. The paper also demonstrates empirical improvements over state of the art detection techniques with higher L2 distortion of perturbed samples. It was interesting to observe the perturbed samples produced by attacking generative classifier.<BRK>Summary:This paper studies the adversarial detection problem within the robust optimization framework. They propose an adversarial detection and a generative modeling technique called asymmetrical adversarial training (AAT). Experimental results are provided on MNIST, CIFAR10 and Restricted ImageNet, compared with CW method as baseline. The paper is well written with detailed experimental results. I d suggest accepting the paper. To my understanding, the objective function of AAT is similar to GAN s, while there is a detector for each class discriminating natural data from adversarially perturbed data instead of generated data. They incorporate the attack into the training objective with three attacking scenarios: classifier attack, detectors attack, and combined attack. They also introduce integrated classification of the classifier and detectors with the reject option. They claim in addition to more robust classification, ATT also gives rise to improved interpretability, which I m not convinced of with given experimental results.
Accept (Poster). rating score: 8. rating score: 8. rating score: 6. <BRK>Novelty:   The paper explores an interesting area of learning representations for cross domain tasks such as image captioning and text to image synthesis. The model is well explained. While the proposed methods beats existing diversity and perceptual metrics, it d be good to also run a human study since these metrics are only a proxy to human judgement. After reading the rebuttal, and considering that they will run the human study to further validate their approach in the final manuscript, I am happy to raise the score.<BRK>Summary:This paper addresses the problem of many to many cross domain mapping tasks (such as captioning or text to image synthesis). It proposes a double variational auto encoder architecture mapping data to a factored latent representation with both shared and domain specific components. The proposed model makes use of normalizing flow based priors to enrich the latent representation and of an invertible network for ensuring the consistency of the shared component across the two autoencoders. The manuscript is well written and easy to follow (provided some technical familiarity with variational inference). I still recommend your work for acceptance.<BRK>The paper introduces a variational model for text to image and image to text mappings. The intuition behind the model is well introduced. Particularly section 3.3 describing the shared component and the global model should be carefully checked. There are some typos or erros, check eq (7), (8), q_phi I instead of q_theta in §3.3. I still have some concerns with confusing notations.
Reject. rating score: 1. rating score: 3. rating score: 3. <BRK>Summary:This paper proposes a new initialization method for recurrent neural networks. Basically, this paper is a combination of [1] and [2]. Strength:The method of initializing LMN using a linear RNN is natural and simple. (section 3.2)The proposed initialization outperforms the baselines on the MNIST dataset. The performance on TIMIT is worse than the baseline methods. The scale of the experiments is too small.<BRK>Summary:The paper proposes an autoencoder based initialization for RNNs with linear memory. Pros:1.The paper is well written, the motivation and methods are clearly described. 2.The experiments on the copy task only showed results for length upto 500, which almost all baseline models are able to solve. The task/ dataset does not require long term memorization. However, it is still a little how this experiment corresponds to the messsage that the authors are attempting to deliver at the end of the introduction. Equation 1 and 2 suggest that there is a vector m^t per timestep (as oppose to having 1 for the entire sequence).<BRK>This paper proposes an initialization scheme for the recently introduced linear memory network (LMN) (Bacciu et al., 2019) and the authors claim that this initialization scheme can help improving the model performance of long term sequential learning problems. My concerns lie with the novelty of the proposed model and the insufficiency of the experiments. Even for the TIMIT dataset, the results are a bit far from state of the art which makes the paper s claim less convincing. Overall I think the novelty contribution is marginal and I suggest the authors to test their models on larger scale real problems.
Reject. rating score: 3. rating score: 6. <BRK>This paper is extremely interesting and quite surprising. In fact, the major claim is that using a cascade of linear layers instead of a single layer can lead to better performance in deep neural networks. Moreover, the proposed approach is extremely simple and it is well explained in Section 2 with equations (1) and (2). In fact, the model presented in the paper has a major obscure point. Equation (1) and (2) are extremely clear. Since the authors are using inner matrices with a number of dimensions higher than the number of dimensions of the original matrix, there is no approximation and, then, no selection of features or feature combinations. If this does not lead to the same improvement, there should be a value in the expansion. The training should be done by using the small network.<BRK>This paper proposes linear over parameterization methods to improve training of small neural network models. The paper is easy to understand and follow. These are some concerns on the paper:1) The effectiveness of the approach is not necessarily significant in all experiments. Table 2 has "N/A". Knowledge transfer methods should be the baseline of the paper.
Reject. rating score: 3. rating score: 6. rating score: 6. <BRK>The paper proposes a multi source and multi view transfer learning for neural topic modelling with the pre trained topic and word embedding. However, it is not clear to me that the improved results are resulted due to multi source multi view transfer learning or for the better leaning of the single source model due to the incorporation of the regularizer.<BRK>The combination of both local and global view transfer to enhance a topic model is the main contribution of this paper, especially when using multiple sources (therefore the title: multi source multi view transfer). Edit after rebuttal: In my review I did not value the contribution of the transfer learning approach enough. The main contribution is the usage topic models in a transfer learning framework.<BRK>This work proposes a novel method to use pre trained topic embeddings and pre trained word embeddings obtained from various corpora in the transfer learning framework. Since this is the first approach to use topic embedding in the transfer learning field, the simplicity of the proposed method is somewhat necessary. Even though comparison with the naive baseline (data augmentation) seems too obvious, I think the results clearly show their claim about the importance of using transfer learning in neural topic modeling domain.
Accept (Poster). rating score: 8. rating score: 6. rating score: 3. <BRK>This paper proposes a novel architecture of integrating neural models with logic inference capabilities to achieve the goal of scalable predictions with explanatory decisions, which are of significant importance in the real deployment. 4.For the experimental section and related work, another existing work is missing, i.e., "Neural Logic Machines". 3.The proposal of a general logic operator defined in eq5 is crucial for formula generation in a differentiable way.<BRK>However, my major critique of the paper is in its clarity. I would be willing to accept the paper if the authors improve the notation and presentation significantly. It is not clear from the figure itself. Given that they are parameterised with a predicate matrix, and that matrix is trainable? Do you have a softmax on the output?<BRK>The paper proposes to determine explanations for predictions using first order logic (FOL). The paper then proposes a parameterized logical operator and describes their architecture for training these using attention and transformers. I found the paper to be very difficult to read. I can t make out what the parameterization of the logical operator is. I can t also connect section 4 to the parameterized logical operator in section 3. This is not my favorite way to write a paper. I couldn t make out exactly what the authors meant with this statement from the Appendix:"The main drawback of NeuralLP is that the rule generation dependents on the specific query, i.e.it’s data dependent.
Reject. rating score: 3. rating score: 3. rating score: 3. <BRK>The paper proposes to combine a VAE model with the Optimal Transport to approximate some components of the model. The authors evaluate their approach on semi supervised problems and claim to obtain very competitive results compared to literature. Unfortunately, the paper is very unclear and hard to follow.<BRK>This paper aims to solve the existing problems in semi supervised learning by employing optimal transport theory and proposes a one stage training VAE, OSPOT VAE, which has a tighter ELBO and demonstrates better performance on benchmark datasets like CIFAR 10 and CIFAR 100 than benchmark methods. My main concern is that it appears in the paper the authors are trying to use a linear transport to approximate the intractable posterior distribution p(z|X) and p(c|X).<BRK>The derivation seems different from a standard semi supervised VAE. The use of OT seems novel in this context. (2) Writing: The paper is generally well written. However, I also found section 3 is hard to follow, with details in the Weaknesses section. (3) Experiments: The results look impressive.
Accept (Poster). rating score: 8. rating score: 6. rating score: 6. <BRK>Interestingly, the paper proposes to remove all stochasticity from an encoder and a decoder and use different regularizers instead. Could the authors comment on the choice of hyperparameters (weights in the objective)? (An open question) The marginal distribution p(z) allows to turn the AE into a generative model. I still believe that this paper is extremely important for the AE/VAE community and it sheds some new light on representation learning.<BRK>The paper studies (the more conventional) deterministic auto encoders, as they are easier to train than VAE. Experiments are mostly around contrasting VAEs with the proposed RAEs in terms of comparing the quality of the generated samples. The paper is trying to answer an important and meaningful question, i.e.can a deterministic auto encoder learn a meaningful latent space and approximates the data distribution just as well as the much more complicated VAEs. The methods developed in the paper are mostly easy to understand and also well motivated in general.<BRK>This paper propose an extension to deterministic autoencoders. Motivated from VAEs, the authors propose RAEs, which replace the noise injection in the encoders of VAEs with an explicit regularization term on the latent representations. Can you provide a theoretical analysis on that? I agree that the additional experiment results help to support the claims from the authors, especially for the CV VAE for the structured data experiments and the AE + L2 experiment.
Reject. rating score: 3. rating score: 3. rating score: 6. <BRK>Because the whole process is differentiable, the appropriate precision to each layer can be found fast. However, the relationship is unclear and cannot convince the reviewer that the newly proposed loss (equation 3) is differentiable. (2) The preliminary experiment in section 4.1 seems to claim that a single cursor leads a network to local minima, by only showing training loss. The reviewer thinks that the authors need to show validation loss as well to claim the failure of the single cursor. The followings are minor comments. * To the best of the reviewer s knowledge, the term "cursor" is the authors  original one. For example, [He et al.2015] is accepted to CVPR 2016 but is not mentioned. The reviewer thinks that the method proposed in this paper requires more comprehensive experiments.<BRK>This paper is about using quantization to compress the DNN models. The main idea is to use NAS to obtain the mixed precision model. More specifically, it adaptively chooses the number of quantization bit for each layer using NAS by minimizing the cross entropy loss and the total number of bits (or model size) used to compress the model. I have somes questions on this paper:1) Is Eq(1) standard for quantization optimization? Normally, we aim to minimize the loss on the training set, not the validation set,  or sometimes generalization loss on data from the data distribution. Although we might not care much about the training time for compression task, I just want to have a sense of how training works. Do these two layers taken into account in the final compression ratio computation?<BRK>The paper proposes a novel way to determine automatically the quantization level at each layer of deep neural networks while training. On the positive side, the authors present very good experimental results on popular networks like ResNet, and improve on other method s compression accuracy, with little to no reduction in accuracy. is the gradient multiplied by the loss?? I assume that there is a price in power/memory/speed etc. This is not a critique of this particular paper, but of the entire framework of flexible quantization. The English of the manuscript could be greatly improved. But the loss for one integer is actually lower   so why wouldn t it be better to use this one? the number of bits for all the weights in each layer?
Reject. rating score: 1. rating score: 3. rating score: 3. <BRK>The authors propose using implicit human feedback, by means of error potential (ErrP) measured using an EEG cap, as an additional input in deep reinforcement learning. The use of ErrP in RL appears novel, however, there are several concerns that prevent me from recommending acceptance:1. The authors claim that it is better to use ErrP than explicit feedback by users. 3.Given the minimal methodological contribution, the experiments should be much more thorough. 4.The authors only provide a hand wavy visual validation of the consistency of the potentials. Mor generally, the authors do not provide sufficient details of their experimental setup and evaluation metrics.<BRK>This paper introduces several methods for training reinforcement learning agents from implicit human feedback gained through the use of EEG sensors affixed to human subjects. I like the idea of using EEG a way to reduce the burden of collecting human feedback for training reinforcement learning agents. While I like the paper as an interesting idea and proof of concept, there are some flaws that make me doubt it would be realizable for more complex tasks. Why are no results shown for Catch or Wobble? 6) At an action speed of 1.5 seconds per action, I imagine that EEG is not much faster than having a human subject press a button to indicate their label. It s my feeling that the experiments are more of a proof of concept and many open questions exist about whether this method would scale beyond these simple domains that DQN masters in ~300 episodes.<BRK>This paper tackles the problem of obtaining feedback from humans for training RL agents, in a way that does not require too much time or mental burden. The goal of this paper is to show that using implicit human feedback via ErrPs is a feasible way to speed up the training of RL agents. Towards this, the paper makes two main contributions: (1) experimental evidence that the ErrP for a human can be learned for one game, and transferred zero shot to other games, and (2) two approaches of collecting this implicit feedback from humans for a smaller number of trajectories, instead of collecting it for all trajectories encountered by the agent during learning. This paper is well motivated, and tackles an important problem. I have doubts that this would generalize to more complex, long horizon tasks, in which trajectories are at least hundreds of timesteps long. It would be useful to be able to characterise / predict how good generalization will be from one type of game / situation to another. In the paper, incorrect actions appear with a probability of 0.2, but there s no explanation for how this number was selected, and whether others were tested.
Reject. rating score: 1. rating score: 1. rating score: 3. <BRK>The paper proposes an approach called omnibus dropout. Cons:1.The paper could be improved in writing, especially on justification why this kind of dropout combination gives better performance. 2.The results are mixed and the improvements are not significant. So I am not convinced the proposed approach is always a better strategy. 3.The proposed approach is simply a combination of the existing dropout methods. The contribution of the paper is very limited. In table 1, different dropout rate is chosen for different method.<BRK>“Dropout uncertainty can be obtained sequentially”. The paper needs to be rewritten to improve presentation, and state motivation, problem statement, and contributions clearly. 3.The entire proposed method is described in one line, “we propose a novel omnibus dropout strategy, which merely combines all the aforementioned methods”. Hence, it seems that the proposed method provides no performance improvement.<BRK>This paper proposed to use multiple structured dropout techniques to improve the ensemble performance of convolutional neural networks as well as the calibration of their probabilistic predictions. The approach, which is termed omnibus dropout and combines multiple existing dropout methods, is reasonable and straightforward. My main concern is about the technical novelty of the paper. I could not find it in Figure 5.
Reject. rating score: 3. rating score: 3. rating score: 8. <BRK>This paper proposes a hybrid approach for adding noise to training images of an image classification model. Instead of either cutting out a patch or adding gaussian noise, the authors propose to adding a patch of gaussian noise to the images. The experiments are rather limitted to support the claim.<BRK>A few examples/pictures of success cases (when the method works) and failure cases (when the method doesn’t work), may help readers (I’m not an expert) to better understand the approach and get more intuitions? The frequency analysis seems quite intuitive. This does support the authors’ claim that current methods haven’t reached the robustness/accuracy tradeoff boundary yet. However, the presentation of the experiments just seems to aim for the best numbers one can get (I’m not certain how significant the numbers are to this field though).<BRK>It seems reasonable that while adding Gaussian noise makes the model robust to high frequency noise, since Gaussian noise is not added everywhere, the model is able to exploit high frequency signal when available in the input. The paper is reasonably well written and the experimental validation is convincing.
Reject. rating score: 1. rating score: 1. rating score: 3. <BRK>In this paper, the authors present an approach for semi supervised learning which combines noisy labels with boosting. The paper does not mention how (or if) a validation set was used to select these. Additionally, the notation in the paper is not consistent.<BRK>The authors propose a new semi supervised boosting approach. In any case, the theorem is not clearly described enough to help understand the contribution of the paper.<BRK>The paper proposed a method combining boosting with semi supervised learning to handle classification problems when only partial data points have labels available. What is the gap like?
Reject. rating score: 1. rating score: 3. rating score: 3. <BRK>This paper proposes a simple method for stabilizing the off policy deep reinforcement learning algorithm, which updates the target network only when the online network performs better than the target network in order to ensure the stability guarantees. More specifically, at every T time steps, they execute both the online network and the target network so as to evaluate the performance of each policy. The experimental results show that the proposed Conservative TD3 (C TD3) is less prone to performance degradation during training. While the stabilizing off policy reinforcement learning algorithms would be a significant problem, I have some concerns regarding the presentation and the limitation of the proposed method. Algorithm 1, 2, 3 and 4 are not directly related to the proposed method, thus they can be omitted from the paper. Instead, it would be great to devote more space to the proposed method such as a more detailed theoretical analysis or the pseudo code of the proposed algorithm. If we see Figure 4, in Walker2d, the mean performance of TD3 reaches 4000 at 400k steps, while the performance of C TD3 is even less than 3000. In the experiments,  discarding failure seeds  cannot be a proper treatment.<BRK>[Summary]This paper proposes an approach called conservative policy gradients to stabilize the training of deep policy gradient methods. After rebuttal: I have read the authors  response, the other reviews, and the revision. The target policy is then updated to match the current policy only if the current policy is better than the target policy. Experiments show that the proposed method, when applied to TD3, reduces the variance in performance through the training. At fixed intervals, the current policy and a separate target policy are evaluated with a number of rollouts. However, this stability comes at the cost of extra computation and interaction with the environment (to evaluate the policies). Does stability in this proposition mean that the performance will reach a stationary distribution with bounded support? How can a stable target policy result in more stable performance if it is not used to take actions? The paper claims that the proposed method results in improvements in stability and overall performance. In Figure 3, the proposed method is more stable than the baseline but the overall performance is not better. The experiments do not seem to compare these two methods with the same number of samples or with the same amount of computation. [Minor comments]In the learning curves in Figure 1, what is the measure of performance and how is it estimated? A description of the plotted measure is necessary to show that the drops in the estimated performance are indeed due to policy degradation rather than poor estimation.<BRK>This paper proposed to use target network policy as a conservative policy for performance evaluation. As for the experiments, the authors evaluate the proposed method on a variant of TD3 (Conservative TD3) and the experimental results indicate the proposed method indeed reduces the variance of the expected return. Besides the promising results, I believe there are several concerns that should be clarified before we can conclude that the proposed method can improve the stability. Stability Measurement: While the experimental results show that the proposed method can reduce the variance of expected return, it is not a direct measurement of the stability or the robustness of the learned policy. More related work should be compared. The authors only compare the original version of TD3 and the modified proposed method. How does the proposed method perform in the noisy MDP settings? It would be more convincing if the method can still perform well in such settings to support the claim. The author clarified one of my main concern, but the other reviewers point out that the comparison is not fair (using only 5 seeds and discarding the failure seeds).
Reject. rating score: 3. rating score: 6. rating score: 6. <BRK>This article present an approach to assign an importance value to anobservation, that quantifies its usefulness for a specific objective task. It follows that the DVE is trained with a RL signal, that followsthe variation of the loss throughout the learning process. The method proposed by the authors is new and show very significant resultsover existing methods. The paper is very well written, and the method is illustrated on several datasets,from different domains. However, it seems that many approaches that did not suffer from the same complexitydrawbacks of LOO and Data Shapley were not compared to this work.<BRK>The results show that the proposed method outperforms a number of existing approaches. I think the proposed method is reasonable, and the results look promising. However, I m concerned that there s limited ablation study provided to show how each design choice impacts the performance. But this is not discussed in paper. How does it impact performance? Overall, after rebuttal, I d like to recommend "weak accept" for this paper.<BRK>This paper proposes a meta learning approach based on data valuation for reinforcement learning tasks. The authors motivate this construction with the goal to filter out unreliable and corrupted data. It s well established that RL poses a difficult learning problem, and as such the goal to improve the RL process is definitely a good one. To the best of my knowledge the approach proposed here is new. In addition, the submission contains a thorough evaluation of the method. Another cricital aspect for meta learning approaches such as this one is also the training time. Here, the text gives a factor of about 3, which is noticeable, but not overly big.
Reject. rating score: 1. rating score: 3. rating score: 6. <BRK>The paper proposes a deep visualization technique for black box image classifiers that feeds modified versions of the original input by means of an off the shelf (black box too) image inpainting approach (DeepFill v1), in order to capture changes in the classification performance. There are problems in the paper, major and minor. Major: 1)	The technical contribute is minimal, the author combine two already existent techniques. 2)	The results are not convincing: a) the comparison are not fair, since on two out of three techniques the authors consistently change the competitors, letting them work with zero value occluders or random noise. The title is misleading, the authors are talking about generic feature removal but in reality we are considering the image domain only.<BRK>The work argues that the use of inpainting in Chang et al (focused on keeping the salient object and removing background) was invalid as the inpainter model is not trained to do such a task. Thanks!Fig 3 results: MP is more robust than MP G and I couldn t find any explanation of why this method behaves specifically different than the other two in the experiments section. The idea has already been discussed out in the literature and the novelty of the work seems to be twofold: They introduce the same method in a way that is not curated for a specific perturbation based method and could be concatenated with "ANY" given (or future) perturbation based local explanation method (which authors notate by calling it ${existing_method} G, they study robustness to hyper parameter choice.The paper is quite well written and the experiments are comprehensive. This work seems not to be concerned with these specifics and directly feeds one of such samples.<BRK>The paper proposes to improve perturbation based explanation techniques by complementing the perturbation step with an inpainting step that removes artefacts caused by it. The approach is sound and intuitive. It is unclear what is the computational cost of the inpainter. Evaluation experiments are not fully conclusive. Perhaps the deletion metric should have been equiped with inpainting as well in order to avoid deletion artefacts.
Reject. rating score: 1. rating score: 3. rating score: 3. <BRK>Based on the above arguments, I would like to recommend a reject for this paper.<BRK>3.The experiments of this paper are not very convincing.<BRK>This paper introduced a novel parametrized graph operation called bipartite graph convolution (BGC). Overall, reviewer is very positive about the technical novelty of the paper. The ablation studies on graph structure (e.g., number of layers) are currently missing (Figure 4 and Table 1).
Accept (Poster). rating score: 8. rating score: 8. rating score: 3. <BRK>The paper offers a nice review of online linear optimization and its relationship to CFR, making its proposed algorithm easily digestible. This will be an important direction for future work.<BRK>The paper proposes an improvement to Counterfactual Regret Minimization, avoiding traversing the whole tree on each iteration. However, this implementation comes at the cost of additional memory requirements.<BRK>This paper presents a variant of the counterfactual regret minimization (CFR) algorithm called Lazy CFR for two player zero sum imperfect information games. They conduct experiments using Leduc hold’em and show that the proposed approach gives significantly better results than existing CFR based algorithms. The downside of the proposed algorithm is that it requires a memory of O(|H|) for bookkeeping, which can be very large and makes it difficult for the algorithm to be applied to large games.
Reject. rating score: 1. rating score: 6. rating score: 6. <BRK>Paper Summary The paper considers a new take on active learning for image classification: given a large fully labeled dataset, identify a subset of the data that, when training on that subset alone, yields similar performance as training on the (much larger) full dataset. This subset is then used to train a "subset" model (again, an ensemble of DNNs).<BRK>Paper Summary:This paper proposes a new method that uses uncertainty estimation to do ensemble active learning on image classification tasks. The proposed method mainly consists of two steps: 1. select a subset of data based on the uncertainty estimation from an ensemble model. Experiments are comprehensive and convincing in terms of the performance.<BRK>This work makes use of uncertainty estimation methods from active learning to select a subset of training data that produces models with similar (or better) performance compared to models trained on the full training set. The first "are" in this sentence is unnecessary.
Reject. rating score: 3. rating score: 3. rating score: 6. <BRK>It shows standard feature normalization methods in supervised learning is indeed not effective for RL settings, due to the fact that a and a’ are from very different distributions with different dynamics. This paper proposes a normalization method, by merging a and a’ into a * single* update step to the batch normalization layer. I would like to first point out the pros of this paper from my perspective then explain my main concerns point by point. Given that the empirical result is pretty much the only support of the claim in this paper, the lack of more diverse experiments would weaken the contribution. It needs not to be a weakness for the algorithm itself as we appreciate simple but effective algorithm. However this makes the problem itself more like a design weakness of BatchNorm and a simple patch to fix it. I doubt how much algorithmic insight this paper could contribute, to inspire related research. In general, I’d like to see a more clear analysis about the dilemma of BatchNorm in off policy data, and why the two simple ways won’t work.<BRK>The paper introduces a new normalization scheme, cross normalization, that stabilizes the off policy reinforcement learning algorithm. The results show that by simply performing batch normalization, where the mean and variance statistics are computed with both behavior and target action samples, it can increase the performance of DDPG and TD3 algorithm consistently. The results are surprisingly good when the simplicity of the algorithm is considered. Also, the paper does not seem to be a complete draft   there are many points that seem to be incomplete. Here are some problems with the paper I found:1. In the introduction, the paper says that the paper investigates convergence: where is the convergence investigation? 4.While it is claimed in the paper that TD3 + CrossRenorm (alpha 0.99) performs well, it is not really justified why (alpha 0.99) is crucial. The paper also lacks experiments about BatchRenorms on DDPG and TD3, which would be a fair comparison against CrossRenorm. 6.Below eq (2), the paper says about big \Phi, but it is never defined and not used anymore. Why do we need variance normalization then?<BRK>The paper first shows the effects of using existing normalization methods (batch and layer normalization) in the context of OPTD methods. Those approaches are shown to be inferior to target networks, because the data (actions in off policy transitions and on policy transitions) is coming from two different distributions. To tackle this problem, the authors introduce a cross normalization scheme that works across the two datasets in this context. The weight of the contribution of those distributions is handled by a hyperparameter, which defaults to 0.5; thus using a balanced influence on both distributions (CrossNorm). In further experiments, it is shown that CrossNorm stabilizes learning in most contexts, but does not guarantee to converge in all settings. Furthermore, empirical results are not only shown, but also analysed.
Reject. rating score: 1. rating score: 6. rating score: 6. rating score: 6. <BRK>This paper sets up a couple discrete communication games in which agents must communicate a discrete message in order to solve a classification task. The paper claims that networks trained to perform this case show a tendency to use as little information as necessary to solve the task. With the experiments being as small scale as they were, why not explicitly marginalize out the message (as I did in the notebook)? The primary question the paper addresses is an interesting one. In particular there was one thing I was worried about upon reading the paper again, and is similar to the point raised by the other reviewers. In Figure 1, we are shown the entropy only of those networks that have succeeded. The claim of the paper is that under normal training procedures it seems like we don t find those solutions and instead seem to find minimal ones. The reason the solutions look minimal in Figure 1 is probably because the initialization chosen for the encoder they used in the paper tended to start at low entropies.<BRK>This paper investigates the phenomenon of entropy minimization in emergent languages. Given the above points, I’d say the paper is borderline it its current form, with a tendency towards rejection. The authors describe this by saying: “With a low temperature (more closely approximating a discrete channel), this is hard, due to stronger entropy minimization pressure.” But this seems misleading to me, since it’s very different from the ‘entropy minimization pressure’ that was discovered in the first result (which comes about when both agents are able to solve the task, but the listener has redundant information).<BRK>why are there different numbers of seeds for each experiment? The paper is clearly written, the tasks are simple and minimalist (in a good way) and the experiments are often followed by the exact ablation I was just noting that I wanted to see. The authors present a series of simple experiments to characterize the objective that emergent languages are optimizing and why we see various behaviors (not aligned with natural language) when training them to play these language games. Should I assume one per seed?<BRK>The paper studies whether discrete communication channels between agents are low entropy. Does the paper support the claims? The entropy of the messages was only analyzed for the runs where agents have successfully learned to communicate in order to solve the task. Buf if that s the case, then there s a controllable parameter that implicitly controls an entropy constraint and it s no longer clear to me that low entropy is emerging. In particular, I have some doubts about the claim that entropy regularization is unnecessary.
Reject. rating score: 3. rating score: 3. rating score: 3. <BRK>I m a bit unsure of how to evaluate this paper. However, this work, as far as I can tell, is separate from the MARL problem. On the other hand, there is the application of the hierarchical setup to the MARL problem. Why does partial parameter sharing make it easier for each agent to estimate other agents potential changes in behavior? I do apologize if my original comment wasn t clear regarding the contribution part of the paper. What I was trying to say is not that I didn t see the individual contributions of the paper, but instead that the paper does multiple things simultaneously, without comparing against the relevant baselines for any of the individual contributions. However, what was not clear to me is how this reduces the non stationarity of MARL. I feel like in the paragraph in questions, a lot of causes and effects are mixed up and more careful descriptions of the benefits of the algorithm would help.<BRK>The submission proposes a method for hierarchical RL in multiagent settings. The model is trained via PPO with GAE and evaluated on a small set of multi agent locomotion tasks. To evaluate the impact of the proposed changes in this paper, one would have to perform extended evaluations and ablations for the submission. Regarding the challenges (and focus on learning simple tasks), reference [3] might be of interest to the authors. ‘Our proposed approach represents the first physics based simulation of its kind that supports MARL.’ This sentence remains unclear as the authors do not propose a simulation engine. Text on experiment figures is much too small.<BRK>This paper proposes a multi agent hierarchical reinforcement learning algorithm so that multiple humanoid robots can navigate in multi agent settings (e.g.avoid collisions, collaboration, chase and escape) in a physically simulated environment. The key difference of this paper with the prior work on MARL is that it used an accurate physics simulation of humanoid robots. This is the main reason of using the hierarchical RL. The results look appealing, too. First, the technical contribution is lean. I think that these are important details and may also be the contributions of this paper. Most of these should be moved to the main text. Here are some more suggestions on writing:1) Certain paragraphs in the main text can be significantly shortened, such as the reward shaping in Section 5.2.
Reject. rating score: 3. rating score: 3. rating score: 6. <BRK>Conventionally, GCNs use information from all the neighbors up to a certain depth; in which case, with consideration of each further hop, the neighborhood size increases exponentially. In Eqn: 2, I believe you are providing an equation for GS GCN. The sampling of nodes at a layer, ‘l’ is based on the transmission probabilities of the nodes at layer ‘l’ and their immediate neighbors sampled earlier in layer ‘l+1’ from both directions of diffusion. Weaknesses of this paper:  Novelty: The idea is incremental. Experimental results:       (a) Inconsistent baseline results: The performance of baselines reported here on standard train/test/val splits are significantly lower than the ones reported in the original papers. This would be a fair comparison to FastGCN and AS GCN. Writing:      The paper is not well written. (c) why sub graph methods are not effective ? You can see that by simply removing the nonlinearity+weights and recursively expanding the GCN equation.<BRK>The authors propose a sampling method for graph neural networks which is applicable to very large graphs (where not all nodes can be kept in memory at the same time). The method uses the transition probabilities of a random walk to construct a sampling probability of the nodes in the lower layer given the nodes in the upper layer. Since this samples nodes which can be one or multiple hops away, an attention mechanism is used to weight updates from connected nodes. Experiments show that this method is promising. In its current state I would be inclined to reject this paper, but I could be convinced otherwise. Similarly, results are badly presented. For example, table 3 should probably be given as relative speedups with the best results bold faced rather than a long table with numbers.<BRK>This paper was an interesting read. The idea of this paper is to challenge the use of Laplacian matrix in GCN. In particular, this typically leads in Euclidean case to learning isotropic filters (because the euclidean Laplacian is isotropic). (in the Euclidean case, that could correspond to the selectivity to orientations   no selectivity would lead to a difference of Gaussians) Furthermore, for non sparse graphs, computing the iterations of the Laplacian matrix can require a significant computational power. At a given layer, the diffusion factors is based on the interaction with other layers of the GCN. Then, a layer wise attention mechanism that will allow to weight the graph connectivity of the sampled nodes is used, which is supervisedly learned. The paper is clearly written, the numerical experiments are convincing and the authors address a difficult problem with a simple method: I m leaning toward an "Accept". Post discussion:The other reviewers have made some good point, and thus I decided to lower my score. I still find the paper address an interesting problem.
Reject. rating score: 1. rating score: 1. rating score: 1. <BRK>The lack of internet infrastructure results in farmers in rural areas using community radio to share concerns related to agriculture. Training on the full corpus would allow the network to see more training data and consequently might result in more accurate models. This evaluation strategy roughly simulates the streaming conditions in which this model is intended to be deployed.<BRK>This paper tries to design an effective and economical model which spots keywords about pests and disease from community radio data in Luganda and English.<BRK>The paper describes an approach to analyze radio data with ML based speech keyword techniques. They only demonstrated results in 1 dataset, which did not indicate the effectiveness of the model. Therefore, we highly recommend rejecting the paper. The paper lacks complete discussion on their empirical results.
Reject. rating score: 3. rating score: 3. rating score: 8. <BRK>In terms of novelty, the proposed approach is a combination of several past works so the technical novelty is limited.<BRK>The idea of the paper is to learn a distance function between observed and the agent’s behaviors.<BRK>The clarity of the technical presentation could be improved, however.
Reject. rating score: 3. rating score: 3. rating score: 6. <BRK>This paper studies the accuracy vs model size trade off of quantized CNNs under different channel width multipliers. These weakness makes it a borderline paper. StrengthThe paper is well written and motivated.<BRK>This paper investigated the Pareto efficiency of quantized convolutional neural networks (CNNs). The authors pointed out that  when the number of channels of a network are changed in an iso model size scenario, the lower precision value can Pareto dominate the higher ones for CNNs with standard convolutions.<BRK>The author studies the quantization strategy of CNNs in terms of Pareto Efficiency. Through a series of experiments with three standard CNN models (ResNet, VGG11, MobileNetV2), the authors demonstrated that lower precision value can be better than high precision values in term of Pareto efficiency under the iso model size scenario.
Reject. rating score: 1. rating score: 3. rating score: 6. <BRK>The second contribution of learning a sampler given a density estimate is interesting but likely suffers from all the instabilities of GAN training, and does not compare to related work on distilling energy based models. My major concern with this paper is that the “denoising density estimator” proposed here is identical to denoising score matching (which is not cited or discussed). There are no comparisons to any GAN based approaches anywhere in the paper.<BRK>The derivation in Section 3 is nice and clear, and the derivation gives a nice alternative of using Gaussian noise for score estimation. In Section 4, how is the proposition 2 applied to the sampling, and what is the situation that the condition in Eq.(12) is satisfied? Any of these relationships are not explained well, and unfortunately it is hard to capture the contribution of this paper though with some interesting properties.<BRK>The very fact that the estimator is trying to imitate a kernel density estimator implies that the assumptions made on the underlying data are that the data comes from a smooth distribution. This paper provides a method for density estimation in high dimensional domains. I like the ideas of the paper and the exposition in this paper.
Reject. rating score: 3. rating score: 3. rating score: 3. <BRK>This paper propose a heuristic algorithm for deciding which random variables to be Gaussianized early in flow based generative models. However, for a multi layer flow model. However, the proposed algorithm is still more expensive than a fixed multi scale architecture, right? Update: thank for the authors for their significant effort on revising this paper.<BRK>The strategy relies on a heuristic that employs the availability of per dimension log determinant terms (e.g.as affine couplings) to decide which half of the dimensions to factor out. If not, this set of experiments should be performed. However, it depends on the availability of per dimension log determinants. b) In Section 3 “Nevertheless, such an analogy can be extended for other flow models which involve a multiscale architecture [...]”. Section 3 “In a multi scale architecture, it is apparent that the variables getting exposed to more layers of flow will be more expressive in nature [...]”. 4.In Section 4 the authors imply that the contribution of a dimension towards the total likelihood is dominated by the log determinant (“[...] as the contribution by the variable (dimension) towards the total log det (~ total log likelihood)”).<BRK>This paper presents a new multi scale architecture for flow based generative models. The features are chosen for further processing based on a heuristic motivated by each feature s contribution to the total likelihood. If the authors had used their method to improve upon the SOTA then their experiments would have been considerably more convincing.
Reject. rating score: 1. rating score: 1. rating score: 3. <BRK>This submission proposes to consider to put attention on "phrases" in NLP. I don t know how much the previous argument is still valid. In each "layer" of the proposed phrase transformer, it has actually two self attention layers, but the baseline has only one self attention layer. It only compares to transformer and semantic phrase transformer.<BRK>The architecture part is complicated to follow and I don t understand the big contribution of this paper. In each "layer" of the proposed phrase transformer, it has actually two self attention layers, but the baseline has only one self attention layer.<BRK>The motivation of the paper is very clear, and I love this kind of paper; with a simple idea, making a huge impact on the field.
Reject. rating score: 1. rating score: 3. rating score: 6. <BRK>The paper proposes an unsupervised feature selection method by minimizing reconstruction error with restricted autoencoders. 1) The novelty of the paper is very limited: using reconstruction for unsupervised feature selection has been explore in many papers, such as [1][2][3]. 3) Experimental results are not convincing at all due to the following reasons:  a) Only one baseline (out of 9) was proposed after 2011. There has been much progress on unsupervised feature selection and several hundred papers in this area have been published in the past 5 years.<BRK>The fundamental of this paper is built on the argument that the optimal approach is to select a set of features that can accurately reconstruct all the remaining features for the settings where they will be used in downstream prediction tasks. Some concerns are listed as follows:1. the theorem based on strong assumptions that all learned models are optimal. It is just a special case study. Please clarify it in details. This is different from RFE where a single SVM optimization problem is used and the ranking score is solely based on the learned SVM classifier. In the experiments, authors did not mention the parameter settings of all compared methods. It is known that the unsupervised feature selection methods incorporate priors with usually various parameters.<BRK>The paper is concerned with unsupervised feature selection, in a lossy compression perspective. The idea is good, but the the same idea (same reconstruction goal, also relying on auto encoders) was published this year, see here: https://ecmlpkdd2019.org/programme/awards/Algorithmically speaking, the approach is very close to the above paper; as far as I can tell, the main difference lies in the recursive feature elimination heuristics. I feel that the originality of the paper is thus severely undermined. The analysis definitely is a good point of the paper; however, parts of it are straightforward (Thm1).
Accept (Poster). rating score: 8. rating score: 6. rating score: 6. <BRK>I think this paper should be accepted, because it presents a novel and non trivial concept (rotation equivariant self attention). [Original review]The authors propose a self attention mechanism for rotation equivariant neural nets. They show that introduction of this attention mechanisms improves classification performance over regular rotation equivariant nets on a fully rotational dataset (rotated MNIST) and a regular non rotational dataset (CIFAR 10). If the authors can address these concerns, I am willing to increase my score.<BRK>Applying this approach to several different models on rotated MNIST and CIFAR 10 lead to smaller test errors in all cases. The paper is also relatively long, going onto the 10th page. It includes an extensive and detailed bibliography of relevant work. It can be applied to additional transformations beyond rotation and mirroring.<BRK>[Post rebuttal update]Having read the rebuttals and seen the new draft, the authors have answered a lot of my concerns. The basic premise is to use a group equivariant CNN of, say, Cohen and Welling (2016), and use self attention on top. That would resolve some of my confusion with the maths. The paper is well structured. In the co occurence envelope hypothesis, for instance, what does it mean for a learned feature representation to be “optimal in the set of transformations that co occur”. I found the section “Identifying the co occurence envelope” very confusing. of visual features from different parts of the face, independent of global rotation?
Reject. rating score: 3. rating score: 3. rating score: 6. <BRK>This paper provides an empirical evaluation of the privacy implications of releasing updated versions of language models. I would also note that the motivation, a predictive keyboard, is not a situation in which maximizing accuracy is generally desirable: users tend to find this creepy rather than helpful. The methodology is sound, but the synthetic experiments around which much of the paper is based may not be sufficiently novel and give little indication of broader implications.<BRK>This paper studies the privacy issue of widely used neural language models in the current literature. The updating setting considered in this paper is kind of interesting. 5.What is the perplexity of the trained models? 8.$\epsilon 111$ seems that the model will provide no privacy guarantee according to the definition of differential privacy?<BRK>Overall I think this work is interesting and I would encourage the authors to try and add as much quantitative evaluation as possible, but also try and include qualitative information regarding specific sequences after prodding the models. Those could go a long way in strengthening the paper. This is an important problem and I think this paper does a good job investigating this in the context of language modeling. It discusses the privacy concerns thoroughly and look at language modeling as a representative task.
Reject. rating score: 3. rating score: 3. rating score: 3. rating score: 6. <BRK>The paper proposes a model building off of the generative query network model that takes in as input multiple images, builds a model of the 3D scene, and renders it. This can be trained end to end. Summary of negatives:  The method is quite complex and explained, in my view poorly (although I m open to the other reviewers  opinion on the matter). In more detail:Method:  I found the method section quite difficult to read, in part because the method is quite complex. However, I think even if I m just the slow one, the authors should think about writing this more clearly and using consistent notation and function names. But when there are no ablations, it s unclear what parts of the changes are important and which parts aren t.(c) It s not clear whether the GQN and ROOTS are being trained fairly   do they have the same capacity? The object detection quality experiment is incomplete   I just do not know how to parse the numbers that are presented without some sort of simple baseline in order to make sense of things. The qualitative experiments are nice but would be substantially improved by showing that:(a) that GQN doesn t do any of these (b) that ROOTs can train on 2 3 objects and test on 3 5 objects by simply changing the prior on K   this is one of the primary advantages of object centric representations of scenes (the ability to handle arbitrary numbers of objects). Two gratuitous examples: 1) The title is "Object oriented representation of 3D scenes", which covers decades of work in robotics and vision. Is the manuscript and the Papon paper the same at all? The claim that the method is unsupervised when it has access to precise camera poses seems a bit like a stretch to me. This deserves some further thought. Table 1   table captions go on top Post rebuttal Update: AC: I would give a rating of 5 if the full revision is considered acceptable (since the paper is more clear), and increase to 4 if it is not (since there are some more experiments, although I think they are still quite weak). I m still inclined to reject the paper on the grounds of experimental comparisons and the open question of whether  but recognize that my concerns are ones which may not be shared by all communities (and that this is not my community). Indeed: the experiments in the revision show that ROOTS often does *worse* in generalization performance to previously unseen objects (improving in only 6/9). I appreciate the author s response that there s a latent variable of # of objects that needs to be adjusted in the case of ROOTs, but this should be investigated. The same thing goes with the claim that an ablation study is only necessary for improving results. This is just baffling   is it possible that only certain parts of the method are necessary? Surely this is a problem that is worth studying. The ICLR 2020 guide is unclear how you should treat this (the AC guide says "you can ignore this revision if it is substantially different from the original version."). f_{3D 2D} There are multiple camera models.<BRK>TLDR: Interesting idea that seems promising, but lacks the maturity required to pass the ICLR bar: Lacks proper citations, comparisons with the latest works, no ablation study of their contributions. The paper presents an extension of the Generative Query Network to incorporates 1) 3D grid for the state representations 2) hierarchical representation and 3) unsupervised model for explicit object representation (which is tied to 1.The unsupervised representation is interesting, but this is a minor contribution on top of GCN and 3D representations have been widely studied in the vision community. This leads to the question of why the authors did not perform ablation studies on each component. Finally, it seems that the authors did not add proper citations. First, the 3D representation has been studied widely in the vision community and 3D R2N2, ECCV 16 proposed using an RNN to encode a series of 2D images to learn 3D grid representation which seems quite similar to what the authors are proposing as an encoder and representation. Secondly, there are numerous methods on 3D neural rendering such as DeepVoxels, CVPR19 and all of the baselines in their experiments. The paper seems to completely ignore the works in this field. Is the y in Eq.3 the same y defined in the preliminary? There are almost two pages between the definition and Eq.3. Minor:Why learn the camera projection?<BRK>The paper presents a framework for 3D representation learning from images of 2D scenes. The proposed architecture, which the authors call ROOTS (Representation of Object Oriented Three dimension Scenes), is based on the CGQN (Consistent Generative Query Networks) network. The representation is 1. factorized to differentiate objects and background and 2. hierarchical to first have a view point invariant 3D representation and then a view point dependent 2D representation. MuJoCo: A physics engine for model basedcontrol. +Learning 3D representations from 2D images is an important problem. While the paper takes a step towards a potentially impactful work, I cannot recommend it for publication in its current form. 1.There are claims in the paper that are not supported by the experiments. For example, “As seen in Figure 2, ROOTS has clearer generations than GQN. ” However, Figure 2 does not show this at all. It shows no difference between ROOTS and GQN. 2.The paper can benefit from further clarity throughout—in general it seems a bit rushed. Its also unclear what is depicted in each of the columns in Figure 3. This should be clearly explained. 3.I suggest clarifying Figure 1 further and referring to it in section 3. Table 2: Why not compare to CGQN? Minor There are typos throughout the text.<BRK>* The GQN, as a conditional → is a conditional* Target observations (in section 2) does not need a capital. Unfortunately, the paper is very poorly written. If the authors can address the issues below and improve the quality of the writing, I would recommend that this paper be accepted. However, in its current state, I recommend that this paper be rejected. The writing should still be improved further and suggest that the authors fully revise the paper before the camera ready version, if the paper is accepted. There is MONet and Iodine that can identify objects without supervision in 2D projections of 3D scenes. This claim should be revised. The authors are using non standard GQN notation and their notion is not consistent. The authors should make their notation consistent or use the standard GQN notation. Section two would not make sense to people that are not already familiar with GQN. The scene volume map is interesting, the inductive bias preventing two objects being present in the same location is interesting, however one could imagine a case where uncertainty in the model could lead to two objects being represented in the same cell. Could this be related to bias in your data? Also, what is the object invariant object encoder? The results are very impressive: being able to swap in and out objects in a scene, showing the 2D renderings of single objects from different view points and the scene decompositions and predicting where the “missing” object is (Figure 6). Minor:The introduction could be strengthened with additional references. There are claims that object wise factorisation will help with transfer, it would be good to have references to other work that supports this view. Also the claim that humans have 3D representations for objects requires a reference.
Reject. rating score: 3. rating score: 3. rating score: 3. <BRK>This paper argues that active learning (AL) methods shold combine unsupervised and semi supervised learning during the iterative training process. I think the characterization of AL is not quite right on page 2. I would not characterize the gains brought by unlabeled data here as "spectacular". Some specific comments and questions:  The authors have decided to frame this paper in terms of improving AL using un/semi supervised learning.<BRK>This paper explores the setting where unsupervised/semi supervised learning is combined with active learning. This paper is interesting in that it provides additional experiments for the intersection of active learning and unsupervised/semi supervised learning. The paper does not claim to provide anything new algorithmically (other than jLP which appears to work no better than random and isn t really advertised as the point of this paper).<BRK>In my mind, this paper has two primary components: (1) taking the position that semi supervised and unsupervised learning can improve overall performance and, in principle, help with active learning and (2) propose jLP, which is a learning algorithm agnostic approach to spanning the manifold space. Thus, the main result is the first point — updating previous (pre deep learning) results on SS/US AL to deep learning. The second result is that active learning in deep learning (at least for this application) hasn’t kept up. If the only claim is pre training or pre clustering, people certainly do this — just often not as a point of emphasis. — I don’t understand the ensemble model analogy in the abstract; is it because it is a ‘meta algorithm’?
Reject. rating score: 3. rating score: 3. rating score: 6. <BRK>Both models are connected by a patch extraction routine. The main contribution of this paper is to provide a way to propagate errors through this non differentiable patch extraction scheme. However, it suffers from the fact that it does not differentiate between model architecture and the overall approach.<BRK>This paper proposes a method for multi instance object classification and reconstruction that does not require any location based supervision. The results on SVHN are a little bit confusing, and it’s unclear what the “Supervised” method is, specially knowing that there are available methods that do obtain much better performance on this task using all the supervision available.<BRK>* All patches need to be squared   again, for real world tasks this is not the case. * In the case of image reconstruction, what is the  reconstruction error  exactly?
Reject. rating score: 3. rating score: 3. rating score: 8. <BRK>There are 4 major issues I have found so far. I have to say this may not be enough for ICLR that should be a more deep learning conference. However, the obtained loss is very similar to the general loss correction approach, see https://arxiv.org/abs/1609.03683 (CVPR 2017 oral). This fact undermines the novelty of the paper, significantly.<BRK>There are also methods proposed for this. It seems the theories of the paper depends on a very strong assumption, i.e., "Suppose S(.) Overall, this is an interesting paper but needs to be improved. The paper is not well presented.<BRK>First, a scoring function is introduced, minimizing which we can elicit the Bayes optimal classifier f*. The calibration and generalization abilities are also discussed in section 4.3. Nevertheless, Some parts of this paper may be confusing:  The computation of the scoring matrix delta is not that clear. Can the authors provide the detailed computation steps of the example? I would be appreciated if the authors provide the sensitivity experiments of alpha to show its fluence for the final prediction.
Accept (Talk). rating score: 8. rating score: 8. rating score: 6. <BRK>These experiments highlight new findings about the role the rank of the Q matrix plays in planning convergence and learning rates. Highlighting the role of this rank and the corresponding empirical analysis estimating it in benchmark RL and control tasks is, to my knowledge, novel. Presentation of results is rigorous, too, and provide strong evidence that the method works. I take this analysis to be out of scope for this paper, but could see the work motivating future investigation into these questions. I recommend accepting the paper. Comments:	C1: The visuals throughout the paper are helpful! Consider changing: "...why not enforcing such a structure throughout the iterations? It is not strictly necessary, but I could see multiple uses of "rank" appearing in the RL literature as a means of exploiting structure for faster learning being confusing. Additionally, the study of sparsity in value function representation was studied by Calandriello et al.2014.If space permits, the paper might benefit from some discussion of the relation to this work.<BRK>The study is motivated by the observation that the Q value matrix in reinforcement learning problems often has a low rank structure. The Q value matrices and functions considered in the problem have a special structure as they result from Markov Decision Processes.<BRK>They also provide a deep RL extension   SV RL, that can be applied to value based methods. Overall this paper presents an interesting idea, that also scales to the deep RL algorithms. Given these clarifications in the author s response, I would be willing to increase the score. The authors are proposing a form of regularization that enforces low rank structure for the value function (and target Q values in particular for deep RL agents). It will be useful to have some analysis on how far it deviates from the optimal value function.
Reject. rating score: 3. rating score: 3. rating score: 3. <BRK>This work analyzes the consistency regularization in semi supervised semantic segmentation. Based on the results on a toy dataset, this work proposes a novel regularization for semi supervised semantic segmentation, which is named CowMix. Cons:The writing is not clear. It is a little hard for me to fully understand Figure 2.<BRK>This paper provided first provided analysis for the problem of semantic segmentation. The paper also illustrated how to perturb the training examples so that consistency regularization still works for semantic segmentation. The analysis part seems interesting and innovative to me. This should be the most important contribution of the paper. The writing of the paper is very clear and easy to follow.<BRK># SummaryThis paper proposes a method for semi supervised semantic segmentation. The authors argue that the cluster assumption (to which effectiveness of consistency regularization has been partially attributed) does not hold in semantic segmentation.
Accept (Poster). rating score: 8. rating score: 8. rating score: 3. <BRK>WeaknessWhile the paper is reasonably readable, there is certainly room for improvements in the clarity of the paper. The feature smoothness indicates how much information can be gained by aggregating neighboring nodes while the label smoothness assesses the quality of this information. Another suggestion about the flow is to separate section 2.2 into two subsections for features smoothness and label smoothness (also the title of this section need to be refined).<BRK>The writing is mostly smooth, and the authors seem to provide enough detail of the experiments performed. The paper compares different techniques and shows that when neighboring labels are not smooth, techniques such as label propagation does not help.<BRK>Overall, the paper is well organized and clearly written. Given these aspects, it is not that convincing that the introduced smoothness metrics are necessary. I would like to recommend a weak reject for this paper.
Reject. rating score: 1. rating score: 3. rating score: 3. <BRK>My objection is not that this work was started a long time ago, but that it has not been updated in years. The main reason is the fact that the paper is outdated by two/three years.<BRK>Further, for the word embedding model, the notation is over complicated. For the deGAN, only generating bag of words may not be that attracting. The final output is just a combination of the shared components and the unique features. Please fix the formatting issues. I think here should be (6).<BRK>Strengths+ The paper clearly mentions all the experimental details+ The paper has a nice set of qualitative examples that probe what the proposed model is learningWeaknesses* It is not clear what problem the paper is trying to solve.
Reject. rating score: 1. rating score: 1. rating score: 6. <BRK>Summary This paper proposes a framework for privacy preserving training of neural networks, by leveraging trusted execution environments and untrusted GPU accelerators. This is a valuable and hard to reach goal. This corresponds to the so called "honest but curious" threat model which often appears in the MPC literature, and this should be acknowledged and motivated. The experimental setup is quite confusing. Why not run all experiments on the same setup?<BRK>The paper proposes a method for privacy preserving training and evaluation of DNNs. As a byproduct, there is a confusing "scaling factor" described by the authors that is applied to the timings. A brief mention is made of the fact that the proposed system does not in fact provide correctness guarantees (unlike CaffeSCONE), but this is dismissed by reference to utilizing the same trick used by Slalom.<BRK>The paper builds a privacy preserving training framework within a Trusted Execution Environment (TEE) such as Intel SGX. There appear to be typos and grammatical errors at many places in the paper. Here, it s not clear which mechanism is inefficient. Overall a strong contribution with supporting experimental results, but the certain parts need further explanation or rewriting for higher rating. This isn t clear from the experiments section.
Reject. rating score: 1. rating score: 3. rating score: 6. <BRK>(c) Can authors please explain why they have used ResNet 110 on CIFAR100 and ResNet 50 on ImageNet? Pros:(+): The paper is well motivated. Unfortunately the analysis section (6) acts very poorly in providing a thorough exploration into their method. (e) Title for Section 5 as the main contribution of this paper should be changed to reflect that.<BRK>On the other hand, if I don t misunderstand, ECE did not omit a lot of predictions according to the top right figure if there is no label noise.<BRK>After an interesting review of calibration methods, the paper describes two new methods for assessing calibration. The paper is interesting and relatively well written. (1) OK, the ECE omits a lot of important predictions (e.g.fig 1)  > give a real application where this matters(2) OK, adaptive binning seems a sensible approach.
Accept (Spotlight). rating score: 8. rating score: 6. rating score: 6. rating score: 3. <BRK>This is an interesting paper that uses two techniques from neuroscience (eletrophysiology) to interpret CNNs. The paper is well written and the experiments are thorough.<BRK>With detailed and complete psychophysics experiments, several interesting properties of the filters and the biases of the neural network classifiers are revealed. Strengths:The paper is well written and easy to follow.<BRK>The broad goal of the paper is to add more tooling to add interpretability and robustness to a neural network. The paper is well written.<BRK>One interesting aspect is that the classification image technique isquite good (better than the average gradient of the classificationloss with respect to the image) at revealing trojan (adversarialpatch) networks   networks trained with trojan patches that change theclassified class of the image   (and the area of the trojan attack).
Accept (Talk). rating score: 8. rating score: 8. rating score: 6. <BRK>The paper develops a cyclical stepsize schedule for choosing stepsize for Langevin dynamics. Many experimental results, including ImageNet, are given to demonstrate the effectiveness of the proposed method. Here I suggest that authors also need to point out that the continuous time MCMC is the Wasserstein gradient flow of KL divergence. The bound derived in this paper focus on the step size choice of gradient flows. This could be a good direction for combining gradient flows studies in optimal transport and MCMC convergence bound for the choice of step size.<BRK>The key idea is to not keep lowering the step sizes, but   at pre specified times   go back to large step sizes. My key concern is that with MCMC sampling it is often quite difficult to tune parameters, and by introducing more parameters to tune when step sizes should increase, I fear that we end up in a "tuning nightmare". The authors does a comparison to this and show that their approach is significantly faster due to "warm restarts". It is argued that the cyclic nature of the algorithms gives a form of "warm start" that is beneficial for MCMC. My intuition dictate that this is only true of the modes of the posterior are reasonable close to each other; otherwise I do not see how this warm starting is helpful. This would improve readability (which is generally very good). I expect that the Bayesian nets still are subpar to non Bayesian methods, and I think the paper should report this.<BRK>This article presents cyclical stochastic gradient MCMC for Bayesian deep learning for inference in posterior distributions of network weights of Bayesian NNs. The posteriors of Bayesian NN weights are highly multi modal and present difficulty for standard stochastic gradient MCMC methods. The proposed cyclical version periodically warm start the SG MCMC process such that it can explore the multimodal space more efficiently. The proposed method as well as the empirical results intuitively make sense. The standard SG MCMC basically has one longer stepsize schedule and is exploring the weight space more patiently, but only converges to one local mode. The cyclical SG MCMC uses multiple shorter stepsize schedules, so each one is similar to a (stochastic) greedy search.
Accept (Poster). rating score: 6. rating score: 6. rating score: 6. rating score: 3. <BRK>This paper deals with turning a 2.5D video representation into a 3D representation of an environment or a scene. They then assess their approach on numerous tasks such as 3D object detection, 3D moving object detection, and 3D motion estimation. The authors also evaluate the transferability of the features in a challenging sim2real setting. I am pretty convinced with the experiments, especially Sim2Real, in Tab1, where the baselines are clears and make sense. In visual CPC papers [1] (or since the early days of visual representation learning!), data transformation has been applied to improve model performance. First of all, the machine learning novelties are rather small, contrastive losses are now widespread, and the models are closed to Tung et al.as mentioned by the authors. Having said that, the proposed approach is pretty generic, can be applied to RGB D (more common in ML), and require few expert knowledge in vision (only the Egomotion module).<BRK>The paper proposes a view prediction inverse graphics model that takes input RGBD streams and produces a 3D feature map of the scene, including regions that are unobserved due to occlusion. This model is used to learn a 3D visual representation that can be applied for semi supervised 3D object detection, and for unsupervised 3D moving object detection. Moreover, the proposed model is evaluated on the downstream 3D object detection tasks to demonstrate the utility of the learned 3D visual representation. Experiments are performed to evaluate the proposed method against baselines on 3D object detection (both in the semi supervised setting and unsupervised moving object detection), 3D motion estimation, as well as sim to real transfer results (training in CARLA and testing on the KITTI dataset). I am positive with respect to accepting this work, but find that there are a few unclear points in the evaluation that should be clarified to strengthen the empirical results. It would have been informative to provide some statistics about observed vs occluded object surface area to elucidate the dataset construction. This aspect of the dataset likely impacts the performance of the method significantly and should thus be addressed a bit more clearly.<BRK>This paper studies the problem of visual representation learning from 2.5D video streams by exploring the 2D 3D geometry structures in the 3D visual world. Compared to previous work (Tung et al.2019), view contrastive inverse graphics networks decode in the feature space rather than RGB space. Results demonstrate the strengths of the proposed view contrastive framework in feature learning, 3D moving object detection, and 3D motion estimation. Overall, this paper studies an important problem in computer vision with a novel solution using unsupervised feature learning. While the benefit of unsupervised feature learning has been demonstrated, it would be more convincing to compare against the following papers (at least with a paragraph of discussion). Also, because the view contrastive loss is applied at feature level, reviewer would like to know performance on detecting small objects.<BRK>This paper show that view prediction learning to help 3D detection. In general, the whole model is quite straightforward, and quite heavy, while most components come from existing papers. It’s really a good engineering work in term of integrating them together. I have several comments. how’s the proposed model different from [1], and [2]? (2) The model is still built upon Tung et al.(2019)  with several novel components, including handling more general camera motion beyond the a 2 degree of freedom sphere locked camera. Particularly, how’s the performance of the model variant without using this component (just 2 degree of freedom sphere locked camera) ? (3) In  Fig.5, it is very interesting that, the results of  Estim. What dataset are used as the labeled, and unlabelled images? (5) Any chance to give some evaluation about the significance of components introduced in the model? NeurPIS 2019
Reject. rating score: 3. rating score: 3. rating score: 6. <BRK>In this paper, the authors study the online knowledge distillation problem and propose a method called AFD (Online Adversarial Feature map Distillation), which aims to transfers the knowledge of intermediate feature map (first propose) using adversarial training. Ablation study on CIFAR100 shows that the adversarial training in AFD can improve the accuracy significantly, while the direct method such as using L1 distance is worse. The comparison experiments with several online distillation methods also show the effectiveness of proposed method. (ii) The details of the experiments such as parameter configurations are missing, which makes the results not easy to be reproduced.<BRK>Instead of direct feature map alignment, the algorithm tries to transfer the distribution of the feature maps. The idea is understandable but some issues remain:1 	Training GAN is by itself an expensive task and optimization is difficult, so how computationally expensive is this online KD compared to the offline one? 2 	It is not clear form the paper feature maps from which layers are being used? 3 	What would be the performance difference compared to offline knowledge distillation?<BRK>Most approaches in this relatively new line of research rely on logit based KD for transferring knowledges between networks, and the paper demonstrates that by an additional feature map level KD the performance can be further improved. 3) Limited novelty and performance improvement  The main idea is already introduced in previous work on the task and the feature map level KD has been studied widely for various applications, their combination is somewhat new though. However, the submission is weak in terms of novelty and the manuscript should be polished carefully.
Reject. rating score: 1. rating score: 3. rating score: 6. <BRK>This paper presented a crowdsourced dataset for evaluating the semantic relatedness, similarity, and contextual similarity of source code identifiers. Cons:1) The proposed datasets are very small. Therefore, it is hard to fully evaluate the embeddings quality of various methods with high confidence. 2) The whole paper is mainly about the data collection as well as a few of evaluations of several existing code embedding techniques. It would be nice to put these efforts to have a competition of code embedding techniques and this paper could be served as a technical report on this direction.<BRK>This paper proposed a benchmark dataset for evaluating different embedding methods for identifiers (like variables, functions) in programs. Experiments are carried out on several word embedding methods, and it seems these methods didn’t get good enough correlation with human scores. However I have sevarl concerns:1) why the identifier embedding is important? As pre trained word embeddings are useful for many NLP downstream tasks, what is the scenario of identifier embedding usage? People have trained the language model with GPT2 (TabNine) that works well with code.<BRK>The labels determine how much the corresponding embeddings are useful for determining their meaning in context of code, a setting that has sufficient differences from a natural language. A significant part of the paper is devoted on data cleaning and relating the computed metrics to similar efforts in natural language processing. While there is not much novelty in this part of the paper, it is doing a good job at addressing many possible questions on the validity of their dataset.
Reject. rating score: 1. rating score: 3. <BRK>Even the structure of networks does not change at all. The authors use mutual information to analyze the network for its improved generalizing capabilities. I think the authors should change this explanation to a more convincing one. 2.The analysis by mutual information makes the paper hard to follow.<BRK>I m inclining to reject this paper given that the results on the main hypothesis (i.e.transferability of features) seem to provide only marginal improvement, and we have no idea about the repeatability of the results ( how many times did the authors run the experiments for figures 3,4,6,7,9?What s the spread of the results? The paper continues this analysis to the transfer setting, first to the same initial task, and then the a new task.
Reject. rating score: 3. rating score: 6. rating score: 6. <BRK>This paper presents a rotation equivariant model for use with point clouds. The method is evaluated on ModelNet40, obtaining much improved results in the case where rotation orientation is arbitrary. Unfortunately, I found too many important parts of the method difficult to understand, enumerated below, and am also not very clear on the details of how they fit together.<BRK>This paper proposes a quaternion equivariant network for 3d point clouds. It builds on the capsule networks and dynamic routing, but instead of using arbitrary 4x4 transforms, they propose to use quaternions to represent 3D rotations. Invariances and equivariances are very important in DNN for classification/detection, so extending these properties to 3d point clouds based applications is interesting and important. The proposed approach is novel and seems to be equivariant to SO(3) rotations, translations, and set permutations. The first is that the details of the architecture are very unclear. Or do you assume the same LRFs are known? The main selling point for equivarince and learning the separation of pose and object class representation is in the hope of more accurate classification.<BRK>This work builds a capsule network, for use with point cloud data, that has units that are equivariant to SO(3) 3D rotations. I found that the authors made the case for rotation equivariance well and I liked the analysis of the dynamic routing approach and its mapping to the Generalized Weiszfeld Iterations. Could the authors show empirical results that indicate that other representations for the equivariance do not work as well? My biggest concern about the paper is the presentation of the results. Are the same objects used in training and test but with different rotations?
Reject. rating score: 3. rating score: 3. rating score: 8. <BRK>The paper proposes an ensemble method for reinforcement learning in which the policy updates are modulated with a loss which encourages diversity among all experienced policies. It is a combination of SAC, normalizing flow policies, and an approach to diversity considered by Hong et al.(2018).The work seems rather incremental and the experiments have some methodological flaws. All the environments studied in the paper have action space [ 1, 1]^n which is a convex set. 6.I think the point about having only a single critic would deserve more discussion: what policy is this Q value of? I m still concerned about comparing to different codebases hence I m keeping my score as is.<BRK>To remedy this, the paper proposes a method for population based exploration. I have 2 issues with this paper:1. Lack of novelty – Most of the components of the proposed algorithm have been researched in other works. This is very misleading and non standard in RL. The number reported for ARAC is 816 which is the peak performance during training. drops to ~600 at end of training.<BRK>In this paper, the authors build a population of SAC NF agents with different parameters for the NF, and use an attraction repulsion approach to ensure diversity and performance of the population. Thus I m in favor of accepting it. In the most recent one, the value function approximator has been removed (see "Soft actor critic algorithms and applications"). Fig.4 is of much interest.
Accept (Poster). rating score: 6. rating score: 6. rating score: 6. <BRK>I found the idea to be elegant and well explained, and overall the paper is well written.<BRK>Also the results in the paper beat those of the GResNet on Cora, but not on PubMed and , which is a recent paper not cited by this work. The fingerprint is the vector of weights of all nodes in the local neighborhood.<BRK>Motivation:  I believe the paper is not well motivated from an applications perspective.
Reject. rating score: 1. rating score: 3. rating score: 3. <BRK>This paper proposes a model to embed states and action sequences into latent spaces in order to enable efficient estimation of empowerment in a reinforcement learning system. But overall, it is not ready to publish yet for the following reasons: 1. What is the detailed definition of empowerment, i.e., how to spell out the formula of mutual information I? How is the learned empowerment used in (training) policy? They indeed mention that ``a state is intrinsically safe for an agent when the agent has a high diversity of future states’’ and that ``the higher its empowerment value, the safer the agent is’’. The authors should really clarify this.<BRK>The idea is to map the raw action sequences and states to a latent space where learning would force that linear property to be appropriate. I like the general idea of the paper (as stated above) along with its objectives but I have several concerns. But note how this does not give a reward for policies, as such. In other words, is the encoder able to map the raw state and actions to a space where the linearity assumption  is correct, and thus where equation (3) is satisfied. I suppose the authors refer to something in particular but it was not clear to me what.<BRK>This paper proposes an approach based on empowerment for reinforcement learning applicable to the cases that the dynamical system is unknown. The model is estimated by a water filling algorithm and is evaluated on two RL tasks. The paper is weak in terms of writing and motivation. Moreover, the paper lacks motivation of the design choices. They are not really "AI benchmark problems". I d rather the paper focuses on its contribution: direct and concise.
Reject. rating score: 1. rating score: 3. rating score: 3. <BRK>The abstract does not read well. The connections between this work and MAML in section 3 is not clear to me. In eq 1, the denominator should be p(D_{t+1} | D_{1:t}). The paper talks about flat loss surface and sampling a small subset of the Hessian   I’m not sure I understand these connections.<BRK>The key advantage of the proposed approach is the low storage requirements. Existing works like Elastic Weight Consolidation (EWC) (Kirkpatrick et al., 2017) have proposed a Bayesian framework to lessen such forgetfulness by condensing the information of the previous tasks and supplying it as a prior for the new task.<BRK>The proposed method is not competitive with state of the art. Then they build a diagonal approximation to the Hessian.
Reject. rating score: 1. rating score: 1. rating score: 1. <BRK>I think the authors could make a more substantive argument why this would be the case in the introduction, but they do a nice job of situating their work in the context of the present literature. This should be addressed in the paper.<BRK>The paper seems to be have been written in a rush. 3.It is not clear to me what is the benefit of gr HMRNN.<BRK>(a), (b), (c). Clarity of the paper: The paper is clearly written. As for the results, even unregularized LSTM performs better than the baseline in this paper.
Reject. rating score: 6. rating score: 6. rating score: 6. <BRK>The authors propose a GAN architecture that aims to align the latent representations of the GAN with different interpretable degrees of freedom of the underlying data (e.g., size, pose).<BRK>This is much stronger than just consistency, which is the weakest part of the approach. The idea is to align points whose corresponding latent points are the same. A paper demonstrates a clear motivation and a working solution for the problem. A paper s idea is to train joint Wasserstein GAN for k datasets given in R^n (corresponding, e.g., to different classes of objects), together with manifold alignment of k manifolds.<BRK>It would be helpful to study their effect of different values on the training and encoding. The method utilizes a special regularizer to guide the training. Overall, this is an interesting idea with a motivated approach, however, I would like several points to be addressed before I could increase a score. 1.It seems that the method makes direct use of the number of classes in the datasets used.
Reject. rating score: 1. rating score: 3. rating score: 3. <BRK>This paper presents a model based RL architecture based on World Models which additionally makes use of an ensemble of models. In general, how many seeds were used to perform evaluations? The ensemble of models are used to train what is referred to as a “collective” policy in the CarRacing and VizDoom environments. While I think the paper tackles an interesting question how to leverage multiple models of the world that may be trained on different data and with different capabilities I unfortunately do not feel that this paper is ready for publication at ICLR, and thus recommend rejection. I think that addressing #1 and #3 alone would substantially improve the paper. Deep reinforcement learning in a handful of trials using probabilistic dynamics models. Specifically, each model in the ensemble receives different partially observed observations from the environment. There are some additional issues with clarity of the model explanation; for example, it’s not clear how the translator component is trained (the paper states that it is trained with meta learning, but this is vague). Moreover, while it seems to work ok in the CarRacing domain (though the ensemble trained policy does not perform as well as a non ensemble agent which is trained with full observations) it does not seem to work well in the VizDoom environment. As mentioned above, this method is an ensemble method but there is no discussion of other model based RL ensemble methods (e.g.[1] and [2]).<BRK>Each agent uses model based learning, learning a representation of the world, then learning a controller against the learned world model (learned with an evolutionary strategy of CMA ES). Different agents will naturally learn world models with different biases   this paper proposes a method that learns a collective policy from the biased models of the different agents, by letting each agent observe imagined rollouts from other agents  world models, training on that data. Many terms are immediately abbreviated into non conventional single letter abbreviations (C for controller, T for Translator). This extends to the results table   it s quite difficult to understand the results in Table 1 and Table 2 without a careful reading to find what what the abbreviations mean, and I still don t understand what the Overlap (%) and Cov. The analysis section also makes a distinction between delusional agents and cheating agents, which I also didn t understand. Overall, the question I found myself asking the entire paper was, "what does this add on top of existing work on training against an ensemble of world models?" This does not require learning O(N^2) translator models translating z_i to z_j for each pair of world models. Edit: have read author reply, no changes to review.<BRK>Each agent has an observation space that is differently impaired, such as by blocking out some specific set of pixels in its image observations. Each such agent learns its own dynamics and reward models based on its own observations, and goes on to train a policy based on simulated rollouts using these learned models. * Baselines: The proposed collective policy is evaluated against the individual policies of agents in its population, which is a very weak baseline since the individual agents are artificially impaired. I would suggest the following additional baseline:       individual agents without observation impairments, trained in simulation (essentially the world model paper this approach builds on). a policy trained with a model that is a composite average of the individual world models ["model ensemble"]. This is the in the spirit of prior work on model ensembling, such as Chua et al 2018, " ... handful of trials." * Environments: The results presented so far indicate the proposed approach works on one environment (CarRacing) and does not on another (VizDoom). (3) It does not sufficiently motivate its setting: when is it true in realistic settings that it would make sense for different agents in an environment to be artificially impaired in different ways? I also have a somewhat related suggestion: perhaps future versions of the paper might focus on using different modalities (such as RGB images and depth) for the different agents. In other words, the method requires the translator T to take one view of an agent and transform it into (a feature representation of) the view of another agent.
Accept (Poster). rating score: 8. rating score: 6. rating score: 6. <BRK>Summary of the paper s contributions:This paper proves a result on the l_p robustness (p \neq 1, \infty) of a piecewise affine classifier in terms of its l_1 and l_\infty robustness. The paper then proposes a regularization scheme called MMR Universal for a ReLU neural network that simultaneously trains for l_1 and l_\infty robustness. This scheme is based on maximizing the linear regions of the network as in Croce et al., AISTATS 2019. (2) The experiments show that the proposed regularization scheme is indeed effective in simultaneously guaranteeing robustness with respect to all l_p balls. Could this be formalized in theory? In Figure 2, the red curves are missing in the first and third plots.<BRK>Overview:The paper is dedicated to developing a regularization scheme for the provably robust model. The author proposes the MMR Universal regularizer for ReLU based networks. It enforces l1 and l infinity robustness and leads to be provably robust with any lp norm attack for p larger than and equal to one. 3.It is the first robust regularizer that is able to provide non trivial robustness guarantees for multiple lp balls. 2.Try a few more classical CNN architectures. Recommendation:Due to the logical derivations and supportive experiment results, this is a weak accept.<BRK>Summary:The author proposed MMR regularization for the provable robustness of union of l 1 and l infty balls, which is robust to any l p norm for p> 1. The paper is well organized. 3.The proposed method presents a significant improvement over SOTA. The author should provide more empirical analysis on the MMR regularization, like how it changes during the training process.
Reject. rating score: 1. rating score: 3. rating score: 3. rating score: 3. <BRK>This paper has set a new problem: singing voice synthesis without any score/lyrics supervision. The authors provide a significance of such a problem in section 1. This paper may serve as baseline results for the proposed problem in the future. Despite the significance of the problem and the novelty of the solution, this paper aims to solve too many problems at once. The authors should be able to support all of the claims they’ve made, which sometimes require experimental results. 3.“Our conjecture is that, … compressing the output of G(·) may have lost information important to the task.”: PatchGAN (Isola et al., 2017) had already addressed this issue. The authors may want to cite PatchGAN to support their conjecture or compare against PatchGAN to show their own architecture’s strength. 4.The ‘inner idea’ concept in the “solo singer” setting looks vague and contradicts with the main topic since it uses chord sequences to synthesize singing voice. Things to improve the paper that did not impact the score:1.<BRK>This paper claims to be the first to tackle unconditional singing voice generation. It is noted that previous singing voice generation approaches leverage explicit pitch information (either of an accompaniment via a score or for the voice itself), and/or specified lyrics the voice should sing. The authors first create their own dataset of singing voice data with accompaniments, then use a GAN to generate singing voice waveforms in three different settings:1) Free singer   only noise as input, completely unconditional singing sampling2) Accompanied singer   Providing the accompaniment *waveform* (not symbolic data like a score   the model needs to learn how to transcribe to use this information) as a condition for the singing voice3) Solo singer   The same setting as 1 but the model first generates an accompaniment then, from that, generates singing voiceFirstly, the authors have done a lot of work   first making their own data, then designing their tasks and evaluating them.<BRK>This paper tries to addresses an interesting problem of generating singing voice track under three different circumstances. Some of the problems that this paper deals with is a new problem and introduced first in this paper, which could be a contribution as well. Although the paper is fairly well structured and written, especially in the early sections, I am giving a weak reject due to its weak evaluation. Evaluation is almost always difficult when it comes to generative art, but I am slightly more concerned than that. The literature review can be, although it is nicely done overall, improved, especially on the neural network based singing voice synthesis. I appreciate the authors tried to find a great problem and provided a good summary of the literature. It is also nice to see some interesting approaches towards objective evaluation. Below are my comments. The difficulty to discriminate them doesn t seem to be (strongly) related to their variable length for me because a lot of papers have, at least indirectly, dealt with such a case successfully. I think there should be only 12 keys. I d say the distribution of pitch values can be interesting metric to show and discuss, but not as a metric of vocal track generation.<BRK>In this paper, authors explore the problem of generating singing voice, in the waveform domain. There exists commercial products which can generate high fidelity sounds when conditioned on a score and or lyrics. This paper proposes three different pipelines which can generate singing voices without necessitating to condition on lyrics or score. Overall, I think that they do a good job in generating vocal like sounds, but to me it s not entirely clear whether the proposed way of generating melody waveforms is an overkill or not. I am voting for a weak rejection as there is no comparison with any baseline. I think in the waveform domain may even be undesirable to work with, as you said you needed to do source separation, before you can even use the training data.
Accept (Poster). rating score: 6. rating score: 6. rating score: 3. <BRK>Since the performance provided in the baseline Resnet models can be slightly improved with further training and training schedule tuning. The numerical results show that the proposed method only introduces a small accuracy loss and sometimes even improve accuracy. Compared to the other methods, it also shows better performance. Also, a detailed comparison of the quantization size comparison should also be provided.<BRK>After the rebuttal  I thank the authors for their rebuttal. Strengths of the paper:  The problem is clearly stated (in particular the two questions about the clipping operation and the quantization levels are clearly presented in the Introduction). They answer to all the raised concerns with clarity and concision and performed additional experiments. Thus, with the classifier, its size is 2.8+1.95   4.75 MB, which is roughly a x10 compression factor.<BRK>The paper presents an approach based on power two quantization to compress the weights of neural networks. The results are better than the non compressed baseline (CIFAR with 3 and 5 bit/flotat, ImageNet with 5bit/float.) This seems surprising. Shouldn t the performance decrease as a result of the compression? Thus, the advantage of the exact scheme that is proposed compared to other methods is unclear to me.
Reject. rating score: 1. rating score: 3. rating score: 3. <BRK># IntroductionThe motivation of this paper relies on the dynamics model not being accurate enough, which leads to compounding errors. Current model based RL methods that learn a Q or value function take into account the uncertainty (i.e., STEVE, MBPO). Those methods are not “less competitive in terms of asymptotic performance.” There has been work on learning a parametric policy from MPC. More recent work has also used similar formulations [2]. Is it necessary? Which environments are risk prone/adverse? Overall, the paper is not mature enough to be accepted: there is not enough novelty, and the results lack of novelty, enough delta in performance from prior work, and have high variance.<BRK>I enjoyed this paper overall, and I think the idea is a good one. However there remain significant issues with the paper that preclude me giving a good score. The dependence on random seeds is worrying, and isn t as common in model free algorithms as you claim, which are mostly robust to seeds (the good ones at least). Is it the case that the policy is updated *only* using the model based rollouts? I.e., the reward signal is never used in the policy gradient but only used to train the models? It would appear that you are missing a reference to the very relevant UBE paper, which also deals explicitly with the uncertainty of the value function estimates: https://arxiv.org/abs/1709.05380In fact I would be curious to see any way that these two approaches could be combined (though that would be follow up work).<BRK>Summary:The main contribution of this work is introducing the uncertainty aware value function prediction into model based RL, which can be used to balance the risk and return empirically.
Reject. rating score: 3. rating score: 3. rating score: 6. <BRK>I am not sure about the novelty of the method, seeing that there is a very similar paper from earlier this year (Mathieu et al.2019).The experiments are lacking a little. While there are parts of this paper I like very much and I think it is technically sound, I feel the experiments section is lacking somewhat (see comments below).<BRK>(3) The experiments are questionable. My main concern is the motivation of the paper and the experiments. The authors may want to provide more discussion regarding this in the literature review.<BRK>The paper is well organized and easy to read. The notations are clear. Overall, I think this work is interesting. My main concern is about the experiments. As the authors mentioned, a potential reason is that the MNIST is not a typical hierarchical dataset.
Reject. rating score: 3. rating score: 3. rating score: 6. <BRK>The authors propose to learn a Transformer in embedded spaces defined via $m$ hash functions, with application to (fixed length) sequence to sequence classification for NLP. The method differentiates itself, in large part, by formulating model outputs in terms of $m$ distinct softmax layers, each corresponding to a hash of output space $\mathcal{Y}$. This issue begins with the chosen notation, which routinely obscures otherwise simple points. Similarly, the section on test time predictions (Sect 2.5) is needlessly hard to follow. Some unanswered questions I had include:  a) What happens if two tokens  hashes collide all $m$ times? c) How does the test time throughput of the proposed compare with that of alternatives? Questions:    Did you compare against different approaches to sparse softmax, such as LSH based methods [1]? What was the impact of approximate vs. exact inference and which was used during experiments? How important were embedding matrices $E^{I}, E^{O}$?<BRK>The paper proposes to use codes based on multiple hashing functions to reduce the input and output dimensions of transformer networks. The novelty of the idea is mostly to integrate these codes with transformers. While the technical contribution is limited, because most of the principles are already known or straightforward, the main contribution of the paper is to show that random hash functions are sufficient to create significantly shorter codes and maintain good performances. On the other hand, the experiments are carried out on non standard tasks without previously published baselines, and it is unclear why. This makes the experiments in the paper look more like "proofs of concept", and they are less convincing than they should be.<BRK>This work presents Superbloom, which applies the bloom filter to the Transformer learning to deal with large opaque ids. Quantitative results demonstrate it can be efficiently trained and outperforms non hashed models of a similar size. The size of the vocabulary could be largely reduced through hashing, which makes a larger embedding dimension and more complex internal transformation eligible and thus better performance. That is, models learned with the bloom filter applied to the whole vocabulary and models learned with the word pieces only. I think the empirical contribution is above the bar, but I do not think the authors gave enough credit to (Serra & Karatzoglou, 2017). It seems that the technical part of Superbloom, including the hashing and the inference, is the same as those in (Serra & Karatzoglou, 2017).
Reject. rating score: 1. rating score: 3. rating score: 3. <BRK>This paper criticizes existing High Dimensional BO (HDBO) via linear embedding literature for the following reasons:  Points in the embedded space projected mostly to the facet of the bounding box in the original space. The projection induces a distorted space which is not fit to be modeled by a GP. Linear embeddings have low probability of containing an optimum. What is wrong with the points being projected mostly to the facet of the original bounding box? I understand that the authors did not project Ay back to Blike REMBO did, but the authors also gave me no reason to believe that this will improve things either. This seems like a very qualitative claim. There is only one set of experiments showing performance ofALEBO against other methods and it was done on a very small extrinsic dimension too (D   100). What about rotational invariance? It also does not offer strong empirical evidence (too few experiments). Given these reasons, I do not think the paper is not ready to be published as it is.<BRK>The authors investigate pitfalls common to random embedding based approaches to high dimensional Bayesian optimization (HDBO). c. Embedded spaces potentially failing to contain optima are handled by constructing an     estimator for the probability of this happening, which is then be used to pick better     embeddings. Specifically, the degree of novelty on offer seems minimal and the empirical results are underwhelming. If nothing else, it would be good to clarify this matter: why did preceding works chose not to explore this direction and/or how did you make it work here? 2) Regarding use of Mahalanobis distances, evidence here (as provided in A.2) seems thin. This issue is allegedly improved by marginalizing Mahalanobis parameters $\Gamma$; however, the appropriate baseline here would be ARD with marginalized lengthscales (which, to my knowledge, is not shown). Why was a Laplace approximation used lieu of, e.g., slice sampling?<BRK>This is a well written paper and I enjoyed reading it. This projection is not enough good. However, as the paper mentioned, HeSBO have a limitation is that the probability that the embedding will contain an optimum can be quite low! To avoid (1), the paper use equation 1 (please find in the paper). LineBO is a good solution for high dimensions without any assumption on structure like low effective dimensionality. Thus, LineBO is a stronger contender to the proposed algorithm. I will lift my rating if the author provide their response to this point. Moreover, because the problem of the paper is high dimensional Bayesian optimization under the assumption of low effective dimensionality, they should compare to other strong algorithms under the same assumption such as SI BO algorithm( NIPS 2013) that used active learning to learn the low dimensional subspace instead of using random embedding like REMBO.
Reject. rating score: 3. rating score: 3. rating score: 8. <BRK>The paper proposes a technique based on the introduced concept of "backward value functions". The method is evaluated on gridworlds and mujoco tasks, showing good performance. The problem solved by SPI (constraint for each state) seems much more conservative than the constraint at the bottom of page 4 (single constraint based on average over all states). In it current form, the algorithm appears to be overly conservative, to the point that it may converge to a policy very far from optimal. Why is this needed?<BRK>In this work, the authors studied solving the CMDP problem, in particular to model the constraint propagation using the idea of the backward value functions. The idea of using backward value functions to model constraints in CMDP is interesting and so far I have not seen it in other places.<BRK>Their theoretical results show that the same properties for forward value functions hold here for backwards ones. The results show that the method does a better job of meeting safety constraints than the Lyapunov based method. The backward value function idea is a nice novel way of addressing the cumulative cost constraint problem. The paper is clearly written and the results are nice. For the empirical results, it would be great to see something about the computation or wall clock time required. Does this method run more quickly than the Lyapunov based method?
Reject. rating score: 3. rating score: 3. rating score: 8. <BRK>This paper presents a new RL library called « SLM Lab ». In spite of the above, I do not mean to criticize SLM Lab too heavily as from what I can tell it seems to be a a solid library with many useful features, and I am sure many researchers will find it useful in their day to day work. It seems like it is possible to have multiple agents in one environment, but is that enough for general multi agent RL?<BRK>Anonymity violationQuestions:  How difficult it is to implement distributional algorithms in your framework? Pros:  I agree that reproducibility is an extremely important question for the RL research, and thus such a code library is very beneficial for the community. I am not sure that ICLR is the right venue for such paper.<BRK>SLM Lab is a software framework for reinforcement learning, which includes many different algorithms, networks, and memory types. Thus, it is easily extendable for anyone and can be a pinnacle for future RL research. It is well written, easy to read, and provide a valuable platform / framework to the community, both the scientific community as well as practitioners. Although the scientific contribution may be low in the paper, I think the significance and potential impact of the paper outweigh that.
Reject. rating score: 1. rating score: 3. rating score: 3. <BRK>The paper introduces Dose Response Generative Adversarial Network (DRGAN) that is aimed at generating entire dose response curve from observational data with single dose treatments. This work is an extension of GANITE (Yoon et al., 2018) for the case of real valued treatments (i.e., dosage). The proposed model consists of 3 blocks: (1) a generator, (2) a discriminator, and (3) an inference block. In this paper, GANITE’s generator and discriminator architectures are modified to be able to handle real valued treatments. + Yoon et al.(2018) claims that optimizing a GAN with this G and D, the generator will become better and better at estimating accurate counterfactual outcomes. Looking at the objective function in Eq.(4), it is not clear why D should learn to distinguish factual from counterfactual outcome as opposed to learning the treatment selection bias as t.log(D(x ,y))+... would suggest. In summary, it seems that the adversarial training designed in GANITE and consequently DRGAN provide no advantage in terms of accurate estimation of counterfactuals, which in turn, nullifies the entire claimed contribution of these two works.<BRK>The paper proposes a generative adversarial net model for heterogeneous dose response causal effect estimation. The idea is to generate counterfactual dose response curves using the generator that can fool the discriminator that tries to distinguish between factual data and counterfactual data. Factual data along with counterfactual data generated by the GAN can then be used for heterogeneous causal effect estimation. Related to this concern, do the authors also implicitly reason under the consistency assumption (the observed/factual samples are the realizations potential outcomes of the corresponding treatment dosage pairs in the potential outcome framework) ? 3.It will also be interesting to see whether the proposed method can be degenerated to handle simpler cases such as heterogeneous treatment effect estimation of binary treatment. Or alternatively, when there is only one treatment option, where when treated there is a dosage response curve while when not treated the dosage is zero.<BRK>Generator is modeled as a multitask neural network and the discriminator is modeled as hierarchical network consisting of treatment and dosage discriminator. This indicates the proposed methods are better for dose response estimation. Novel neural network architectures used with counterfactual generator and counterfactual discriminator are interesting and these can be extended to other tasks. Useful for the community in general? Is the model better in terms of stability? Minor variation of previous work? While this is a very interesting task, this paper falls under incremental improvement as it improves GANITE framework to solve dose response estimation. As authors use GAN for counterfactual estimation which is not novel(GANITE) and estimating dose response estimation is not novel(DrNet).
Accept (Poster). rating score: 8. rating score: 6. rating score: 6. <BRK>The paper proposes a method for learning partial orders based on learning fuzzy pairwise comparisons (smaller/greater/approximatively equal to), and the retaining of a set of representants in each chain in order to allow consistent result by consistency maximization. Extensions proposed are the learning of multiple disjoint chains on this dataset, manually partitioned or learned through an iterative assignment algorithm that has similarities with a soft expectation maximization principle. Extensive experiments are done on the age estimation problem with comparison to exisiting approaches, and an application to aesthetic assessment is proposed in appendix. The paper is very well written and contains very extensive and relevant experiments. 2) It would be good to include the details of Appendix C in the text, as is, it looks like an artificial space saving trick. 3) typo in Section 3.2: "we assume that the chain, to which the instance belong, is known"  > "we assume that the chain to which the instance belongs is known"<BRK>This paper presents an order learning method and applies it to age estimation from facial images. The paper also provides an extended version for multiple disjoint chains, where each chain may correspond to a higher level attribute class, for example, gender or ethnic group. One concern about the method is that it imposes the geometric ratio (log distance) between the class distances in age estimation, considering the difference between 5 and 10 year old instances is easier to detect than 65 to 70 year old instances as stated in the paper. However, as far as it is understood from the provided discussion compared SOTA methods are not retrained or fine tuned in the same manner. Another concern is that the performance is not stellar as the presented method underperforms in comparison to DRF (Shen et al.2018) on MORPH II. The presentation of the results for FG Net is neither complete nor included in the main paper, and results for CLAP2016 are missing. BridgeNet, according to its results, is on par with the proposed method. Also, the numbers reported in this paper and the numbers in [1] have a discrepancy, which causes confusion.<BRK>This paper departs from traditional age estimation methods by proposing an ordering/ranking way. What more promising is the unsupervised chains, which could automatically search for a more optimal multi chain division scheme than the pre defined data division. The paper is well written. I have a few concerns as the following:1. The results shown in Table 6 show a better number for Geometric, but it is tested on FG NET which presents unbalanced age distribution (ages up to 40 years are the most populated). Therefore, the improvement may be caused by the data distribution that is more suitable for geometric ratio, i.e., more younger faces and less older faces. Such random initialization works for multiple chains with small mutual shift (the distance of references between chains). As indicated in Table 3, more chains will not significantly affect performance, which implies a relatively small mutual shift. However, if encounter a large shift, e.g., other properties would significantly affect the target property that you want to classify (ranking soccer teams where gender makes difference), will the proposed unsupervised learning converge?
Reject. rating score: 1. rating score: 1. rating score: 3. <BRK>Furthermore, it doesn t make that users want to prevent other from copying them. While this could be interesting, the work presented falls short of what it promises, i.e.to develop a practical model of fake news on social networks,  because many of the assumptions made about the phenomena under study are unrealistic. Assuming that the social network graph remains fixed over time is also unrealistic.<BRK>The proposed method is built upon a multi agent reinforcement learning method. The main contribution of novelty of the work, as claimed by the authors, is that they come up with a practical solution for fake news detection with deep reinforcement learning.<BRK>Update: I thank the authors for their response and for improving the paper. However, I maintain my position that the paper lacks a significant technical contribution to learning algorithms and that the applicability of the proposed approach is remains questionable in the current state. Summary:The paper proposes the  use of deep multi agent reinforcement learning (DMARL) for modelling fake news propagation and detection in social networks.
Reject. rating score: 3. rating score: 3. rating score: 6. <BRK>2) pruning before model saturation may result in accuracy degradation. The author also conducts extensive experiments with various ResNet architectures on both CIFAR 10 and ImageNet, achieving state of the art results with only 1/5 of the total epochs for iterative pruning. Finally, they conduct experiments to confirm it. 3.The early winning tickets in this paper achieve state of the art results with only 1/5 of the total epochs for iterative pruning. For lottery tickets, especially for early winning tickets, I think there is a lot of randomnesses.<BRK>This paper attempts an in depth study of the lottery ticket hypothesis. The lottery ticket hypothesis holds that sparse sub networks exist inside dense large models and that the sparse sub networks achieve at least as good an accuracy as the underlying large model. However, my main issue is with both the originality and significance of this work.<BRK>This paper carefully observes the behavior of weight magnitudes during training, finding the is a stage of saturation that is closely related to the winning lottery tickets drawing. Overall I am very happy with the interesting observations and analysis of the dynamics of weight magnitudes and how it can be related to the early winning lottery tickets drawing. The curve of retraining with smaller LR (0.01) has the save trend as the baseline and retraining with larger LR (0.1). It would be better to have these results for clear comparison. Overall, I love the empirical observation of weight magnitudes and think it would help the community to understand lottery tickets and training process of deep models.
Accept (Poster). rating score: 6. rating score: 6. rating score: 6. <BRK>The paper describes an approach to improve the confidence on deepneural networks (DNN).<BRK>The selected approach is to use the distance to (previously computed) class centers in multi class classification to help compute the confidence interval. (or ellipse) estimation is not new, but an effective and compact representation of this intuition in a DNN context is a valuable contribution, well placed in the literature   I recommend acceptance.<BRK>Would this generalize to other image classification tasks and datasets.
Reject. rating score: 3. rating score: 6. rating score: 6. <BRK>In this paper, the authors consider the neural amortized inference for clustering processes, in which the number of cluster can be automatically adapted based on the observed samples. The learning procedure and experiments are clear now. The major contribution of the paper is the design of the posterior parametrization so that the posterior satisfies the permutation invariant within a cluster, between clusters, and unassigned data, based on the DeepSet method.<BRK>The paper presents a neural network based clustering process where the number of clusters is not known a priori. Understanding model’s biases based on training data is one area I feel is important and the paper could add to.<BRK>The authors also suggest an extension to models of random communities and a novel approach to neural spike sorting for high density multielectrode arrays based on the proposed method. Strengths:The paper is generally well written and the relationship to previous works is well described. Empirical results seem quite convincing, for example, the clustering results presented in Fig.2 and Fig.3 clearly show not only the inferred number of clusters, but also the posterior probability which indicates that reasonable samples are assigned higher probability.
Reject. rating score: 3. rating score: 3. rating score: 3. <BRK>The article presents an approach to reduce the precision of weights, activations and gradients to speed up the training of deep neural networks. The precision of these values is increased according to a dynamic schedule such that the original classification accuracy is reached after training. •	Table 1: For the baseline experiments, the precision is switched from 8 to 32 bits, for MuPPET from 8 to 12 bits (see main text). •	Please clarify “distribution approach”.<BRK>However, the results are not superior enough compared with the state of the art. Positive points: 1. This paper proposes a new reduced precision training scheme to speed up training by progressively increasing the precision of computations from 8 bit fixed point to 32 bit floating point. The authors describe “This approach enables the design of a policy that can decide at run time the most appropriate quantization level for the training process”. 3.The presentation of the precision switching policy is confusing and the notations are unclear.<BRK>Overall an interesting paper, though I wished a more detailed presentation of the reasoning behind the algorithm would have been provided. When you say you look at the diversity of the gradients over the epochs, is this the batch gradient !? I find the justification for AlexNet to be adhoc (it switched at the wrong time, but that allowed to take more advantage of computation in the low precision hence it was faster). Or if you have high variance you could argue that the expected gradient would be 0 and hence you are not really making progress, i.e.you are just moving left right.
Accept (Poster). rating score: 6. rating score: 6. rating score: 6. <BRK>SummaryThe authors tackle the problem of skill discovery by skill chaining. Overall:		+Break the assumption that all options are everywhere. +Chains skills in a smart way   which could be very useful for lifelong learning, but the approach, as it is, is limited by the goal of the MDP (whatever that means in this context, state or a Reward). It would be very useful to discuss, clarify and contrast what is novel and what is borrowed from Konandiais 2009b as is. Initiation set classifier: This is an interesting approach. In particular, authors should compare their method and discuss other skill chaining approaches such as [1,2,3].<BRK>Initially the goal state is is the default termination condition. As new options are added to the pool, their terminal condition simply correspond to the initial condition of their subsequent option, which consist in a learnt classifier that acts as indicator function during the learning phase. The experiments are sound (and all averaged over 20 different seeds except for one). From a performance perspective, DSC is comparable to the state of the art most of the time, and even improving upon it on some experiments. However, I am not totally convinced that the learnt skills can be interpretable as clearly indicating different regions of the environment.<BRK>This paper studies the problem of learning suitable action abstractions (i.e., options or skills) that can be composed hierarchically to solve control tasks. The starting point for the paper is the (classic) observation that one skill should end where another can start. The procedure is repeated, yielding a sequence (a "chain") of skills that extends from the initial state distribution to the terminal state. The paper compares the proposed algorithm against state of the art hierarchical baselines on five continuous control tasks of varying complexity; the proposed method outperforms baselines on 2   4 of the 5 tasks (depending on the setting considered). I would be more confident about this paper if it added a comparison with HIRO. My reason for not voting for "accept" (rather than "weak accept") is that the experiments are not the most complex, and are all navigation esque (reacher is effectively a point mass).
Reject. rating score: 3. rating score: 3. rating score: 6. <BRK>This paper focuses on the implementation and some empirical evaluations of a class of algorithms designed to find optimal strategies/values of large MDP. The theoretical formulation of \kappa PI and \kappa VI involves solving, at each iteration, another auxiliary MDP problem (where the discount rate is of order \kappa\gamma). This is basically what this paper suggests to do, and implements. The experiments are a bit difficult for me to read, as the baselines (\kappa 0 and  1, say) are compared with "the best \kappa" which seems to be problem dependent, so I do not know if there is a clear message.<BRK>The main contributions of this paper are k PI DQN and k VI DQN, which are model free versions of dynamic programming (DP) methods k PI and k VI from another paper (Efroni et al., 2018). The deep architecture of the two algorithms follows that of DQN. Although this paper is going one step further extending from tabular to function approximation, I feel that the paper just combined known results, the shaped reward from Efroni et al (2018a) and DQN. The experiments were only comparing their methods with different hyperparameters, with only a brief comparison to DQN.<BRK>This would complement the Atari and MuJoCo experiments by showing improvements in performance with a higher confidence. The multi step dynamic programming algorithms proposed by Efroni et. How big were the networks that you used for k PI DQN? al.(2018a) also showed an equivalence between h step optimal Bellman operators and k Policy Iteration (k PI) and k Value Iteration (k VI) algorithms, which, similar to TD( 𝜆 ) but for policy improvement, take a geometric average of all future h step returns weighted by k. The paper extends the work from Efroni et. al.(2018a, 2018b) to the deep reinforcement learning setting by proposing an approximate k PI and k VI algorithm based on DQN and TRPO. For this reason, my rating of the paper is weak accept. Empirical Evaluations  First, my main complaint is the complicated architectures and complex domains used to gain insights about k PI and k VI with function approximation.
Reject. rating score: 3. rating score: 3. rating score: 3. <BRK>In this paper, the authors propose a reinforcement learning method for multi robot scheduling problems. How about thousands and millions of robots/tasks, e.g.routing planning or dispatching for vehicles in a ride sharing platform? My major concerns are as follows. 1.The paper is not easy to read.<BRK>Paper addresses the problem of centralized multi machine task assignment in an RL setting ("multi robot reward collection"). Appendix C seems to do part of this, and probably should be integrated into the body of the paper. Some figure refs are broken.<BRK>Not being an expert in RL, my assessment should be discounted. However, I am not sure I follow properly the main idea of the paper.
Accept (Poster). rating score: 8. rating score: 6. rating score: 3. <BRK>This paper describes a method of training neural networks to be robust to a worse case mixture of a set of predefined example attributes. This is done with a loss in accuracy in the average case but improvements in the worse case. I will keep my rating. The algorithm has a downside in that the groups must be known a priori, is it possible for these groups to be learnt?<BRK>To the best of my knowledge, this is the first paper to carefully address and propose an algorithm (with guarantees) for distributionally robust learning in the overparametrized regime, which is typical of modern large deep neural networks. Following other work, the paper formalizes distributionally robust learning as the minimization of a worst case loss over a set of possible distributions. Furthermore, the paper proposes a new stochastic optimization algorithm to minimize the loss that corresponds to distributionally robust learning and gives convergence guarantees for the proposed algorithm. I think this is an interesting and solid paper, with a clear presentation style, and well supported contributions.<BRK>This paper proposes a novel training method based on group DRO that makes it possible for overparameterized neural networks to achieve uniform accuracy over different groups of the data, where the data is required to obey a mixture distribution composed of these groups. In addition, the authors identify that increased regularization plays a critical role for worst group performance in the overparameterized regime by a series of empirical studies. For example the concept of  "group" is not defined properly. Cons:* The title is a little bit vague and may overstate the paper s contribution.
Reject. rating score: 3. rating score: 3. rating score: 8. <BRK>By utilizing mutual information constrain on the encoder, supervised probabilistic constraint on the class conditional probability, and introducing a margin to the maximum likelihood, the proposed method can manage a high classification accuracy and detect the out of distribution data. The motivation is good, the writing is OK. The structure of the paper needs to be refined. The experiments are not very strong. Several concerns are listed:1. It will be better if the author can show a structure map for Encoder, classification, class condition embedding, and mutual information evaluation networks, to clarify their relationships. 2.In Table 2, what is the accuracy for the rejection (outlier detection)? 4.The overall loss functions have three components, what is the contribution of different components to the final performance experimentally? 5.The architecture for encoder is large. Is it proper for Mnist which is a relatively simple dataset? 6.The comparison talked in the paragraph “Is Fully Generative Model Necessary for Generative Classification”, are these accuracies obtained from a comparable network size? It only makes sense if they are obtained using a comparable parameter size.<BRK>The paper proposes a scalable approach to train generative classifiers using information maximizing representation learning, with the motivation that generative classifiers could be more robust to adversarial attacks than discriminative classifiers. Then, class conditioned generative models of the representations are learned avoiding full generative modeling of the images. An additional loss is used to train the generative classifier which maximizes likelihood margins. Finally, percentile based thresholds of the class log probabilities is proposed to be used to reject classification for out of manifold inputs. The paper cites existing literature which indicates that generative classifiers might be more robust to adversarial attacks, and uses recently proposed representation learning techniques to scale up learning generative generative classifiers. The motivation of the proposed technique is clear, and the problem itself is relevant. Similar prior work use generative modeling at the pixel level. Generative modeling of representations is novel, afaik. The paper needs to have a clearer explanation and interpretation of the results. It’s not clear what the logits in table 4 and table 6 are, and what is being shown by the comparison. Summary: The authors propose a new technique for training generative classifiers with the aim to improve robustness to adversarial attacks and confidence on out of distribution samples. The method is well motivated and explained, but the experiment section is not very clearly written and I’m not confident whether the technique represents an advancement in the state of the art or not.<BRK>This paper studies classification problems via a reject option. The classification procedure studied in this paper builds on three components 1. An auto encoder that obtains a latent low dimensional representation of the data point 2. A generative model that models the class conditional probability model and 3.  a margin based loss function that learns a classifier that provides a large probability mass to the class conditional distribution corresponding to the correct class. The final decision procedure is to reject an input if the best class conditional probability is small and to use the class corresponding to the best class conditional probability otherwise. I have a few comments1. I would like to see what is the log likelihood assigned by the proposed procedure on OOD samples and would like to see a comparison of the log likelihood assigned by other procedures.
Reject. rating score: 1. rating score: 3. rating score: 8. <BRK>The paper proposes a cross lingual data augmentation method to improve the language inference and question answering tasks. The core idea is to replace a port of the input text (such as one of the sentence in a sentence pair in the language inference tasks) with its translation in another language. The authors empirically show that deploying the XLDA data augment improves the baseline methods for both the XNLI language inference data sets and the SQuAD task. In this sense, using some other data augmentation methods as comparison baselines would be very helpful.<BRK>The paper provides an analysis of a cross lingual data augmentation technique dubbed XLDA, which consists of replacing parts of an input text with its translation in another language. Building on the mBERT approach, the authors show that at fine tuning time it is beneficial to augment the training set of XNLI with cross lingual hypotheses and premises instead of in language pairs. The paper explores an interesting idea and shows that cross lingual data augmentation works well.<BRK>Finally, this paper is really fun and well written, thanks for the effort! mattered more for NLI performance. you say this: "The BLEU score of the translation system has little effect on a language’s performance as a cross lingual augmentor. " you also say this: "for every language a XLDA approach exists that improves over the standard approach", what a tantalizing statement! Are there any generalizations over whether typologically similar languages are better augmentors for each other than they are for really different ones? I feel like if you could redo your XLR method (fig.4) by adding augmentors in order from most similar to least (or vice versa), and you might find the answer to this.
Reject. rating score: 1. rating score: 6. rating score: 6. <BRK>The paper proposes learning a TSP solver that incrementally constructs a tour by adding one city at a time to it using a graph neural network and MCTS. Results on synthetic TSP instances with 20, 50, and 100 cities show that the approach is able to achieve better objective values than prior learning based approaches. Applying AlphaZero like approaches to TSP is an interesting test case for understanding how well they can work on hard optimization problems. Novelty is fairly low. The changes in SEGNN compared to previous works are incremental or not novel, and the overall idea is the same as AlphaGo/Zero. While I don’t think novelty is a strict requirement, if it is absent, then it should be compensated with strong empirical results, but the paper lacks that as well. The ML community needs to move away from evaluating on small instances if the long term goal is to beat state of the art solvers with learning.<BRK>For this learned heuristic, the authors propose a Graph Neural Network derived approach, in which an additional term is added to the network definition that explicitly adds the metric distance between neighboring nodes during each iteration. They perform favorably compared to other TSP approaches, demonstrating improved performance on relatively small TSP problems and quite well on larger problems out of reach for other deep learning strategies. I believe that the paper is built around some good ideas that tackle an interesting problem; the Traveling Salesman Problem and variants are popular and having learning based approaches to replace heuristics is important. It should be made clearer that the algorithm is referring to a "partial tour" as opposed to the final tour. As written, this information is not included in the main body of the paper yet is critical for the implementation. I also like the comparison between the proposed work and AlphaGo, which popularized using deep learning in combination with MCTS; this enhances the clarity of the paper.<BRK>EDIT: After the authors response and update of the contributions to indicate that the main contribution of the paper is the application of GNNs and MCTS to the TSP (rather than the original claims that that the model architecture and search approach were novel contributions), I increased my score from Weak Reject to Weak Accept. The authors propose an MCTS based learned approach using Graph Neural Networks to solve the traveling salesman problem (TSP) agents. At train time, this network is trained to predict the probability of each unvisited node to be next in the optimal path. At test time, they use MCTS with a variant of PUCT, where the pre trained SE GNN is used as the prior policy, and there is a selection strategy during search that balances the prior probability, and the Q values estimated by MCTS, using max based updates (e.g.during back up new Q estimates replace old estimates if and only if the are larger than the previous ones). Authors show that the approach beats other learned solvers in the TSP problem by a large margin in terms of optimality gap. In my opinion the main contribution would be the state of the art performance at solving the TSP using learned methods.
Reject. rating score: 3. rating score: 6. rating score: 6. <BRK>This is interesting, and different from previous methods, which use either an explicit vector of factors that is input to a generator function, or object slots that are blended to form an image. 3.The most serious concern is that although empirical results are promising, I have concern about the correctness that Eq.(6) realizes disjunciton, and Eq.(8) realizes negation. The authors seem to assume that the normalizing constants for p(x|c_i) are equal. Convincing quantitative experiments on disjunction and negation lacks. Minor: in the updated paper, "The equal sign in Eq.(4) should be \proto" is not fixed.<BRK>The paper is very hard to reproduce, which hurts the reliability of the experimental results. Writing can be improved. Original Review  This paper proposes several rules to compose different energy based models for compositional image generation. The authors propose empirical rules for conjunction, disjunction, and negation, which in combination can be used to derive arbitrary logic expressions. Why is this the case?<BRK>The paper talks about training multiple energy based models for each concept and then combining them in different ways to construct a composite model. Regarding the usage of energy based models, the idea is interesting. However, I am not sure about the existing works using alternative approaches. So what do you mean by this: "We can then obtain maximum a posteriori (MAP) estimates of concepts by minimizing the energy of the above expression."
Reject. rating score: 3. rating score: 3. rating score: 6. <BRK>Consequently, the paper claims that this CEB model improves general test accuracy and robustness against adversarial attacks and common corruptions, compared to the softmax + cross entropy counterpart. In overall, the manuscript is easy to follow with a clear motivation. I found the experimental results are also promising, at least for the improved test accuracy and corruption robustness. I would like to increase my score if the following questions could be addressed:  It is not clear whether adversarial training is used or not in CEB models for the adversarial robustness results. Personally, this is one of the reasons that makes me hard to understand the values presented in the paper. If so, it feels so awkward for me, and I hope this results could be further justified in advance.<BRK>This paper studied the effectiveness of Conditional Entropy Bottleneck (CEB) on improving model robustness. The proposed approach sidesteps such a problem since it does not require adversarial retraining. One of my major concerns is that it has a large overlap with Fischer (2018) in terms of methodology, experiment settings, and empirical observations, which limits the general contribution of the paper. Although Fischer (2018) is an unpublished work, I think that it is fair to consider that as a prior work since it is properly cited in the paper. My other concern is that the experiment misses comparisons against other adversarial defense approaches, which makes it difficult to understand the degree of robustness this model can achieve. But it would also be great to see justifications from various angles (e.g.in the context of adversarial defense).<BRK>TITLECEB Improves Model RobustnessREVIEW SUMMARYAs a thorough empirical evaluation of a new method for training deep neural networks, this paper is a useful contribution. PAPER SUMMARYThe paper presents an empirical study of the robustness of neural network classifiers trained using a conditional entropy bottleneck regularization. CLARITYThe paper is well written and easy to follow. Results are presented in a way that gives the reader a good overview, but figure quality / legibility could be improved. SIGNIFICANCEThe primary contribution of the paper is a thorough experimental evaluation of the particular regularization method presented in the paper. While this is certainly useful, it would have been even better with more comparison with other regularization methods. The paper clearly demonstrates the benefits of CEB, which is of interest in itself. "VIB" Abbreviation not defined in the text.
Reject. rating score: 3. rating score: 3. rating score: 6. <BRK>This paper considers Bayesian Reinforcement Learning problem over latent Markov Decision Processes (MDPs). 1.The Bayesian Reinforcement Learning problem this work considered is important. Update Thanks for the rebuttal. I suggest the authors do more detailed analysis. The experiments shows that it can work in some cases, but I do not see an explanation (the "residual learning" paragraph is high level and I do not get an insight from that.).<BRK>In this paper, the authors motivate and propose a learning algorithm, called Bayesian Residual Policy Optimization (BRPO), for Bayesian reinforcement learning problems. The paper is well written in general, and the proposed algorithm is also interesting. However, I think the paper suffers from the following limitations:1) This paper does not have any theoretical analysis or justification.<BRK>3.Why and how was the reward for the cheese finding task determined? 4.I would be helpful to provide some intuition about \psiOverall an interesting paper, but not sure how well it would perform on a wider set of tasks.
Reject. rating score: 3. rating score: 3. rating score: 6. <BRK>This paper addresses a limitation of BatchNorm: vulnerability to adversarial perturbations. Specifically, the authors observe that the statistics of BatchNorm for training and inference are different, resulting in different data distributions for training and inference. The paper is well written and the contributions are stated clearly. Further experiments would have made this claim more convincing. The variance of input is of the same order of magnitude as (max(x) min(x))^2.<BRK>Review: This paper investigates the reason behind the vulnerability of BatchNorm and proposes a Robust Normalization. The paper is clearly written, easy to read. Strengths: Explore the cause of adversarial vulnerability of the BatchNorm and assume that the tracking mechanism used in original BatchNorm leads to the vulnerability from experiment results.<BRK>This paper proposes an interesting perspective that BatchNorm may introduce the adversarial vulnerability, and probes why BatchNorm performs like that (the tracking part in BatchNorm). In experiment, the robustness of the networks increases by 20% when removing the tracking part, but the test accuracy on the clean images drops a lot. Detailed Comments: + The paper is well written.
Reject. rating score: 3. rating score: 3. rating score: 6. <BRK>In this paper, the authors propose a new gradient compression method, which is called nonuniform quantization. The algorithm is a reasonable variant of SGD with uniform quantization. The experiments show good performance. In this paper, a very important reference and baseline is missing, which is call error feedback SGD [1]. Since [1] provides the SOTA results for quantized SGD, the proposed algorithm should be compared to it in the experiments. 2.This paper claims to have strong theoretical guarantees. However, the theoretical analysis only works for convex functions. Note that the theoretical analysis in [1] also works for non convex functions. the proposed algorithm, NUQSGD, does not show improvement on the convergence, compared to the baseline QSGDinf. What I really what to see is training loss (or testing accuracy) vs. training time (or communication overhead, such as number of bits), so that we can evaluate the trade off between communication overhead and the convergence, compared to the baselines. Minor issue (I hope the authors can consider the following suggestions in a revised version.However, since the issue is minor, it doesn t affect the score):!.<BRK>In this distributed system, there is a trade off between the *communication cost* from sharing the stochastic gradient and the *variance* from gradient quantization. This paper is a follow up of Alistarh et al.(2017).It proposes a non uniform (logarithmic) quantization scheme (NUQSGD). This paper provides theoretical analysis of the variance and communication cost of NUQSGD. Originality and significance:This paper follows up on the parallel SGD framework proposed by Alistarh et al.(2017), where the authors proposed QSGD using a uniform quantization. This paper proposes NUQSGD using a non uniform quantization method. The quantization of the stochastic gradient amplifies the stochastic variance, which influences the rate of convergence of SGD. On the other hand, it is also important to decrease the communication cost. NUQSGD does not provide significant improvements in terms of the variance and communication cost. Compared to QSGD, we can see that NUQSGD improves the dependence on s for the variance term, but it has a worse (exponential) dependence on s for the communication cost. However, NUQSGD has the same dependence on d as QSGD in terms of both variance and communication cost. It would be great to add learning curves with the ‘time’ being the x axis as well. Also, I would suggest the authors to record the time needed to proceed one iteration for each parallel algorithm to compare the communication cost.<BRK>They show that it yields stronger theoretical guarantees than QSGD while showing a great empirical performance. The main difference between their scheme NUQSGD and QSGD is that they use nonuniform quantization (0, 1/2^{s},  …., 2^{s 1}/2^{s}, 1) instead of uniform quantization (0, 1/s, …, (s 1)/s,1). They also establish theoretical results for the variance upper bound and expected communication cost of their scheme. However, there are several issues and questions that if fixed or illustrated could be a great paper. It would be great to include more theoretical analysis which demonstrates the importance of variance upper bound for convergence speed guarantee. This may cause tuning biases (the setting may favor one method but hurt others  performance). 3) Although the paper mainly focuses on comparing with QSGD, there are several relative communication efficient training algorithms which I think are worth to compare empirically (at least one of them). For example:		a. I agree with the authors  point that it s worth to explore the interaction between NUQSGD with more complex reduction patterns like ring based. Since the ring based algorithm like all reduce is more popular in practice nowadays, interacting with it would have a better practical meaning.
Accept (Spotlight). rating score: 8. rating score: 8. rating score: 6. <BRK>Summary:The paper improves upon Hamiltonian Neural Networks to model physical systems from observed trajectories. My Comments:I come from RNNs background and I am not an expert on physical systems. But the paper is extremely well written that I can easily follow.<BRK>This paper proposes to represent a Hamiltonian model of a physical system by a neural network. The proposed approach is shown to outperform the recent work "Hamiltonian Neural Networks" by a large margin on mass spring chain dynamics and three body systems. The approach can even handle stiff dynamical systems such as bouncing billiards. Overall, the work is solid contribution and a reasonable improvement over the recent work on HNNs is demonstrated. Therefore I recommend acceptance of the paper. It could very well be, that the law cannot be described by a Hamiltonian system.<BRK>This paper introduces SRNN to model the hamiltonian of a dynamical system. The paper is well written and the ideas are communicated properly. To me, it seems like a boundary on the noise variance should be assumed. Is this true or am I missing something? The conclusion section is missing from the paper.
Accept (Spotlight). rating score: 8. rating score: 8. rating score: 6. rating score: 6. <BRK>This paper makes present an original way to encode the position of the token when encoding them in a sequence. The classical additive encoding of positions creates several issues, such as the lack of flexibility when dealing with pooling layers, and the authors refer it as the position independence problem. This enables the embedding of a word to be dependent on the position in a non linear manner. In short, it is quite rare to find such a clear and simple idea with so much empirical gains.<BRK>This paper proposes to learn position varying embeddings of words using complex numbers. Results are shown on text classification baselines and improvements are small. Now, with large periods, for a finite practical length value, the periodic effects might end up not being observed but is that the case in the models that this approach learns? It would be great if authors could characterize the contexts/ words for which the periods are small and the contexts for which they are large. Finally, the "iff" proof needs to be cleaned up because I am not still not convinced if the proof holds oin both the directions and I believe there could be other functions with desired properties.<BRK>### Problem and Previous ResearchThis paper tackles the problem of incorporating the sequential structure of words for text processing. Experimental results on text classification, machine translation, and language modelling show gains over classical and position enriched word embeddings. ### Pros and ConsOverall, the paper tackles an important problem in word embeddings and proposes a principled approach to the problem. between nodes to model the sequential structure between words. relevant since they show improvements on machine translation.<BRK>### DetailsThe paper "Encoding word order in complex embeddings" presents a method for making word embeddings position dependent. The frequency is dependent on each word and each dimension in general. ### Possible improvements to the paper1. The main weakness of the paper is that the authors repeatedly mention that encoding the position as a multiplicative factor which is multiplied to the frequency gives leads to a more decoupled/interpretable embedding but their experiments are solely focused on accuracy measurement. We expect the performance to be bad but how bad?
Reject. rating score: 1. rating score: 3. rating score: 3. <BRK>In general, the rebuttal is intended to be a conversation to clarify understanding of the work. What is the purpose of Proposition 1? The results on Mujoco suggest not. Finally, I have some concerns with the results as presented in the paper. There are some details lacking, for example how specifically are the meta test MDPs chosen for the Mujoco experiments?<BRK>Thanks for your comments. The algorithmic ideas feel a bit too incremental and the experimental evaluation could be stronger I d recommend trying the method on more complicated environments and including ablation studies. I disagree that the cheetah and hopper environments are "challenging" they re one of the simplest MuJoCo environments.<BRK>This paper studies the meta RL problem in the off policy, batch learning setting. * The approach is general insofar as it could presumably be used with other batch RL methods (for first stage of algorithm), or even non batch RL methods. I ve given a weak reject mainly because 1) auxiliary loss has not been experimentally shown to be crucial and therefore the technical novelty may be relatively limited, and 2) more comparisons are needed, to alternatives or other baselines.
Reject. rating score: 3. rating score: 3. rating score: 6. <BRK>They designed a task called  lift the flap , where the human or the model are given an image with one of the object blacked out. The authors has collected human data using mTurk, and also trained a model (named ClickNet) to perform the task. I find the paper is clearly written and quite easy to follow. However, I am not sure it would be of general interest to the ICLR audience. Can the author comment on this decision? But, it is now shown in the results. Minor:1.It is impressive that the authors have considered a wide range of control models.<BRK>I think this is an ambitious study and find the employed experimental methods interesting. In the last section, there is a statement “the model adequately predicts human sampling behavior and reaches almost human level performance in this contextual reasoning task.” I think that at least the first half is overstatement; it is not well validated by the experimental results. For instance, contrary to the authors’ claim, I do not think the effectiveness of the recurrent connections in ClickNet is sufficiently validated. I think a yet another baseline is missing in the experiments, which is a strategy of clicking points in the periphery of the black box while avoiding (or minimizing) overlaps of the regions deblurred by previous clicks. Additionally, I am somewhat skeptical if a pretrained VGG can really extract useful features from (partially) blurred images, even though it is not trained on blurred images. It is widely known that CNNs for visual recognition tasks are vulnerable to image blur, noise, etc.<BRK>The experiment is done by "lift the flap"   masking out a region of interest in the image and let either the human or a convNet based model to guess what it is by checking the context. Both of them are first shown with a blurred image with masked region, and then start to guess by clicking on surrounding regions and unblur them. It would be interesting to see how it works for objects that are out of context though:http://people.csail.mit.edu/myungjin/outOfContext.html+ A like it that a great set of baselines and analysis are done for the paper. I think the paper needs to have a baseline for the upper bound as well: what is the accuracy if the region is seen? 2018.Just from the results (e.g.Fig 3) it seems our current models are already doing pretty well!
Reject. rating score: 3. rating score: 3. <BRK>The writing generally is unclear, full of ungrammatical sentences and extremely hard to read: e.g.see examples below. The evaluation section is very rambling and contains unnecessary parts that need not be included, e.g.convergence of mean squared error loss between GRU output and latent space z. Questions  The internal reflection function part, the core of this paper, is not properly explained until the end. This seems an important part of evaluation but not explained at all.<BRK>This seems to be an ambitious paper that claims to replicate the phenomenon of “spontaneous” learning of compositional language (as in Choi 2018) under relaxed constraints. Overall I have had a lot of difficulty understanding the proposed method, and would have needed more time (or more background knowledge) in order to properly evaluate this paper.
Reject. rating score: 1. rating score: 3. rating score: 3. <BRK>Paper summary: This paper proposes an alternative approach to batch normalization called POP Norm. They then empirically examine their method on CIFAR10, CIFAR100 and ImageNet classification. In fact, the effects of gradient variance on convergence in optimization have prompted a whole sub field on variance reduced gradient descent methods such as SVRG. However, it seems that they take the l2 norm of the random variable after mean subtraction not before (Equations 10 and 12). The authors are comparing two upper (or two lower) bounds which is insufficient to draw any meaningful conclusions. Since the authors do not really tune over hyperparameters (they use a fixed learning rate in all their experiments) when comparing the different approaches, it is not clear whether the chosen learning rate is optimal for BatchNorm. Overall, I think there are significant concerns with the paper, both in terms of writing and the soundness/novelty of the technical results and experiments.<BRK>Based on this observation, the authors show that by applying POP norm which is quite similar to the Batch norm applied before the activation functions, the gradient Lipschitz constants and the gradient variances can be lowered. I must first confess that I’m not an expert in the relevant area. Having said that, I don’t have enough background knowledge to evaluate the novelty of the paper. Are there any papers or standard textbooks to be noted? The theorem 2 and 3 are based on the full dataset statistics, but because they are infeasible in practice, the authors propose to use the batch statistics. The experiments provide results to demonstrate the effectiveness of 1), but not clear about 2). Overall, despite the lack of background knowledge, I find this paper to be borderline, but more close to the reject, because of some unclear points. I would like to hear back from the authors and other reviewers. Shouldn’t it be the repetition of b_k for m times?<BRK>This paper proposed a new normalization method for accelerating the training of deep neural networks. Then, the authors show that the normalization step can reduce the variance of gradients. The experimental results show some improvement. 2.If my understanding is right, Theorem 2 and 3 actually can also show that other normalization techniques have these effects. 3.This paper claims that the effect of batch normalization will be destroyed by Tanh and Sigmoid activation function. It s better to show the performance of POP Norm for the network with Tanh or Sigmoid. 4.How about its performance for large networks, such as ResNet 50?
Accept (Poster). rating score: 6. rating score: 6. rating score: 3. <BRK>Thank you for an interesting read. As far as I understand, this paper presents DAMS which is a MAML like algorithm but applied to posterior sampling. The meta sampler is designed as an inverse version of the neural auto regressive flow (NIAF), and the task specific sampler is based on the Wasserstein gradient flow (WGF). This paper provides a nice complement by considering fast adaptation of posterior sampling. As far as I understand this baseline approach is different from ABML/PMAML that has been reported in the paper. clarity  The presentation needs to be improved.<BRK>Baselines:  A lot of this paper relies on comparison to baselines, which are chosen to be mostly from the meta learning field. However, in practice the goal of the paper is Bayesian Inference in a particular class of models. Main Comments to authors:Pros: interesting combination of techniques (IAF flows and WGF/Stein inference) to do meta sampling empirically appealing results as pertaining to raw performance metrics like accuracyWeaknesses:  The evaluation is focused on #of steps during testing, but not on # of datapoints required for eval on new domains. Putting them together and making it work is nontrivial and the authors demonstrate in their experiments that they get strong performance metrics. I would appreciate more details and careful analysis. This object is highly nontrivial and not analyzed in terms of performance at all here.<BRK>This paper proposes a Bayesian meta sampling framework consisting of two main components: a meta sampler and a sample adapter. It seems the performance of this Bayesian method is inferior to the state of the art meta learning methods. What are the specific model structures for T and G in NIAF? The experiments can be improved.
Reject. rating score: 1. rating score: 1. rating score: 3. <BRK>The paper studies how different settings of data structure affect learning of neural networks and how to mimic the behavior of neural networks seen on real datasets (e.g.MNIST) when learning on a synthetic one. I would recommend rejecting the paper due to several issues pointed out below. After a quick search, for example, [1] provides an in depth analysis and generalization bounds for two layered neural networks and provides a data complexity measure that can discriminate between random labels (which are equivalent to the outputs of randomly initialized fixed teacher networks in this work) and true labels on structured datasets like MNIST and CIFAR. 2.The paper claims to experimentally identify key differences in the training dynamics of neural networks in teacher student setup and on an MNIST task (binary classification into even and odd numbers). I would also like to see concrete examples when and how the hidden manifold model may benefit theoretical understanding or practical knowledge on, for example, how to cook a dataset or check if the dataset admits/affects learning.<BRK>This paper studies influences of data structures on neural network learning. The data structures discussed in this paper are structured inputs (concentrating on a low dimensional manifold) versus unstructured ones, as well as the teacher task (labels are obtained as a function of high dimensional inputs) versus the latent task (labels are obtained as a function of the lower dimensional latent representation of inputs). The observation that typical real world datasets are concentrated on a lower dimensional manifold is not novel, and it is also well expected that networks trained with such a dataset would exhibit different behaviors for inputs outside such a lower dimensional manifold. The possible novelty of this paper would thus be in the proposal of the hidden manifold model, but I am not convinced with the significance of the latent task. Because of these, I would judge possible contributions of this paper rather weak, so that I would not be able to recommend acceptance of this paper.<BRK>The authors consider the general problem of "structure" in datasets particularly, what are the features of datasets that govern the learning dynamics of neural networks trained to classify that data. Finally, they identify that "structure" in the input space, and a notion of "latent" ness in the task seem crucial for a synthetic dataset to recapitulate the learning dynamics of a real world dataset. My only reservations are about the scope of the experimentation / strength of conclusions of the paper for generally structure data. The primary weakness of this paper is the over reliance on the MNIST dataset, which is very nearly linearly separable. I would raise my score to an accept if the authors carried out their analysis on (at least) CIFAR 10 as well, and even higher if the authors greatly expanded their experiments.
Reject. rating score: 3. rating score: 3. rating score: 6. <BRK>You never report how well C_\psi is doing, and it s not clear to me why C_\psi is needed at all. I assume that you acutally optimize (8) with L^label? The source of datasets should be clarified. (see detailed comments)It s interesting that this method can leverage these very sparse or poor quality annotations, but it would be helpful to get a sense of how good the annotations provided here are. What s the threshold were annotation s quality is too low to be helpful? Plus, an indication of variance would be nice. Where does it come from?<BRK>The paper demonstrates some experiments on a basketball environment and a halfcheetah environment, showing that the agent will perform according to corresponding styles. My main concern here is the technical novelty of the proposed method: it seems that once we have the labels (which are limited to programmable functions), all we need to do is to learn a policy that conditions on the labels. In this case, we are not concerned with the latent variables whatsoever, therefore it seems that the CTVAE baselines are overkill for the task (learning latent variables that are not actually needed). At least from how the policy is described pi(\cdot |y) does not depend on unsupervised latent variables z. I wonder if the basketball kl should be multiplied by 10 and the cheetah ctval style NLL is a typo? This seems technically viable but unclear empirically.<BRK>The authors propose learning generative models for long term sequences that can take a style argument and generate trajectories in that style. (Real valued labels are allowed but are not considered in this work). It s argued that learning a model helps with credit assignment, from an RL perspective credit assignment seems like the wrong word. Nothing about learning the model makes it easier to assign credit, the main gain it gives is making the problem differentiable. It would be good to define this. I am concerned at all the requirements though   the method assumes the dynamics of the environment are learnable, and that we can define labeling functions that cover the space of styles we want, both of which seem like strong requirements.
Reject. rating score: 1. rating score: 1. rating score: 1. <BRK>The paper tries to analyze LSTM and GRU through differential equations and proposes an alternative formulation. Comparing the EINS to LSTM regarding the number of parameters is a bit misleading, as the right comparison is with GRU which is the common "slim" alternative.<BRK>It’s interesting to look for new building blocks for neural nets. The provided experiments do not strengthen the claim that EINS is “simpler” for the practitioners. Section 5 (Experiments) To me, the main contribution of this work lies (potentially) in the newly proposed EINS unit.<BRK>And then the author proposes three simplifications on LSTM, which are suggested by the formulation of the dynamic Hopfield model. A wide range of tasks are included in the experiment and the results prove the effectiveness of the simplification. Furthermore, there is not even a related work section in the paper.
Reject. rating score: 3. rating score: 3. rating score: 6. <BRK>Authors propose a GAN based adv imitation attack that can use less training data to produce a replica of the model. The idea of using a GAN in producing adv examples in quite interesting. But the proposed approach is closely related to the following paper:https://arxiv.org/pdf/1801.02610.pdfTherefore, I am not sure about the novelty of the proposed approach. Also can authors comment on the stability of GAN s training? How does the proposed approach relate to the adversarial distillation literature?<BRK>The authors propose to use a generative adversarial network to train a substitute that replicates (imitates) a learned model under attack. They then show that the adversarial examples for the substitute can be effectively used to attack the learned model. The proposed approach leads to better success rates of attacking than other substitute training approaches that require more training examples. So it is not surprising that the proposed approach can be better than substitute models.<BRK>This paper proposes a new approach to conduct adversarial attacks, where an imitation classifier is trained to mimic the behaviours of the targeted/attacked classifier and adversarial attacks can be generated with existing attack methods on the imitation classifier. The reasons that I am going towards accept are as follows:1. I feel that the idea of learning an imitation classifier is somehow novel and intuitive, which would improve the applicability of existing adversarial attacks in more realistic cases. In addition, using GAN framework in the training of the imitation classifier is also interesting. It is a bit unclear of the model configuration of the generator, which seems to be not introduced in details. Minor:"sometimes the ability of G is much stronger than G"
Accept (Poster). rating score: 8. rating score: 6. rating score: 3. <BRK>This work proposes CROWN IBP   novel and efficient certified defense method against adversarial attacks, by combining linear relaxation methods which tend to have tighter bounds with the more efficient interval based methods. One of the primary contributions here is that reduction of computation complexity by an order of \Ln while maintaining similar or better bounds on error.<BRK>This paper proposes a new method for training certifiably robust models that achieves better results than the previous SOTA results by IBP, with a moderate increase in training time. The CROWN based bound uses IBP to compute bounds for intermediate pre activations and applies CROWN only to computing the bounds of the margins, which has a complexity between IBP and CROWN. The improvement is significant enough to me and I tend to accept the paper.<BRK>This paper proposes a new variation on certified adversarial training method that builds on two prior works IBP and CROWN. They showed the method outperformed all previous linear relaxation and bound propagation based certified defenses. The method achieved SOTA. It is a straightforward combination of prior works, by adding two bounds together.
Reject. rating score: 1. rating score: 1. rating score: 3. <BRK>The paper a new method for evaluating prediction uncertainties for regression and classification tasks. I argue this work should be rejected. I am unclear as to how the classifier uncertainty is measured. I am unclear as to what statistical test you perform to accept/reject the chi square hypothesis.<BRK>I am tending to reject, because although each of the two distinct contributions are good starts on interesting approaches, neither provides a convincing solutions for the main question, and the two contributions are quite distinct, so that the paper lacks a consistent thread. Mahalanobis distance increases monotonically with negative log likelihood, so is there any reason to expect it is a better way to compare models based on their uncertainty? Can the statistical test be adapted to the classification setting? The conclusion that CVAE is better than MC for this problem is solid, but is there a more general conclusion to be drawn? E.g., could a CVAE model yield realistic estimates in the regression setting?<BRK>The paper makes the following two contributions: 1) a new metric to measure the realism of uncertainty estimates for regression problems which uses a Mahalanobis distance based statistical test. After reading the paper it still remains unclear to me why the proposed statistical test is superior to other popular metrics, such as the log likelihood or the metrics proposed by Mukhoti and Gal. This is somewhat confusing.
Reject. rating score: 3. rating score: 6. rating score: 6. <BRK>This paper proposes a simple modification to the ubiquitous Transformer model. Noticing that the feed forward layer of a Transformer layer looks a bit like an attention over "persistent" memory vectors, the authors propose to explicitly incorporate this notion directly into the self attention layer. This involves concatenating the contextual representations with global, learned memory vectors, which are attended over. The model is tested on widedly utilized character/word level language modeling benchmarks, where it is found to outperform, or be on par with, existing models while using fewer number of parameters. I had some further questions/comments:  I found the motivation of the persistent memory vector as replacing the FF layer somewhat tenuous. What are some examples in which the persistent memory vectors are attended to most?<BRK>This paper considers an architecture change to the transformer in which they swap the feedforward subcomponent of the standard transformer with an "attention only" variant that includes persistent "memory" vectors. The model is evaluated against a suite of baselines on the tasks of character  and word level language modeling. Overall, I m on the fence regarding this submission. One thing I would have liked is more motivation. A question for the authors: did you consider a vanilla Transformer with persistent memory vectors added?<BRK>This paper mainly proposes a modification of well studied Transformer architecture that is widely used for the text generation tasks, i.e., language modeling and machine translation. In other words, the proposed method discards the feed forward layers and augment the self attention layers with persistent memory vectors. They conduct experiments on character and word based language modeling and show the on par or slightly better performance comparing with the standard Transformer language model and other similar Transformer modifications. The proposed method is just a modification of the existing neural network architecture. However, the Transformer architecture is currently applied to a wide variety of tasks in text generation.
Reject. rating score: 1. rating score: 3. rating score: 6. rating score: 6. <BRK>I have two general concerns, the first is related to the presentation and the second to the relevance of the results. * The point of Fig.1 and Fig 4 is not clear. * Explanation of experiments is a little bit confusing (e.g.what does it mean top1 and top5 in tables?). * What is the CFL condition?<BRK>The paper proposes a spectral non local block, which is a generalized method of the non local block and non local stage in the literature. 2", "more liberty for the parameter learning. What is the conclusion from Table 4? However, I am afraid that the idea is not well explained and supported, thus I gave a weak reject to encourage the authors to further improve the paper.<BRK>In this paper, authors propose a spectral nonlocal block. Overall, the paper is written well. I like the idea to interpret the nonlocal operation in the graph view. More important, the resulting formulation is quit concise for implementation. However, my main concern is the experiment, which should be further enhanced by perform large scale video classification like Kinetics400.<BRK>The experiments seem convincing enough that the authors made enough effort to prove their method might work. Feature maps to show robustness of method is a good point  CIFAR 10 and CIFAR 100 are certainly a good start, but might not be the best datasets to test for image classification, in lieu of ImageNet and others. Classification itself is a good start, it might be interesting to think about using this in a generative model such as GAN. The content reminds me of Self Attention GAN which uses a similar non local block (self attention).
Reject. rating score: 1. rating score: 1. rating score: 3. <BRK>The paper additionally presents RANs (similar to GANs but with an AlgoNet embedded) and Forward AlgoNets (where the the AlgoNet is embedded in a feedforward net). I could not see however any such arguments in this paper.<BRK>The work that is discussed is very narrow in scope (see below). The paper concludes by describing versions of AlgoNet for various different algorithms. However, the paper does not go beyond conceptualizing how this might be done in a hand wavy manner.<BRK>Can you provide empirical comparisons with other differentiable renderers? The smooth renderer, and in particular the advantages it has over existing differentiable renderers, seem like the most important contributions of the paper (although they re relegated to the appendix).
Reject. rating score: 3. rating score: 3. rating score: 6. <BRK>The paper claims that for invertible neural networks, mathematical guarantees on invertibility is not enough, and we also require numerical invertibility. However, the decorrelation task introduced in section 4.2 is puzzling. Overall I believe the experimental section can be largely improved, and given that the motivation of the paper is nice and the paper is clearly written and nicely presented, it would be a shame to leave the experiment section as it is.<BRK>This paper analyses the numerical invertibility of analytically invertible neural networks (INN). The authors write in the abstract and conclusion that they show how to guarantee invertibility for one of the most common INN architectures. This work is interesting and could be important to many researchers working with INNs. However, I have some concerns regarding the experimental evaluation.<BRK>This paper points out invertible neural networks are not necessarily invertible because of bad conditioning. It shows some cases when invertible neural networks fail, including adding adversarial pertubations, solving the decorrelation task, and training without maximum likelihood objective (Flow GAN). The paper also shows that spectral normalization improves network stability. Update:After reading other reviewer s comment I agree with other reviewers that the experimental section is problematic. It seems to be unrelated with the theoretical results proposed in this paper.
Reject. rating score: 3. rating score: 6. rating score: 6. <BRK>This paper introduces Skip gram style embedding algorithms that consider attribute distributions over local neighborhoods. In this case, the AE process naturally goes back to plain network embedding, where DeepWalk and WalkLets are proposed for.<BRK>This paper proposed an attributed network embedding method by leveraging a node’s local distribution over attributes. I examine the results anyway, and I found the performance gain is very limited, and on some datasets, the proposed methods even perform inferiorly.<BRK>This manuscript introduces embedding algorithms that consider attribute distribution. In MUSAE, what is the intention that the tuples are added to different sub corpus for source and target nodes?
Accept (Poster). rating score: 8. rating score: 6. rating score: 3. <BRK>The authors discuss an interesting idea for memory augmented neural networks: storing a subset of the weights of the network in a key value memory. Could you clarify or include a reference? This is similar to having programs/subprograms in a classical computer. However, the authors should not forget to cite the original fast weight paper by Malsburg (1981). However, the von Neumann architecture assumes that the program and the data are stored in the same memory, which is not the case for what the authors propose (a significant limitation is that the vector size of the memory doesn’t match the number of network weights). Please correct! And one more thing: in the revised version they write "More recent works implement fast weight using outer products," citing papers from 2016 17, but this was actually first done in the 1993 paper they already cite. In Figure 5, what is the average performance supposed to show? The method is mainly tested on methods used to test MANNs. Please make this consistent. In figure 7 etc. it would be nice to have a title for the topmost subplot, too. Why?What drives the network to use new memory keys in this case?<BRK>I found the exact details of number of programs versus number of heads (and the type of those heads) a bit confusing. The IN also receives the recurrent controller output, and produces an interface vector which is used to read or write to the regular memory. The definition is sufficiently general to allow the computation between an interface vector and the regular memory to be done according to NTM/DNC/etc schemes. Experiments on continual learning, few shot learning and text based question answering back up the wide applicability of this technique, with many strong results including new SOTA on bAbI. It would be good to see a line added to the paper justifying this choice. This simplicity is a positive, and the authors make a convincing argument that NSM can be applied to any MANN solving a sufficiently complex problem. Combined with the learning curves / results in the various tables, it is to me clear that the model is performing as designed. The formatting of some figures and graphs could be improved:* Figure 1   the graph could use some more text labels, rather than mathematical notation which needs to be referred to below. I was not sure at first whether this should be interpreted as indicating something important. There are some other issues, such as the $r_t$ value which should really come from the main memory as a result of the interface vector. Text labels with the various arrows (eg "vector used to lookup in NSM", "used as network weights for Interface Network") may improve clarity.<BRK>Strong/Weak Points+ The idea of generalising NTMs to "universal" TMs is interesting in itself ...  ... however, the presented solution seems to be only half way there, as the memory used for the "program" is still separate from the memory the NUTM operates on. Hence, modifying the program itself is not possible, which UTMs can do (even though it s never useful in practice...)  The core novelty relative to standard NTMs is that in principle, several separate programs can be stored, and that at each timestep, the "correct" one can be read. The current paper does not try show the way there. The writing is oddly inconsistent, and important technical details (such as the memory read/write mechanism) are not documented. I would prefer the paper to be self contained, to make it easier to understand the differences and commonalities between NTM memory reads and the proposed NSM mechanism. RecommendationOverall, I don t see clear, actionable insights in this submission, and thus believe that it will not provide great value to the ICLR audience; hence I would recommend rejecting the paper to allow the authors to clarify their writing and provide more experimental evidence of the usefulness of their contribution. This seems pure speculation, not a fact.
Reject. rating score: 3. rating score: 3. rating score: 3. <BRK>* Table 1   How did the baseline algorithms perform on this task? While this idea has been explored in prior work (as noted in the related work section), the proposed idea seems like a useful contribution to the literature. The experimental results are quite strong. My main concern with the paper is a lack of clarity. I currently have enough reservations and questions (listed below) about the experimental protocol that I am learning towards rejecting this paper. * In equation 2, I think it d be clearer to write p^(1+\alpha). * In Section 4.3, it s unclear whether the physical robot was successful at solving the task. However, since a large fraction of my concerns were not addressed, so I am inclined with stick with my original vote to reject the paper. It seems like the method depends critically on the balance between these hyperparameters.<BRK>The paper compares to RND (a method that is representative of state of the art in exploration bonuses, I believe) and also to Skewfit, and outperforms these methods in two non standard environments: door opening and point mass navigation. While the I think the empirical contributions are solid, and authors provide code to reproduce the results, I found the paper a bit hard to follow and understand, and different subsections in the technical section (Section 3) did not seem to have a unified narrative running through them.<BRK>However, the theory seems a bit off and there are a few experimental details that make me hesitant to increase my score. It would be good for the authors to address this concern, given that the claim of the paper is that they are "maximizing the entropy of the visited states." Experiments:Can Table 1 be replaced with the learning curves? Can the authors summarize the difference between coverage and entropy in the main paper? Given that the authors did not use all 8 pages, it would be good to explain it there rather than in the Appendix. How sensitive is the method to the hyperparameter alpha? 2.Address the concerns raised over the experiments.
Accept (Poster). rating score: 8. rating score: 8. rating score: 6. <BRK>I have generally positive feelings about the paper. This is just a sequential application of two class of methods, and not what I would understand as a "unified framework" (which suggests some form of generalization or at least a closer interaction to me). To be honest, I do not like the way the authors frame their work (e.g.the way the method is motivated in Section 2.3 or calling it a "unified framework"), but the actual method they propose does make sense, the experimental evaluation is solid, and the results are generally convincing.<BRK>This paper compares to approaches to bilingual lexicon induction, one which relies on joint training and the other which relies on projecting two languages  representations into a shared space. It also shows which method performs better on which of three tasks (lexicon induction, NER and MT). Please address:  I would like more intuitive motivation for (6). do you have any analysis exploring what is shared?<BRK>In light of the problems with silver standard MUSE datasets (see also the recent work of Kementchedjhieva, EMNLP 2019), I would suggest to run BLI experiments on more language pairs: e.g., there are BLI datasets of Glavas et al available for 28 language pairs. Adding one such experiment would make the paper more interesting. For instance, English Chinese is included as a more distant language pair, and the combined method seems to work fine, but the reader is left wondering why that happens. Also, can we use the same self learning technique with the combined method here?
Accept (Poster). rating score: 8. rating score: 6. rating score: 3. <BRK>It is a simple yet novel way to incorporate a model, and I appreciate the thorough results with multiple baselines and additional ablations to show importance of each component of the method. The authors suggest back propagating through a learned dynamics model and provide a derivation on monotonic improvement of the proposed objective. They show increased sample efficiency, asymptotic performance matching model free methods, and ability to scale to long horizons. They provide bounds on the error of the gradient when using the learned model and Q function and the total variation distance between policies trained using true dynamics versus the learned dynamics model. This leads to a theorem showing monotonic improvement of the policy under their algorithm, MAAC. The work contains both theoretical and empirical results on back propagating through a learned dynamics model compared to other model based and model free methods. I will increase my score if these concerns are addressed.<BRK>This paper presents MAAC, a model based reinforcement learning algorithm which makes use of path wise derivative to optimize the policy. The dynamics model is learned by the same method in PETS. The theory shows that if learned model and learned Q function have similar derivative as the real one, the improvement of policy can be lower bounded. Experiments show that MAAC achieves the state of the art and can be further improved by adding MPC. The emperical results are very good and are claimed to be the new state of the art. What s underlying state distribution of H(pi_theta)? 5.Does MAAC work for Humanoid?<BRK>#rebuttal responsesThanks for the clarification! However, I will keep my original score for these reasons:(1) Only 3 random seeds are used for each environment, which is not convincing as the variance of MAAC is large in some figures. (2) Baselines are only trained with 10^5 steps and do not converge. Thus it not fair to say that MAAC matches the asymptotic performance of model free algorithms. The theoretical guarantee of the error of the model based gradient is presented. However, the paper is badly written. First of all, the authors claim that the pathwise derivate method is applied to optimize the objective function. The paper does not clearly show the advantage of each component:(1) I want to see the experimental results of MAAC optimizing the objective without the entropy. (2) The SVG(1) algorithm should be compared as a baseline as the SVG(1) also uses the gradient of the learned model to optimize the policy. I am happy to increase my score if the authors justify these questions.
Reject. rating score: 3. rating score: 6. rating score: 8. <BRK>The VTAB judges whether the learned representation is good or not by adapting it to unseen tasks which have few examples. This paper conducts popular algorithms on a large amount of VTAB studies by answering questions: (1) how effective are ImageNet representation on non standard datasets? (2) are generative models competitive? The authors need to compare the conclusion obtained with other works. It seems that there is no new founding in this paper.<BRK>The authors define a good representation as one that can adapt to unseen tasks with few examples. The benchmark tasks are constrained to unseen tasks, which seems obvious but is often violated when evaluating representations  It does a good attempt at covering a large spectrum of realistic domains (19 tasks!) Extensive study is conducted, covering the published state of the art methods in each domain. The study leads to interesting finding, such as promising results on self supervision and negative results on generation. Overall, I believe the paper is an important contribution as it provides some interesting analysis of the current state of the art for visual representation learning.<BRK>This is problematic since an easier way of improving results on VTAB is to just collect more data/ data that is more representative of evaluation tasks. The paper presents a new benchmark (VTAB) to evaluate different representation learning methods. I feel that a much better way of defining the benchmark would be to restrict representation learning to Imagenet. 1.The experimental evaluation in this paper is outstanding. I m also not convinced by some of the choices made in designing the benchmark (Such as the models can be pre trained on any data as long as there is no overlap with the evaluation data). However, I don t see how this can be avoided for an open ended benchmark. I m not aware of any existing work that compares various representation learning methods on a diverse benchmark fairly and comprehensively, and I m sure I ll be using the results in this paper to support my claims in the future. ### Questions for Authors 1.
Accept (Poster). rating score: 8. rating score: 8. rating score: 3. <BRK>The paper proposes a new way of estimating treatment effects from observational data, that decouples (disentangles) the observed covariates X into three sets: covariates that contributed to the selection of the treatment T, covariates that cause the outcome Y and covariates that do both.<BRK>Summary:   The authors consider the problem of estimating average treatment effects when observed X and treatment T causes Y. Observational data for X,T,Y is available and strong ignorability is assumed. I vote for accepting the paper.<BRK>The paper proposes an algorithm that identifies disentangled representation to find out an individual treatment effect. A very specific model that tries to find out the underlying dynamics of such a problem is proposed and is learned by minimizing a suggested objective that takes the strengths of previous approaches.
Accept (Poster). rating score: 8. rating score: 6. rating score: 6. <BRK>In general, this is paper is an interesting paper. And there are two typos. This motivate them to decouple representation learning and classification. Experiment:Since this paper decouples the representation learning and classification to "make it clear" whether the long tailed recognition ability is achieved from better representation or more balanced classifier, I recommend that authors show us some visualization of the feature map besides number on performance. The authors conduct experiment with ResNeXt {10,50,101,151}, and mainly use ResNeXt 50 for analysis.<BRK>The paper considers the problem of long tailed image classification, where the class frequencies during (supervised) training of an image classifier are heavily skewed, so that the classifier underfits on under represented classes. On a validation set?<BRK>This paper proposes to tackle long tailed classification by treating separately the representation learning and the creation of a classifier for test time. The paper also provides interesting insights about long tailed recognition in general like the effect of the different samplings with the proposed method. It is also important to know it as it has an impact on how useful is this approach in practice. Because if the coefficients are very different for all the datasets, then the method requires a validation set to find this hyperparameter.
Reject. rating score: 3. rating score: 3. rating score: 3. <BRK>Summary:This paper gives a new MLP formulation and architecture that is ostensibly suited for data where the label s dependence on the input feature has form of sparsity. The paper reports the performance of this architecture on a few standard datasets and outperforms other baselines. The empirical results also do not look particularly convincing, as there is just a few percentage improvement over MLP (except CIFAR10). If it is different, the entire rationale of Group select breaks down, so I am assuming it is the same, but this must be clarified. 4.The visualisation for MNIST Group select seems very limited compared to what is there in the architecture.<BRK>This paper studies supervised classification problems where features are unstructured. For these problems, the authors propose a new neural network architecture that first reorganize the features into groups, then builds feed forward networks on top each group, and finally aggregate the hidden nodes of each group to produce the final output. Empirical and ablation studies are conducted to show the performance of this approach. My detailed comments are as follows. If the feature is very high dimensional, it seems that the normalization factor in equation (2) might be hard to compute. 3.In terms of the experiments, it seems that the results are very similar to that of the MLP, although slightly better. Moreover, the datasets used seem small, with no more than 10^6 data points in all datasets. The largest dataset is the Permutation invariant CIFAR 10, which has 50000 data with 3072 features.<BRK>This paper addresses the problem of learning expressive feature combinations in order to improve learning for domains where there is no known structure between features. Results are shown as comparisons on 5 real world datasets, and intuitive visualizations on two other datasets. Considering that the premise of the paper is that MLP’s are not good enough when dealing with data in which the relationships between features are unknown, it seems like these are definitely not good datasets on which to demonstrate this notion of “there has been little progress in deep reinforcement learning for domains without a known structure between features.” The MNIST visualization of group select felt informative, but the XOR example for grouping visualizations seemed too easy. It seems like different sizes per group would be a more realistic expectation, and that perhaps this should be worked into the algorithm. Minor: Equation 8 did not fully make sense to me.
Accept (Poster). rating score: 8. rating score: 8. rating score: 3. <BRK>SummaryThe surprising generalization properties of neural networks trained with stochastic gradient descent are still poorly understood. DecisionThe present work proposes a plausible, simple mechanism that might be contributing to the generalization of Neural Networks trained with gradient descent. Overall, the paper is of high quality and provides an interesting perspective on an important topic, which is why I think it should be accepted. Questions for the authorsThe coherent gradient hypothesis seems equally valid in the absence of stochasticity. However, the latter is often seen as an explanation of the generalization performance of SGD.<BRK>Overall, the paper is of very high quality, and I recommend its acceptance. One criticism perhaps is whether these results are sufficiently significant. It s clear that the authors themselves are aware that it s of interest to extend their results to more realistic settings, and regardless I think that this paper stands alone as is and should be accepted to ICLR. For instance, Figure 1 provides compelling evidence for the coherent gradient hypothesis and in particular motivates the way phenomenon of early stopping arises a natural consequence.<BRK>The paper studies the link between alignment of the gradients computed on different examples, and generalization of deep neural networks. Also, citing in this context Keskar is misleading. Hence, "Coherent Graident Hypothesis" should be mostly considered an explanation for why simple examples are/simple function are learned first. Similarly, [3] proposes that SGD learns functions of increased complexity. A detailed comparison between these hypotheses is needed. However, [5] should be cited. I would recommend moving half of them to the Supplement? Please note there is very little work on training with GD large models.
Reject. rating score: 3. rating score: 3. rating score: 6. <BRK>This paper studies the noise injection as defense methods against adversarial perturbations. Thank you for the clarification on (3 1) in the initial review. This paper should be rejected because (2 1) some conclusions are not well supported by the experiments, and (2 2) the paper fails to demonstrate the novelties and their importance of some conclusions.<BRK>The experiments and reasoning of the paper should be sufficient without citing other work in the author response. Specifically, for standard methods for crafting adversarial examples, the authors evaluate whether the crafted examples remain adversarial under (stochastic or deterministic) transformations such as Gaussian noise and SVD compression. The authors argue that different transformations have a similar impact on the perturbed inputs. c) The instability of adversarial perturbations can be explained by a larger gradient norm and Hessian spectrum (figure 2, 3).<BRK>The paper is quite well justified and addresses a very interesting question by unifying the existing results of the literature in one place. First of all, comparing the accuracy robustness trade off of different methods are useful. The perturbation analyses along with the results in Figures 2, 3 are novel and can be useful for further theoretical investigation of the source of instability of adversarial images. * It should be made clear in the text that methods like randomized smoothing are designed for the purpose of  certified  accuracy and being robust against attack sizes more than the certification threshold was not part of the method s contributions. * Fig 1 should be larger.
Reject. rating score: 1. rating score: 1. rating score: 3. rating score: 3. <BRK>The mathematical notations are also inconsistent throughout different places in the paper. The extensive literature of modelling time series with seasonality trends, both in the statistics and the machine learning community, is severely under represented in the motivations and related works.<BRK>4.ATR CSPD is undefined in the abstract. The paper proposes to use an auto encoder to find the "main pattern" within each seasonal window, and to use total variation penalty (l1 norm on the change) of the hidden state in the auto encoder to encourage a smooth state sequence which allow breaks. While the proposal is sensible and the paper is reasonably readable, I find the paper lacking in several respects, and recommend to reject it. Various nonlinear DL approaches to TS with seasonality have also started to appear. 2.Baseline methods for detecting seasonal patterns are naive   clearly applying methods that are not aware of seasonality will fail when there is strong seasonal components.<BRK>The paper investigates the important problem of detecting changes in seasonal patterns, and proposes ATR CSPD to learn a low dimensional representation of the seasonal pattern and then detects changes with clustering based approaches.<BRK>I have a few concerns about some technical details of the paper, as explained below:1) The paper motivated the new model with difficulty in detecting change points in seasonal time series. It would be nice if the authors can make a connection between the two. Fast gradient based algorithms for constrained total variation image denoising and deblurring problems. Hence it is not obvious to the reader that the proposed model outperforms the baseline. 4) In the appendix B, the authors explained the architecture of the autoencoder used in the paper.
Reject. rating score: 1. rating score: 3. rating score: 8. <BRK>This paper studies the mildly over parameterized neural networks. In particular, it shows that when the width is at least O(sqrt{n}) where n is the number of samples, PDG fits the training data. For 2 layer network, that means the first layer weights are trainable. The quadratic activation is probably the main limitation of this paper, and fixing the last layer is the second major limitation. It also only requires O(sqrt{n}) neurons. The paper tries to compare with “a series of work (Du et al.(2019); Allen Zhu et al.(2019c); Chizat & Bach (2018); Jacot et al.(2018)” which uses a large number of neurons, and thus claims “mild over parameterization”. It is not sure how useful this result will be, and how can this be extendable. The 3 layer proof may still be interesting.<BRK>I’m not giving a high score for now but I will reconsider my review once I hear back from the authors. Comparison to Oymak & Soltanolkotabi 2019: my understanding is that this paper prove convergence to a global minimum for a neural network where the number of parameters is only twice the number of datapoints so aren’t your results “worse” in that sense? The text in the related work seem to say the opposite, so I’m rather confused by your statement, please clarify. In fact the paper by Soltanolkotabi et al (see reference below) proves this for a similar network with a quadratic activation function and arbitrary data. Theoretical insights into the optimization landscape of over parameterized shallow neural networks. Three layer neural net1) This network uses activation function of the form x^p. 2) Limitation quadratic activation function.<BRK>The paper studies the amount of over parameterization needed for a quadratic two (three) layer neural network to memorize a separable training data set with arbitrary labels.
Reject. rating score: 3. rating score: 3. rating score: 8. <BRK>In this paper, a new network architecture called EVPNet was proposed to improve robustness of CNNs to adversarial perturbations. The proposed network and the methods are interesting, and provide promising results in the experiments. However, there are several issues with the paper:  The authors claim that Gaussian kernels are replaced by convolution kernels to mimic DoGs. It is also claimed that “a 1 × 1 conv layer, can be viewed as a PCA with learnable projection matrix”. However, this statements is not clear.<BRK>This paper presents a SIFT feature inspired modification to the standard convolutional neural network (CNN). Specifically the authors propose three innovations: (1) a differences of Gaussians (DoG) convolutional filter; (2) a symmetric ReLU activation function (referred to as a truncated ReLU; and (3) a projected normalization layer. The paper makes the claim that the proposed CNN variant (referred to as the EVPNet) demonstrates superior performance as well as improved robustness to adversarial attacks. Beyond these issues, the paper is relatively clear in the presentation of the material. Can this architecture be competitive with the state of the art? The current paper in it s current Most of the results relate to the claim that the proposed model is robust to adversarial examples.<BRK>This paper proposes a network model named EVPNet, inspired by the idea scale space extreme value from SIFT, to improve network robustness to adversarial pertubations over textures. To achieve better robustness, EVPNet separates outliers (non robust) from robust examples by extenting DoG to parametric DoG, utilising truncated ReLU, and then applying a projected normalisation layer to mimic PCA SIFT like feature normalisation, which are the three novelties that the authors claim in this paper. 2.Truncated ReLU is a modified ReLU. The authors state only its difference to global average pooling but not to batch normalisation which should be a better comparison. There is a more "non parametric" block.
Accept (Poster). rating score: 8. rating score: 6. rating score: 3. <BRK>This paper proposed a learning algorithm to recover the events of using an appliance and as well as the location of the appliance in a home by using smart electricity meter and a motion sensor installed a home. In the model, the input is a window of electricity energy consumption and context and the output of the model is the location collected by the motion sensor. This provide an alternative approach to understand how energy was used in a house and so could be used potentially for energy efficiency and malfunction detection of appliances. The data is also new to the community.<BRK>Authors proposed a multi modal unsupervised algorithm to uncover the electricity usage of different appliances in a home. The detection of appliance was done by using both combined electricity consumption data and user location data from sensors. The unit of detection was set to be a 25 second window centered around any electricity usage spike. Authors used a encoder/decode set up to model two different factors of usage: type of appliance and variety within the same appliance. This part of the model was trained by predicting actual consumption. Then only the type of appliance was used to predict the location of people in the house, which was also factored into appliance related and unrelated factors. Locations are represented as images to avoid complicated modeling of multiple people.<BRK>Comments from other reviewers and revisions have deliberately not been taken into account. ### Conclusion  This needs to be developed a bit more: what are some downsides to your method? ## Overall**Summary**The authors introduce a new method for classifying which appliance was turned on by looking at the change in total household electricity consumption and tracking people s coarse positions in the house. And what are future directions based on this work? What does energy consumption have to do with classifying which appliance was turned on? Is this a training set vs test set artifact? The rest are minor issues. I could imagine, since the location is noisy, a person could jump in and out of the visible area and cause training instability. But shouldn t the signal add up to explain the entire scene?
Accept (Spotlight). rating score: 8. rating score: 6. rating score: 6. rating score: 3. <BRK>This also makes me ask why didn t you compare to NALU on more tasks presented in the paper? The paper presents a strong case that the new models outperform NALUs in a few metrics: rate of convergence, learning speed, parameter number and model sparsity. I am quite fond of the analysis and the informed design of the two new models, as well as the simplicity of the final models which are fairly close to the original models but have been shown both theoretically and practically that they work. The change makes it hard to compare with the results in the original paper, and you do not present the reason why.<BRK>The authors extend the work of Trask et al 2018 by developing alternatives to the Neural Accumulator (NAC) and Neural Arithmetic Logic Unit (NALU) which they dub the Neural Addition Unit (NAU) and Neural Multiplication Unit (NMU), which are neural modules capable of performing addition/subtraction and multiplication, respectively. The authors show that their proposed modules are capable of performing arithmetic tasks with higher accuracy, faster convergence, and more theoretically well grounded foundations. The new modules modules are relatively novel, and significantly outperform their closest architectural relatives, both in accuracy and convergence time. For these reasons I believe this paper should be accepted.<BRK>What would be the use case for such a unit? The NMU on the other hand simply learns a product of affine transformations of the input. This choice prevents the model from learning divisions, which the authors argue made learning unstable for the NALU case, but allows for an a priori better initialization and dispenses with the gating which is empirically hard to learn. The paper is mostly well written (one notable exception: the form of the loss function is not given explicitly anywhere in the paper) and well executed, but the scope of the work is somewhat limited, and the authors fail to properly motivate the application or put it in a wider context.<BRK>This paper aims to address several issues shown in the Neural Arithmetic Logic Unit, including the unstability in training, speed of convergence and interpretability. The improvement for the NAC addition is based on the analysis of the gradients in NALU. The proposed neural addition unit uses a linear weight design and an additional sparsity regularizer. From the experimental perspective, it seems to work well. I think the paper can be made more self contained.
Reject. rating score: 3. rating score: 6. rating score: 6. <BRK>In this work authors present a regularized, variational autoencoder method for speech synthesis. To endow the latent space with more capacity, the authors employ a modified variational autoencoder objective, which uses a learnable Lagrange multiplier to impose a capacity limit on KL divergence between latent posterior and prior. In numerical experiments the authors evaluate their approach on a number of speech synthesis tasks involving same text prosody transfer, inter text style transfer, inter speaker prosody transfer. They also analyze speech samples generated from latent samples drawn from the prior. The paper is well written and easy to follow, but the significance of the main contribution of the paper remains unclear to me. The authors propose to use an augmented standard VAE loss with a capacity hyperparameter and a Lagrange multiplier, but such modifications have been used before and it is not clear to me where is the novelty in there? However, the application alone is in my opinion not a contribution that is significant enough for publication.<BRK>1.Summary: This paper proposes Capacitron, a conditional variational latent variable model for TTS which allow for controllable latent variable capacity. They optimize the Lagrangian dual of the ELBO and restrict the capacity of the rate term through a learnable, non negative multiplier. They demonstrate the effectiveness of their approach on a range of TTS tasks such as same text prosody transfer and inter text style transfer, and provide extensive analyses on their latent variable capacity (in addition to comparisons to non variational approaches based on Tacotron). From what I understood, this work’s contribution is not so much the introduction of the conditional generative model but identifying the effects of controlling the rate term in the ELBO decomposition.<BRK>This paper introduces a new embedding method in expressive TTS by focusing on the capacity of hidden variables introduced in variational autoencoder. Such a method is supported by the KL divergence and lower bound theory and the paper well formulates/describes the capacity concept. In the experiments, the proposed method is compared with other conventional methods with well designed subjective and objective metrics and shows the effectiveness in terms of the style transfer. The paper is well written in general. Another discussion of this paper is that it is not explicit to provide the meaning of the latent variables (this is a general issue in VAE) and I could not be fully convinced by the discussion and analysis based on the latent variable.
Accept (Poster). rating score: 6. rating score: 6. rating score: 6. <BRK>This paper studies a very interesting phenomena in machine learning called VSP, that is the output of the model is highly affected via the level of missing values in its input. The authors demonstrate the existence of such phenomena empirically, analyze the root cause for it theoretically, and propose a simple yet effective normalization method to tackle the problem. Several experiments demonstrate the effectiveness of this method. I admit that I’m not an expert in the field of recommendation but still think that more recent, and powerful baseline algorithms should be applied on to further demonstrate the true effectiveness of Sparsity Normalization.<BRK>This paper provides a novel solution to the variable sparsity problem, where the output of neural networks biased with respect to the number of missing inputs. The authors proposed a sparsity normalization algorithm to process the input vectors to encounter the bias. The paper describes a clear and specific machine learning problem. Therefore, my opinion on this paper leans to an acceptation. 2) In the experiments, you compare your model with zero imputation (Please correct me if w/o SN is not zero imputation). I m interested in knowing whether filling with mean/median values work with these datasets. 3) In section 4.5, you mentioned that "SN is effectiveeven when MCAR assumption is not established". However, I m still not clear about the reason. 4) Does your model assume all input values are numerical but not categorical?<BRK>Zero imputation is studied from a different view by investigating its impact on prediction variability and a normalization scheme is proposed to alleviate this variation. This normalization scales the input neural network so that the output would not be affected much. Maybe it attributes to the missing features. It is argued in the paper that with high dimensional data, your algorithm is more acceptable, but how would it be with in other cases?
Accept (Poster). rating score: 6. rating score: 6. rating score: 6. <BRK>The method is clearly positioned with respect to previous work (in particular using \ell_2 regularization of the gradients)   Experiments demonstrate the effectiveness fo the method.<BRK>It will be good if the authors can discuss in what practical scenario their algorithm can be applied.<BRK>I think the authors do address some questions but the paper still has some weakness in terms of performance.
Reject. rating score: 3. rating score: 6. rating score: 6. <BRK>This paper proposes and analyzes a new loss function for linear autoencoders (LAEs) whose minima directly recover the principal components of the data. The core idea is to simultaneously solve a set of MSE LAE problems with tied weights and increasingly stringent masks on the encoder/decoder matrices.<BRK>This paper proposes a new loss function to compute the exact ordered eigenvectors of a dataset. The loss is motivated from the idea of computing the eigenvectors sequentially. A proof of the correctness of the algorithm is given, along with experiments to verify its performance. The loss function proposed in the paper is useful, and the decomposition in Lemma 1 shows that it is not computationally expensive. The main result (Theorem 2) is stated twice.<BRK>This paper proposes a new loss function for performing principal component analysis (PCA) using linear autoencoders (LAEs). With this new loss function, the decoder weights of LAEs can eventually converge to the exact ordered unnormalized eigenvectors of the sample covariance matrix. The main contribution is to add the identifiability of principal components in PCA using LAEs and. This paper is well presented. It s better to add experimental results on big data sets with larger dimension.
Reject. rating score: 3. rating score: 6. rating score: 6. rating score: 8. <BRK>This paper extends the unpooled mention pair model of annotation (MPA) (Paun et al., 2018b) with the hierarchical priors (e.g., mean and variance) on the ability of the annotators. To control the sparsity of the dataset, the authors split annotations from larger player workloads into smaller batches, and assumed that each batch was produced by a different player. This paper also includes a discussion about the inferred community profiles. The comparison with the traditional approaches that consider communities showed that the proposed method is comparable to the traditional approaches. I have two questions here: why is it effective to consider community for reducing the problem of the sparsity? This treatment introduces quite a few shadow users whose capabilities are exactly the same, and deviates from the reality of the user community. Does this treatment favor the proposed method over MPA more than necessary? I am also wondering why evaluating portions of the dataset where annotations were made by  sparse  users would not work to highlight the effectiveness of the proposed method for sparse users.<BRK>This paper presents a method to get a more qualified output from multiple crowdsourced mention pair annotations for coreference annotation. The work in interesting and much needed if SOTA indeed is from 2015, but the CommunityMPA results are not strong enough for me to recommend full accept. Caption of Table 1: (Moreno et al., 2015) > Moreno et al.(2015)Table 1: Boldface best result per dataset Small commentsFigure 4 is an interesting insight into the four community profiles (but why not include adversaries?)<BRK>The paper presents an interesting crowdsourcing approach as an alternative to expert annotation. Different from the conventional crowdsourcing approaches that concentrate on classification tasks with predefined classes, the proposed Community MPA, as an extension of MPA can address anaphoric annotation well where coders relate markables to coreference chains whose number cannot be predefined. * The paper is an incremental work to the existing one by Paun et al.(2018b).It considers the underlying communities of different workers, which may lead to better annotations when the data is scarce. It may take quite a while for readers with only machine learning background. * Authors claimed that “We let the number of communities grow with the data, a flexibility that we achieve using a Dirichlet process mixture,” but this is not clear in the modeling in section 2.1. Which parameters control such increases and how they were adjusted? More explanations are needed.<BRK>The paper proposes an improvement over the mention pair model for anaphoric annotation by Paun et al.to mitigate the effects of sparsity that are inherently present in some crowd sourcing environments. THe coverage of related work is extensive and the contribution placement apt.
Reject. rating score: 1. rating score: 3. rating score: 8. <BRK>Summary: This paper introduces a variation on measuring catastrophic forgetting in sequential learning at the representation level and attempts to resolve forgetting issue with the help of a meta learner that predicts weight updates for previous tasks while it receives supervision from a multi task learner teacher. The new method is evaluated on sequences of two tasks while task 1 data remains available at all times to the teacher. Regarding the task number, this is indeed not an issue for your approach with 2 tasks. "Packnet: Adding multiple tasks to a single network by iterative pruning." (+): Tackling continual learning from a meta learning approach is novel and not yet well explored. Pros:(+): This paper is very well written and very well motivated. The experiments are accurately described and performed but authors have only considered sequence of 2 tasks which is far from being considered as a continual learning setting. Another drawback in the experiments is about the baselines. I would be happy to change my score if authors can address the above concerns about considering distinguishing multi task learning from continual learning and providing a realistic evaluation setup with more than 2 tasks and comparison with current state of the art methods. So if it is not fair, it would be for those methods given the computational expenses of this paper. has very similar performance to EWC in their paper. We have cited this work already. So it has its own advantage.<BRK>###  Summary  The paper demonstrates that neural networks that appear to have forgotten an old task still contain useful information of that task in their representation layers. 2  While the underlying idea in the paper for learning an update rule is promising and sound, the paper is missing baselines that also use the meta training dataset in some way. In this paper, the meta train and meta test settings are too similar to see if that is the case. 1  The paper claims that catastrophic forgetting in a neural network is partly due to miscalibration of the last layer, and the representation layer of the neural network still contain useful information. A more interesting question is the difference between the accuracies when the network has been trained on Task B till convergence. The empirical results, consequently, are not very convincing. ### Additional evidence that can change my evaluation 1  Showing that the meta learned update rule can be applied to different architectures/non linearities/datasets (Train on one dataset, test on another). ### Minor comments that did not play a part in my decision, but should be addressed nonetheless. The paper, in its current form, needs to be proofread and reorganized. Inspired by this fact, we propose a novel meta learning algorithm that tries to mimic a multi task teacher network’s representation, an offline oracle in our sequential learning setup, since multi task learning has simultaneous access to all tasks whereas our sequential learning algorithm only has access to one task at a time." I hope that authors would fix these issues during the writing process. If the update rule is tied to a data distribution, it is not extremely useful. In the public discussion phase, the authors addressed both of my concerns. However: 1. The baselines perform very close to the proposed method at a fraction of meta training cost. Moreover, the authors do not combine these baselines with existing methods to mitigate interference (Such as LwF) which can easily be done (and would probably increase the performance of the baseline noticeably)2. The meta training and meta testing datasets are still too similar in the new experiment (A downscaled TinyImagenet is very similar to CIFAR). R2 s review (and the response to the review) also highlights an important problem   the authors didn t tune the optimizers used by the baselines on the meta training dataset. As a result, I m keeping my initial score.<BRK>Some discussion to this end I think would be helpful. I am not docking this work for not doing this type of generalization work though as we must start someplace and meta training on similar data distributions is a logical place to do so. The one piece of motivation I did not fully understand is why not forgetting on the feature space is so important. Overall:I would recommend this paper for acceptance as it presents an interesting approach to solving the catastrophic forgetting issue with a compelling set of diverse experiments. I find the distillation based learning to be a clever alternative to the computationally heavy optimizing over past performance. Experiment 1 demonstrates the principles. In my opinion you should caution the reader given the meta train, meta test split. I believe the authors are aware of this though as this issue is addressed in experiments 2 and 3. Experiments 2 and 3 are interesting and demonstrate the method on a more realistic setting. Figure 6 is not referenced in the text. Overall, I believe the baselines could be made considerably stronger. Where there learning rate schedules also tuned? Questions / concerns: Cost of running this not discussed. I am curious in particular One motivation for meta learning update rules in this way is that this cost can be amortized ahead of time and the learned update rule can transfer to new very different tasks.
Accept (Poster). rating score: 8. rating score: 6. rating score: 6. <BRK>Weaknesses:It took me a lot of effort to figure out that the transformer XH is only applied to the bridge questions, and part of the overall gains are due to a better retrieval pipeline on the comparison questions. Would be good to clarify this.<BRK>The paper is proposing an extension of the Transformer matching and diffusion mechanism to multi document settings.<BRK>This paper introduces the Transformer XH model architecture, a transformer architecture that scales better to long sequences / multi paragraph text compared to the standard transformer architecture. The paper tackles an important problem (learning from long sequences) and achieves good empirical results on HotpotQA.
Accept (Spotlight). rating score: 8. rating score: 6. rating score: 6. <BRK>** SummaryThe paper proposes a novel zeroth order algorithm for high dimensional optimization. Can I make the probability arbitrarily close to 1? The proposed algorithms are tested on synthetic optimization problems and in a few Mujoco environments for policy optimization.<BRK>In summary, I think this is a good paper and tend to accept it. The theoretical analysis of the main results is based on a geometric perspective, which is interesting.<BRK>This paper focuses on derivative free, or zero th order, optimization. The paper proposes an algorithm for "gradientless" descend. Strengths:+ The derivations and the theorems are non trivial.
Reject. rating score: 1. rating score: 6. rating score: 6. <BRK>The paper proof properties of counter machines that have in recent work be suggest that LSTMs can model those. The papers starts to mentions related work that relates automaton s and counter machines with LSTMs. They provide mostly empirical evidence that some behaviour is related to performance seen in LSTMs, GRU, etc. The paper makes then the point to take  the counter machine as a simplified formal model of the LSTM. However, I would read Merrill 2019 that counter machines could be model via LSTMs but are not limited or they are not an upper bound what they can compute. The authors does some proves on counter automata and hopes to gain insights into the properties of LSTMs used for NLP or semantic analysis and this would provide insights for the use in NLP. It seems to me that the paper claims that counter automatons are an upper bound for the computation power of LSTMs. That one can use LSTMs to compute languages that counter automata can do too means not that they could do more.<BRK>Motivated by a link between LSTMs and counter machines (suggested by recent work, e.g.Merrill, 2019 et al.), this paper studies the formal properties of counter machines (and LSTMs by extension) as grammars, in hopes of discovering why LSTMs perform particularly well in language tasks despite having no obvious hierarchical structure. It shows that: (1) many variants of counter machine converge to the same formal language, (2) the counter languages are closed under common set operations (e.g.intersection, union, and complement), (3) counter machines are incapable of evaluating boolean expressions, and (4) only a weak subclass of CLs are sublinear (and most are not). As this paper is far from my area of expertise, I m willing to change my score based on my co reviewers.<BRK>This is motivated by recent indications that LSTMs have comparable expressivity to counter machines, so that the formal properties of these machines might provide indirect insights into the linguistic suitability of LSTMs. Nonetheless, I think that this paper, albeit a bit of a gamble, would make for an interesting addition to the program. Without this link, the value of the paper is unclear. The semilinearity result is less impressive because of how limited the machines are that it applies to, and I m not sure that the proof provides a good basis for generalization to more complex machines. 3) No investigation of linguistically important string languagesAs the authors make claims about linguistic adequacy, it is surprising that there is no discussion of TALs, MCFLs or PMCLFs. It should also be possible to generate the linguistically undesirable MIX language, which is a 2 MCFL but not a TAL. Minor comments   As noted in my SCiL review, your definitions still differ from those of Fischer et al.1968.What is the reason for this? The CL can do that.
Reject. rating score: 3. rating score: 8. rating score: 8. <BRK>Cohen et al.recently proposed the "Gauge equivariant CNN" framework for generalizing convolutions to arbitrary differentiable manifolds. The sphere is the simplest natural non trivial manifold to try out gauge invariant networks on, and spherical CNNs have several applications. However, other than the details of the interpolation etc., there is really very little in this paper that is new relative to the original paper by Cohen et al., it reads a bit more like an extended "experiments" section. Unfortunately the experimental results are not all that remarkable either, probably because the tasks are relatively easy, so other SO(3) equivariant architectures do quite well too.<BRK>The paper proposes SO(3) equivariant layer, derived from the recently introduced Gauge equivariant CNN framework. Do you have any explanation for this? For the most part, the paper is very clearly written despite the challenging and technical nature of the topic. However, the experiments, while satisfactory, are not impressive: one issue is that the results are mostly compared to the results of two relevant papers by Cohen and colleagues.<BRK>(1.1) The authors could explain more about why we would want to consider tensor features. 3.They conducted experiments on different datasets, including the MNIST dataset. They achieved good results comparing to baseline spherical CNNs. However, the advantage of this method over S2CNN can be further elaborated, as S2CNN already achieved high accuracy; it seems like the one improvement is the complexity (improved from S2CNN s $O(N \log N)$ to their model s $O(N)$), but the reduction of complexity is not significantly reflected in the training time per epoch (from 380s to 284 s).
Accept (Poster). rating score: 8. rating score: 6. rating score: 3. <BRK>The  paper proposes a new approach to multi actor RL, which ensure diversity and performance of the population of actors, by distilling the policy of the best performing agent in a soft way and maintaining some distance between the agents. The authors show improved performance over several state of the art mono actor algorithms and over several other multi actor RL algorithms.<BRK>The authors propose another method of doing population based training of RL policies. The paper claims that a natural approach is to have a chief job periodically poll for the best worker, then replace the weights of each worker with the best one. In its place, the authors propose a soft update in the chief. Instead of replacing the parameters exactly, worker i s loss is then augmented by beta * D(pi_i, pi_B), where D is some distance measure that is measured over states sampled from the replay buffer. The "soft" update encourages individual workers to match pi_B without directly replacing their parameters, which maintains diversity in the population. I thought this paper was interesting, but thought it was strange that there were very few comparisons to other population / ensemble based training methods.<BRK>Maintaining a population of agents can help mitigate issues due to convergences to local optima. But the technical innovation is fairly modest. The proof of guaranteed improvement largely follows the proof from other trust region methods, by replacing the old policy with the best policy from the previous iteration. The experiments did compare with a number of previous algorithms, but the comparisons are all to algorithms that train a single agent at a time, and no comparisons are made to other distributed RL algorithms. Including some comparisons to other distributed methods can help strengthen the claims in favour P3S. But with more thorough evaluation and polishing, I think this work can make for a strong submission. It might also be informative to compare the wall clock time of the different methods, as well as including comparisons to other distributed frameworks. Some experiments to show how performance scales with the number of learners can also be helpful.
Accept (Poster). rating score: 8. rating score: 8. rating score: 3. <BRK>I think the paper should be accepted for the following reasons: 1.<BRK>I encourage the authors to have this disclaimer in the end of the paper so that the community does not falsely conclude that MAML cannot do rapid learning. All these datasets are artifically created from the same dataset and hence it might be very easy to reuse features to get good performance.<BRK>After rebuttal:I do think it s important to also have that kind of analysis works. My main concern is with how ANIL and NIL are introduced as new algorithms and not just an ablation of the MAML method. This paper is exploring the importance of the inner loop in MAML.
Accept (Poster). rating score: 8. rating score: 6. rating score: 6. <BRK>I am in favour of this work acceptance, overall the paper introduces very interesting insights to explain the usefulness of self training for auto regressive generation tasks and could inspire future work along this line in designing better ST algorithms and/or adoption of self training in low resourced generation tasks.<BRK>This paper presents a self training approach for improving sequence to sequence tasks. Finally, this paper reports empirical results (on WMT 2014 English German, FloRes English Nepali, and English Gigaword (summarization task)) of self training strategies presented in this paper. This paper is well structured and well written. It is interesting to see the improvements of the sequence to sequence tasks by using the self training approach. (2) We can view the self training strategies presented in this paper as regularization methods.<BRK>This paper investigates why self training helps in machine translation and text summarization tasks, identifying that auxiliary noise can amplify the benefits of this process. The paper is well structured and clearly written, and conducts a fairly thorough analysis of the issue at hand. Some mathematical analysis would be nice to have here as well.
Reject. rating score: 3. rating score: 3. rating score: 8. <BRK>This paper proposes a method of learning loss functions in addition to the learning of predictors. The design of the surrogate loss is problem dependent, and handcraft is required. However, the proposed method in this paper seems an ad hoc approach rather. See the following paper for details. 101, No.473.On the other hand, the current approach does not have such a theoretical guarantee for each learning problems. This is not a practical operation in the learning algorithm<BRK>In this paper, the authors propose to learn surrogate loss functions for non differentiable and non decomposable loss. 2.Learning a unified loss for different tasks is interesting. The benefit of the trade off is not systematically evaluated. Or, does it have the same differentiable form for different tasks? Please provide the details of the function g for the different losses in the experiments. 5.The time complexity analysis treats extracting function g as a black box function. However, the complexity of function g depends on the tasks.<BRK>In this paper authors propose to jointly optimize a critic, that estimates some non differentiable objective (or an objective with intractable derivatives and/or derivatives that are 0 almost everywhere). I find it a bit disappointing, that authors did not try to analyse trained models, it would be invaluable to see what kind of aggregation g and h came up with, that is well aligned with losses such as F1 or JAC. **Update**Based on discussion, corrections and new additions, I recommend acceptance of the above work.
Reject. rating score: 3. rating score: 6. rating score: 6. <BRK>The baselines the authors compare to are quite weak. Why is this? Can it match the results of the ground truth simulator but do it faster, or with fewer resources?<BRK>I hope the authors can also clarify these points in the rebuttal. The figure uses the split from equations (6,7), so it would be good to make this clear via the notation. The paper certainly does not aim for new insight for deep learning methods in general, but provides an interesting application for turbulent flows that is evaluated with a nice amount of detail.<BRK>Small remarks:   quantifies : few times, did not not mean  quantities ? Would it still be applicable? The method is tested on a synthetic example, showing interesting results and comparing with a large set of baselines. However, I am a not totally convinced since (i) the authors said that their goal is to emulate numerical simulations given  noiseless observations : how can this be used on a real case then?
Accept (Poster). rating score: 6. rating score: 6. rating score: 6. <BRK>This paper proposes two methods named OSG and OAdagrad for solving stochastic non convex non concave min max problems. Quality:The work is of good quality and is technically sound.<BRK>Notably, rather than inventing a new algorithm (OSG), the paper proposes a way to combine variance reduction with Optimistic mirror descent of Deskalakis et al such as to achieve state of the art convergence rates which so far were only known for the variance reduced extragradient method. The fact that all experiments seem to be conducted without constraints and constant batch size further strengthens this impression. There is a typo in the definition of pseudomonotonicity.<BRK>This paper proposed a new algorithm (Optimistic Stochastic Gradient) for solving a class of non convex non concave min max problem. after the rebuttalI thank the authors for their response and it addressed most of my concerns.
Reject. rating score: 1. rating score: 6. rating score: 6. rating score: 6. <BRK>The statement could very well be phrased in terms of any two models, one of which has a lower loss than the other. The approach is based on reweighting the train examples according to the ratio of confidences of the complex and simple models, and then retraining the simple model on the reweighed dataset. * Limited novelty. * There are various other technical aspects of the paper that are either non rigorous or lack details.<BRK>The experiments show favourable performance of the proposed method over baselines, including the distillation approach of Hinton. You are leveraging the predictions of a complex model to enhance those of a simple model? Surely they can also be used with simple models as ones you consider in the paper?<BRK>The main methodological contribution is the formal justification of the proposed weighting scheme for samples which utilizes the ratio of conditional probabilities p_complex(y|x)/p_simple(y|x) in the Lemma 3.1 and 3.2. Experiments for different benchmark datasets and different complex simple model combinations nicely demonstrate that this method is indeed quite useful in practice.<BRK>This paper presents a simple yet effective trick to reweigh the samples fed into a simple model to improve the performance. It s difficult to pinpoint how we replaced the complex model with the graded classifiers. The reweighting scheme looks very related to importance sampling ratios in off policy evaluation in reinforcement learning and counterfactual analysis. The introduction of the graded classifiers was very sudden.
Accept (Poster). rating score: 8. rating score: 6. rating score: 6. <BRK>This is a really interesting paper, and can really shine some light on what was going on in neural link prediction over recent years. that is really needed right now in the field.<BRK>Summary The paper conducts a thorough analysis of existing models for constructing knowledge graph embeddings. This is a very interesting section. Furthermore, the paper is well written and easy to follow, and should become a good reference for future works on KGEs.<BRK>The paper presents an experimental study about some KGE methods. While it is necessary for efficient exploration of larger search spaces [1], and should be the standard methodology for hyperparameter search of a new method, the interpretability of the comparison of two runs suffers. "Simple embedding for link prediction in knowledge graphs."
Accept (Poster). rating score: 6. rating score: 6. rating score: 6. <BRK>This paper proposes a new method for making 3D point clouds by automatically completing 3D scans. Were such experiments considered? The authors use a generative adversarial network (GAN) to “generate” complete point clouds from noisy or partial point clouds obtained by 3D scanning. Shouldn’t it rather say that it is plausibility comparison on real life scans and synthetic data? The main weakness is limited novelty in terms of the techniques used in the proposed method. The proposed method outperforms existing methods in real life data scenario. The main contribution of this paper is training with unpaired data, which enables training on real life data, leading to better results on real life scans.<BRK>While the idea of doing the unpaired shape completion has been known since the introduction unpaired image based methods (e.g., [1]), the application is novel and the methods are formulated using the language known in the point cloud learning literature (e.g., EMD losses and point based autoencoders). Regarding the evaluation procedure, the authors demonstrated convincingly that the proposed approach is feasible. Overall, I believe that the paper does a good job of combining the established components into something new and useful, particularly, the problem is well defined, the method is intuitive and extends the state of the art, and the evaluation looks convincing. The paper is well written and easy to follow, too. Learning Representations and Generative Models for 3D Point Clouds.<BRK>This manuscript focuses on reconstructing 3D shapes from point clouds, with applications for instance to 3D scanners. The contribution builds on an adversarial formulation of the reconstruction, as in GANs. The training loss is based on an Earth Mover s Distance between points. The method is benchmarked on simulated and real data. The methods are well formulated and make sense. The experiments show the interest of the contributed method.
Reject. rating score: 3. rating score: 3. rating score: 8. <BRK>The main idea is to have several different buckets in each block of the generator and then train the generator with random paths. To interpret the features captured by each block, the authors unfreeze one block and show the variance of the generated images via different buckets. The contribution of the paper is limited. And the authors did not clarify why their methods are better than the previous work.<BRK>The main weaknesses:(1) Although the authors provide experimental examples showing images associated with various paths through the architecture, is not clear how interpretations can be associated with these paths. Overall, in its current state (not least due to presentational issues) the paper appears to be below the acceptance threshold. I am not sure that their conclusions adequately capture what is going on here, nor am I convinced that they generalize to other situations.<BRK>The authors also add a diversity term to force the different blocks to have different blocks. The paper presents an empirical evaluation of what each switch does, and show that concepts are well disentangled between layers (for instance, one layer changes the background whereas another one changes the color of the foreground).
Reject. rating score: 1. rating score: 1. rating score: 3. <BRK>However, I still think that the motivation of the paper is not clear: the authors may have a good algorithm. But the value of adding entropy on SVG or training the policy on imagination data is not well justified. Experimental results show that S2VG beats SVG, SAC, DDPG, and SLBO on three MuJoCo tasks. Then the novelty of this paper is adding an entropy regularizer to improve the robustness under the model estimation error. However, this point is not clarified in the paper. Thus I do not find the value of adding a policy entropy regularizer on the SVG method. S2VG should be compared with STEVE ((Buckman et al., 2018), the state of the art model based method. That is, the claim that S2VG beats state of the art model based methods is not appropriate. Finally, the performance improvement is somewhat weak as S2VG only beats other baselines on three MuJoCo tasks.<BRK>This paper presents a slightly new model based reinforcement learning algorithm, claiming that the new method is elegant and combines the merits of earlier methods. In particular, it claims that the new method is more sample efficient and computationally efficient than previous methods.The novelty of the new method has to do with how the model participates in “the policy improvement step”. The presentation of the method is not clear; it uses an informal notation and leaves out many steps; in many cases the statements are not formally correct. I believe that many of these weaknesses are not introduced in this work but are present also in the works that it builds on. It is possible that someone intimately familiar with the previous works would be able to fully understand this method, but perhaps not, and certainly it does not stand alone. Do they have the same distribution?<BRK>This paper proposes an off policy model based reinforcement learning approach. To leverage the learned dynamics, the proposed approach adopts value expansion to substitute the learned reward function and forward dynamics function in the Bellman equation for the value function. I feel the novelty of this approach is a bit limited, considering that the novelty mainly comes from embedding a more powerful maximum entropy RL formulation upon the value expansion [Freinberg et al., 2018] and stochastic value gradient[Heess et al., 2015], rather than from the perspective of model based modeling. Even though the experiment result is still very promising and it s good to see the method could outperform SLBO in 3 out of 4 cases. Some other comments:  The presentation in Sec 3.2 is not clear. For Walker2d, more training steps need to be shown. Robustness of the algorithm: are the results in Figure 1 derived from different random seeds? If so, how many of them? The authors may consider to include more task domains, e.g., swimmer, and humanoid as well. The curves in Figure 2 are not shown with an uncertainty plot.
Reject. rating score: 1. rating score: 3. rating score: 8. <BRK>This paper proposes a new perspective on understanding knowledge distillation as a transfer of information defined with respect to the neural tangent kernel. Overall, I think the paper lacks justification (and explanation) for its main statement on how knowledge distillation is related to the early stopping of the teacher network. Especially, it is confusing since Section 2 and 3 make different statements. Specifically, Section 2 shows that early stopping "helps" knowledge distillation while Section 3 shows that knowledge distillation can "replace" early stopping. However, there is no elaboration on how the knowledge distillation process leads to the transfer of such information, i.e., there is no connection between the neural tangent kernel and the knowledge distillation process. The label refinery algorithm for the noisy labeled dataset is interesting, but it is not evaluated thoroughly enough to demonstrate its superiority over existing algorithms. It also does not have much originality when compared to similar algorithms [1, 2].<BRK>This paper proposes a self distillation algorithm for training an over parameterized neural network with noisily labelled data. Experiments on CIFAR 10 and Fashion MNIST are provided, which show that the self distillation algorithm is effective for label noise with relatively little tuning. Although the theoretical part of the paper has a large overlap with [Li et al.2019], I find the self distillation algorithm very interesting and it s nice that it can achieve zero l_2 loss w.r.t the correct labels. However, I think the paper could still use some improvement. Can it be generalized to multi class classification? 3.Section 2 is not very satisfying. In particular, the concept of AIR is nothing new and its connection to NTK top eigenspace has already been written in previous work (e.g.[Arora et al.2019]).I d suggest to not have this section and to make Section 3 the main contribution of this paper. update:Thanks to the authors for the response and for adding a generalization bound.<BRK>This paper provides a theoretical framework to understand the regularization effect of distillation. Then the author consider to use distillation to learning with corrupted labels inspired by the previous works using early stopping to clean label noise. The author also tries to formulate the “informative information” via the NTK theory. While the previous result was on convergence in 0 1 loss, which means the distillation can enlarge the margin the classifier. From this proof, aurthors also make an interesting discussion to show how distillation introduces further information rather than just early stopping. Authors also demonstrate their result on Fashion MNIST and CIFAR 10 to convince the reader the benefit of the algorithm. The paper is interesting and brings new theoretically thoughts to understand the distillation method. The relationship between early stopping and distillation can inspire the machine learning researchers to explore more about distillation both empirically and theoretically. The analysis is based on the assumption that the teacher is overparameterized. What will happen if the teacher network is not overparameterized? in  Algorithm1 what is $\mathcal{N}(x_i, w_t)$ and what is base network and mother network? These should be instead by student network and teacher network. 4.Author claims that early stopping is hard to tune while introducing extra hyper parameters in the self distillation algorithm. Would the extra hyper parameter makes the algorithm might be even harder to tune?
Accept (Poster). rating score: 8. rating score: 6. rating score: 6. <BRK>Experiments: The authors provide a number of illustrative experiments that demonstrate the efficacy of the approach across a number of tasks. For example, there will be large imbalances in the number of training samples used for the different discriminators   how does this affect stability?<BRK>The paper is tackling the problem of training generative adversarial networks with incomplete data points. It is hard to evaluate the comparative performance of the method without the comparisons mentioned above.<BRK>Well written, good initial results. It is a nice formulation, and the experimental validation of the technique has been strengthened. The title is the same as an the arxiv paper title, and so the double blind requirement is trivially violated.
Accept (Spotlight). rating score: 8. rating score: 6. rating score: 6. <BRK>The authors propose with this paper a simple extension of DARTS, a popular neural architecture search (NAS) method. This achieved in a simple way. Instead of using all channels only a random subset is used. To account for that, the authors propose a method to normalizes edges. Concluding, this might not be a ground breaking paper but it is well made and I see no obvious flaws so I do not see any reason to reject it.<BRK>** Summary ** This paper proposes to improve the previously work DARTS in terms of the training efficiency, from the large memory and computing overheads. The authors are inspired by ShuffleNet or related research topics. 2.The motivation of edge normalization is somehow weak, as the authors are aware of this can also be applied to the original DARTS. 3.In imageNet results, it seems P DARTS significantly outperform PC DARTS in terms of the search cost, and the accuracy is similar. This makes PC DARTS approach to be embarrassing. What’s the most advantage of PC DARTS compared to their method?<BRK>rebuttal clears my concerns. Summary:The paper proposes a partially connected differential architecture search (PC DARTS) technique, that uses a variant of channel dropout for each node s output feature maps, and a weighted summation of concatenating all previous nodes. It would significantly strengthen the effectiveness of the proposed approach. My main concerns are about the incremental novelty and experiments are heavily done on one search run, especially the search space is not the same as baseline DARTS. Experiments comparing to the baseline is not fair. However, in the least scenario, the experiment comparison should be in a fair way.
Reject. rating score: 3. rating score: 6. rating score: 6. <BRK>This paper proposes to use the autoencoder to measure the complexity for identifying the cause and effect in the univariate case. However, my main concerns are regarding the assumption of this work, seeing that the assumption h(X)>h(Y) in the univariate case is easy to violate.<BRK>Therefore, I have increased my score to 6. Examples include:http://www.jmlr.org/papers/v12/shimizu11a.htmlIn conclusion, since the idea of using independence relations for learning the causal directions is not a very new idea and a lot of discussion of the theoretical analysis is still missing.<BRK>Edit after author rebuttal and author additions:I have updated my score from a weak reject (3) to a weak accept (6). I believe more discussion between the authors and reviewers is necessary here.
Reject. rating score: 1. rating score: 1. rating score: 1. rating score: 3. <BRK>The main idea, as laid out in §1.1, is to observe that the parameter update depends mainly on the way a small perturbation in parameters is reflected as a variation in the optimal trajectory (by asking for the probability of a trajectory, this variation becomes the probability of a nearby trajectory). 3.In light of all this, I find the evaluation really weak. There are some extensions, such as using a local GP model instead of a local linear model and consideration of ways in which the system might not be exactly repeatable given initial states.<BRK>*Decision*I vote for rejecting this paper. While the idea is interesting, the paper lacks precision in key areas and the method is not placed in context among related work. The evaluation is unclear, at least to a non expert in robotics.<BRK>As such, the method should beof practical interest in order to be accepted. It does not appear to be referenced in the text and it is not clear what is	  being shown. Without much more extensive experimental validation, this paper should be rejected.<BRK>The motivation is not too convincing without showing some results on hard tasks: model based RL methods work great in this setting, and are very likely to outperform the method proposed in the paper. But there are a few reasons why I believe this work is not ready for publication.
Accept (Poster). rating score: 6. rating score: 6. rating score: 6. <BRK>This paper studies an interesting phenomenon in neural network models that the classifier s prediction at a one input will not be significantly perturbed after the classifier is updated via sgd at another input that is dissimilar from the former one. This phenomenon is termed as the local elasticity, which provides another perspective seeking to interpret the neural networks. They present that this local elasticity characteristic does not hold for linear models. To further investigate this property, the paper introduces the relative similarity and kernelized similarity based on which a k means like clustering algorithm is developed to further find fine grained clusters within a coarse grained category, like dogs and cats from the mammal category. In the experiments, it will be interesting to further investigate how the local elastic property changes with large batch size in that large batch size may encourage more diversity of the examples in a batch. Furthermore, it will be even more interesting to explore how these similarities can improve the performance of a simple k nearest neighbor classifier.<BRK>[Summary]This paper proposes and studies the “local elasticity”, a quantitative measure for the ability of neural networks to only locally change its prediction (around x) after a stochastic gradient step at x. The paper verifies experimentally that nonlinear neural nets are locally elastic through showing that an elasticity motivated similarity score can perform clustering well. [Pros]The notion of local elasticity is interesting and has the potential of opening up lots of further directions. The way I understand it is to relate to memorization (as the authors have indeed discussed)   I think “local elasticity” can be viewed as some sort of “local memorization ability”, in that the NN is able to change its prediction only in a small neighborhood of x without affecting predictions at other remote x’s after one SGD step on x. Conceptually this is something not covered by the existing narratives in deep learning theory, yet the phenomenon itself is quite convincing and could provide a new perspective into lots of things.<BRK>The paper contributes to the understanding of neural networks and provides a new clustering technique:  1) The paper introduces the interesting notion of local elasticity which considers the relative variation in output values for two different inputs before and after an SGD style update;  2) The derived similarity metric between input samples (obtained by as SGD unfolds) can be used for clustering and is amenable to a kernelized formulation;  3) Empirical measurements on visual classification tasks show that the new similarity metric can offer a better clustering performance than PCA + K means. I found the paper very interesting but the writing appeared somewhat unclear at times. I believe that some rewriting is needed for the authors to argue that the newly introduced elasticity metric provides a significantly new understanding of neural networks. In particular, I did not find the argumentation around explaining generalization to be very convincing or clear. The kernelized formulation of the elasticity metric seems compelling and I found that turning the insights developed by the theoretical section of the paper into an actionable algorithm for clustering was a nice contribution.
Reject. rating score: 1. rating score: 3. rating score: 3. <BRK>This paper studies the problem of domain adaptation via learning invariant representations. Overall the paper is well written and easy to follow. Compared with the original generalization upper bound [3], the one proposed in this paper contains a constant $\lambda$ that contains FOUR optimal error terms. Note that the original one in [3] only contains two such terms. The experiments on using different number of layers of the network as feature extractors are quite interesting.<BRK>Positive points: (a) This paper proposes an interesting insight that the complexity of embeddings is also important in domain adaptation. Although this bound involves new insight, the novelty is limited if it is looser than existing upper bound. (b) There is no analysis about the generalization when estimating this upper bound from finite samples. It seems that if the embedding complexity of each layer is a constant, minimize divergence of a single layer can further minimize the minimum. Furthermore, there are previous method that minimizes divergences on all layers [A]. Although the insight is interesting, the novelty of this paper is not enough for being accepted by ICLR. So I vote for rejecting this submission.<BRK>This paper studies the impact of embedding complexity on domain invariant representations. By incorporating embedding complexity into the previous upper bound explicitly, the authors demonstrate the limitations of previous theories and algorithms. The proposed algorithm can achieve similar performance as DANN with manual selection of embedding depth. The paper is well written and the contributions are stated clearly. The proposed upper bound is insightful, but it has several limitations. 2.On the claim of implicit regularization. However, they do not validate this regularization effect. 3.The proposed MDM method seems to be incremental.
Reject. rating score: 3. rating score: 3. rating score: 6. rating score: 8. <BRK>This paper proposed a white box (known network architecture, known network weight) data free (without need to access the data) adversarial attacking method. The method is compared with existing methods (only one existing approach for the problem, GDUAP by Mopuri et al.2018) in terms of the fool rate. Also on some other settings (black box testing, less data) the proposed method outperforms GDUAP. 1), the novelty of the proposed idea seems relatively limited. 2), it was mentioned that compare with GDUAP, this paper has more theoretical analysis. But this is not very convincing to me. But these steps can hardly be called "theoretical analysis". 3) I do like the experimental results. But the baselines are really limited (granted, there are not many existing approaches).<BRK>Summary:This paper proposed a method to generate universal adversarial perturbations without training data. My major concern is that there is not much novelty in the proposed method compared with GDUAP. This paper provides a theoretical explanation of the dilate loss function and an improvement on the non linearity function, which, however, is not convincing. The proposed method performs better than GDUAP in the data free and black box setting. The writing is good. Cons: 	The theoretical analysis is based on many strong assumptions/criteria. o	It would be better if the authors show that in what cases these assumptions can be satisfied. In the initialization part, why can we start learning p from the result of the previous layer?<BRK>The experiments results seems solid as the numbers show that their method is much better in many cases. It needs complete access to the model and relies on properties of ReLu. They gave a well motivated derivation going from the data matrix, the data mean and to data free. While it is interesting, the paper did not establish that universal adversarial perturbation is well motivated and why data free is more important that model free or targeted perturbations.<BRK>In this paper, a new data free method is proposed to create universal adversarial perturbation without using data. There are some similarities with GDUAP though, authors also make some crucial improvements. The authors provide a detailed theoretical analysis and systematic experimental results to demonstrate their arguments, which is convincing. What’s more, the proposed method achieves state of the art data free fooling rates on the large scale dataset, which strongly demonstrates the effectiveness of their method.
Reject. rating score: 1. rating score: 1. rating score: 3. rating score: 6. <BRK>This paper studies the solution of neural network training in the NTK regime. The trained network can be written as the sum of two terms   the first is the minimum RKHS norm interpolating solution, and the second term depends on the initialization. The bounds in later part of the paper are also straightforward. update:I have read the authors  response. My assessment stays the same since I still think that the technical contribution of this paper is quite limited.<BRK>[Summary]This paper studies the impact of initialization noise on the theories of wide neural networks in the Neural Tangent Kernels (NTK) regime. The paper proves that the difference between the trained neural net and the kernel interpolator (with the NTK) can be bounded by O(\sigma^L + 1/\sqrt{m}), where \sigma^2 is the initializing variance of each individual weight entry. Relationships between the generalization error of these two functions are derived from the above bound. To bound the difference in squared loss we have a^2   b^2 <  O(1) * |a b| (if a, b are bounded by O(1)). ***I have read the authors  rebuttal and the other reviews, and I m glad to see the issues with Theorem 3 and 4 pointed out above are fixed in the revision.<BRK>This paper studies overparameterized fully connected neural networks trained with squared loss. The authors show that the resulting network can be decomposed as a sum of the solution of a certain interpolating kernel regression and a term that only depends on initialization. Moreover, the generalization bound given in this paper does not seem to be very complete and significant, since the authors do not show when can L_{test}^{int} be small.<BRK>The study uses tools from recent literature on the generalization of overparameterized neural networks, i.e.neural tangent kernels and interpolating kernel method, to provide useful insights on how the variance of weights initialization affects the test error. Addressing the following points will improve the exposition of the paper. Could this be verified with a similar experiment as for MSE? 3.To what extent this result is observed in not as strongly overparameterized settings?
Reject. rating score: 6. rating score: 6. rating score: 6. <BRK>Overview: In this paper, a meta learning approach is proposed to perform link prediction across multi graphs with scarce data. It would be better to provide more explanations on the graphical model for meta graph and the meta graph architecture in the context. The idea of formulating link prediction as a few shot learning problem and solving it via multi graph meta learning is novel. I don’t think the current experimental results can well explain the property of fast adaptation.<BRK>This paper presents a new link prediction framework in case of having seen only a small fraction of the graph. The paper is conceptually solid, however, it is hard to claim novelty in individual ideas in the paper.<BRK>This paper proposes to provide a novel gradient based meta learning framework (Meta Graph) for a few shot link prediction task. The authors validate the proposed model through several experiments. Moreover, it would be nice if the authors could also provide some ideas for future research directions, such as the prospects of using their approach for improving link prediction models and incorporating Meta Graph in other domains like molecules structure.
Accept (Poster). rating score: 6. rating score: 6. rating score: 6. <BRK>+ I would say that the paper is rather on the light side regarding experiments. The proposed algorithm adds momentum and importance sampling.<BRK>The paper proposes a stochastic derivative free optimization algorithm. The contribution is two fold: first, the paper introduces the heavy ball momentum into the STP framework; second, the paper fulfills both the importance sampling and heavy ball momentum in the STP framework.<BRK>The authors extend recent the recent stochastic three point (STP) method to allow for Polyak style momentum, as well as momentum with importance sampling. Overall, I think this is a strong paper, and a very interesting topic, and hence I support a "Weak accept". I think all the reviewers were curious how Bayesian optimization (BO) would perform.
Accept (Poster). rating score: 8. rating score: 6. rating score: 3. <BRK>This paper studies leveraging mode connectivity to defend against different types of attacks, including backdoor attacks, adversarial examples, and error injection attacks. In general I like this paper. Meanwhile, the attacks studied in this work include various settings, i.e., poisoning attacks, error injection attacks, evasion attacks, and also adaptive attacks where the adversary knows that the defender will use path connection to improve the robustness. These make this paper a good reference as a systematic study of their proposed topic. While it is helpful to show negative results if this is indeed the case, do you have some possible explanation why the models on the path are less robust than the two end models? Post rebuttal commentsI thank the authors for clarifying my questions, and I keep my original score.<BRK>The paper repurposes results on mode connectivity (that is, minimal loss paths between local optima) to improve adversarial robustness. Comments:The claim that all models on paths have similar test losses (bottom p8) seems a bit of a stretch given the bottom panels (middle and right) of Figure 4.<BRK>This paper proposes an adversarial defense method based on mode connectivity. The goal of the method is to repair tampered networks using a limited number of clean data examples. The authors consider two types of adversarial attacks: backdoor attacks and error injection attacks. In the experiments the proposed method shows better results compared to baseline defense techniques including fine tuning, training from scratch, and pruning followed by fine tuning. The paper also analyzes evasion adversarial attacks from the perspective of mode connectivity and observes the existence of barriers in the landscape of robustness loss on the paths connecting regular and adversarially trained models. Including the error bars would help to quantify the statistical significance of the results.
Accept (Poster). rating score: 8. rating score: 6. rating score: 6. <BRK>The authors propose a novel method for continual learning with neural networks based on a Bayesian approach. To address the continual learning setting, the authors propose to multiply the learning rate of the mean parameters in the posterior approximation by the corresponding standard deviation parameter in the posterior approximation, while the learning rate for the variance parameters in the posterior approximation is not changed. The method proposed is well described and it would be easy to reproduce. Novelty:The proposed method is novel up to my knowledge. The methodological contributions do not seem very sophisticated, but the experiments show that the proposed method, despite being very simple, works very well in practice. This indicates that the proposed method will be relevant to the community. In my opinion, this work is highly significant.<BRK>I believe it is a good addition to the community of continual learning. ** post rebuttal end **  Summary:This paper proposes to use a way to improve continual learning performance by taking "Bayes by backprop" method. Experimental results on several benchmarks show that their method outperforms few state of the art methods. 1.The proposed method is simple but effective. An ablation study with different choices of the importance measure (maybe \mu can also be incorporated as well as \sigma?) would be good to see. 2.Survey and comparison with memory based methods are limited. Pruning is not beneficial in terms of the performance.<BRK>**** End ****The paper presents a simple yet effective way to avoid catastrophic forgetting in a continual learning setting. The main idea of the approach is to weight the learning rate of each parameter in the neural network by the standard deviation of its posterior distribution. This leads to regularizing parameters that are "important" to tasks seen earlier and thus avoiding forgetting. Why using uncertainty to define importance works better than using online VI in VCL or fisher information in EWC? I am not sure why weighting the learning rate would be a good idea? Is there a constraint on the standard deviation? I think the method would be very sensitive to the initialization of the standard deviation. Overall I think the idea of using uncertainties for continual learning is interesting.
Reject. rating score: 1. rating score: 1. rating score: 1. <BRK>  This paper simply proposes to use UNet for the segmentation of stagnant zones in X ray CTs. Overall, the quality of the paper is below the standards of ICLR (content, technical contribution, length). The submission is not anonymized (authors included their names and affiliations).<BRK>I support desk rejection since violating double blind rule, wrong format and insufficient length.<BRK>More importantly than all of that, this paper violates the double blind review rule and is same with [1]. So I think this paper is not suitable for acception.
Reject. rating score: 1. rating score: 1. rating score: 6. <BRK>Although the samples sound OK, given its current form, the paper needs significant re work. what does it mean? But the paper has a lot of issues both technically and grammatically, which makes the paper hard to follow. The authors give the mathematical formulation of the problem and provide the implementation of the so called AdaGAN.<BRK>Cons:This paper is poorly written and difficult to follow. For example, I could not accurately identify the major contribution & novelty after reading the abstract and introduction. For example, it seems utterance U_i is from speaker Z_i in the dataset, but there are n speakers and m utterances.<BRK>The rest of the machinery is well motivated and well executed, but less novel. This would suggest that for training input and output need to be synchronized.
Reject. rating score: 1. rating score: 6. rating score: 6. <BRK>This paper introduces a way to decompose features for better domain adaptation via learning domain invariant representations. Compared with [1], the main novelty of this paper lies in a new feature decomposition of the traditional convolution layer. My main concern is that this paper seems to miss a significant line of recent work on learning domain invariant representations [2 3]. Basically, it has been shown that invariant representations provably hurt generalization on the target domain when the marginal label distributions are different between the source and target domains.<BRK>The paper proposes an approach to learning domain invariant representations using the adaptive decomposition of the convolutional filters. Overall, the paper was well motivated and easy to read. "Co regularized alignment for unsupervised domain adaptation."<BRK>I would have wanted to see MNIST >SVHN for the unsupervised case as well, as this is a particularly hard one. * The paper proposes to perform domain adaptation via separating domain specific and cross domain features, by what is referred to as "domain adaptive filter decomposition". Provide additional feedback with the aim to improve the paper.
Reject. rating score: 3. rating score: 6. rating score: 6. <BRK>This paper proposes a method to improve the interpretability of a convolutional neural network (CNN). The main idea is to force the CNN filters to be class specific, i.e.to be associated to a specific class. This would make the model more interpretable by allowing to check which filters/classes have been activated. Instead, in the actual implementation, this idea is applied only at the last layer of a CNN. To me, instead of evaluating the activation of the classification layer, it is possible to check the activation of the filters of the last convolutional layers. However, evaluating the proposed approach on a single dataset and only one network is not enough for a clear evidence. Additionally, the obtained performance is below the standard performance of ResNet on CIFAR10 without a clear reason.<BRK>Contributions: The paper proposes a novel Label Sensitive Gate (LSG) structure to enable the model to learn disentangled filters in a supervised manner. The novelty of the paper is to introduce the Label Sensitive Gate path during the training, on top of the standard training path. However, for the plot at the right, it is titled with "False Positive". If we train less filters for each class, will that hurt the performance? It might be a good direction on how to interpret filters from one class and how to make intra class filters more interpretable. Overall, the novelty of the paper is limited by the scope of the experimental results. Why would people care about interpretability of CNN filters is not explained clearly. The paper shows interpretable filters could help with localization. However, this is the only example in the paper I find useful. A more theoretical/intuitive explanation would also make the paper stronger as well.<BRK>This paper proposed an interesting idea of using Label Sensitive Gate (LSG) structure to enforce models to learn disentangled filters for better interpretability of the DNN model. By periodically training with the sparse LSG structure, the model is forced to extract features from only a few classes. The model is trained efficiently in an alternate fashion (with respect to both the network parameters and the sparse gate matrix.) By disentangling the class specific filters, the model becomes less redundant and more interpretable. I  like the idea of imposing a class specific gated structure for disentangling the representation. And numerical experiments verify the effectiveness of the proposed method in terms of1) Improved performance2) Disentangled representation (a small L1 norm on the gate matrix G)3) Consistent class activation map for different inputs. I do have some question though1) Table 1 also reports the L1 norm and \Phi of a STD CNN.
Reject. rating score: 3. rating score: 3. rating score: 3. <BRK>Many techniques have been used for modelling this similarity, including predictive models, cliques and graphs.<BRK>In contrast to this prior work, the proposed model is based on the selection of a previously available pixel and the modeling of the differences between the old pixel and the new one. I am rating the paper "weak reject" mostly due to the limited set of comparisons in experimental results. Furthermore there is no discussion of these comparison results   i.e., what the proposed algorithm contributes given that it s outperformed by the sparse transformer.<BRK>In this paper the authors present a new way to use autoregressive modeling to generate images pixel by pixel where each pixel is generated by modeling the difference between the current pixel value and  the preexistent ones.
Accept (Poster). rating score: 8. rating score: 6. rating score: 6. <BRK>The paper is well written, clearly motivated, highly novel and significant not only in a theoretical sense but also in a practical sense. Surprisingly, the authors proved that for some simple binary classification with label noise, standard gradient clipping does not provide robustness; on the other hand, a simple variant of gradient clipping is robust, and is equivalent to suitably modifying the underlying loss function. Is this a typo?<BRK>[Summary]This paper studies the relationship between gradient clipping in stochastic gradient descent and robustness to label noise. There is little discussion on how this parameter is chosen in the experiments. [Decision]The first contribution, that gradient clipping does not induce robustness to label noise, is an important negative result given the prominence of gradient clipping and datasets with noisy labels. Overall, I recommend acceptance.<BRK>Summary:Gradient clipping has been studied as an optimization technique and also as a tool for privacy preserving, but in this paper, it studies the robustness properties of gradient clipping. More specifically, the main question of the paper is: Can gradient clipping mitigate label noise? Experiments show that the proposed variant works under label noise. The proposed method is robust to label noise and has theoretical guarantees.
Reject. rating score: 3. rating score: 6. rating score: 6. <BRK>This paper investigates the design of adversarial policies (where the action of the adversarial agent corresponds to a perturbation in the state perceived by the primary agent). In particular it focuses on the problem of learning so called optimal adversarial policies, using reinforcement learning. I am perplexed by this paper for a few reasons:1)	What is the real motivation for this work? And so what is really the use of learning so called optimal attacks? It quickly proposes one possible definition (p.4): “the adversary wishes to minimize the agent’s average cumulative reward”. So overall, while I find the general area of this work to be potentially interesting, the current framing is not well motivated enough, and not sufficiently differentiated from other work in robust MDPs and multi agent RL to make a strong contribution yet.<BRK>Summary:The authors of this paper propose a novel adversarial attack for deep reinforcement learning. Different from the classical attacks, e.g., FGSM, they explicitly minimize the reward collect by the agent in a form of Markov decision process. The proposed attacking diagram can be devised in a pure black box setting and also can be incorporated with white box attacks. 3.With such a strong attack, the authors derive an upper bound on the impact of attacks and shed light on new research studying the robustness of deep RL approach. I would like to see the clearer comparison of optimal attack in a pure black box setting with gradient based attacks. 2.Though FGM is not as efficient as the proposed optimal attack, they are simpler than a learning based approach.<BRK>In particular, they devise optimal attacks in the sense that e.g.the agent’s objective should be minimised by the attacker. It is assumed that the attacker can manipulate the observed state of the agent at each time step during testing in a restricted way such that perturbations are small in the state or feature space. Experiments show that a) attacks that are trained for optimality w.r.t.to minimising the average reward of the agent outperform a baseline method that only optimises a related criterion. Overall, I think this paper can be a good contribution to the subfield of adversarial attacks on MDP policies. However, I would have liked to get a better understanding and motivation for the investigated problem setting, such as projected applications and state or features spaces that could be manipulated in the proposed way. Cons: No real application was presented. This reiterates my application argument.
Accept (Poster). rating score: 8. rating score: 8. rating score: 6. <BRK>The model shows rather compelling results on small datasets, and is very simple, with very strong parallels to active contours, which is a strength. “The recent learning based approaches are either non competitive or proven to be effective in the specific settings of building segmentation".<BRK>This paper investigates an image segmentation technique that learns to evolve an active contour, constraining the segmentation prediction to be a polygon (with a predetermined number of vertices). The performance compared to prior learning based active contour methods is impressive.<BRK>The approach taken here is end to end learning with an active contour type approach. The authors should be more careful to include their reasoning in the actual text, which I believe this is essential for proper, easy understanding of the paper. The inclusion of an evaluation on a larger size data set is highly appreciated, and seems to indeed validate the robustness of the method.
Reject. rating score: 3. rating score: 3. rating score: 3. <BRK>At infinite depth, the NTK is just a trivial kernel Theta^*, as noted in the paper. The paper draws connection between these behaviors with the trainability and generalization of corresponding neural networks. NTK has been a popular subject of research in deep learning theory, and it s an interesting direction to study the NTK in large depth. I hope the authors could further improve the exposition of this paper.<BRK>This paper studies the relation between trainability and generalization ability in deep neural networks. The paper clarified that the spectrum of the NTK and NNGP has an important role in investigating the generalization and trainability, i.e., the condition number of the NTK. In this paper, some existing theoretical results on deep neural networks were combined to extract new insight. Thought the attempt of this paper is interesting, the readability of the paper is not necessarily high.<BRK>This paper studies the evolution of Neural Tangent Kernel (NTK) at large depth regimes. Furthermore, they also analyze the influence of pooling and flattening in CNNs and identify potential regimes where pooling hurts the generalization. However, I think this paper is worth of more revisions because many theoretical analyses are unjustified. Because q* q*_{ab} as you stated nearby, do you mean xi_1   xi_c ? Generally speaking, I think the paper needs careful revisions to support its theoretical analyses.
Accept (Poster). rating score: 6. rating score: 6. <BRK>I feel the current version of the paper does not build upon these insights to propose a new method. Summary: This paper presents a detailed empirical study of the recent bonus based exploration method on the Atari game suite. This also leads to the conclusion that recent results on the game Montezuma’s revenge can be attributed to architectural changes instead of the exploration method. I think this is a ok paper in that it does what it says it does. One of the comparisons I did not particularly find fair was when the hyperparameters of various methods were tuned to play MR and then the hyperparameters were fixed and the method were tested on other ATARI games. It would have been interesting to know how the methods performed when combined with the original DQN algorithm.<BRK>The authors combine Rainbow with different exploration methods, such as count based bonus methods, curiosity driven methods, and noisy networks. Results show that these methods fail to beat epsilon greedy on other Atari games, even if the parameters of these methods are tuned. The paper is very well written, and they claim that evaluating the exploration methods on the Montezuma s Revenge and tuning parameters on this environment are not suitable for the total ALE environments. The claim is very interesting and important for the exploration community. Then results in easy games and other games are presented.
Reject. rating score: 6. rating score: 6. rating score: 8. <BRK>Inhibitory neurons are not essential (at least when training with batch norm). The object classification task is not really relevant to elicit the observed behavior and 2. 3.)The results of Fig.11 suggest that the number of inhibitory channels is not important. Did the authors try a stripped down version with only excitatory cells?<BRK>The case the authors cite   similarity of architectures of convolutional neural networks and the pinwheel architecture of visual cortex V1/V2 areas, is one of the few examples of convergent architectures of real world and artificial neural networks specializing in a task. Finally, the role of Inhibory neurons in the visual cortex seems to bewell understood, both biologically and mathematically (see for instance https://www.sciencedirect.com/science/article/pii/S0896627303003325 or https://www.sciencedirect.com/science/article/pii/S0896627303003325 or https://www.sciencedirect.com/science/article/pii/S092842570300072X)Overall, the quality of the article at least in its current state, does not seem to be ready for acceptance to ICLR, but I m willing to adjust my opinion after reading the opinion of more qualified reviewers  in this area and the authors response. The authors claim that they observe similar patterns emerge in the Neural Networks when it comes to the interaction and topological properties of excitatory and inhibitory neurons. Unfortunately, the paper s claims seems to be undermined by several factors.<BRK>This paper explores why the brain might have separate excitatory (E) andinhibitory (I) cells and how their different properties affect theirnetwork connectivity. The paper is novel and addresses an interestingquestion. From theseconstraints they found that some of the other observed differencesbetween E and I cells developed, specifically greater selectivity of Ecells and greater connectivity of I cells. Both subtractive and divisive inhibition are modeled in the Standard model. Through careful controls, they show that increased selectivity of Eneurons depends on having a lot of E neurons (but there was not acorresponding effect for I neurons).
Reject. rating score: 3. rating score: 6. rating score: 6. <BRK>What are the conditions for the existence of a stationary state? Is bounded noise sufficient? Equation 4 and local approximationRegarding the standard decomposition of the Hessian in eq 4 (sometimes referred to as Gauss Newton decomposition), the discussion is not precise. If of course all **depends on the size of the approximation region**. The authors simply claim that the loss can be locally approximated while I think these assumptions should be clearly stated.<BRK>At the current level of writing in the paper, I can not formally verity the theorems. I find the theorem in the paper quite interesting, especially Lemma 1 stating that the loss function can be locally approximated by a quadratic function whose variables are the non degenerate directions and the coefficients only depends on the degenerate directions. After Rebuttal: I have read the authors  responses and acknowledge the sensibility of the statement. With this Lemma, it is then easy to see that as long as the noise is aligned with the non degenerate directions, then the trace of the Hessian is decreasing in expectation at every step.<BRK>The authors also show that by changing the type of noise that is added to GD, one can get different functions of the Hessian to reduce with SGD iterations. Can this be replicated for deep networks, or are the two quantities (trace vs determinant) closely related? I have a few concerns though:1.
Reject. rating score: 3. rating score: 3. rating score: 6. rating score: 6. <BRK>The paper builds on existing translation models developed for molecular optimization, making an iterative use of sequence to sequence or graph to graph translation models by wrapping them in a meta procedure. The primary contribution is really just to apply the translation models iteratively, i.e., feeding translation outputs from the models back in as inputs for retranslation. Other properties are not additive in the same sense, e.g., drug likeness or QED, and the method doesn t appear to improve it (though, to be fair, there may be a ceiling effect for QED in particular). The authors use essentially the score from the model itself, similarity to input, and some basic chemistry metrics to do that. Multi property optimization would be one possible setting since de novo models have a hard time to reach intersections of different property constraints. E.g.,Brookes et al., Design by adaptive sampling, arXiv:1810.03714The paper is clearly written but for such a simple method one would need really convincing results and experiments. Maybe better as a workshop submission?<BRK>This paper presents a translation based method for molecular property optimization. 7: Figure 4 says these are "ablation" experiments. Both of these are computed properties that have known issues (see the discussion in [3]), but I understand that these properties are used in many publications and are thus easy to compare. They claim superior results on the logP task, but I have concerns about the fairness of the comparison since logP can be exploited by very simple models if there are no limits on the size of the generated molecules (or, similarly, the number of tokens/generative steps allowed for each molecule). Additionally, the authors claim to perform multi objective optimization but do not actually do this. The iterative nature of this method is very interesting. 9: These experiments are not multi property optimization. 5: The results in Table 1 would be more compelling if they were not divorced from their starting points. 5: "Consistent with the literature we report diversity as...".<BRK>The authors frame molecule optimization as a sequence to sequence problem where a source molecule is translated to a target molecule with improved properties. The authors extend existing methods for improving molecules by applying them recursively over multiple rounds, and show that it is beneficial for optimizing logP but not QED. Is there a significant difference in the complexity between the top 100 (for example) molecules? How does the trace of the best molecule shown in figure 3 look like? The method is also closely related to Zou et al (https://www.nature.com/articles/s41598 019 47148 x) and Mueller et al (http://proceedings.mlr.press/v70/mueller17a.html), which are not cited in the text. QED combines several molecular properties, including logP, and is therefore more suited for quantifying drug likeness. Are molecules with the highest QED in the training dataset? This would contradict several recent papers on graph based representations.<BRK>This paper presents a novel approach to generating molecules using Black Box Recurrent Translation. The authors also analyze the decoding strategy, and how the process generates interpretable molecular traces. I am leaning towards an accept for this paper, since not only does the technique presented seem general, the authors does in depth analysis into the model and how it affects drug discovery. The model seems to reach a significantly better state of the art on the metrics proposed. The authors provide an in depth discussion about how having molecular traces would hhelp in drug design. This makes the tool seem more widely appealing and useful. Are these only used in Figure 4? This would highlight that the advantage of stochastic decoding is really online in the context of recursive translation, not generally.
Reject. rating score: 3. rating score: 3. rating score: 3. <BRK>This paper studies hypergradient descent for precondition matrices. Specifically, they take the gradient of the loss wrt the precondition matrix and update the precondition matrix to decrease the loss. In cases improving by 2%. The improvement is not significant. Figure 2: On mnist after 20 epochs the model has not reached 1% test error. I am also not convinced about the proof of Theorem 1. Hence, the first bound in Theorem 1 in this paper cannot simply be the same as in Baydin et al.2018.<BRK>This paper proposes an interesting optimization algorithm called first order preconditioning (FOP). The empirical studies on CIFAR 10 and ImageNet validate the effectives of the proposed algorithms. How does this term affect the update procedure? Since P changes over the course of training, it is difficult to check weather the result of Theorem 2 is stronger than gradient descent method. 4.Why the experimental results not include the other second order optimization algorithms such as K FAC and KFC? Minor comment:The notations M in (1) (2) and (5) are ambiguous.<BRK>“k” should be “k+1”. Pros:This paper extends the idea of hypergradient descent in [Almeida et al., 1998; Maclaurin et al., 2015; Baydin et al., 2017] with a preconditioning method. Cons:1 	The novelty and contribution is not clear. Therefore, I tend to give this paper a Weak Reject score.
Reject. rating score: 3. rating score: 3. rating score: 8. <BRK>Although some of the results are interesting, I have lots of concern on the motivation of this work. Maybe better called off policy policy improvement with deep neural networks. I feel it s not so hard to derive a bound that combines statistical error and algorithm error for [1]. I hope the authors can clarify in their paper. However, it is not proper to claim as a theoretical analysis of Deep Q Learning.<BRK>This result is motivated by the problem of understanding why deep Q learning works, which the authors relate to the problem above via certain simplifying assumptions. The authors also extend this result to give similar guarantees for two player zero sum stochastic games. Review:This paper addresses an important and challenging problem, and the results appear technical sound. This assumption is standard in the analysis of fitted Q iteration for off policy RL (eg, Munos and Szepesvari  08), but it implies that the algorithm does not need to solve a challenging exploration problem, since the data gathering policy has good coverage. Unfortunately, the authors do not justify why this assumption should hold for DQN.<BRK>The authors provide a theoretical analysis of deep Q learning based on the neural fitted Q iteration (FQI) algorithm [1]. The strengths of this paper are as follows. Moreover, the authors establish the algorithmic and statistical errors of the neural FQI algorithm.
Accept (Poster). rating score: 8. rating score: 6. rating score: 3. <BRK>The paper provides an unsupervised domain adaptation approachin the context of deep learning. In parts  I am willing to accept such kind of gray literature provided by well known authors but  this should not become a standard habit  I am happy to see that the code will be published   I hope this is really done, because  from the material it maybe hard to reconstruct the method<BRK>Authors should try to explain this behaviour since it is quite counter intuitive. The paper uses the same amount of layers for all domains making the amount of computation exactly same.<BRK>In this paper, the authors proposed to address the information asymmetry between domains in unsupervised domain adaptation. Do you need some constraints?
Reject. rating score: 3. rating score: 3. rating score: 8. <BRK>This paper studies the training of over parameterized two layer neural networks with smooth activation functions. In particular, this paper establishes convergence guarantee as well as generalization error bounds under an assumption that the data can be separated by a neural tangent model. As is discussed in the paper, results on both convergence and generalization have already been established in earlier works even for deep networks. However, all other results in Table 1 are for ReLU networks, and it has been discussed in Oymak & Soltanolkotabi, 2019 that the over parameterization condition for smooth activation functions are naturally weaker than that for ReLU networks. This seems to be a much simpler setting than many existing results. The improvement of requirement in network width, which is the major contribution of this paper, might not be very meaningful if it only works for shallow networks.<BRK>This paper studied the generalization performance of gradient descent for training over parameterized two layer neural networks on classification problems. The authors proved that under a neural tangent based separability assumption, as long as the neural network width is $\Omega(\epsilon^{ 1})$, the number of training examples is $\tilde\Omega(\epsilon^{ 4})$, within $O(\epsilon^{ 2})$ iterations GD can achieve expected $\epsilon$ classification error. Overall this paper is well written and easy to follow. The theoretical results on the neural network width and iteration complexity are interesting. Second, they use ReLU activation functions, which brings in the nonsmoothness along the optimization trajectory. Can you provide some examples regarding which type of data can satisfy Assumption (A.4) with constant margin $\rho$? Towards moderate overparameterization: global convergence guarantees for training shallow neural networks. An improved analysis of training over parameterized deep neural networks.<BRK>The authors study the problem of binary logistic regression in a two layer network with a smooth activation function. They introduce a separability assumption on the dataset using the neural tangent model. This separability assumption is weaker than the more Neural Tangent Kernel assumption that has been extensively studied in the regression literature. Under the separability assumption, the authors prove convergent gradient descent and generalization of the ensuring net, while assuming the two layer networks are less overparameterized than what would have been possible under the Gram matrix perspective. While the work only applies to smooth activations and to logistic loss classification problems, it can inspire additional work both in rigorous guarantees for training neural nets in regression and classification.
Accept (Spotlight). rating score: 8. rating score: 8. rating score: 6. <BRK>This very nice paper tackles the integration of of domain knowledge in signal processing within neural nets; the approach is illustrated in the domain of music. * are useful  > is useful ? The argument is that a "natural" latent space for audio is the spectro temporal domain. How sensitive to this parameter ?<BRK>This paper develops a framework for for audio generation using oscillators with differentiable neural network type learning. I have one important question though: how susceptible do you think the system is robust with respect to f0 and loudness encoders?<BRK>The model is trained by mining an L1 loss between the synthesised audio and the real training audio. Is it really a “bias” if every object in the universe vibrates? Bias as used in the paper, seems to have several means.
Reject. rating score: 1. rating score: 1. rating score: 6. <BRK>The paper proposes a  deep learning architecture for forecasting Origin Destination (OD) flow. The model integrates several existing modules including spatiotemporal graph convolution and periodically shifted attention mechanism. The experiments are not convincing.<BRK>The paper s first contribution is the design of a convolutional neural net architecture that uses non rectangular receptive fields more suitable to origin destination data. A periodically shifting attention mechanism is further used to measure long term data s similarities to short term data and weight accordingly. ## E2: Lack of error bars / uncertainty quantificationA natural question is, can we reliably tell that the difference between (for example) RMSE 2.49 and 2.54 is significant and not noise? RMSE error comparing to several classic (e.g.ARIMA) and deep feature learning (e.g.LSTMs and SRCN) baselines, with primary results in Table 1.<BRK>Summary: This paper proposes a latent spatial temporal origin destination model to address the OD flow prediction problem. The efficiency comparisons between the proposed model and the baselines are missing. 3.The authors design a periodically shift attention mechanism to model the long term periodicity.
Accept (Spotlight). rating score: 8. rating score: 6. rating score: 3. <BRK>POST REBUTTAL COMMENTSI appreciate the response from the authors. I particularly like the comparison table in the response to the other reviewer and ought to be highlighted in the paper. PRIOR FEEDBACKThe contribution of this work lies in providing a library for working with the existing variants of infinite width neural networks and avoiding the need to derive the NNGP and NT kernels for each architecture by hand. The authors then go into some implementation details with their library. On the overall, I like this effort which is timely. It would also be interesting to see how the posterior variance (e.g., Fig.1 right) evolves over the entire space during training. I would have preferred a more detailed discussion about the implementation on transforming tensor ops to kernel ops in Section 3. For example, is the 4th feature (i.e., exploring weight space perspective) demonstrated in the paper? Can the authors elaborate on the ease of expanding their library for the new developments in this field?<BRK>This work develops a library for working with a class of infinitely wide neural networks, in particular those corresponding to neural tangent kernels (NTKs) and neural network Gaussian processes (NNGPs). It s really nice to see the development of such a library, which I believe could benefit the deep learning community a lot, especially for theoretical research on NTK. I appreciate this work a lot. The theory and formulae of NTKs and NNGPs were well developed. The research contribution is relatively low. 2.As commented in the paper and if I understand correctly, the current library cannot scale to large datasets for CNNs with pooling.<BRK>Due to the correspondences with infinite neural network kernels (NNGPs), these kernels are also able to be computed for (essentially) free. Layers which do not emit an analytical form (e.g.Tanh or Softplus) can be implemented using Monte Carlo estimates. These results definitely show the potential software promise of the codebase and open some interesting new research questions as a result. Edit: post rebuttal, I m bumping my score to a weak reject but would have minimal qualms if this paper were to be accepted. I find the further experiments performed by the authors of very good quality overall, but I m still not particularly satisfied by their `argument that the codebase itself is distinct enough from separate related work. I could definitely see myself using this library in the future for research work. However, my primary concern with this paper is that it’s not sufficiently distinct from the previous work of Lee et al, 2019. One other potential example would be a kernel SVM in this manner on CIFAR 10. Originality: Again, a very efficient and easy to use implementation of neural tangent kernels would be a great boost to the community. This is doubly so as Jax is easy and pretty straightforward to use and is quite numpy like. Quality: I find the experiments performed to be very well constructed.
Accept (Poster). rating score: 8. rating score: 6. rating score: 3. <BRK>Summary: This paper proposes a general method for eliminating noisy labels in supervised learning based on the combination of two ideas: outputs of noisy examples are less robust under noise, and noisy labels are less likely to have a low loss. The results show pretty convincingly that one of the new methods (LTEC) that uses past networks outputs to build an ensemble performs really well. * “over 4 runs” did you randomize the data noise in each run (and in the same way for each method?), or only the network initialisation? Comments:* The readability of the paper could be dramatically improved by reporting results visually (eg bar plots) and moving all tables into the appendix.<BRK>In this paper, the authors proposed to identify noisy training examples using ensemble consensus. Rather, the authors hypothesized that examples with high noise require memorization, which is sensitive to perturbations. Thus, the authors proposed to identify and subsequently remove those examples from training by looking at the loss after small perturbations to the model parameters. Examples with consistently low training loss are retained for training. The authors also provided several alternatives of perturbations, including examining the consensus between an ensemble of networks, between multiple stochastic predictions, or between predictions from prior training epochs. Finally, the authors demonstrated the performance of their procedures using numerical studies. This paper is well motivated and clearly presented. The idea of identifying noisy examples through ensemble consensus is novel and plausible. The numerical studies are relatively comprehensive and in depth.<BRK>This paper presents a method for filtering out noisy training examples, assuming that there is a known percentage of label noise present in the data. The gist of the method is to repeatedly perturb the weights in the network slightly, and only accept as training data examples that consistently get small loss across all perturbations. It seems pretty unrealistic to randomly corrupt 20% or more of your data. In what scenario will you actually have data that has 20%+ label noise? One example of a real scenario where you might have a very high degree of label noise is in weakly supervised semantic parsing. This paper (https://openreview.net/forum?id ryxjnREFwH), for example, uses confidence thresholding (though without an ensemble) to filter the training data. Section 4.3   citations for the baseline methods should be in the main paper, not only in the appendix.
Reject. rating score: 3. rating score: 6. rating score: 6. <BRK>This paper considers extremely multi label learning (XML) where the label size is very large . This paper proposes several tricks to handle the issue for efficiently learning both neural network parameters and classification weights of extremely large number of labels. Overall the paper tackles the problem well. And the empirical results show improved results. This makes the overall algorithm/model very complicated and introduces a lot of hyper parameters, for example, head label portion, L h , c, beta, s neural network hyper parameters and so on. c. The largest label size in the experiments is 3M. 3.The writing and the organization of this paper needs to be improved. Several method names are not defined.<BRK>The paper presents a deep learning method for extreme classification and apply it to the application of matching user queries to bid phrases. On the positive side, given that there are relatively few successful approaches for deep learning in extreme classification, the main contribution of the paper is towards making an attempt towards this goal. 4.Related to above is impact of data pre processing for different methods.<BRK>This paper introduces a new algorithm for extreme multi label classification. The key novelty in this paper is to split the labels into two buckets: head labels and tail labels. For the tail labels, the embeddings from the head labels are used as the input for another classifier which is trained on only the tail.
Reject. rating score: 1. rating score: 3. rating score: 6. <BRK>Summary This paper proposes to make neural networks robust to adversarial examples via dimensionality reduction. The authors then mention a relation to the Wasserstein distance and to Wasserstein GANs. This seems like a very convoluted way of justifying the use of the most standard loss function in ML. The very high accuracy even for large epsilon suggests that the evaluated attacks are not evaluating the right objective to fool the classifier.<BRK>The main comments are listed as follows:(1) To the reviewer, the adversarial framework to use a simple prior to align the hidden features seems new. The evaluations may be more convincing  if more attacks can be tested on the proposed method. Thought these response resolved some of my concerns, it is still not very convincing. On one hand, it seems that the authors did not  comment (4) and (5). On the other hand, the paper may be still in lack of comparisons and/or discussions with the related work.<BRK>This paper proposes a new regularization technique called Embedding Regularization to improve the adversarial robustness. The idea is to use generative adversarial networks (GAN) to perform inference on the latent space by matching the aggregated posterior of the hidden space vector with a prior distribution. By the way, how is lambda determined in the current experiments? The following are my detailed comments.
Accept (Poster). rating score: 6. rating score: 6. rating score: 6. <BRK>This paper introduces a novel method for an adversarial attack named mixup inference (MI). This paper should be accepted because the proposed method is super simple but effective for defending from adversarial attacks under different threat conditions.<BRK>Both Cifar 10 and Cifar 100 are relatively small scale datasets. The authors utilize mixup not for training but for inference (MI; Mixup Inference). I lean to accept this paper. The proposed method is simple but effective, moreover well motivated.<BRK>Why does mixup inference hurt the clean accuracy by 10% (table 2)? The novel procedure presented is "Mixup Inference". Two mechanisms are proposed for why this could help. Review: This paper was interesting, because it has nice experimental results and seems like a good idea, but I feel like the paper needs to be improved.
Reject. rating score: 3. rating score: 3. rating score: 6. <BRK>This paper proposed a graph net based approach for subgraph matching. 2.It seems there s a mismatch between training and inference. Overall the paper is well motivated. 4.The graphs used in experiments are too small.<BRK>This paper proposes a novel algorithm NeuralMCS for maximum common subgraph (MCS) identification. The proposed algorithm consists of two components. This paper proposes an interesting method for MCS detection which would have large application interests. The algorithm has two parts, and the first NN part to learn a matching matrix is mostly based on the already existing algorithm of GMN (Li et al, 2019).<BRK>The authors proposed a novel method to find the Maximum Common Subgraph (MCS) of two graphs. Some suggestions:The GCN based GMN might not be the best choice for graph embedding. The similarity matrix is then normalized using a similar way as the sinkhorn procedure in [1 3]. Overall the paper is well written, and the experiment is good and solid.
Reject. rating score: 3. rating score: 3. rating score: 6. <BRK>I lean towards rejecting this paper for one main reason: the contributions are not enough for this venue. Both the introduced method and the metrics are slight modifications of what already exists and the experimental results do not convince me that the introduced method tackles an important problem with the existing methods. The authors refer to their method as being applied to black box models. This is not a black box method. The advantage of the introduced methods is not very clear on the provided subjective examples. * It might be a wrong intuition but it seems like the fine tuning step seems to make the output scores curated for the M_F metric which would make the results in Table 1 not quite fair.<BRK>Paper Summary:This paper proposed a method to produce instance wise saliency map for image classification tasks. The proposed method develop a U net based generator for saliency mask, where the important modifications are (1) a skewness inducing activation function for mask generation (i.e.controller), which is either a ReLU function or a scaled sigmoid function raised to a certain power. I m leaning toward rejecting this paper in its current form. While I think this paper proposed an interesting strategy in improving the faithfulness of the explainer (i.e.training on the cross entropy loss with respect to original classifier), the rest of the two modification either already exists in the literature (generate mapping at coarser resolution is an idea from Dabkowski & Gal (2017) / Du (2018) as pointed out by the author) or suffers potential technical issues that can benefit from further methodology improvement/empirical justification (please see Major Comments). However, this approach may not be valid in practice. In this work author used only (1).<BRK>The predictor is optimized solely under the classification loss without additional constraints, which therefore improves the faithfulness of mimicking target black box models. This paper introduces two metrics to evaluate the proposed mask predictor and the experiments demonstrate the proposed method outperforms others in terms of faithfulness and explainability. Overall, this paper has some advantages:(1) the method could be a significant contribution to improve the local explanation for black box. All the experimental results demonstrate that the performance of the proposed method is better than others. (3) the paper states the motivation and the proposed algorithm clearly. However, for the experiments, the following should be addressed. (2) More detailed analysis should be added on the experimental results. Moreover, there are some minor comments:(1) Please state more details about the innovation of the method.
Reject. rating score: 1. rating score: 3. rating score: 6. <BRK>This paper carries out several kinds of analysis on the GAT networks of Velickovic (2018), which augment GNN updates with multihead self attention. First, since the graphs studied in this paper are, if not generally sparse to begin with at least they only include connections that are meaningful, the sparsification experiment is a bit hard to understand. I would encourage the authors to continue this line of work so that it can be used to provided guidance to those who would like to make more effective use of GNNs.<BRK>This paper analyzes attention in graph neural networks. It makes two major claims:(1) Datasets have a strong influence on the effects of attention. I have some concerns about this paper: (1) the analysis lacks theoretical insights and does not seem to be very useful in practice; (2) the proposed method for graph sparsification lacks novelty and the experiments are not thorough to validate its usefulness; (3) the writing of this paper is messy, missing many details.<BRK>This paper presents an empirical study of the attention mechanism in the graph attention networks (GAT). The authors further tried to utilize these findings and attempted to do attention based graph sparsification, and showed that they can get a similar level of performance with only a fraction of the edges in the original graph if they do the sparsification based on the attention weights. Given the popularity of the GAT model in the graph neural networks literature and the effectiveness of the attention mechanism in many deep learning architectures, the empirical study presented in this paper focused on attention is valuable. The experiments are clearly motivated and executed, which I appreciate. Normalizing by 2|V| does not guarantee the score is in [0, 1] as claimed in the paper.
Accept (Poster). rating score: 6. rating score: 6. rating score: 6. <BRK>Summary: This paper provides a method to train neural networks with guarantees on outputting probabilities/scores close to uniform on inputs that are out of domain. The paper combines a simple generative model (mixture of Gaussian) for modeling in distribution vs. out of distribution. However, I have some concerns regarding experiments and comparison to previous work. Overall, this idea is a promising approach to obtain networks that are provably under confident far from training examples. — During evaluation, how do you ensure that the radius is not too large such that it has images that the model should actually be confident on (close to in distribution samples).<BRK>The authors present a novel approach for OOD detection; in particular their approach comes with worst case guarantees without compromising on performance. The manuscript is clearly written and I only have some concerns regarding the evaluation. First, while the authors include with MCD an uncertainty aware training approach, I miss more state of the art methods with substantially better OOD performance, including Evidential Deep Learning (Sensoy et al, NeurIPS 2018) and Deep Ensembles. In particular a comparison to EDL would be interesting, since a similar entropy encouraging term in the loss function is used during training, resulting is maximum entropy for OOD samples.<BRK>Experiments do not make unreasonable assumptions such as the ability to peak at the test data, unlike much previous work. My primary concern is that they should show performance on CIFAR 100 not just CIFAR 10, and I certainly hope these experiments will be included during the rebuttal. Small comments:> adv OOD detection with uniform ball perturbedThis is a good way of formulating adversarial OOD detection. This paper should cite _Open Category Detection with PAC Guarantees_ by Liu et al.(ICML 2018) since this also involved provable guarantees for OOD detection. Update: my concerns are addressed but my sentiment is still that this is a 6.
Reject. rating score: 3. rating score: 3. rating score: 8. <BRK>However, due to the limitation of novelty and poor organizations, this paper cannot meet the standard of ICLR. For example, the graph clustering experiments are not convincing.<BRK>In the original Top K paper (Gao et al , Graph U Net, ICML2019), the reported results for Proteins and DD datasets are 77.68%, 82.43%, which are significantly better than the results reported in this paper. 2.The motivation is not clear.<BRK>The math is solid and the concept is well substantiated by results. Results show good performance improvement on different tasks of graph clustering, node and whole graph classification. The paper is well written and clear to read.
Accept (Poster). rating score: 8. rating score: 8. rating score: 8. <BRK>This paper propose to study the generalization properties of GANs through interpolation. Finally they show that the quality of the interpolation can be improved by learning the interpolation and generator jointly.<BRK>This work explores the extent to which the natural image manifold is captured by generative adversarial networks (GANs) by performing walks in the latent space of pretrained models. In order to increase the extents to which images can be transformed, it is shown that GANs can be trained with an augmented dataset and using a loss function that encourages transformations to lie along linear paths.<BRK>Authors experimentally show dependence of range of possible attribute manipulations on the diversity of the dataset in terms of that attribute as well as propose techniques to improve it. Several questions I would like the authors to address to make some details more clear and the paper more complete:1. Why the color distribution of generated images is evaluated on a sampled subset of pixels, not full images?
Accept (Poster). rating score: 6. rating score: 6. rating score: 6. <BRK>Curvature graph networkThis paper proposes a novel network architecture “curvature graph network” that incorporates the Ricci curvature to fully utilize the graph structure. This is very important information especially for extracting hidden clusters in the graph. The proposed curvature driven graph convolution network shows good experimental results on theoretical synthetic data sets and real world benchmark data sets. I recommend “weak accept” to this paper since this paper proposes an interesting concept and it looks promising. Pros.The authors introduce a very useful metric “Ricci curvature” CurvGN 1 and CurvGN 2 outperforms existing graph convolution algorithms.<BRK>The work presents a graph neural network that incorporates graph curvature. The proposed model is able to explore the neighborhood structure of each node, by using the curvature of edges in the proposed framework. Extensive experimental results show the efficacy of the proposed framework. All I can say is the approach is intuitively appealing, the text is well written and easy to follow, even for an outsider.<BRK>In this paper, the authors proposed a novel graph neural network (GNN). The combination method is simple but effective, which is easy to implement and is compatible with existing GNN models like GCN. Experimental results show the potentials of the proposed model to node classification. Overall, I think the idea of the proposed method is interesting. Introducing Ricci curvature to GCN is a potential method to take advantage of structural information beyond the adjacency matrix.
Accept (Poster). rating score: 6. rating score: 6. rating score: 6. <BRK>This paper proposes a first step in the direction of  real world relevant RL approaches  in the sense of considering environments that don t halt their execution until an agent has finished its optimal action computation and execution but actually just go on being an environment. For this, the notion of a concurrent action is introduced. The paper focuses on value based RL approaches. From a theoretical perspective the resulting Bellman operators (for both continuous and discrete time) remain contractions and thus maintain q learning convergence guarantees. Qlearning models are adopted to support concurrent actions and the experiments  demonstrate that the suggested enhancements are working well.<BRK>This paper considers the theoretically interesting and practically important problem of concurrent deep reinforcement learning (DRL), i.e., DRL in which the agent has to decide the next action while performing the previous one. Contraction properties are shown for both the continuous time and discrete time concurrent Bellman equations, and a value based DRL algorithm based on the concurrent Bellman equations is proposed and tested on a few tasks. In addition, the numerical experiments do show that a consistently improved performance of the proposed approach on both synthetic and more real world robotic control tasks. However, there are several significant issues about technical clarity or even correctness in this paper, which I elaborate below:1. The authors need to make these much more clear, and should clearly state the main setting/model that the paper is considering (which seems to be the concurrent discrete time case, but also not very clear to me).<BRK>The paper tackles the problem of making decisions for the next action while still engaged in doing the previous actions. The paper shows how to model such delays within the Q learning framework, show that their modelling preserves desirable contraction property of the Bellman update operator, and put their model into practice by an extensive set of experiments: learning policies for several simulated and a real world setting. The writing of the paper is lucid and sufficient background is provided to make the paper self sufficient in its explanations. These areas of research have also been interested in problems of the "thinking while moving" nature: that of reinforcement learning in the context of neurons where the neurons act by means of spikes in response to the environment and other "spikes" [1, 2]. "Reinforcement learning using a continuous time actor critic framework with spiking neurons."
Reject. rating score: 3. rating score: 6. rating score: 6. <BRK>In this paper, the authors propose a physical driven architecture of DeepSFM to infer the structures from motion. 3.The experiments in section 4.3 are also expected to be improved. In general, the paper is clearly written but I still have several concerns.<BRK>Strengths:The proposed model is well motivated and shows strong performance and generalization ability on several datasets. Weaknesses:The authors claim that the LM optimization in BA Net is memory inefficient and may lead to non optimal solutions. It’s not clear to me that the proposed method can guarantee optimality any better.<BRK>3.In the introduction, the Authors point that prior methods have trouble dealing with textureless, reflective or transparent approaches, but it s not clear form the paper where it addresses these cases, and if yes, what is the mechanism for that. For example, "our network learns a cost volume of size L × W × H using several 3D convolutional layers with kernel size 3 × 3 × 3"    more details about this network are needed, as well as the others in the paper. Training is supervised, and the the results are evaluated on multiple datasets. Is there a mechanism to protect from interpolating across discontinuities?
Reject. rating score: 3. rating score: 6. rating score: 6. <BRK>This paper tackles vulnerability to poisoning. The authors propose using a GAN to generate poisoning data points, as an alternative to existing methods. While most (or all) of the paper is devoted to illustrate the effectiveness of the approach against *non protected* ML. My only and biggest concern with this paper is that no defense mechanism has been tested against, and there are many in the literature. Thus my question for the rebuttal period: How would pGAN perform when defense mechanisms are deployed during the learning phase?<BRK>This paper proposed a method pGAN based on Generative Adversarial Networks to generate poisoning examples in order to degrade the performance of classifiers when trained on the poisoned training data. The authors evaluated pGAN on both synthetic datasets and commonly used MNIST and Fashion MNIST datasets in machine learning. The detailed questions are as follows:Q1: Has the authors tried more complicated datasets such as CIFAR 10 to evaluate the pGAN method? It would make the paper more convincing to add results on more complex datasets. This observation is interesting as it finds that the poisoned input tends to lie on the overlap of two classes. This defense mechanism can be used together with other sanitization approaches. Thanks for the rebuttal.<BRK>The experimental results show that the hyperparameter \alpha significantly affects the poisoning data distribution and pGAN leads to specific error in a classification task. This paper should be weekly accepted, considering the following aspects. (2) The error specific and performance control characteristics of pGAN seem to be interesting. Negative points: (1) The authors should provide more justification on equation 3. (2) The function of the discriminator is not very clear, especially for the classification error test. It would make more sense if the classification error measured from the data the discriminator selects. (3) pGAN can produce error specific attack without sufficient justifications. Is it possible for pGAN to control the specific error tendency?
Accept (Poster). rating score: 8. rating score: 8. rating score: 3. <BRK>The motivation is to adapt or transfer quickly by discovering the correct causal direction and learning representation based on it. The idea of disentangling the marginal and conditional factors to reduce the sample complexity and thus achieve fast adaptation is novel and insightful. The structure causal model is parametrized and then optimized in a meta learning procedure. Here are some concerns about the proposed algorithm: 1). In this case, learning an encoder to infer the correct causal relation is not that difficult. It is highly recommended for the authors to provide discussions about real data tasks with neural causal models in future work.<BRK>This idea is used to develop a “meta transfer” objective function for which gradient ascent on a continuous representation of the model structure allows learning of that structure. It is clear, well motivated, well written, does a good job of connecting to related work, and presents an interesting method for structure learning. Questions and comments:  All else being equal, the speed of adaptation between two very similar models will serve as a good proxy, as shown in this paper. The parameter counting argument is not nearly so strong if what actually changes is the conditional p(A|B). I would still expect more data to be beneficial here.<BRK>The paper proposes a method of discovering causal mechanisms through meta learning, by assuming that models transfer faster if their causal graph is correct. Hence, how would the online likelihood be computed for cyclic models? The meta learning approach appears to be a novel contribution. The authors provide a theoretical argument by counting ‘effective parameters’ to suggest why models using the right causal model obtain a higher online likelihood. For finite training data, the models are distinguishable, while for infinite training data they are not [Fig B.1]. Does the method then still work? 1.4) No experiments for more than two random variables are performed in this paper. Subsequently a claim is made that this effectively reduces the number of parameters and thus that adaptation is faster. However, the zero expectation gradient may still be non zero and even large on the small intervention sample.
Accept (Poster). rating score: 6. rating score: 6. rating score: 3. <BRK>This work proposed a method to reconstruct machine learning pipelines and network architectures using cache side channel attack. Overall, the paper is well written and easy to read. Therefore, I think this is a promising direction for future work. I hope the authors can address my concerns as follows:Q1: What is the knowledge of the attacker? This is important in evaluating this work. For example, one can add some null/useless operation during execution to make the reconstruction process harder? There is a bunch of other architectures found by NAS, e.g.MNas, ENas?<BRK>This paper proposes a way to attack and reconstruct a victim s neural architecture that is co located on the same host. I am wondering compared to just doing NAS yourself, how much gain in terms of resources and time this attack can give?<BRK>Summary This paper proposes to use a computer security method, "Flush+Reload" to infer the DNN architecture of a victim in the setting where both the attacker and the victim share the same machine in a cloud computing scenario. The paper is overall clear and well written. However, I do have some practical concerns about the applicability of the method. Most DNNs, even for inference, are run on (one or multiple) GPUs. Conclusion While the paper, proposed to use Flush+Reload for recovering DNNs architectures and succeeds for at least 2 non trivial architectures, I do not recommend acceptance. Can this realistically happen in a real life scenario?
Reject. rating score: 1. rating score: 3. rating score: 6. rating score: 6. <BRK>This paper proposed a post processing method for improving group based recognition tasks. Several manually designed features based on the pretrained networks are supervised trained on a light weighted network like the teacher student module. This paper should be rejected because (1) the novelty of the algorithm is limited: only using the well known intra class distance and inter class distance as features. Besides, the superiority of such features should be better explained; (2) Similarly, the discriminability of testing image is too complicated, such as Eq.(9, 10).Why these params are designed in such ways? Explain them; (3) Why don’t you learn the discriminability by directly operating on the extracted pretrained features? The used two distances certainly are not the best choices compared with learning methods. You should compare the performance with your methods of training under the manually designed features.<BRK>This paper proposes a discriminability indicator using embedded class centroids on a proxy set, and show the discriminability distribution w.r.t.the element space distilled by a light weight auxiliary distillation network, which is called discriminability distillation learning (DDL). However, to me this is a supporting experiment and it is more important to show the real necessity of the proposed method by comparing it with SOTA methods. 3.8) “The discriminability distillation learning is more practical to untrimmed video action recognition since there are more diversity videos chip with ambiguous content and visual blur problem”  This means that DDL should not be used in some tasks, it is not generalizable, task specific? 2) The related work section should include studies more related to the motivation behind DDL (“….to select represent samples from the whole set efficiently for group understanding.”). For instance, attention models, saliency detection, key frame detection methods, outlier detection methods, quality aware networks, etc. should be discussed and compared with DDL. Instead, authors more focused on face recognition and action recognition tasks themselves. check them all. In other words, the experimental analysis should be extended. One thing authors should consider is including more datasets, especially more challenging datasets i.e., the ones not already saturated. However, only for one dataset such a comparison was performed.<BRK>This paper studies how to aggregate features from group inputs. The paper proposes  Discriminability Distillation Learning (DDL) to compute the aggregation coefficients. The method assumes that each sample has a discriminability property that is directly related to the task. The authors define this property and propose to learn such property by an auxiliary network. Such a network can be used in many models without affecting their original training procedure and is able to improve the performances on many tasks, including set to set face recognition and action recognition. The experimental results are comprehensive and convincing. The idea is simple yet interesting and the results are good, so I tend to give a positive rating. But overall, the paper is not well written, and there are some questions:1. For example,    a. only frames **will** high scores will be weighted and aggregated    b. 2.It seems that the proposed method is limited to tasks that have the concept of “centroids”. Many tasks may not have well defined centroids. The authors are encouraged to discuss more the differences to highlight the novelty and contributions. I’m just wondering if the authors have considered cascade training, i.e., use the discriminability information to train better centroids, which then will define discriminability better.<BRK>In this paper, the authors proposed a discriminability distillation learning (DDL) method for the group representation learning, such as action recognition recognition and face recognition. The main insight of DDL is to explicitly design the discrimiability using embedded class centroids on a proxy set, and show the discrimiability distribution w.r.t.the element space can be distilled by a light weight auxiliary distillation network. The experimental results on the action recognition task and face recognition task show that the proposed method appears to be effective compared with some related methods. The detailed comments are listed as follows, There are many grammar errors and typos in the current manuscript, such as  	Our key insight is to explicitly design the discrimiability using embedded class centroids on a proxy set…The authors proposed DDL based on the principle of the intra class distance and inter class distance. How about the influence of the model if we ignore this normalization? Some ablation study whether or not the normalization is missing.
Reject. rating score: 3. rating score: 3. rating score: 8. <BRK>This paper proposed to address a compound problem where missing data and distribution shift are both at play. The major problem here is the problem appears to be underspecified, and its not clear under what conditions if any the proposed methods are valid. If the data is not missing at random then there is presumably confounding. In short the paper addresses an under specified problem with a heuristic technique based upon domain adversarial nets which have recently been shown have a number of fundamental flaws. Some minor thoughts:“some components of the target data are systematically absent”>>> 	Not clear what “component” means at this point“We propose a way to impute non stochastic missing data”>>> 	What does this mean? What is the pattern of missing ness conditioned on? “This key property allows us to handle non stochastic missing data,” >>> 	again what precisely does this mean? The authors do not address the primary concerns clearly and do not point to specific improvements in the draft that might cause me to change my mind.<BRK>*Summary.* The paper presents and addresses the problem of performing domain adaptation when the target domain is systematically (i.e., not the result of a stochastic process) missing subsets of the data. The issue is motivated by applications where one modality of data becomes unavailable in the target domain (e.g., when deciding which ads to serve to new users, the predictor may have access to behavior across other websites but not on a specific merchant s website). The proposed method learns to map source and target data to a latent space where the representations for the source and target are aligned, the missing components of the target can be inferred, and classification can be performed successfully. *Review.* While the problem statement is novel, I am unconvinced that the advertising experiment includes both a domain adaptation and imputation problem. For this reason, I am giving the paper a weak reject. a specific partner and the target domain is the traffic of the users who have not interacted with that specific partner. In this case, it is not obvious to me why there is a domain shift between these two groups of users. Out of curiosity, do the authors have an explanation for this discrepancy?<BRK>The submission describes an approach for unsupervised domain adaptation in a setting where some parts of the target data are missing. The novelty here comes from the properties that a) domain adaptation and data imputation are handled in a joint manner, b) the missing data in the target domain is non stochastic, and c) imputation is performed in a latent space. This maps to a fairly specific, but realistic enough set of real world problems; the authors give an image recognition as well as an advertising prediction related problem as experimental examples. I d rate the novelty as medium (smart combination of existing methods), but the exemplary experimental evaluation elevates it to more than a systems paper. The method is described clearly in Section 3, and the joint training makes sense. The ablation study in Section 4.4 is interesting w.r.t.the trade off it shows between stable, consistent, "average" results from an MSE loss term, vs. high variance (and on average better) results when a choice of mode is forced using an adversarial loss term. Section 5.2: type "impainting"  > "inpainting"  Appendix, section  Pre processing : It seems to me that there is a clear assumption made that the target set is balanced, since training happens with a balanced source set. Is this realistic in practical scenarios?
Reject. rating score: 1. rating score: 3. rating score: 6. <BRK>More specifically, it formalized a knowledge graph completion problem as an estimation of the optimal embedding by assuming the canonical distribution derived from the embedding and derived the upper bound of misclassification rate of entities under the missing completely at random assumption. In this aspect, this paper gave insights into how we can give justifications to existing embedding methods. However,  I think it is hard for those who are not familiar with this field to follow the logic of the paper, as I write in detail in the following sections. Section 7, Theorem 4	  The upper bound is in terms of the estimator which maximizes the expected log likelihood. The small terms in the upper bound (i.e., first and second terms) depend on the number of objects $|\mathcal{O}|$ and not on relations $|\mathcal{R}|$. We know that the authors used the maximum likelihood estimator if we read the proof of the theorem.<BRK>This work conducts a theoretical study of the generalization bounds for the number of wrongly predicted triples of KG embedding methods. Pros:  This work raises a novel problem, i.e., analyzing the theoretical generalization ability of existing KG embedding methods. The paper is well motivated with clear writing and technically sound with rigorous formula derivation. So in practice, "``missing completely at random" assumption may not hold for KG. Although the authors mentioned "``log loss can in principle be also used and it was observed by Trouillon & Nickel (2017) that the margin based loss functions, used by many knowledge graph embedding methods, are more prone to overfitting compared to log likelihood , more rigorous theoretical analysis is suggested to verify (or refine) the applicability of the proposed analysis 3) It would be better to see more examples that applying the proposed analysis to different KG embedding methods, corresponding comparisons help to get a deeper understanding for existing KG embedding methods, and may shed further light on how to design a KG embedding method that achieves a good enough generalization with fewer model parameters.<BRK>The paper presents a study on bound of the number of the expected triples wrongly predicted by general graph embedding methods. The paper considers methods that follow the "completely at random" assumption and aim to maximize the likelihood (or minimize the log loss). The study considers the learning from positive and unlabelled data problem and theoretically demonstrates the correctness of the bounds on the number of triples that embedding methods can add during the completion of the knowledge graph. The paper is well written, theorems and proofs seem to me to be mathematically sound and correct. However, I have to admit that I am not an expert on the problem, I am more aware of the practical part of this field and less aware of the theoretical part.
Reject. rating score: 3. rating score: 3. rating score: 6. <BRK>This paper proposes a new neural network for tabular data which uses sequential attention to perform instance wise feature selection which can help learn superior decision rules, in addition to facilitating interpretability of the resulting model. If the authors just mean standard training, I recommend they use this name instead.<BRK>This paper proposes a novel deep machine learning model called TabNet for learning from tabular model. The paper proposes a network architecture which seems to achieve good performance on a set of experiments. However, I am missing a motivation for parts of the model.<BRK>I am in favor of this paper as it proposes an interpretable method for feature extraction in tabular learning. It would be better if the authors can run the baselines on the datasets. It seems non trivial to me.
Accept (Poster). rating score: 8. rating score: 8. rating score: 3. <BRK>The adversarial examples are built by training, for each target class, a binary classifier for the class based only on the features of that layer. The paper does not mention adversarial training. This could increase the significance of the paper if working with intermediate feature representation makes the attack more robust to this type of defense. Generally, not using the output layer at all is an interesting approach that deserves in my opinion to be discussed and investigated further. The paper is clearly written, and notation is rigorous.<BRK>This paper presents an adversarial attack based on the feature representations at different layers given the classes. The paper is clear and well written and different interesting experiments support the claims. 1 	Intermediate feature space is not fully independent of the architecture of  the model. I am wondering how different the results would be if adversarial attack was trained on the black box model. In other words, as a simple testing for example, how much higher the success rate would be in the case of DN121 >RN50 if you trained the noise on RN50 itself? How possible do you think it is to make it black box model agnostic?<BRK>The paper proposes a new transfer attack method (on undefended models), called FDA. The method uses auxiliary class wise binary classifiers attached on intermediate layers, and optimizes the targeted probability of the binary model. But I have some serious concerns about the method and results. I m not too familiar with the targeted adversarial attack literature, but one of the very early paper on targeted attack [1] seems to report much higher target matching rate (e.g., see their Table 3, ~70,80%). Why is this the case? 3.One drawback of the method is it needs to train a custom model by oneself first, with one binary model for each class we want to attack or target. This can be very cumbersome in practice. [1] Delving into transferable adversarial examples and black box attacks.
Accept (Spotlight). rating score: 8. rating score: 8. <BRK>In tackling a curious construction by Telgarsky regarding a certain class of functions that can be represented by deep networks (but not shallow networks (unless those shallow networks have exponentially many units)), the authors derive depth width tradeoff conditions for when relu networks are able to represent periodic functions using dynamical systems analysis. (possibly on a synthetic dataset?)<BRK>Sharkovsky s theorem is leveraged to characterize the depth width tradeoff in the ability of ReLU networks to represent functions with periodic points. A lower bound on the depth necessary to represent periodic functions is also provided. The theoretical background is very interesting, but it would be better to start from the contribution to ML and get into the math later on.
Reject. rating score: 3. rating score: 3. rating score: 3. <BRK>Given certain assumptions, they show that this added regularization can improve generalization to unseen problem settings. Also as mentioned in the paper Disentanglement will not work in stochastic environments.<BRK>For a paper that has very little theory and thus most of the value is in the empirical evaluation, I think that is a problem. It seems to me that this paper would exactly fit within that scope?<BRK>Empirical results show reasonable improvements in the StarIntruders task. Exploration of the limitations of the approach. Currently, this paper should be judged solely for its empirical improvements, because there is little formal analysis or rigorous derivations.
Reject. rating score: 3. rating score: 6. rating score: 6. <BRK>Summary:This paper proposes a relation based ZSL model which can effectively alleviate the domain bias problem. Y  union(y_s  y_u), but not intersection(y_s  y_u)5.2) How is the class separation formulated in the framework? Next, the paper hypothesizes that the domain bias problem is due to the overlap between seen and unseen classes in the shared space, and explicitly introduced a discriminator to separate the two domains. The paper performs experiments on ZSL benchmark datasets and shows that the proposed method outperforms other relation based methods. Besides, the domain discriminator which can be applied to other models demonstrates its effectiveness in reducing domain bias given the experimental results. Clear writing logic. However, the domain discriminator has certain novelty and can be applied to other methods. The overlap among seen and unseen classes is an important problem (domain bias problem named by the author) and the add of the domain discriminator to distinguish whether a sample is from seen classes or unseen classes is reasonable, which can provide better class separability (among seen and unseen classes). 3.The novelty of this paper is somewhat limited while missing some relevant works, e.g.[r1, r2].[r1] learns a latent space where the compactness within class and separateness between classes are considered. In arXiv 2018. This is quite problematic.<BRK>The paper presents a novel approach for (generalized) Zero shot learning (GZSL). The main key of the method is using Variational Inference, variational autoencoders. Moreover, the authors also propose to take into account a kind of biasness domain into the learning procedure, which details in adding a regularization of the domain discriminator into the objective function. The paper is nicely written, espcially with a clear formal introduction to the problem of GZSL. How do you know your method works well? I can not find where you have defined a kind of loss so that we can compare the predicted labels \hat{y}_j ? (In Section 2.) 3) It is not enough detail on how do you optimize the objective (8) ?<BRK>The main topic of this paper is generalized zero shot learning. This paper also proposes a domain discriminator to enhance class separability of learned features to avoid unseen classes to be covered by seen classes. Experiment results show their efficiency under relation based setting. Pros:1.This paper proposes an important insight that in generalized ZSL, the unseen classes may be dominated by seen classes in the feature space. Comments:1.The proposed MCMVAE is No longer a VAE but an AE with attribute matching loss. 3.Why discriminator is harmful for PSE method?
Reject. rating score: 3. rating score: 3. rating score: 3. rating score: 6. <BRK>Summary:This paper proposes to impute multimodal data when certain modalities are present. The authors present a variational selective autoencoder model that learns only from partially observed data. Strengths:  This is an interesting paper that is well written and motivated. Some of the datasets the authors currently test on are quite toy, especially for the image based MNIST and SVHN datasets. One drawback is that this method requires the mask during training. In other words, we only see multiple modalities as input, but we are not sure which are noisy and which are not?<BRK>The paper proposes a novel training method for variational autoencoders that allows using partially observed data with multiple modalities. The key idea is to use two types of encoder networks: a unimodal encoder for every modality which is used when the modality is observed, and a shared multimodal encoder that is provided all the observed modalities and produces the latent vectors for the unobserved modalities. The “ground truth” values for the unobserved modalities are provided by sampling from the corresponding latent variables from the prior distribution once at some point of training. * The baselines in the experiments could be improved. First of all, the setup for the AE and VAE is not specified.<BRK>The paper proposed variational selective autoencoders (VSAE) to learn from partially observed multimodal data. Overall, the proposed method is elegant; however, the presentation, the claim, and the experiments suffer from significant flaws. 2.In my opinion, the idea is elegant. Second, the synthetic image pairs are not multimodal in nature. [Cons]1.[The claim] One of my concerns for this paper is the assumption of the factorized latent variables from multimodal data. I do expect the paper be a strong submission after a significant effort in presentation and experimental designs. The presentation is undesirable. I list some instances here.<BRK>To infer latent variables from partial observation data, they introduce the selective proposal distribution that switches encoders depending on whether each input modality is observed. This paper is well written, and the method proposed in this paper is nice. In particular, the idea of the selective proposal distribution is interesting and provides an effective solution to deal with the problem of missing modality in conventional multimodal learning.
Accept (Poster). rating score: 8. rating score: 8. rating score: 6. <BRK>***This is an excellent paper that integrates inductive biases from physics into learning dynamical models that can then be integrated into deep RL based control tasks. Can the authors comment on how this model would scale to larger (e.g.multiple joints) dynamical systems? The dynamics function is explicitly written as the equations of the Hamiltonian dynamical system, involving the 1) inverse of the mass function, 2) potential energy and 3) control function, in a complex graph (Figure 1) that transforms positional and angular coordinates and momenta x and the external control into f(x, u). The derivation of the method is long but very well written and didactic.<BRK>In this paper, the authors propose a framework for learning the dynamics of a system with underlying Hamiltonian structure subjected to external control. *************************************The authors addressed my comments and answered  my questions clearly. This is very interesting and clear. In particular, I appreciate that the paper is pretty much self contained and that the authors derive the theory from first principles.<BRK>  Update  The authors have done a good job at addressing my concerns, and the revised version of the submission is substantially improved. I have adjusted my score and recommend accepting the paper. The paper shows that this approach does not have a problem with generalizing to large angles that the naïve approach does. The paper shows evaluation of the learned dynamics system for control on two examples, an inverted pendulum and CartPole. There are some promising leads explored in this paper for learning physical system dynamics effectively with neural nets. Hamiltonian Neural Networks.
Accept (Poster). rating score: 6. rating score: 6. rating score: 3. <BRK>The paper presents a new NN architecture designed for life long learning of natural language processing. Compared to the old methods that train a separate generator, the performance of the proposed method is noticeably good as shown in Fig 3. This demonstrates that the new life long learning approach is effective in avoiding catastrophic forgetting. The motivation of the paper is clear. As the authors have discovered, the performance is highly dependent on implementation.<BRK>Summary:The paper proposes to use the same language model to learn multiple tasks and also to generate pseudo samples for these tasks which could be used for rehearsal while learning new tasks. Please change the title! Language modeling is NOT all you need for lifelong language learning. 6.On page 5, you mention that MTL is used to determine whether forgetting is caused by a lack of model capacity. 10.I assume that the authors will release the code upon acceptance of the paper.<BRK>This paper studies the problem of lifelong language learning. Consider the NLP tasks as QA and then train a LM model that generates an answer based on the context and the question; 2. to generate samples representing previous tasks before training on a new task. The baseline from using real data is missing;3. Compare the proposed method with existing baselines.
Reject. rating score: 3. rating score: 3. rating score: 6. rating score: 8. <BRK>The author also derived a variational representation of the optimal accuracy complexity region, which also expresses the optimal encoder and decoder map as the solution of the optimization problem. I incline to reject the paper, for the following reasons. Even if the precise accuracy complexity region is obtained, it says little about the sample complexity needed by a learning algorithm to achieve this region. If this is the case, I think the proposed method may have some value in practice. I encourage the authors to study the gap between the variational lower bound and the optimal region, and maybe do more experiments to find a good use case of the proposed method.<BRK>It also proposed an algorithm to learn the distributed representation without any prior knowledge of the data distribution. The multi view learning problem has been quite well studied in the literature. The paper reformulated the multi view learning problem as a Bayesian inference problem and provided solid analysis for it. The writing of the paper was pretty hard to follow for me, with a lot of notations that are not defined clearly. * What is \Omega in Theorem 2? The algorithms in the experimental result are not very clearly defined.<BRK>I am not an expert in this area and the paper involves a lot of derivations and proofs, but I did not check the correctness of those derivations. To make this idea to work, the paper used mutual information as the objective function to control the accuracy if the model, and at the same time to avoid overfitting the paper proposed to use MDL as a measure to control the complexity of the model. The consequence is that it make the paper to be hard to read. 3) Experiments: there are a lot of papers describing to integrate data sources for at least the MNIST example. I do not understand these notations. I am confused by these notations7) In equation (6), it is more readable to explicitly define the Shannon Mutual Information.<BRK>In this paper, the authors studied the distributed representation learning problem, where multiple sources of data are processed to provide information about Y. They studied this problem from information theoretic point of view. 1.The optimal trade off between the accuracy and complexity were studied for discrete memoryless data model as well as memoryless vector Gaussian model. 2.A variational bound were constructed in order to connect the optimal encoder and decoder mappings with the solution of an optimization algorithm. In general, I think the paper is well organized. The definition of the problem and the motivation of the approach are clear.
Reject. rating score: 1. rating score: 3. rating score: 6. <BRK>I went over this work multiple times and had a really hard time judging the novelty of this work. The paper seems to be a summary of existing work reinterpreting variational autoencoding objectives from an information theoretic standpoint. In particular, the paper seems to follow the same analysis as in Wasserstein Autoencoders (Tolstikhin et al., 2017) and InfoVAE (Zhao et al., 2017). More importantly, the claims around "more disentangled representation" are imprecise in light of this work. A proper discussion on the contributions of this work as well as discussion on the above related works would be desirable on the author s end.<BRK>The paper develops an information theoretic training scheme for Variational Auto Encoders (VAEs). In the absence of a quantitative evaluation metric, they are not informative. This submission cannot be treated as a contribution without showing an improvement on top of this extremely closely related work. The problem is the same, the solution is almost the same, and the theoretical implications of the solution are also the same. For instance see:Dinh et al., Density Estimation Using Real NVP, ICLR, 2017The presence of such two strong alternative remedies to the problem addressed by this work makes its fundamentals shaky.<BRK>Overview: This paper describes the Variational InfoMax AutoEncoder (VIMAE), which is based on the learning principle of the Capacity Constrained InfoMax. Contributions: The authors clearly state the contributions of the paper themselves, but in a summary: a derivation of a variational lower bound for the max mutual info of a generative model, definitions and bounds estimation for a VAE, associations for generative quality and disentanglement representation to mutual information and network capacity, and finally proposing the Capacity Constrained InfoMax. You mention both InfoVAE and β VAE yet only test your models against a β VAE model. The semi supervised learning experiment is interesting (the one based on Zhao et al.) I’d like to hear a discussion in the paper about future work and how the authors believe this could be applied elsewhere.
Reject. rating score: 1. rating score: 3. rating score: 3. <BRK>This paper extends GraphSAGE in several dimensions: 1) applying attention when aggregating neighbors (already used by GAT and many other approached); 2) Ensembling node embedding by applying DualENC multiple times on positive pairs selected by random walk (this is doing aggregation of neighborhood again); and 3) adding global bias. All this makes the proposed method an incremental extension of existing solutions. There is no theoretical justification why these extensions should work. However, there are many other inductive graph embedding learning approaches, such as (Hamilton et al.(2017b) Velickovic et al.(2017) Bojchevski & Gu ̈nnemann (2017) Derr et al.(2018) Gao et al.(2018), Li et al.(2018), Wang et al.(2018) and Ying et al.(2018b)).It is necessary to compare with these approaches.<BRK>This paper proposed a dual graph representation method to learn the representation of nodes in a graph. The experimental result demonstrates some improvement over existing methods. But the novelty is limited.<BRK>This paper presents a unified framework CADE for unsupervised graph representation learning, which combining the local neighbors and global information. 2.Based on GraphSAGE, the module DualENC can include positive pairs simultaneously, capturing the context aware information. 3.In extensive experiments on Pubmed and PPI for link prediction, they demonstrate that their model beats the state of the arts. In other words, this paper lacks novelty, which seems incremental to the existing works. For example, in section 3.2, equations 2 6 are reduplicative that have already appeared in algorithm 2. 3.The experimental comparison methods and results are not complete.
Accept (Poster). rating score: 6. rating score: 6. rating score: 6. <BRK>The paper gave a very detailed analysis of the value function of the gamblers problem. The exposition is clear and well structured. The paper presented both an interesting issue and a clear analysis, and should be accepted.<BRK>The paper derives the optimal value function for the continuous and discrete versions of the Gambler s problem.<BRK>The paper revisits the Gambler s problem. It studies a generalized formulation with continuous state and action space and shows that the optimal value function is self similar, fractal and non rectifiable. Overall, the paper is extremely well written.
Reject. rating score: 1. rating score: 1. rating score: 3. <BRK>Summary: The authors propose a deep reinforcement learning method for training how to choose pivot rules for the simplex algorithm for a set of LP instances. The artificial TSP instances used in the experiments are not convincing for practical applications. So, the proposed method is far from practical yet. As a summary, the idea could be interesting, but the paper is not mature enough. I think the authors  work is valuable and should be investigated further, but the scalablity issues make me feel that the paper is still premature.<BRK>The paper proposes learning a policy for selecting a pivoting rule to apply at each iteration of the Simplex algorithm for linear programming. I still believe the paper is not ready for publication, so I ll keep my rating unchanged. Cons:  The results on the 5 city synthetic TSP instances are not sufficient. Additional comments:  Although I’m recommending rejection, the problem is interesting and I hope the authors will continue working on it to develop the ideas further and collect results on larger scale problems.<BRK>The authors present a learned method for speeding up optimization of LP, with an application to the TSP problem. The paper presents a quite interesting approach to solving the TSP, using NNs on top of the tableaus. It seems yes, but it is not clear. Experiments are somewhat weak, and seem to indicate that the method does not have much practical value.
Reject. rating score: 1. rating score: 3. rating score: 6. <BRK>We think that the paper is not of enough quality to be accepted in ICLR. As a consequence, the contributions are not clear. The authors compare the proposed method to only two spectral clustering methods, which as the standard kNN and the Consensus kNN from 2013. However, we think that it is still of not sufficient quality. There have been many spectral methods in graph learning for large scale datasets.<BRK>This paper proposes a scalable approach for graph learning from data.<BRK>This paper presents a scalable spectral approach for graph learning. The authors claim that the proposed graph learning approach is highly scalable.
Reject. rating score: 3. rating score: 3. rating score: 3. <BRK>This work aims to produce reinforcement learning methods that are ‘distributionally robust’. I went through this section, the main results, and the proofs in the appendix. There are a great many grammatical errors throughout the paper. The experimental results are very minimal and not very convincing.<BRK>This would definitely improve the quality of the paper. And support it better with more comprehensive and convincing experimental results. “Algorithms for CVaR Optimization in MDPs”. The algorithm is new, although its section (Sec.3) is not well written.<BRK>In this work the authors studied the robust reinforcement learning problem in which the constraint on model uncertainty is captured by the Wasserstein distance.
Reject. rating score: 6. rating score: 6. rating score: 6. <BRK>Perhaps a paper organization more amenable to this venue would be to shift some of the lengthier equations into an appendix and use the space of the paper to discuss a more conceptual and contextual understanding of why this technique is desirable, at each step, relative to other possible quantum techniques. The overwhelming majority of quantum machine learning references in this paper appear in physics journals (all but one, which was ICML 2019). (I have a background in physics but not quantum computing.)<BRK>While the main selling point of quantum LS SVM is that it scales logarithmically with data size, supervised algorithms shall not fully enjoy logarithmic scaling unless the cost for collecting labeled data is also logarithmic, which is unlikely. Technically, there are two main contributions. The first is the method of providing Laplacian as an input to the quantum computer. My main concern about the paper is on its organization. Some background in quantum computing offered in page 3, 4, 5 are quite nice, but for a conference paper, I think this is an overkill. Specifically, Generalized LMR technique and Hermitian polynomials in Kimmel et al.(2017) could be discussed in more detail.<BRK>Strengths: This is an interesting emerging research topic that has its significance. Also, this paper prodives a nice tutorial on the key ideas of quantum machine learning and provides detailed derivations and analysis on the proposed algorithm. Weaknesses: The novelty of this work seems to be incremental. It largely extends the existing algorithms such as HHL and LMR.
Reject. rating score: 3. rating score: 3. rating score: 6. <BRK>But for now, I am inclined to not change my score. They demonstrate that their method achieves comparable performance with much speedup. Strengths, 1, This paper is well written and the ideas are well presented. Overall, this paper is very comprehensive. 2, As evaluated and validated in the experiments, the proposed method vastly reduces the inference time at test phase. And I think it is probably unfair, as distilling the single point distribution to the Dirichlet under KL, EMD, MMD might require large amount of particles, which they don t have. The paper claims to achieve 500x speed up, while I reckon the performance of MCDP and SGLD won t deteriorate a lot if you use only fewer particles.<BRK>Overall I liked several results presented in this paper. As demonstrated by BDK SGLD vs. BDK DIR SGLD. The proposed idea is a simple and meaningful improvement over previous works. However, I do find some discussions in the paper unnecessary and would expect for more technical contributions. Everything seems straightforward given the hypothesis F is of enough capacity, which obviously does not hold in practice. * The concentration model is parameterized using an exponential activation, how does this activation affect the performance? * MMD/wasserstein distances are cool but they require also samples from the student, which adds more variance to the distillation process. What is "uncertainty measures", are they used as metrics for detecting out of distribution data, how are AUROC/AUPR calculated using the uncertainty measures? * I found most numbers convincing except that sometimes BDK SGLD outperforms BDK DIR SGLD, if I understand it right, the predicted mean of BDK DIR SGLD should be as good as BDK SGLD? Minor:* On page 4, above Eq.(4) there is a broken figure link.<BRK>This paper studies the problem of avoiding Monte Carlo (MC) estimate for the predictive distribution during the test for Bayesian methods. The authors propose One Pass Uncertainty (OPU) methods to approximate the predictive distribution through distillation. Although the restriction of the student distribution to be tractable seems to limit the design of the student model significantly. And this restrictive distribution family may cause large amortization error, as suggested by Lemma 1 in the paper. The experiments are well conducted, and the proposed method is well evaluated. Significance:This paper studies an important problem in Bayesian machine learning and the proposed method can be combined with many Bayesian methods to reduce the computational cost during the test.
Reject. rating score: 1. rating score: 3. rating score: 6. <BRK>Distillation proceeds by minimizing the KL divergence between the predictions of each ensemble member with the corresponding head in the student network. Experiments illustrate that the multi headed architecture approximates the ensemble marginally better than approaches that use a network with a single head. This improved fidelity, however, comes at the cost of increased computation and storage requirements (which scale linearly with the size of the ensemble). The paper would have been more interesting if the authors had managed to demonstrated significant improvements over competitors on not toy (MNIST / CIFAR) problems. Unfortunately, this is not the case and the fact that similar ideas (Lan et al.)<BRK>Summary & Pros  This paper proposes a simple yet effective distillation scheme from an ensemble of independent models to a multi head architecture for preserving the diversity of the ensemble. This paper experimentally shows that multi head architecture performs well on MNIST and CIFAR 10 in terms of accuracy and uncertainty. As the authors mentioned, it also used for online distillation [1]. This paper provides experiments on only small sized 10 class datasets, MNIST and CIFAR10. To overcome this gap, the proposed method requires more parameters.<BRK>Overview:This work introduces a new method for ensemble distillation. The method itself is a simple extension of earlier “prior networks”: the original method suggested to fit a single network to mimick a distribution produce by given ensemble, and here authors suggest to use multi head (one head per individual ensemble member) in order to better capture the ensemble diversity. Writing:The paper is well written, illustrations are good. However, I am a bit concerned that the method itself seems like a trivial extension of the prior work, and does not really provide much addition insight.
Accept (Talk). rating score: 8. rating score: 8. rating score: 8. <BRK>The paper proposes a model that can perform multi hop question answering based on a textual knowledge base. The paper is a good one and I vote for its acceptance.<BRK>This paper introduces a new architecture for question answering, that can be trained in an end to end fashion.<BRK>They start with linking mentions to entities in a knowledge base. The architecture is essentially end to end trainable (except for specialized pretraining discussed below).
Reject. rating score: 3. rating score: 3. rating score: 3. <BRK>Update: I thank the authors for their response. I believe the paper has been improved by the additional baselines, number of seeds, clarifications to related work and qualitative analysis of the results. I strongly believe the baselines should be tuned just as much as the proposed approach on the tasks used for evaluation. I am concerned this leads to an unfair comparison given that different models may work better for different sets of HPs. 2.Using only 3 seeds does not seem to be enough for robust conclusions. What is the learned behavior of the agents?<BRK>Update: I thank the authors for their rebuttal. Having read the other reviews I still stand by my assessment and agree with the other reviewers that the empirical validation should stronger, adding more baselines and conducting experiments on the same environments as your main competitors for fair comparison. The paper has sufficient technical depth. StrengthsInteresting non parametric approach to estimating uncertainty in the agent s forward dynamics modelClearly written paper with sufficient technical depthWell structured discussion of related workWeaknessesMy main problem with the paper is a missing fair comparison to prior work. I understand this would require dealing with discrete action spaces, but I don t see why this would be infeasible.<BRK>It is mentioned as a novel aspect of the work, but never reallyjustified at all. The main differencewith this work and Pathak et al.seems to be that the variance isall coming from one particular conditional distribution rather thanan ensemble of models, but in Pathak et al it is also a distributionover models. This paper would greatly benefit from some explanation.
Reject. rating score: 3. rating score: 6. rating score: 6. <BRK>This paper further shows that using the data pre processing step proposed in (Coates et al., 2011) can boost performance of CNN NNGP/NTK based ridge regression by ~7% which allowed the authors to achieve classification accuracy in high 80s which is AFAIK SOTA on CIFAR 10 when not using learned representations. My current assessment of the paper is “weak accept”. Major comments:  Can you please clarify why you decided to give a new name (Box Blur) to standard average pooling? Why not just use the existing name? Looking at fig.1b, it seems like the images are not as unrealistic as in fig.1a but human eye can still tell they are not realistic (potentially even more so with other images than the one selected for this figure). Unfortunately, I cannot accept these results to be published “as is”. Can you clarify and also report the results of this experiment with all the layers trained please?<BRK>Cons:  Given the claim in the abstract about accuracies, it should be pointed out that: * in the unsupervised setting, with a kernel engineering method, you can obtain ~86% on cifar10 (cf https://arxiv.org/abs/1605.06265 )* in the no data(up to a linear model) setting, it is possible to get ~82% on cifar10 with the scattering networks (cf https://arxiv.org/abs/1412.8659 )Those two works are also mainly empirical, and thus some accuracies of this paper should be compared to them. While this should have been a positive aspect of the paper, I noticed that the accuracies reported here are computed from the test set. Section 5: The paper cites the Local Average Pooling as a "new operation", but this is clearly standard in the literature. "Boxblurring" has always been named average pooling in deep learning, low pass filtering in signal processing. This could be commented. Post discussion:The revision clarifies all my concerns and this work is likely to induce interesting discussions.<BRK>This paper builds on recent developments of CNN GP and CNTKs in multiple fronts obtaining significant performance boost on CIFAR 10 dataset (and some mild boost on Fashion MNIST). The authors also introduce flip data augmentation by doubling the dataset. It would be interesting for future work to find kernel operation similar to GAP that encodes symmetries of the dataset. Also in this regard the title could be misleading. It’s strange to have paper’s strongest result is based on CNN GP while the title only mentions CNTK. As the author’s mention in the paper, Box Blur is just an average pooling operation.
Accept (Poster). rating score: 8. rating score: 8. rating score: 6. rating score: 6. <BRK>I would suggest that for the conference presentation the authors try to bring out the essential within a less formal setting to open it to a wider ML audience. Remarks:The authors should point out how this is specific to convolutional neural networks. For an audiance who are not quantum experts one could clarify a few properties of the described "quantization". It is not able to take multiple images in at the same time as quantum superpositions.<BRK>This algorithm has complexity bounds that would open up (for instance) the possibility of exponentially large filter banks, and the authors show through a simple, classical simulation approach that the resulting network is also likely to be trainable. The discussion of using a sigma based classical sampling rather than the eta based quantum importance sampling mentions a "Section C.1.15" which does not exist (I think you mean the end of Section C.1.5). Re: "We will use this analogy in the numerical simulations (Section 6) to estimate, for a particular QCNN architecture and a particular dataset of images, which values of σ are enough to allow the neural network to learn."<BRK>This submission proposed a quantum convolutional neural network (QCNN). As you both used the term "QCNN", it is better to explain more clearly what is the main difference in the main text. This subject is out of my usual area. However, I tend to think this subject is interesting to the ICLR audience due to the recent advancement in quantum computing.<BRK>But CNNs are notoriously known to reduce the number of weights in a network because of weight sharing. So, it is not all about making one convolution faster but also to compute invariant representations by sharing weights over different parts of the inputs. I would have been interested by a discussion about how quantum noise may impact this property and I didn t find this in the paper nor the appendices. Especially, I found the experimental section was missing details.
Accept (Poster). rating score: 6. rating score: 6. <BRK>This paper proposes a method, hierarchical visual foresight (HVF) that learns to break down the long horizon tasks into short horizon segments. These subgoals are optimized to have meaningful states and used for planning. Also, the paper is well written and easy to understand. Questions/Concerns:  How important is the output quality of the video prediction model? The number of subgoals needs to be fixed.<BRK>The paper is clearly written, although many sub components of the architecture are described in other articles. However, I don t believe that this approach is in any way practically feasible at present, and the paper makes a number of hyperbolic claims that should be toned down to give a more balanced perspective (I appreciate that the authors attempted to do so in their limitations section, but this needs to be done throughout the paper).
Accept (Poster). rating score: 6. rating score: 6. rating score: 6. <BRK>This paper investigates the theoretical support for Generative Adversarial Imitation Learning (GAIL).<BRK>The submission provides theoretical analysis of GAIL regarding its generalization and convergence properties. I would be interested in some insights on the strong oscillatory behavior of the NN reward on the acrobot task.<BRK>Overall, I think that this is an interesting, if somewhat incremental and technical paper about the generalization and convergence of GAIL. First, on the generalization aspects, the methodology here seems to largely parallel Arora et al.s analysis of generalization in GANs.
Reject. rating score: 6. rating score: 6. rating score: 6. <BRK>This paper proposes a way to compress Bert by weight pruning with  L1 minimization and proximal method. This paper is one of the first works aiming at  Bert model compression. The authors think the traditional pruning ways can not work well for Bert model, so they propose Reweighted Proximal Pruning and conduct experiments on two different datasets. The authors propose a new method RPP for Bert model compression. 2.In the experiments, the authors only compare RPP with self designed method NIP instead of any existing pruning method. 5.It is not clear what the authors want to express in Figure 2.<BRK>This paper has proposed using proximal gradient descent to find sparse weights for BERT to reduce the number of parameters and make the model smaller. therefore, they propose to use reweighed sparse method and optimise it using proximal gradient descent which provides a closed form solution for sparse constraint. they have also provided some visualisation for the weight matrices after sparsification. The paper is well written and easy to follow with nearly comprehensive related work. However, there are some drawbacks:1. 2.It should be explained clearly about all the matrices included in the sparsification steps, despite only saying “parameters of the model”. 3.More analysis is required on the results, specially the diagrams for fine tuning over different datasets.<BRK>The paper proposes a new approach to prune weights that is designed keeping large scale pre trained language representations like BERT. Such a method is desirable for deploying such models on devices with limited memory like phones etc. Experiments on Squad and Glue datasets show that a pruned version of the model maintains high accuracy for these tasks. Pros1.Pretty high pruning ratios (80%) can be used for many datasets (except Squad). Modest technical contribution. Unclear what weights participate in the pruning objective. This needs more elaboration.
Reject. rating score: 1. rating score: 3. rating score: 6. <BRK>First, f and g are functions, not kernels. The work is now improving, and is on the right track for publication at a future conference. The authors propose a procedure for improving neural mutual information estimates, via a combination of data augmentation and cross validation. See the NeurIPS paper for details.<BRK>The first contribution is based on adapting the MINE energy based MI estimator family to out of sample testing. I would have been interested in "false detection" experiments: comparing estimator in a variety of problems where the mutual information is zero, but for different marginal distribution. Hence the theoretical sample complexities contributed are not comparable to those of MIME.<BRK>The paper proposes a neural network based estimation of mutual information, following the earlier line of work in [A]. 2.The results on the synthetic datasets show that the resulting estimator does have low variance and the estimates are less than or equal to the true MI value, which is consistent with the fact that it is a lower bound estimation. Some improvement suggestions:1. I am inclined to accept (weak) the paper for the following reasons:1.
Reject. rating score: 6. rating score: 6. rating score: 6. <BRK>One of my main complaints arose from a misunderstanding that none of the baselines model worker competencies and task difficulty. Given that and the other additions to the draft, I am changing my assessment from 3 to 6. In this paper, the authors extend the classical probabilistic model of Dawid Skene (DS) for predicting the final label of crowdsourced tasks. The authors show that the proposed approach outperforms several baselines on 5 different datasets. The presentation suggests that somehow the proposed approach is novel in its end to end framework. Carpenter, B.<BRK>This is a solid piece work that deals with a very practical problem – training ML models with crowdsourced data with imperfect annotations. The proposed method is not completely new: many ideas have been adopted from previous works. I have a number of concerns:1, One of the main claimed novelties of the paper is an “end to end” approach that unifies ground truth label prediction and label aggregation. 2, The authors made a claim on the ability to estimate the quality of the annotator. Also it is not clear how this vector embedding (Sec.3.2) is implemented. 3, In the implementation, the strong BERT model was used for language but VGG was used for image representation.<BRK>What the authors propose is a framework which allows one to combine a direct graphical model of how human annotations are produced with model training. The graphical model introduces latent variables for the difficulty of an instance and the competence of an annotator as well as the (unobserved) true label. This way one can potentially benefit from the meta information about the annotators (e.g., their demographics) and improve upon the majority voting baseline of aggregating available annotations. I am not an expert in this area, but as far as I am concerned the contributions are sufficient to accept it. However, I have the following questions and concerns:1. 4.I have not followed this topic much but it seems to me that there should be more related work on modelling annotators  competence and item difficulty for crowd sourced annotations.
Reject. rating score: 1. rating score: 3. rating score: 3. rating score: 3. <BRK>It seems to me that the data and thus the analyses lack the generality needed for the purpose of understanding behaviors of neural networks on real tasks/data. The same is true for “Neural network design” (p.3), in which 13 experiments conducted in this study are explained.<BRK>The authors studied the local codes in neural networks through a set of controlled experiments (by controlling the invariance in the input data, dimensions of the input and the hidden layers, etc.), and identified some common conditions under which local codes are more likely to emerge. It remains unclear whether the presence of those components would change the emergence behavior of local codes, and hence affect some of the conclusions in the paper.<BRK>Paper Overview: This paper aims to study when hidden units provide local codes by analyzing the hidden units of trained fully connected classification networks under various architectures and regularizers.<BRK>This paper studies the emergence of local codes in neural networks on a synthetic dataset. and figure ?? The paper is very difficult to read for me, partly due to its writing in a language (local codes) that I m not familiar with.
Reject. rating score: 1. rating score: 1. rating score: 3. <BRK>This paper compared several classification methods, including deep neural networks (DNN), to identify hate speech texts. The research topic is important from the sociological viewpoint. I m not sure whether this paper suits to the publication from ICRL. Overall, this paper did not provide any useful knowledge, while this paper introduced some statistical methods and showed numerical results. I recommend the authors to add more beneficial insight and to submit the paper to other conferences that deals with sociological issues.<BRK>I m sorry to say that this paper is not ready for publication. I think it s an important area and the dataset collected could be quite valuable for tackling hate speech. The paper does not follow the style guide, is full of typos or  kkkkk  tokens indicating missing values. Codeswitched needs to be mentioned more specifically in the introduction. How many of the tweets were multi lingual? The paper did not feel sufficiently anonymized.<BRK>Comments:  This paper considers codeswitched hate speech texts from an NLP perspective. Focuses on kenyan presidential election. The performance of the CNN does not seem very strong. If the paper is proposing a new task with many baselines, it s also important to release the dataset and code in my opinion (I believe ICLR allows this to be done in a de anonymized way). Review: This paper deals with an important problem in social media analysis. Unfortunately this paper needs more polish to be appropriate for ICLR.
Reject. rating score: 1. rating score: 3. rating score: 6. <BRK>The authors analyze the stability of a randomly initialized ResNet in terms of the scaling of the network and its depth. These results, along with some further results, are used to extend convergence proofs of ResNets towards more standard initializations. Unfortunately, this paper is not a good contribution at the moment, as the work, although potentially interesting, is somewhat incremental and poorly presented. The union bound argument should be more clearly spelled out, especially given the fact that some smoothness should also be established for this argument (e.g.the trivial sub multiplicative bound). The presentation of the current results could be improved by including standard deviation when averages are reported, and changing figure 4 with axes starting at zero (bar charts which do not start at zero are extremely misleading).<BRK>Based on this analysis, a linear convergence rate of the gradient descent for the squared loss is also shown. Improve the depth dependence in the network width and the iteration complexity compared to existing studies. Indeed, a simple way of using a natural spectral bound cannot explain the stability of ResNets with $\tau L^{ 0.5}$. Quality:I think the quality of the paper can be improved. However, the advantage of ResNets is still unclear because high over parameterization is required.<BRK>This is an interesting submission which presents important theoretical results while also showing their practical pertinence via experimental validation. The paper is well written and the presentation is clear. Prior work is reviewed appropriately. The authors further indicate in their experiments that for a given width the convergence of ResNet does not depends *much* on depth, compared to feedforward networks. So there might still be a dependence though much milder than that found by the current theory.
Reject. rating score: 1. rating score: 1. rating score: 3. <BRK>The paper provides a method/metric for comparing sentence embedders, based on a nearest neighbor analysis. The method is straightforward: sample a sentence, embed the sentence and lots of sentences from a corpus, find the k nearest neighbors in the corpus to the sampled sentence, do the same for another embedder, calculate the overlap of the two sets of nearest neighbors.<BRK>The paper proposes a method to estimate the similarity of sentence embedders called N2O with the goal to better inform embedder choice in downstream applications. Hence, two embedders which are similar according to N2O may perform differently in downstream tasks. Hence, the approach is not well motivated.<BRK>The paper proposes N2O, a tool for probing the similarity among sentence embedders. Given two sentence embedders, N2O measures the amount of overlap of the k nearest neighbor sets reported by the two embedders, averaged over a sample of probing queries. The methods in these works are mostly identical to N2O with some minor variations (e.g., using Jaccard distance). However, the results do not offer a lot of new insights.
Reject. rating score: 6. rating score: 6. rating score: 6. <BRK>This paper presents a novel method to extract cross modal text visual embeddings on the HowTo 100M corpus. The paper is well written and explains the main problem well, however I do have a few questions:  I do not understand the sentence "However, for images and videos, the inputs are real valued vectors." It would be good to know how good the ASR is, and if adding in punctuation post hoc works well, and how this influences your use with a pre trained BERT model. at the end? Also, would it be possible to compare the results of your work with some of the work in (Miech, 2019c)   it almost seems that your work avoids comparing your results to this previous work.<BRK>From the paper: "we use 32 Cloud TPUs. However, with that being said, it s a good paper of general interest to the community. The paper focuses on self supervised learning in video, and combines two contributions. The second is a cross modal (BERT) model that requires language and vision. The paper is full of technical details to reproduce the results. This makes the main novelty is actually in showing that this approach works.<BRK>This paper is about a self supervised video representation with a multi modal learning process that the authors then use for performance on a variety of tasks. The main contribution of the paper is a successful effort to incorporate BERT like models into vision tasks.
Accept (Spotlight). rating score: 6. rating score: 6. rating score: 6. <BRK>Overall, this paper was well written with useful illustrations and clear motivations.<BRK>It is a pity that the code is not provided. Experiments are conducted on datasets in the chemistry domain and the biology domain showing the benefit of the pre training. The paper addresses an important and timely problem.<BRK>Experiment: The experiments are overall good. I am relative positive for this work. The most similar paper at the same period is [Z Hu, arXiv:1905.13728].
Reject. rating score: 1. rating score: 3. rating score: 6. <BRK>Apriori, it is not clear why removing the second moment is important. The paper is meandering and confusing. See the detailed review below:  Section 1: "the generalization results (of adaptive methods) cannot be as good as SGD". Please cite the relevant papers, (for example, Wilson.2017)  that show this empirically. Section 1: "the proposed algorithm outperforms Adam in convergence speed"   Please clarify what "convergence speed" refers to. Indeed, this is batch gradient descent. How is this an adaptive method then?<BRK>This paper proposes new stochastic optimization methods to achieve a fast convergence of adaptive SGD and preserve the generalization ability of SGD. In both eq (14) and (15), it is not clear to me why the authors divided the summation into two parts. The regret bound in Theorem 4.2. is not quite satisfactory. However, there is an additional factor of $\sqrt{d}$ in the regret boun in Theorem 4.2, which is not appealing in high dimensional problems.<BRK>Balancing the generalization and convergence speed is an important direction in deep learning. The authors should provide more materials to support this claim, such as a comparison table of different mathods.
Reject. rating score: 1. rating score: 3. rating score: 3. <BRK>Motivation is presented to the agent as a component of its inputs, and is encoded in a vectorised reward function where each component of the reward is weighted. What was the motivation for using the time step as part of the state construction? Review Summary:I am uncertain of the neuroscientific contributions of this paper. It is mentioned that it is a function of time, but there are no additional details. In both the terminating and continuing cases, the choice of inputs is unusual. For these reasons, I suggest a reject. The reward given to the agent is a combination of the reward signal from the environment (a one hot vector where the activation is dependent on the room occupied by the agent) and the motivation vector, which is a weighting of the rooms. The non motivated agent does not have access to mu in its state, although its reward is weighted as the motivated agent’s is. The issue with this example is that the non motivated agent does not have access to the information required to learn a value function suitable to solve this problem. Notes on paper structure:  There are some odd choices in the structure of this paper. How do we know that the non motivated network is engaging in a "non motivated delay binge"? This claim is made, but it is never made clear what the conventional computational models of working memory are, or how they fit into the computational approaches proposed. A single trial is insufficient to draw conclusions about the behaviour of an agent. The graph suggests that 100% of the agents found the shortest path. Pavlovian Conditioning Experiment:In the third experiment, shouldn’t Q(s) be V(s)? The third experiment does not have enough detail to interpret the results.<BRK>This paper builds a model of motivation dependent learning. A motivation channel is provided as an additional input to and RL based learning system (essentially concatenated to state information), similar to goal conditioned approaches (as the authors mention). While the narrative is interesting, I lean towards reject as I believe it failed to deliver on what it promised. The choice to map the four rooms to biological drives is cute, but possibly confusing/misleading since this navigation problem really has nothing to do with these biological drives. A claim is that by providing the motivation as input to the policy, it is more robustly (across seeds) able to learn the "migration" (i.e.cycling) behavior among the rooms. In this setting, a motivation channel was again used. This paradigm already becoming increasingly popular within computational neuroscience. However, while I find the results slightly interesting, but not very significant, as someone interested in the biology of motivation, I question whether the nature of these contributions would be of broad interest at this venue. More fundamentally, I don t believe there is a meaningful ML/AI/RL contribution, and I have some issues with the presentation of the first two examples. Critically, it has not been shown that motivational systems are useful for artificial agents, rather the tasks themselves have been designed to attempt to be models of biological motivation. Personally, I am interested in motivated behaviors and think that future AI developments should take note of this field, but again, the present work does not provide actionable insights into implementing an artificial motivation system. At the same time, this work does not provide interesting enough neurobiological results for those to stand on their own either. I would consider this a fairly abstract model of the task.<BRK>Importantly, these rewards depend on a motivation factor that is itself a function of time and action; this motivation factor is the key difference between the authors  approach and "vanilla" RL. In simple tasks, the RL agent learns effective strategies (i.e., migrating between rooms in Fig.1, and minimizing path lengths for the vehicle routing problem in Fig.5).The authors then apply their model to a task in which the agent is presented with sound cues. It was very clear, and interesting to read. I have some suggestions for how to deepen the connection between the model and the experiment, and some concerns about the necessity of the motivation framework to the Pavlovian task:1) The authors make the prediction that neurons in VP should show (functional) connectivity matching that learned by their model. This could be tested in their data. 3) For the Pavlovian conditioning in the RL agent, I m not sure I d describe this as changing motivation. So the fact that the same network can make predictions in both cases seems more like metalearning than motivation based action selection. For this reason, it s hard for me to connect the two halves of the paper: the first half has nice ideas on motivation based action selection, while the second one has no apparent action selection, and hence no mechanism for the agent s motivation to matter.
Reject. rating score: 1. rating score: 3. rating score: 6. <BRK>The paper proposes a method that aims at encoding trajectories (described as a sequence of actions) into a set of discrete codes with a hierarchical structure. In terms of positioning, I find the idea of the paper interesting (i.e encoding trajectories through discrete symbols) since it uses sparse coding approaches which, as far as I know, are not classical in the RL domain. Is it the case in this paper? what is T_i ? [cite] appears in the introduction<BRK>In general, I think it proposes an interesting view of the temporal abstraction. 3. in the algorithm, T_i is not defined. In this sense, I think it provides some scientific insights that benefit my understanding. Therefore I can not fully understand the paper and can not accept it.<BRK>The paper discusses identifying motifs for aiding in the solving of cognitive tasks when using Reinforcement Learning. The idea seems quite novel, but the presentation seems to be more complicated than it needs to be for the idea. Figure 2 is complex and lacks enough discussion in the text.
Reject. rating score: 3. rating score: 3. rating score: 3. <BRK>They introduce a formal exploration objective that promotes efficient exploration and provides a mechanism for injecting prior knowledge about the task. This seems counterintuitive and I would’ve expected to increase since I do not see why SAC would get better and SMM would get worse. Overall, the paper is clearly written and the authors are quite transparent about the assumptions made. SMM uses prior information about the task in the form of the target distribution. I would expect other exploration methods to  be faster in that case.<BRK>As the learned exploratory policy should correspond to an historical average exploration for the downstream task is achieved by sampling one of the learned policy at the beginning of each roll out. The paper mentions that VAEs are used to model the state distribution. Given that VAEs are not likelihood based, I do not understand how the reward term log(q(s)) can be computed. SMM iteratively optimizes the policy and a model for its induced state distribution.<BRK>### SummaryThis paper proposes to optimize the state marginal distribution to match a target distribution for the purposes of exploration. Overall, the paper is well written. The main reason is that this paper ignores very similar prior work which is not properly credited.
Accept (Poster). rating score: 6. rating score: 6. rating score: 6. <BRK>+ The experiments are extensive and convincing.<BRK>To enforce sparsity in neural networks, the paper proposes a scale invariant regularizer (DeepHoyer) inspired by the Hoyer measure. Therefore I m leaning to accept it. The paper is well written and easy the follow.<BRK>The paper is written very nicely and the experiments are convincing (though you can always show more)In terms of novelty, I shamelessly can say the idea is very simple and the basic form, the Hoyer regularization, was known. This is basically my question/request for the rebuttal.
Reject. rating score: 3. rating score: 3. rating score: 3. rating score: 3. <BRK>If I understand correctly, Sec 5 proposes a method to evaluate topologies. The choice of the matrix reconstruction problem looks arbitrary. There are no experimentation on real benchmarks or comparison to prior work on sparse networks. It should be possible to compare to methods that sparsify networks after training as well as to methods that enforce sparsity during/before training. Other:  it is strange to call a method by the first name of the researcher (Xavier)  there are some grammatical mistakes and problems with articlesEDIT:After the rebuttal period paper still has weak experimentation.<BRK>This paper tackles the problem of finding a sparse network architecture before training, so as to facilitate training on resource constrained platforms. However, it seems the number and width of such layers still need to be pre defined. Correct?Below Eq.6, the authors mention that the networks are still trained using SGD with L1 loss. I believe that it would be worth discussing these architectures and showing the benefits of the proposed approach over them. I therefore feel that this paper is not ready for publication.<BRK>Overall I think this work is an interesting direction for designing static sparse neural network weight topologies, but it’s lacking in empirical evidence of their claims and could do better to tie themselves to existing literature in training sparse neural networks. They demonstrate that their new topology outperforms existing approaches on a matrix reconstruction task. Batch size 1 training is also not desirable. If the authors could strengthen their results by a) experimenting with their newfound topology and initialization on standard sparsification benchmarks like CIFAR, ImageNet, and WMT EnDe b) comparing their approach to other static sparse [1, 2, 3] and dynamic sparse [4, 5] training algorithms this could be a good paper, but without more experimentation it’s unclear what can be taken away from this work.<BRK>This paper proposes to replace dense layers with multiple sparse linear layers. But we could also build hardware to accelerate general dynamic sparsity. Many of these questions could be answered by trying the dynamic sparse techniques and the proposed static topologies on real problems (MB on ImageNet for example), maybe some kind of language modeling task, etc. I find the use of one artificial task (of matrix reconstruction) which serves mainly to confirm the assumptions that are made rather than test them on real data and real problems a big weakness of the paper.
Accept (Spotlight). rating score: 8. rating score: 8. rating score: 6. <BRK>The results seem to clearly support the claims.<BRK>Overall the paper is clearly written and easy to follow.<BRK>I would like the evaluation to be stronger, however. This is an interesting paper.
Accept (Poster). rating score: 6. rating score: 6. <BRK>This paper studies the landscape properties of over parameterized two layer neural networks, and proposes a network width lower bound for guaranteed existence of descent paths that is tighter than existing results. In particular, the authors prove that if the network width m \leq n   2d, then there exist training data sets and initial weights such that the square loss on the neural network has no descent path connecting the initial weights and global minima. The presentation is clear and the logic is easy to follow.<BRK>This paper analyzes the existence of descent paths from any initial point to the global minimum for the two layer ReLU network and gives a better characterization of the network width that guarantees the descent path property. To show the global convergence property of the optimization method, this kind of landscape analysis is very important. Basically, I like this paper and I think it makes a certain contribution to this line of researches.
Reject. rating score: 1. rating score: 3. rating score: 3. <BRK>Following this, I am not sure what the authors mean by "deterministic" and "non deterministic" methods.<BRK>The paper describes a technique to speed up optimizers that rely on gradientinformation to find the optimum value of a function. The authors describe andjustify their method and show its promise in an empirical evaluation.<BRK>It is odd to have a regret bound in Theorem 3 that is completely independent of S. Unless the authors can address these issues I don t think the current paper is suitable for publication yet. Setting the parameter S seems to be difficult and problem dependent.
Reject. rating score: 3. rating score: 3. rating score: 3. <BRK>#rebuttal responseThanks for your response and the additional experiments. I think the response of the authors is not enough to clarify this.<BRK>Such information could be helpful for others trying to reproduce the results.<BRK>Although a very important step, I vote for a weak reject for this paper as I believe the contribution of the current paper is limited.
Accept (Poster). rating score: 8. rating score: 6. rating score: 6. <BRK>The paper addresses the problem of intrinsically motivating in DRL. In particular, it focuses on exploration of procedurally generated environments where many states are novel compared to training experiences. It offers an intrinsic reward based on large movement in a state embedding space where this state embedding representation is co trained on the same data already collected for learning. The need for intrinsic motivation in exploration is well motivated, and the approach for training a state embedding is anchored in multiple past works. This reviewer moves to accept the paper for its contributions to intrinsically motivated exploration with thorough discussion of how the technique addresses shortcomings of past methods. This reviewer is thankful that the authors do not overinterpret the MiniGrid results and that they provide intuition for why the state embedding functions capture what we want them to capture.<BRK>This paper proposes a new intrinsic reward method for model free reinforcement learning agents in environments with sparse reward. The main weakness of the paper seems to be a limitation in novelty. Previous papers such as (Pathak et al.2017) have trained RL policies using an implicit reward based on learned latent states. Previous papers such as (Marino et al.2019) have used difference between subsequent states as an implicit reward for training an RL policy. It is not a large leap to combine these two ideas by training with difference between subsequent learned states. However, this paper seems to be the first to do so. Despite the limited novelty of the IDE reward term, the experiments and analysis provide insight into the behavior of trained agents and the results seem to improve on existing methods. Notes:In section 2 paragraph 4, "sintrinsic" should be "intrinsic".<BRK>SummaryThis paper proposes a Rewarding Impact Driven Exploration (RIDE), which is an intrinsic exploration bonus for procedurally generated environments. Finally, RIDE s intrinsic reward bonus is computed by L2 norm of the difference between the current state feature and the next state feature, divided by the square root of the visitation count of the next state within the episode. Experimental results show that RIDE outperforms the existing exploration methods in the procedurally generated environments (MiniGrd), and is competitive in singleton environments. Comments and questions:  In reinforcement learning, the agent should explore the experiment due to uncertainty. It could be true on the conducted MiniGrid domains, but this assumption may not hold in general. Similarly, in the problems where high impact states have to be avoided, can RIDE still work effectively? Most of my concerns are addressed, and I raise my score accordingly.
Reject. rating score: 3. rating score: 3. rating score: 3. <BRK>UPDATE: I appreciate the authors  discussions and qualitative results. Because this concern was not sufficiently addressed, I maintain my original rating. This I believe is the main limitation of the paper.<BRK>While this generalization finding is undoubtedly interesting, I am not convinced it is enough for a publication in ICLR. Due to global image and feature pooling operations, the proposed approach is computationally efficient. To improve the paper, I recommend considering harder prediction tasks and lifting some of the restrictions of the method. 2.A similar meta learning prediction problem is addressed in Nagabandi’19a,b, which are not cited.<BRK>The proposed machinery is overkill for the very simple experimental setup. Some of the main parts of the paper are not clear. Regarding the evaluation, how does it know which prediction corresponds to the which groundtruth to compute the distance that is mentioned?
Accept (Poster). rating score: 6. rating score: 3. rating score: 1. <BRK>UPDATE: Based on the extensive improvements by the authors, I have updated my rating. This paper introduces a simple method to weight pretrained lexical features for use in meta learning of few shot text classification. While two weight functions are proposed, the majority of improvement comes simply from normalizing IDF with attention. Second, the approach simply trades variance for bias. Given the weak representational power of the model, I believe this is unlikely. The main contribution is at the input representation level, and this should be applicable across algorithms. Please provide the average across datasets in Table 1.<BRK>This paper studies the effects of using function of ngram statistics as feature to generate attention score per word. Main comments:This paper has a clear motivation and decent experimental results (though some concern on baseline models, see below). The introduction of using distributional signature to derive attention scores seems interesting and a novel contribution. If you could compare your model with some latest algorithms proposed in the few shot learning communities, that would be more convincing as well. But I will be willing to revisit the decision after we get feedback from the author(s). is highly correlated with IDF which also indicates general word importance in corpus. My questions are that, 1) regarding ablation test "OUR w/o biLSTM", how is $h$ calculated in this case (without biLSTM)? 2) since each word is represented based on two statistical number (map function by t(.) and s(.)), can you give any intuitive explanation that why getting attention score from that makes sense? 3) do you have any experiments using the distributional signature as a common feature in standard text classification problems? 2019.Few shot text classification with induction network.<BRK>The primary contribution is an attention mechanism based on word statistics   most importantly word frequency. A secondary contribution is the use of ridge regression [1] to perform meta learning for text classification. Aside from the new attention mechanism and the use of ridge regression, the proposed approach makes use of FastText word embeddings or BERT sentence representations, depending on the task. The paper demonstrates significant improvements over baselines that use other methods of aggregating word representations. However, I have concerns about both the clarity of this paper and the lack of clear comparison to previous work. I think the paper would benefit from a more formal definition of the entire learning procedure. Comparison to previous work  This paper seems to be following the standard FewRel experimental setup. Also, the RCV1 experiments seem to follow the [2] which was cited by the in the paper under review.
Reject. rating score: 3. rating score: 6. rating score: 6. <BRK>The proposed approach can imitate the deep policy better compared with Viper while preserving verifiability. Empirically the proposed method demonstrates improvement in terms of cumulative reward and misprediction rate over Viper in four benchmark tasks. Novelty and significanceThe paper addresses an important problem in RL, trying to extract an interpretable and verifiable policy from a deep RL model. This is more like a small modification of the original method than a significant contribution.<BRK>The paper proposes a method (MOET) to distillate a reinforcement learning policy  represented by a deep neural network into an ensemble of decision trees. The main objective of this procedure is to obtain an "interpretable" and verifiable policy while maintaining the performance of the policy. 4) There should be a better connection with the knowledge distillation/model compression literature and in general the important idea that the same computation can be represented by different algorithms.<BRK>The paper suggests learning decision trees to learn policies that can be verified efficiently. The decision trees are learnt by imitation learning (DAGGER) and are guided by a DNN policy (called the oracle) and its Q function. The paper does two contributions:  the modification of Viper to learn a mixture of decision tree policies that better mimics the DRL agent, and  the fact that the model based on Mixture of Experts is still interpretable. I think that would be an important addition to the paper. That being said, the experimental results are of interest as they bring interpretability and verifiable RL to more challenging environments. Minor comments:  The notation "EM" that appears in the fourth paragraph of the introduction is not defined previously.
Reject. rating score: 3. rating score: 3. rating score: 3. rating score: 3. <BRK>“Therefore the training is deterministic” Training is not deterministic if the first stage of training involves training a GAN. I think the authors would do well to use more standard terminology, and to reconsider their description of the model to be more concise and clear. The same general principle holds here.<BRK>My understanding is that the authors tried to achieve this by decomposing the framework into a wasserstein GAN and a standard autoencoder. I believe this paper contains promising idea. I must admit that I do not work on this field, and cannot judge this paper with more details.<BRK>The work in the paper is interesting and seems to show some empirical progress on image generation. In the begining of the experiment section, there are a number of hyperparameter values defined yet what these hyper parameters are is not explained. The latent encoding size use is rather large.<BRK>However, the comparison to VAEs is difficult since the latent representation of the data learned by VAEs differs from the one of LIA. The idea is inspired by the concept of VAEs. However, instead of maximising the ELBO, the authors propose to learn/represent the generative model by a Wasserstein GAN (first stage).
Accept (Poster). rating score: 6. rating score: 6. rating score: 6. <BRK>This paper suggests a framework for answering graph matching questions consisting of local node embeddings with a message passing refinement step. The paper has well written text, offers what appear to be nice experiments validating the method and discusses its own limitations. Many of the expressions are  high entropy   For instance, the embedding network is given as $\mathbf{\Psi}_{\theta_1}$ throughout.<BRK>The authors proposed a message passing neural network based graph matching methods. Using the two embedding the similarity between points can be computed and then the final matching can be generated. The overall structure of this paper is similar to [1] and [2], the authors should discuss the difference of the proposed with these two papers, if it is possible, the authors may try to compare with these two methods in experiments. al."A study of Lagrangean decompositions and dual ascent solvers for graph matching.<BRK>This paper proposes a two stage GNN based architecture to establish correspondences between two graphs. The first step is to learn node embeddings using a GNN to obtain soft node correspondences between two graphs. Experiments show that the proposed algorithm performs well on real world tasks of image matching and knowledge graph entity alignment. I hope the authors justify this. This needs to be shown experimentally by substituting the second stage by GA process. Is this algorithm robust to node addition or removal, occurring in many practical graph matching problems? I hope all the points in the rebuttal are included in the final manuscript.
Reject. rating score: 3. rating score: 6. rating score: 6. <BRK>This paper proposes to use the Neural Tangent Kernel (NTK) with the Upper Confidence Bound for stochastic contextual bandits. The paper instantiates Kernel UCB (Valko, 2013) with the NTK and the novelty is limited from a theoretical point of view. Detailed review below:  Section 2.2 is not relevant to the paper and it might be more useful to use this space to explain NTK better. Please explain NTK before instantiating the algorithm in Section 4. At the moment, it is difficult to parse. Why is there no upper bound? Can it result in linear regret? As mentioned earlier, there is no comparison with Kernel UCB with a fixed kernel, Neural Linear or Thompson sampling, methods that work well in practice.<BRK>This paper proposes Neural UCB for the neural linear bandit setting. The main contribution of the paper is the theorem that the proposed method, Neural UCB, is guarantee to achieved a good regret bound, which for the first time extends bandits result to neural networks. While the result of this paper seems to be interesting, the idea of the paper is simply combining a recent progress on the neural tangent kernel for overparametrized neural networks and a standard linear UCB algorithm. What would be a lower bound for \lambda_0 for eg. It should be explicitly stated somewhere in the paper that x_{t,k} are assumed to be deterministic. It is more important that \theta^* does NOT depends on a_t. Based on the new version of the paper and the discussions, I change the score to weak accept.<BRK>The authors proposed a neural network based UCB algorithm for bounded reward contextual bandit problems with theoretical guarantee thanks to the recent development of Neural Tangent Kernel (NTK). Thus, I think the technique used in this paper is not novel as well. So generally, this is not a problem. Otherwise, there can be some issue on the positive definiteness of the NTK. However, it is still possible to upper bound this distance to derive the remaining proof. I can understand that the authors may use the Cauchy Schwartz inequality, but Frobenius norm cannot be directly upper bounded by spectral norm (though they are equivalent, but we need to add an additional constant like \sqrt{TK}). 9.The authors should better include the kernelized contextual bandits for a fair comparison, as LinUCB and Neural \epsilon greedy both have theoretical issue that can be solved by kernel methods.
Reject. rating score: 1. rating score: 1. rating score: 6. <BRK>The main idea of the paper: When using meta learning there is an inherent incentive for the learner to win by making the task easier. Overall, I found the idea of the paper interesting, but the attempt to generalise the effect from meta learning to general learning setups hard to follow and detracting from the overall value.<BRK>The paper discusses concepts of self induced distributional shift (SIDS) and the hidden incentives when using meta learning algorithms. Overall, the paper touches the important question of distributional shift for machine learning systems but I find the concepts discussed in the paper, such as the focus on meta learning, the "unit test", and "context swapping", less relevant to how we can really mitigate the issues in real systems or how it can provide additional insights about the problem.<BRK>They define the term along with the term hidden incentives for distributional shift. The paper is poorly written. It feels hastily written and should be carefully proofread with the help of a proficient English speaker. Therefore, the example is not helpful, but rather confusing. Is this true for all meta learning approaches?
Reject. rating score: 3. rating score: 3. rating score: 6. <BRK>This paper proposed deep Bayesian structure networks (DBSN) to model weights, \alpha, of the redundant operations in cell based differentiable NAS. Therefore there is a KL term between the variational distribution q(w) and the prior distribution p(w) to regularize q(w). The second major concern is on the experiments. (1) The authors use DARTS as a main baseline and show that DBSN significantly outperforms DARTS. I notice that the DARTS paper has a parameter number of 3.3M, while in the current paper it set to 1M. Given that DARTS is the main baseline method and the same dataset (CIFAR 10) is used, it would make much more sense to use exactly the same architecture for comparison. These results are highly inconsistent with previous work. It would be best (and would definitely make the paper stronger) to include some comparison. Even if Dikov & Bayer 2019 is not very scalable, it is at least possible to compare them in smaller network size.<BRK>This paper proposes to do approximate Bayesian inference in neural networks by treating the neural network structure as a random variable (RV), while inferring the parameters with point estimates. While performing Bayesian inference for the neural network structure is sensible, I am not convinced by the approach taken in this work. The biggest problem is that the model uses a point estimate of the same weights for different, random network structures. Why not just be explicit about it and say that a Gaussian prior is used? Does it refer to the hierarchy in the graphical model? However, given that the authors use point estimates for the weights, it is not clear if that is still true, especially since the number of different architectures used in practice is small. What’s more, the approach uses *the same* point estimates for different structures. This leads to a graphical model, where the weights are not conditioned on the architecture/structure. For this reason, this paper should be rejected.<BRK>Instead of maintaining uncertainty in network weights, the authors propose to retain uncertainty in the network structure. Overall, I liked the paper and vote for accepting it. The notion of maintaining uncertainty about the network structure is a sensible one, and the paper explores an as yet under explored area at the intersection of state of the art network architecture search algorithms and Bayesian neural networks. Moreover, this is accompanied by compelling empirics — results demonstrate gains in both predictive performance and calibration across diverse tasks and careful comparisons to sensible baselines are presented to evaluate various aspects of the proposed approach (Table 1). + How many Monte Carlo samples were used to evaluate Equation 8. It would be interesting to see predictive performance as a function of the number of MC samples for DBSN. While the overall ideas come across, there are several instances where the text appears muddled and needs a few more polishing passes.
Reject. rating score: 1. rating score: 3. rating score: 6. <BRK>The approach is evaluated on synthetic and real datasets. The paper considers and important topic.<BRK>It strikes me that authors make all evaluations based on NMSE.<BRK>The paper is slightly hard to read due to many typos, and hard to read sentences.
Accept (Poster). rating score: 6. rating score: 6. rating score: 6. <BRK>This paper proposes deep covering options. This method extends laplacian based option generation techniques to continuous domains. The method can be applied in continuous state (and action) domains and options can be learnt in an online manner. I favour acceptance of this paper. The effectiveness of the method is demonstrated on a set of very diverse problems that go well beyond the traditional grid worlds often used in option literature. The method is also compared to another unsupervised option generation method. This seems to mainly happen with the option methods in the continuous control domains  Why are there no learning curves for the ATARI domain?<BRK>SummaryThe authors introduce deep covering options, an online mechanism to extend the covering options to large state spaces. They claim their method discovers options that are task agnostic. It might be useful to comment on this and discuss this in the paper. (V) Regarding the connectivity of the states to generate a diverse set of options: there seem to be connections to the work on constructing options using stronger guarantees Castro & Precup, 2011. To overcome this, the authors propose an approximation of the computation of the Laplacian with Eq3. (II) An important step in the algorithm is line 3” identify an under explored region in the state space using the eigenfunctions”. Forex: imagine a lifelong learning scenario where the environment is really big, and it is just not almost impossible to visit all states, how does this objective function of minimizing the upper bound on the expected cover time constitute the right choice?<BRK>The paper proposes an algorithm to extend the recently proposed method of “covering options” from a tabular setting to continuous state spaces (or large discrete state spaces). An online algorithm is also proposed that does the above option learning process intermittently in addition to training for an external task. I vote for weak reject due to (1) the idea of covering options (Jinnai et.al., 2019b) and the approximation for the graph laplacian (Wu et.al., 2019) both have been shown in prior work and the novelty in this paper seems to be limited to putting together these two ideas, and (2) the paper shows quantitative results for options discovered for simple environments whereas only qualitative options are shown for harder exploration tasks, and (3) given that exploration is a key problem being addressed, a comparison to other exploration algorithms which are non option based methods has not been shown   which makes for a weak argument for using an option based method for exploration as opposed to existing methods for exploration.
Reject. rating score: 1. rating score: 1. rating score: 3. <BRK>The paper proposes a new condition: $\gamma$ optimization principle.<BRK>4) The assumption that the author(s) use in the paper, that is, \gamma optimization principle in Definition 1, is indeed strong and not reasonable.<BRK>Pros:1.This paper is well written and the presentation is clear.
Accept (Poster). rating score: 6. rating score: 6. rating score: 6. rating score: 6. <BRK>This paper proposes a new magnitude based pruning method (and a few variants) by extending the single layer distortion minimization problem to multi layer cases so that the correlation between layers is taken into account. (3) The authors introduced LAP with deep linear networks.<BRK>This paper proposed Lookahead Pruning (LAP), a new method for model weight pruning to generate neural networks with sparse weights. The authors interpret the conventional magnitude pruning (MP) as pruning each layer individually while the proposed lookahead method considers neighboring layers  weights. Also the paper is well written.<BRK>(2) Can the authors elaborate what are the advantages of MP/LAP over Hessian based pruning, such as OBD? [Pros]:  The proposed algorithm is simple and easy to implement. In the paper, the operator block consists of 3 consecutive linear layers.<BRK>*Summary*The paper proposes a multi layer alternative to magnitude based pruning. This simplifies to a cost effective pruning criterion. *Rating*The paper has some clear positives, particularly:    + Clear writing and formatting    + Simple method    + Good structure for the experimental analysis (with 5x replications!) Are those cases sufficiently rare or randomly distributed that they are merely noise? Also, is $w   W_i[j,k]$?
Reject. rating score: 3. rating score: 3. rating score: 3. <BRK>More exactly, the lean images are images rendered from a 3D model of a city. For the purpose of the evaluation, no real images are used,  only lean images both at training and test times. The motivation for the study is to understand the use of geometric aspects by a deep network to solve the task. I think the empirical results can be interesting for people working on geo localization, but probably not for the audience of ICLR.<BRK>The paper tackles the problem of geolocating an image from a 3D model of a city. 4.Please provide more literature review and discuss the difference of your work to existing ones. [2] Mousavian and Kosecka. Review:## Contribution of the paper:1. The paper argues that "lean images" (geometric structure of a 2D scene) contains sufficient information for solving the memorization of geolocation tasks. 3.The paper provide experimental validation that the proposed method works fairly on a certain Berlin dataset. ## FeedbackThe paper is interesting but it does not meet the standard of this conference. The writing is poor.<BRK>Therefore, I believe the paper does not meet the standards of ICLR. Can you give more compelling arguments for the problem setup, rather than a mere thought experiment? Other than this particular setup of geo localizing based on lean images only, the novelty of the paper and the approach seem limited and does not offer new insights in geo interpolation methods, geometry aware neural networks, or memorization.
Accept (Spotlight). rating score: 8. rating score: 8. rating score: 6. rating score: 6. <BRK>This work is clearly the work of a large team. the paper clearly defines what is being done. I have spent a lot of effort with MCTS. I would like some elaboration on this. I find the learning in the latent space the important part and there are things like how much simulation is done in the latent learning not clearly spelled out. Massive effort, nice results.<BRK>This paper presents a world model based approach in which behaviours are optimised by rollouts (i.e.imagination) in latent space. There is actually not too much for me to critique and I would suggest this paper should be accepted. could potentially be highlighted more but otherwise the paper was clear. The paper achieves impressive results across a large selection of tasks, both in terms of sample efficiency and final performance.<BRK>The paper proposes Dreamer, a model based RL method for high dimensional inputs such as images. The main novelty in Dreamer is to learn a policy function from latent representation and transition models in an end to end manner. Experiments on a large set of continuous control tasks show that Dreamer outperforms existing model based and model free methods. Comments.Efficiently learning a policy from visual inputs is an important research direction in RL. This paper takes a step in this direction by improving existing model based methods (the world models and PlaNet) using the actor critic approach. The main difference between Dreamer and SVG is that Dreamer incorporates a latent representation model. Since the latent models are learned based on existing techniques, the paper presents an incremental contribution. This is an open issue in model based RL. This restriction should be noted in the paper.<BRK>In contrast to PlaNet, the difference is that this work learns an actor critic model in place of online planning with the cross entropy method. This paper introduced a latent space model for reinforcement learning in vision based control tasks. The method is applied to vision based continuous control in 20 tasks in the Deepmind control suite. It first learns a latent dynamics model, in which the transition model and the reward model can be learned on the latent state representations.
Reject. rating score: 3. rating score: 3. rating score: 3. <BRK>The notion of zero shot tasks are also not clear for multivariate polynomial experiment and card game experiment. Concerns:  A key issue is that the paper claims to be the first to propose a method for zero shot task adaptation (pg 8, para before “Future Directions”). In particular, the CVPR 2019 paper proposes zero shot task transfer using meta mappings. Some of the queries/concerns in the review comments above were clarified. tasks?In an RL setting, do zero shot tasks deal with a new environment?<BRK>In this paper, the authors proposed to address the zero shot adaptation of tasks by the defined and learned “meta mappings”. Cons: 	The major concern for this work is its lack of discussion and comparison with state of the art meta learning baselines. o	Why don’t merge the results Fig.2 and Fig.3 and compare them in a single figure, so that the contribution of meta mappings can be shown.<BRK>This paper presents a method for adapting a model that has been trained to perform one task, so that it can perform a new task (potentially without using any new training data at all—i.e., zero shot learning). Also, I felt that the proposed approach in Section 5 is very similar to MAML intuitively. The premise of the paper is very interesting and the overall problem is definitely of high interest and high potential impact. The method description was a bit confusing and unclear to me.
Reject. rating score: 6. rating score: 6. rating score: 6. <BRK>This paper presents PrediNet: an architecture explicitly designed to extract representations in the form of three place predicates (relations). They show that their architecture leads to useful generalizable representations in the sense that they can be used for new tasks without retraining. The experimental task is interesting (I m okay with synthetic tasks of this form for unusual new architectures like this), but I m not sure what it tests that isn t tested in the CLEVR and sort of  CLEVR datasets which rely on similar relational reasoning to solve. From the softmax product it seems they should be a weighted sum of the pixels that are addressed   implying that it is the weighted average location? Could you provide some intuition for why we would expect such a representation to be useful for the downstream task?<BRK>This paper presents a network architecture based on the multi head self attention module to learn a new form of relational representations. + The major novelty of this paper is a new attention based network architecture that aims to discover objects and the relations between them. The idea of explicitly appending the patch positions to the representations is interesting, though I am not sure whether it can be generalized to real data. It would be better if the authors could extend their method to natural images. A closely related work is the NLM model [1], which can perfectly generalized to new tasks. Please compare the proposed model with it.<BRK>The authors propose a new neural network architecture that learns to form propositional representations with an explicitly relational structure from raw pixel data. This paper is well organized. The applied methods are introduced in detail. The authors showed the improvement using the Relations Game.
Reject. rating score: 1. rating score: 3. rating score: 3. <BRK>The paper studies missing value imputation in univariate time series. The paper compares with a lot of missing value imputation baselines but the experimental setup is actually for extrapolation. The experiments are not well motivated. The authors further propose DISE which generalizes RISE.<BRK>A major limitation of this paper is that it only applies to the univariate time series. Also, it is not enough contribution for a full conference paper. They quantify the performance of the RISE and DISE on two datasets. This work only studies RNNs.<BRK>Overall:Provides a nice summary of different methods for dealing with missing values in neural net time series models and proposes a new technique that does not involve running a series of possibly diverging predictions but rather jumps ahead to reason about arbitrary points in the future in a “single hop”, avoiding the risks associated with compounding errors. Overall, the paper is mostly clear, the technique is reasonable, and the best model does indeed appear to work well. I have only one serious reservation about this paper   and it is an extremely serious concern about the experimental setup, and I would ask that the authors clarify this point for me in a response.
Reject. rating score: 1. rating score: 6. rating score: 6. rating score: 8. <BRK>SummaryThe authors proposed to disentangle syntactic information and semantic information from pre trained contextualized word representations. They use BERT to generate groups of sentences that are structurally similar (have the same POS tag for each word) but semantically different. Specifically, they defined a triplet loss (Eq4) and uses negative sampling. First, what do you mean by structural information without a clear definition? Also, in the method, the authors construct a dataset where each group of the sentence share similar syntactic structures (having the same POS tag). I think a reasonable baseline in all experiments would be the performance based on POS tags.<BRK>Summary: This paper aims to disentangle semantics and syntax in contextualized word representations. The main idea is to learn a transformation of the contexualized representations that will make two word representations to be more similar if they appear in the same syntactic context, but less similar if they appear in different syntactic contexts. The experiments are mostly convincing. Main comments: 1. The main motivation seems to be to disentangle semantic and syntactic information. The random baseline is also a good sanity check to have. However, the different between layer 16 and others is not that large as to warrant emphasizing it so much. I also wonder whether it makes sense to use different layers for different parts of the triplet loss, depending on whether to emphasize syntactic vs. semantic similarity. Even in the LAS, it seems like the difference is very small, ~0.5, although it s hard to tell from the figure.<BRK>CONTRIBUTIONS:Topic: Disentangling syntax from semantics in contextualized word representationsC1. A method for generating ‘structurally equivalent’ sentences is proposed, based only on the assumption that maintaining function words, and replacing one content word of a source sentence with another to produce a new grammatical sentence, yields a target sentence that is equivalent to the source sentence. C2.The ‘structural relation’ between two words in a sentence is modeled as the difference between their vector embeddings. Rather, it is probably meaning, not structure, that makes ‘let’ and ‘allow’ similar. Implementing C3b, the triplet loss penalizes closeness between D and D”, where D” is the difference between transformed word embeddings of a pair of content words in a sentence S” that is inequivalent to S. (Eq.4).C6.(Implementing C5.) Training will use the difference in the transformed embeddings of the words in S with these indices: call this D, and call the set of these (500) D vectors B. Using deep learning to create an encoding of syntactic structure with minimal supervision is an important goal and the paper proposes a clever way of doing this. The experimental results are rather convincing.<BRK>The authors state a clear hypothesis: it is possible to extract syntactic information from contextualized word vectors in an unsupervised manner. The method of creating syntactically equivalent (but semantically different) sentences is indeed interesting on its own. Experiments do support the main hypothesis   the distilled embeddings are stronger in syntactic tasks than the default contextualized vectors. The authors provide the code for ease of reproducibility which is nice. I understand that they are obsolete these days, but on the other hand, they are better researched, so were there any attempts to disentangle syntax and semantics in the classical static word vectors? Overall, I have no major concerns with the paper.
Accept (Poster). rating score: 6. rating score: 6. rating score: 3. <BRK>This paper presents a novel defense method to make the classification more robust. Then, a distribution based method is proposed to classify the distribution of the soft max for the cleaned (or adversarial) image and its transformed images. But I think this paper could be improved by (1) checking the performance over large datasets (such as ImageNet); (2) providing possible analysis on the observation.<BRK>Couldn’t it be that the baseline (majority voting) does a better job than the distribution classifier on the “wrongly classified” clean images and the “correctly classified” adversarial images? I think it s still interesting and informative to train the distribution classifier using both clean and perturbed inputs to compare the performance. This paper tries to mitigate the accuracy drop on clean inputs. It does so by learning an additional distribution classifier which takes as input the distribution of perturbed samples’ outputs. In any case, the details of the distribution classifier is missing from the paper, and section 4 refers to the figure. It could refer to both the number of transformations and also the number of perturbed pixels.<BRK>This is as opposed to original approach of making a prediction for each sample (a random transformation of the input image) followed by majority voting. Does the transformation retain information about the difference between classes? Figure 2 is an interesting example showing the relationship of the softmax distributions for clean and adversarial examples. Is it really the case that the MNIST model only gets 78% accuracy on class label 8 and 36% on class label 6? 1.For the choice of number of transformations and training set size for the distribution classifier, how were they chosen? What s the intuition for this?
Reject. rating score: 3. rating score: 6. rating score: 6. <BRK>2.It is not quite clear about how to use the collaborative objective on the hallucinator. Additionally, a collaborative objective is introduced as middle supervision to enhance the learning capacity of hallucinator.<BRK>This paper proposes a general meta learning with hallucination framework called PECAN. On the whole, the paper is well written, and the proposed idea is novel and interesting.<BRK>This paper describes a method that builds upon the work of Wang et al.It meta learns to hallucinate additional samples for few shot learning for classification tasks. This is fine as it is perhaps outside he scope of their current work.
Reject. rating score: 3. rating score: 3. rating score: 3. <BRK>Summary.Long short term memory (LSTM) networks are trained to learn the underlying dynamics of 3D structures   An encoder LSTM encodes a sequence of 3D mesh representations and two decoder LSTMs reconstruct the input itself and predict the future structural geometry. Such structural deformation sequences are collected from a simulation and used to train and evaluate the model. 1.Deep neural network architecture is applied to learn a 3D mesh structure deformation. 2.The paper shows that LSTM can learn the underlying dynamics of 3D structural deformation (but since it does not provide any comparison with other work, I cannot determine the prediction error is within a reasonable bound for this task). 1.Weak technical and theoretical novelty. An existing LSTM based seq to seq style architecture is simply chosen and applied to the 3D mesh reconstruction and prediction task. LSTM networks have been successfully applied to various tasks, thus it would be less impactful even if a paper shows LSTM works well in the specific task. 2.Missing comparison with existing work. The paper, however, does not thoroughly compare or cite any prior work in this area. For example, the paper mentions (as the main reason to use the deep neural network) that “Since one simulation run takes a couple of hours on a compute cluster, running a large number of simulation is not feasible” (in the 1st paragraph of Introduction). Can the authors provide an analysis of flops (floating point operations per second)? 3.Simulated data for training. 205 samples are collected from a simulation to train and validate the proposed model and half an hour has taken to train this model. Judging from the common practices, does this imply the underlying dynamics function (of a simulation) is trivial to learn? Also, only two parts (i.e.the left and right structural beam) of the whole 3D structure are analyzed. Minor concerns.<BRK>The paper presents an LSTM model to predict the evolution for 3D models for crash tests. The core model is a video prediction model (Srivastava et al.2015).Instead of using the original 3D geometry, the authors propose to predict a feature representation based on prior work (Brezis & Gomez Castro 2017, Bohn et al.2013, Teran & Garcke 2019). The problem of simulating 3D meshes instead of raw pixels is interesting. My main issue with the paper was that I had a hard time seeing a contribution. It is unclear what is technically new, or if everything was just copied from prior work. It was also unclear which parts of the technical approach mattered? Is the LSTM the right solution, or are there other simple baselines to compare to? Finally, the experiments are not very intuitive. It is unclear what impact the presented technique has on the down stream tasks of crash testing. Does it improve the current simulation of crash testing? Is the prediction error acceptable to replace certain safety tests?<BRK>This paper proposes to use autoencoder LSTM to learn car 3D crash simulation in an unsupervised manner. Basically, it proposes to apply autoencoder LSTM, which is an existing and well studied technique, to a very special task of car crash simulation. Neither a new method nor an in depth investigation of the task itself is carried out. It is hard to identify contributions that are significant enough to support the publication in top conferences such as ICLR. Considering the fact that autoencoder LSTM is wildly employed in solving various unsupervised learning tasks, readers can hardly acquire knowledge in terms of novelty.
Reject. rating score: 3. rating score: 3. rating score: 6. <BRK>However, I still think the algorithm in this paper is merely a "clever" version of gradient masking, which does not give the neural networks real robustness, it is just harder to design attacks on all these discrete operations.<BRK>This paper extends the compressive sensing framework introduced in Bafna et al.to handle l1 and l2 attacks. It is indeed interesting to extend the compressive sensing framework to handle l1 and l2 attacks.<BRK>They provide guarantees for some recovery algorithms in the case of different kinds of norm bounded noises. Overall, this paper is a follow up work towards Bafna et al.(2018) but with better theoretical guarantees and ample experiment results to support their robustness against various popular attacks.
Reject. rating score: 1. rating score: 1. rating score: 3. <BRK>I have read the author response, thank you for responding. Original review:This paper presents the extraction of a bibliographic database of Chinese technical papers. This database could potentially be a valuable resource for the community. However, the paper is mis targeted to the ICLR conference, as it does not discuss learning representations or using deep learning (the 36 page appendix does include some material on knowledge graph embedding, but this is not covered in the main body of the paper). The paper refers to their resource as a knowledge graph, but I would say it is more accurate to call it a bibliographic database (it consists primarily of paper titles, authors, and keywords). This is very different from the broader KBs and KGs discussed in the related work.<BRK>This paper introduces a large scale knowledge graph database called TechKG, which is constructed from a massive repository of academic papers in Chinese. The authors have described in details the process and heuristics in use for constructing such knowledge base, and also reported important statistics and characteristics of the database, including duplicate name, imbalance issue etc. Without these experiments, it’s hard to estimate precisely the contribution of the KG database to the machine learning community. In Section 3.1, the authors should specify more clearly the source of the journals collected & the representativeness of the repository. The writing can be polished further.<BRK>The paper talks about creating a Tech Knowledge Graph of 260 million tripes, with 52 million entities coming from 38 research domains. The authors claim this is the first of it s technology specific Chinese KG, so they claim scale, specificity and list out various aspects that makes this distinct to existing KG s. A few things that the paper is missing or not written well:1. By being technology specific how does it help, where is KG being used and what metrics are current KG s fall short etc. 2.There is no novelty in the way KG is built, so there is no technical contribution to this paper making it a very weak submission for ICLR standards. 3.There is a huge appendix section with some results and lot of information. If the authors can address these issues, the value of the work can increase significantly.
Reject. rating score: 1. rating score: 1. rating score: 3. <BRK>This paper proposes 3 such explanation evaluation metrics, correctness, consistency, and confidence. The quality of the paper is my reason for the low rating. Instead, this method finds images with the same ground truth class which the classifier scored the lowest of all such images, forming a low confidence baseline.<BRK>However, I am confused or unconvinced by several arguments made in the paper, and if the authors can clarify them I am willing to increase my review. I think the major issue is that most metrics are justified with flimsy arguments, not compared with prior work, and do not lead to consistent ranking of the models. Correctness: I am not convinced by the correctness evaluation for several reasons1. Which result should a practitioner believe?<BRK>  AFTER rebuttal1) "We identify issues with current masking procedures as proposed in other papers"One of the major issues with the current masking procedure is that the resulting image is out of the data distribution. The experiments given in the paper, it looks like confidence and correctness are positively correlated.
Reject. rating score: 3. rating score: 3. rating score: 6. <BRK>The authors proposed a hybrid method for defending against adversarial attacks called SPROUT. The proposed defense method consists of three main ingredients:1. label smoothing with a learnable Dirichlet distribution2. adding Gaussian noise to input examples3. mixup: augment training examples with their linear combinations The authors  main argument for their method is the speed over adversarial training and its effectiveness. Indeed this is corroborated by Figure 5, when the individual defenses do not have more than 10% accuracy under PGD100 attacks for epsilon 0.4. This is very surprising. Given the surprise in these experimental results, I believe the authors should perform a more detailed analysis on how these ingredients for their SPROUT defense interact to produce such a strong predictor, in addition to doing ablation studies. An attempt should be made to explain why they work so well together when they are quite weak individually as defenses. It is difficult for me to recommend acceptance of this paper without an attempt to explain why it works.<BRK>This work proposes training robust models without explicitly training on adversarial examples and by "smoothing" the labels in an adversarial fashion and by using Dirichlet label smoothing. Training robust models without adversarial training is indeed an important problem as mentioned by the authors since it can potentially (as the authors demonstrate) result in faster model training and less drop in clean accuracy. Overall the idea is interesting but I have some concerns mainly about evaluations and baselines which I am including below. where the authors do adversarial training on ImageNet with no overhead cost compared to natural training. (a), why is the loss for the validation image illustrated so high? 8.In terms of Scalability, its good to mention new scalable methods such as YOPO and Adversarial Training for Free. This illustrates that the performance of the model is very sensitive to \alpha. Do you use multiple random restarts? It is known that random restarts can be more effective than increasing the number of PGD attacks. In previous smoothing methods, the L infinity CW attack seems to be a stronger attack compared to PGD. For this, I do not understand why the authors mention that they only evaluate targeted attacks while they are not doing any adversarial training.<BRK>This paper proposes a novel training method to build robust models. A new framework SPROUT is introduced to adjust label distribution during training. It also integrates mixup and Gaussian augmentation to further improve the robustness. Experiments show that the proposed method significantly outperforms the existing best methods in terms of robustness against attacks. Overall, this paper proposes a novel method with good robustness performance. Experimental results are also very strong to prove the effectiveness of the proposed method. On the other hand, I have some concerns about this paper. 1.How do you perform inference given testing data? It s very important to examine the true robustness of the propose method.
Reject. rating score: 1. rating score: 1. rating score: 3. <BRK>Moreover, the writing of the paper could be significantly improved. At a high level the goal of this objective is to ensure that the classifier is confidently separating samples from different classes. However, for now, I will have to recommend rejection. First of all, I did not find the proposed CCKL objective well motivated.<BRK>In case of this paper an alternative loss function is proposed, based on information theory. Should evidence for that be supported for that the reviewer would be strongly in favour of accepting the paper.<BRK>The work explores the problem of robustness and adversarialattacks in NN. In a multiclass prediction setting the ideais to use a taylor expansion of a loss coined CCKL whichis the KL divergence between predictions for pairs of samplesfrom different classes. Typos   is found these   >  is found that these The mathematical derivations should be made more rigorous.
Reject. rating score: 3. rating score: 3. rating score: 3. <BRK>The paper tries to draw conclusions on how learning should develop across brain regions in infants. In all the networks, it also tested for a relationship in the order in which infants and machines acquire visual classes, and found only evidence for a counter intuitive relationship. LimitationsThe topic is extremely interesting and worth intense study. However, the approach is not convincing. Why would it be related to infant learning? In particular, back propagation is not widely considered possible in biology. Given the learning mechanism may be very different. What is the basis of using DNN to study the infant learning? The findings are also not very surprising and offer much for the community.<BRK>This paper aims to examine whether DNNs are a good model of infantbehavior and development. The paper is very well written and easy toread. The work did show that unsupervised training results in a differentpattern of layer learning than supervised learning, but neither formof learning was able to model the development in children. Perhaps aself supervised multimodal learning system should be tried? The decision to train the DeepCluster type net in a supervised way fora control on training method vs architecture type is nice, but it would alsohave been good to try other kinds of networks. It is not clear to me how the concretenessof a word relates to the strength of visual representation. I don t think there is enough new insight gained from this paper for ICLR publicationat this stage. Funding acknowledgement (especially with grant number) should not bein an anonymous submission.<BRK>The paper measures the amount of class information in each layer over the progress of training. However, the paper doesn t seem to provide any evidence for how the training process used for deep neural networks should correspond to the development of the visual system in infants. In particular, backpropagation is considered biologically implausible [1], whereas backpropagation serves as the main method for learning in the neural networks. Furthermore, neural networks have randomly initialized parameters, whereas it seems unlikely that human infants  brains would lack existing organization to such a drastic extent. In order for the results in this paper to hold greater weight, I would expect to see more evidence about how the neural network training process (also including aspects such as batch normalization, and the self supervised clustering method in DeepCluster) are expected to correlate with learning in human brains. My conclusions above are based on my surface level knowledge of neuroscience, so I welcome any clarifications or corrections from the authors about the above points.