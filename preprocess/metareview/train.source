Accept (Poster). rating score: 8. rating score: 7. rating score: 6. <BRK>This paper studies the issue of truncated backpropagation for meta optimization. This paper highlights this problem as a fundamental issue limiting meta optimization approaches. The paper is generally clear and well written. I couldn t find a mention of the constraint in the paper. I assume the red curve is hiding beneath the blue one but it would be good to see this explicitly.<BRK>The paper discusses the problems of meta optimization with small look ahead: do small runs bias the results of tuning? I would assume from the description that the colors are based  on the final loss values obtaine dby choosing a fixed pair of decay exponent and effective LR. pro:  Thorough discussion of the issue with theoretical understanding on small benchmark functions as well as theoretical work  Easy to read and followcons: Small issues in presentation: * Figure 2 "optimal learning rate"  > "optimal greedy learning rate", also reference to Theorem 2 for increased clarity. * Figure 4 misses the red trajectories, also it would be easier to have colors on the same (log?)<BRK>This paper proposes a simple problem to demonstrate the short horizon bias of the learning rate meta optimization. The second part of the paper seems to be a bit disconnected to the quadratic function analysis. I think it might be of interest to some audiences in ICLR.
Reject. rating score: 3. rating score: 4. rating score: 4. <BRK>While the idea is sound, many design choices of the system is questionable. The problem is particularly aggravated by the poor presentation of the paper, creating countless confusions for readers. I do not recommend the acceptance of this draft.<BRK>The matrix is not isomorphic invariant and the different clusters don’t share a common model. The work itself, however, falls short of the goal. Clarity: The paper organization needs work; there are also some missing pieces to put the NN training together.<BRK>Thus, the comparisons in Tables 2 and 3 are not fair. Almost all of the experiments are qualitative and can be easily made quantitive by comparing PageRank or degree of nodes.
Invite to Workshop Track. rating score: 9. rating score: 5. rating score: 4. <BRK>This is a well written paper, very nice work. While it is not the first to explore this kind of direction,the method is efficient for what it does; it shows that at least for some systems, the physical parameters can be optimized without optimizing the controller for each individual configuration. The introduction could also promote that over an evolutionary time frame, the body andcontrol system (reflexes, muscle capabilities, etc.) (muscle routing parameters, including insertion and attachment points) are optimized along with the control). It would be interesting to compare to a baseline where the control systems are allowed to adapt to the individual design parameters.<BRK>Related to that, it would be interesting to see a visualization of the design space distribution. The algorithm boils down to an alternating policy gradient optimization of design and policy parameters, with policy parameters shared between all designs. However, these designs are not guaranteed to be optimal in the long run, with further specialization. In summary, while the paper presents a simple but possibly effective and very general co optimization procedure, the experiments and discussion don t definitively illustrate this.<BRK>Unfortunately, this paper has a number of weak points that   I believe   make it unfit for publication in its current state. Main weak points:  No comparisons to other methods (e.g.switch between policy optimization for the controller and CMA ES for the mechanical parameters). The basic result of the paper is that allowing PPO to optimize more parameters, achieves better results...  One can argue that this is not true joint optimization Mechanical and control parameters are still treated differently.
Reject. rating score: 5. rating score: 5. rating score: 6. <BRK>Then for every *average threshold* from the CI method, apply that as a static threshold. E.g.add these numbers to Table 1 and Table 2 (for every "our adaptive ensemble", put the equivalent static threshold.) With the above experiments I think this would be a good paper.<BRK>Would this baseline be competitive with the proposed approach? * The overall improvement in computation time seems to be within a constant scale, which can be easily achieved by doing ensemble prediction in parallel (note that the proposed approach would require predicting sequentially). They then address the problem by proposing an adaptive prediction approach.<BRK>In this paper it is described a method that can be used to speed up the prediction process of ensembles of classifiers that output probabilistic predictions. The method proposed is very simple and it is based on the observation that in the case that the individual predictors are very sure about the potential class label, ensembling many predictions is not particularly useful. However, it is true that several similar ensemble pruning techniques exist for multi class problems in which one uses majority voting for computing the combined prediction of the ensemble. The experiments carried out by the authors are convincing.
Invite to Workshop Track. rating score: 5. rating score: 4. rating score: 4. <BRK>I am overall a fan of the general idea of this paper; scaling up to huge inputs is definitely a necessary research direction for QA. However, I have some concerns about the specific implementation and model discussed here. How much of the proposed approach is specific to getting good results on bAbI (e.g., conditioning the knowledge encoder on only the previous sentence, time stamps in the knowledge tuple, super small RNNs, four simple functions in the n gram machine, structure tweaking) versus having a general purpose QA model for natural language?<BRK>The framework does include several components and techniques from latest recent work, which look pretty sophisticated. However, as the dataset is generated by simulation, with a very small set of vocabulary, the value of the proposed framework in practice remains largely unproven. Pros:  1.An interesting framework for bAbI QA by encoding sentence to n gramsCons:  1.<BRK>Question answering over (much more complex) probabilistic knowledge graphs have been proposed before as well (in fact I believe Matt Gardner wrote his entire thesis on this topic). Non of these areas, with the exception of semantic parsing, are addressed by the author. With sufficient knowledge of related works from these areas, I find that the authors  proposed method lacks proper evaluation and sufficient novelty. This, in itself, is not a problem.
Reject. rating score: 6. rating score: 6. rating score: 6. <BRK>The authors propose a new method of defining approximate posteriors for use in Bayesian neural networks. The idea of using hypernetworks for Bayesian inference is compelling, and the authors show some promising first results. I see two issues, and would be willing to increase my rating if these were sufficiently addressed. The instantiation of Bayesian hypernetworks that is used in experiments seems to be a special case of the method of multiplicative normalizing flows as proposed by Louizos and Welling and discussed in this paper. If the variances / sigmas are zero in the latter method, their approximation seems functionally equivalent to Bayesian hypernetworks (though with different parameterization). Is my understanding correct? If so, the novelty of the proposed method is limited.<BRK>This paper presents Bayesian Hypernetworks; variational Bayesian neural networks where the variational posterior over the weights is governed by a hyper network that implements a normalizing flow (NF) such as RealNVP and IAF. The proposed method is evaluated on extensive experiments. This paper seems like a plausible idea with extensive experiments but the similarity with [1] make it an incremental contribution and, furthermore, it seems that it has a technical issue with what is explained at Section 3.3. This would then better illustrate the tradeoffs you make when you reduce the flexibility of the hyper network to just outputting the row scaling variables and the effect this has at the posterior approximation. [1] Louizos & Welling, Multiplicative Normalizing Flows for Variational Bayesian Neural Networks.<BRK>This paper proposes Bayesian hypernetworks to carry out Bayesian learning of deep networks. I like the idea of constructing general approximation strategies for complex posterior distribution and the proposed approach inherits all the scalability properties of modern deep learning techniques. This seems to suggest that also the hypernetwork is infered using Bayesian inference, but if I understand correctly this is not the case. I have some comments on novelty and realization of the experiments. In the positioning of the work in the literature, the Authors point out that hypernetworks have been proposed before, so it is not clear what is the actual novelty in the proposal. The experimental part is interesting as it explores a number of learning scenarios. I believe that this would strengthen the comparative evaluation. I think the paper would have made a stronger case by including other approaches to approximate posteriors using generative models.
Accept (Poster). rating score: 7. rating score: 5. rating score: 4. <BRK>They experimentally demonstrate their method on sequences of MINST digits and the KITTI dataset. Pros:  interesting concept of combining algebraic structure with a data driven method  clear idea development and well written  transparent model with enough information for re implementation  honest pointers to scenarios where the method might not work wellCons:  the method is only intrinsically evaluated (Tables 2 and 3), but not compared with results from other motion estimation methods<BRK>Paper proposes an approach for learning video motion features in an unsupervised manner. Without a comparison of this form the paper is incomplete and the findings are difficult to put in the context of state of the art.<BRK>Q to the authors: what labelled data were used to train the linear regressor in the KITTI experiment? Pros1)The neural architecture for motion embedding computation appears reasonable2)The paper tackles an interesting problemCons1)For a big part of the introduction the paper refers to the problem of `` ````"learning motion” or ` understanding motion”  without being specific what it means by that.
Accept (Poster). rating score: 8. rating score: 6. rating score: 4. <BRK>The paper introduces a novel method for modeling hierarchical data. The aim is to learn embeddings from supervised structured data, such as WordNet. The evaluation consists of hypernym detection on WordNet and graded lexical entailment, in the shape of HyperLex. This is good work: it is well written, the experiments are thorough and the proposed method is original and works well. It s good that LEAR is mentioned and compared against, even though it was very recently published. Or am I missing something? "hypothesis proposed by Santus et al.which says" is not a valid reference.<BRK>The paper presents a study on the use of density embedding for modeling hierarchical semantic relations, and in particular on the hypernym one. n Figure 4 middle, it is not clear whether the location and city concepts are intersecting the other synsets. +++pros: 1) potentially a good idea, capable of filling an ontology of relations scarcely present in a given repository 2) solid theoretical background, even if no methodological novelty has been introduced (this is also a cons!) This aspect is also present in the experiments section, since it is not possible to understand how much the problem (the scarceness of the hypernyms) is present in the HYPERLEX dataset.<BRK>The paper presents a method for hierarchical object embedding by Gaussian densities for lexical entailment tasks.Each word is represented  by a diagonal Gaussian and the KL divergence is used as a directional distance measure. To summarize, I don t think there is enough interesting novelty in this paper. If the focus of the  paper is on  obtaining good entailment results, maybe an NLP conference can be a more suitable venue. Hence, I was not convinced that the propose KL+Gaussian modeling is suitable for directional relations.
Reject. rating score: 3. rating score: 5. rating score: 6. <BRK>The ideas and the presented results are interesting and are clearly of interest to the deep learning community. For instance, the assumption that the stochastic gradient noise being Gaussian is very restrictive and trying to justify it just by the usual CLT is not convincing especially when the parameter space is extremely large, the setting that is considered in the paper. Therefore, presenting it as a new result is misleading. 3) Even if the sigma is taken constant and theorem 1 is corrected, I don t think theorem 2 is conclusive. With authors  claim, the algorithm should spend more time on the wider one, however it is evident that this will not be the case. The experiments are very interesting and I do not underestimate their value. However, the current analysis unfortunately does not properly explain the rather strong claims of the authors, which is supposed to be the main contribution of this paper.<BRK>The theory in section 3 is described clearly, although it is largely known. The use of the Fokker Planck equation for stationary distributions of stochastic SDEs has seen wide use in the machine learning literature over the last few years, and this paper does not add any novel insights to that. Also, though it may be relatively new to the deep learning/ML community, I don t see the need to derive the F P equation in Appendix A.Theorem 2 uses a fairly strong locally convex assumption, and uses a straightforward taylor expansion at a local minimum. There are some detailed experiments showing the effect of the learning rate and batchsize on the noise and therefore performance of SGD, but the only real insight that the authors provide is that the ratio of learning rate to batchsize controls the noise, as opposed to the that of l.r. Overall I think the paper is borderline; the lack of real novelty makes it marginally below threshold in my view.<BRK>“the endpoint of SGD with a learning rate schedule η → η/a, for some a > 0, and a constant batch size S, should be the same  as the endpoint of SGD with a constant learning rate and a batch size schedule S → aS.” This is clearly wrong as there are many local minima, and running teh algorithm twice results in different local optima. Point of departure is an analytical theory proposed by Mandt et al., where SGD is analyzed in a continuous time stochasticformalism. The advantage ofthis theory is that under specific assumptions, analytic stationary distributions can be derived. Overall assessment:I find the analytical results of the paper very original and interesting. The paper could be drastically improved when focusing on the experimental part. Detailed comments:Regarding the analytical part, I think this is all very nice and original. So, why is it a reasonable assumption to still use results obtained from the stationary (equilibrated) distribution which is never reached? Why do the authors focus on two manually selected optima?
Accept (Poster). rating score: 7. rating score: 7. rating score: 6. <BRK>Some Theory and Empirics" considers the important problem of quantifying whether the distributions obtained from generative adversarial networks come close to the actual distribution of images. The authors argue that GANs in fact generate the distributions with fairly low support. The proposed approach relies on so called birthday paradox which allows to estimate the number of objects in the support by counting number of matching (or very similar) pairs in the generated sample.<BRK>Evaluation: Significance: The question whether GANs learn the target distribution is important and any  significant contribution to this discussion is of value. One should note that the birthday theorem assumes uniform sampling.<BRK>The authors also provide a specific theoretical construction that shows bidirectional GANs cannot escape specific cases of mode collapse.
Reject. rating score: 5. rating score: 5. rating score: 7. rating score: 8. <BRK>The paper describes an empirical evaluation of some of the most common metrics to evaluate GANs (inception score, mode score, kernel MMD, Wasserstein distance and LOO accuracy). Given that the underlying application is image generation, the authors move from a pixel representation of images to using the feature representation given by a pre trained ResNet, which is key in their results and further comparisons. First, it only considers a single task for which GANs are very popular. Second, it could benefit from a deeper (maybe theoretical analysis) of some of the questions.<BRK>This paper has some interesting insights and a few ideas of how to validate an evaluation method. The topic is an important one and a very difficult one. The evaluations rely on using a pre trained imagenet model as a representation. The authors point out that different architectures yield similar results for their analysis, however it is not clear how the biases of the learned representations affect the results. The use of learned representations needs more rigorous justification The evaluation for discriminative metric, increased score when mix of real and unreal increases, is interesting but it is not convincing as the sole evaluation for “discriminativeness” and seems like something that can be gamed.<BRK>The paper evaluates popular GAN evaluation metrics to better understand their properties. The "novelty" of this paper is a bit hard to assess. If the authors release their code as promised, the off the shelf tool would be a very valuable contribution to the GAN community. E.g.Danihelka et al proposed using an independent Wasserstein critic to evaluate GANs: Comparison of Maximum Likelihood and GAN based training of Real NVPshttps://arxiv.org/pdf/1705.05263.pdfHow sensitive are the results to hyperparameters? It would be interesting to see some sensitivity analysis as well as understand the correlations between different metrics for different hyperparameters (cf.Appendix G in https://arxiv.org/pdf/1706.04987.pdf)Do you think it would be useful to compare other generative models (e.g.VAEs) using these evaluation metrics? Some of the metrics don t capture perceptual similarity, but I m curious to hear what you think.<BRK>Specifically, the authors pointed out some desirable properties that GANS evaluation metrics should satisfy. Section 4 summarizes the results, which concluded that the Kernel MMD and 1 NN classifier in the feature space are so far recommended metrics to be used. I think this paper tackles an interesting and important problem, what metrics are preferred for evaluating GANs. In particular, the authors showed that Inception Score, which is one of the most popular metric, is actually not preferred for several reasons. The result, comparing data distributions and the distribution of the generator would be the preferred choice (that can be attained by Kernel MMD and 1 NN classifier), seems to be reasonable. This would not be a surprising result as the ultimate goal of GAN is mimicking the data distribution. Overall, I think this paper is worthy for acceptance as several GAN methods are proposed and good evaluation metrics are needed for further improvements of the research field.
Reject. rating score: 2. rating score: 2. rating score: 3. <BRK>The work would be better suited for a focused summarisation workshop, where there would be more interest from the participants. Some of the statements motivating the work are questionable. I don t know if sentence vectors *in particular* have been especially successful in recent NLP (unless we count neural MT with attention as using "sentence vectors"). It s also not the case that the sentence reordering and text simplification problems have been solved, as is suggested on p.2.<BRK>This paper explored the effectiveness of four existing sentence embedding models on ten different document summarization methods leveraging various works in the literature. Evaluation has been conducted on the DUC 2004 dataset and ROUGE 1 and ROUGE 2 scores are reported. In general, the paper seemed to be an ordinary reporting of some preliminary work, which at its current stage would not be much impactful to the research community.<BRK>This paper examines a number of sentence and document embedding methods for automatic summarization. It further provides a number of analyses of the sentence representations as they relate to summarization, and other aspects of the summarization process including the decoding algorithm. However, the signficance of the results may be limited, because the paper does not respond to a long line of work in summarization literature which have addressed many of the same points. The idea that the summary should be similar to the entire document is known as centrality. The impact of frequency on summarization.
Accept (Poster). rating score: 9. rating score: 7. rating score: 3. <BRK>Understanding how and why complex motion skills emerge is an complex and interesting problem. list of pros & cons+ informative and unique experiments that demonstrate emergent complexity coming from the natural curriculum  provided by competitive play, for physics based settings+ likely to be of broad interest  likely large compute resources needed to replicate or build on the results  paper is not anonymous to this reviewer, given the advance publicity for this work when it was released > overall this paper will have impact and advances the state of the art, particular wrt to curriculums    In many ways, it is what one might expect. It is often difficult to decide apriori when a game is balanced; game designers of any kindspend significant time on this. : )The opponent sampling strategy is one of the key results of the paper. If two classes of agents are bootstrapped with different flavours of exploration rewards, how much would it matter? It would be generally interesting to describe when during the learning various "strategies" emerged,and in what order. The paper could comment on the further additional complexity that might result from situationsthat allow for collaboration as well as competition. Also, "$\pi_{\theta}$ can be Gaussian":   better to say that it *is* Gaussian in this paper. It seems that these are unique in that the goals are not symmetric, whereas for the other tasks they are. episodic length T, eqn (1)It s not clear at this point in the paper if T is constant or not.<BRK>This paper demonstrates that a competitive multi agent environment trained with self play can produce behaviors that are far more complex than the environment itself and such environments come with a natural curriculum by introducing several multi agent tasks with competing goals in a 3D world with simulated physics. It makes training more stable by selecting random old parameters for the opponent. Although the technical contributions seem to be not quite significant, this paper is well written and introduces a few new domains which are useful for studying problems in multiagent reinforcement learning. The paper also makes it clear regarding the connections and distinctions to many existing work. In the notation section, the observation model is missing, and the policy is restricted to be reactive.<BRK>In observations: Are both agents given different observations? They study the effect of this annealing process (considered as a curriculum) and of various strategies for sampling the opponents. The main outcome is the acquisition of a large variety of useful skills, just observed from videos of the competitions. Below, I talk directly to the authors. "In this paper, we point out that a competitive multi agent environment trained with self play can produce behaviors that are far more complex than the environment itself." Well, for an agent, the other agent(s) are part of its environment, aren t they? would make the paper better...By the end of Section 5.2, you allude to transfer learning phenomena. It would be nice to study these transfer effects in your results with a quantitative methodology. This is not always true. Was it hard?
Reject. rating score: 4. rating score: 4. rating score: 4. <BRK>This paper considers a new way to incorporate episodic memory with shallow neural nets RL using reservoir sampling. The whole algorithm is tested on a toy problem with 3 repeats. The paper is well written and easy to understand. Typos didn t influence reading. It is a novel setup to consider reservoir sampling for episodic memory. The technically soundness of this work is weakened by the experiments.<BRK>What this means is that the agent has a reservoir of n "states" in which states encountered in the past can be stored. For the latter question, the authors propose using a "query network" that based on the current state, pulls out one state from the memory according to certain probability distribution. This network has many tunable parameters, but the main point is that the policy then can condition on this state drawn from the memory. The first question, had a quite an interesting and cute answer. There is no easy way to fix this and for the purpose of sampling the paper simply treats the weights as immutable. [In general, I also recommend against using figure captions to describe the setup.]<BRK>This paper proposes one RL architecture using external memory for previous states, with the purpose of solving the non markov tasks. The proposed architecture could identify the ‘key’ states through assigning higher weights for important states, and applied reservoir sampling to control write and read on memory. According to that, the experiments they demonstrated in this article are not well designed so that the conclusion they made in this article is not robust enough. They demonstrate that proposed model could work better and rational of write network could be observed.
Accept (Poster). rating score: 8. rating score: 6. rating score: 6. <BRK>The paper introduces a new algorithm for training GANs based on the Earth Mover’s distance. To compute the distance between mini batches, they use the Sinkhorn distance. Would like to see:   * a numerical comparison with Cramer GAN, to see whether the additional  computational cost is worth the gains.<BRK>There have recently been a set of interesting papers on adapting optimal transport to GANs. This makes a lot of sense. The proposal makes sense from the generative standpoint and it is clear from the paper that the key contribution is the design of the transport cost. I am asking this question because solving the OT problem with cost defined as in c_eta is equivalent to using a *normalized squared* Euclidean distance in the feature space defined by v_eta. Otherwise, the contribution has to be balanced. The MMD is the solution of an optimisation problem which, I suppose, has lots of connections with the dual Wasserstein GAN.<BRK>The paper presents a variant of GANs in which the distance measure between the generator s distribution and data distribution is a combination of two recently proposed metrics. In particular, a regularized Sinkhorn loss over a mini batch is combined with Cramer distance "between" mini batches. In the experiment, the size is increased to 8000 instances for stable training. Could you please comment on this? Another issue is the adversarial training of the transport cost.
Reject. rating score: 4. rating score: 5. rating score: 6. <BRK>While the idea is interesting and might be a good alternative to standard CNNs, the paper falls short in terms of providing experimental validation that would demonstrate the latter point. The CIFAR 10, STL 10, and SVHN results are disappointing. Moreover, these tables do not even cite more recent higher performing CNNs. Do such CCNNs achieve performance compatible with ResNet/DenseNet on CIFAR or SVHN? In addition, for such experiments, a clear report of parameters and FLOPs for each network should be included in the results table. This would assist in understanding tradeoffs in the design space. Additional questions:What is the receptive field of the CCNNs vs those of the standard CNNs to which they are compared? If the CCNNs have effectively larger receptive field, does this create a cost in FLOPs compared to standard CNNs? Such dependence for CCNNs appears to be a weakness in comparison.<BRK>The main idea of the paper is to project examples into an RK Hilbert spaceand performs convolution and filtering into that space. for expressivity of hypothesis spaces? This means that they bringall necessary information for rebuilding their continuous counterpart. Hence, it isnot clear why projecting them back into continuous functions is of interest. Another point that is not clear or at least misleading is the so called Hilbert Maps. Hence, the learning framework of theauthors can be casted more as a learning with similarity function than learninginto a RKHS [2]. Learning good similarity functions is also not novel [3] and Equations(6) and (7) corresponds to learning these similarity functions. Part 3 is the most interesting part of the paper, however it would have beengreat if the authors provide other kernel functions with closed form convolution formula that may be relevant for learning. The proposed methodology is evaluated on some standard benchmarks in vision. However, performance results seem to be competitive and that s the reader maybe eager for insights. * Supposingly that the authors properly consider computation in RKHS, then \Sigma_ishould be definite positive right?<BRK>This paper aims to provide a continuous variant of CNN. The main idea is to apply CNN on Hilbert maps of the data. The paper is well written and provides some new insights on incorporating kernels in CNN. In this case, the projections of the data via the kernel are not necessarily in a RKHS. The connection between Hilbert maps and RKHS in that sense is not clear in the paper. In large scale situations, working with the kernel matrix can be computational expensive.
Accept (Poster). rating score: 8. rating score: 7. rating score: 5. <BRK>This paper extends the concept of global rather than local optimization from the learning to search (L2S) literature to RNNs, specifically in the formation and implementation of SEARNN. As noted by the authors, it is a dataset that has been tackled by related papers and importantly a well scaled dataset. For SEARNN and related techniques to see widespread adoption, the scaling analysis this paper provides is a fundamental component. This reviewer, whilst not having read all of the appendix in detail, also appreciates the additional insights provided by it, such as including losses that were attempted but did not result in appreciable gains.<BRK>The paper proposes new RNN training method based on the SEARN learning to search (L2S) algorithm and named as SeaRnn. It proposes a way of overcoming the limitation of local optimization trough the exploitation of the structured losses by L2S. In summary, a very nice paper. Clarity: The paper is well structured and written, with a nice and well founded literature review. Originality: the paper presents a new algorithm for training RNN based on the L2S methodology, and it has been proven to be competitive in both toy and real world problems. Significance: although the application of L2S to RNN training is not new, the contribution to the overcoming the limitations due to error propagation and MLE training of RNN is substantial.<BRK>This paper proposes an adaptation of the SEARN algorithm to RNNs for generating text. Pros:  Good literature review. A lot of important experimental details are in the appendices and they differ among experiments. For example, while mixed rollins are used in most experiments, reference rollins are used in MT, which is odd since it is a bad option theoretically. the two losses introduced are not really new. Note that the notion of dynamic expert is present in the SEARN paper too. Goldberg and Nivre just adapted it to transition based dependency parsing. the top k sampling method is essentially the same as the targeted exploration of Goodman et al.(2016) which the authors cite. Thus it is not a novel contribution. Also, not sure I see why SEARNN can be used on any task, in comparison to other methods. Minor comments:  Figure 1: what is the difference between "cost sensitive loss" and just "loss"? Can t see why SEARNN can help with the vanishing gradient problem.
Reject. rating score: 4. rating score: 5. rating score: 7. <BRK>It discretizes each dimension of the action space, but to avoid an exponential blowup, it selects the action for each dimension in sequence. The paper should discuss this and explain what are the consequences in practice. In the end, the MDP does not become simpler. While the empirical results are good, I don t think the paper should be published until the authors figure out a principled way of training.<BRK>Originality When the action space is N dimensional, computing argmax could be problematic. The paper proposes to address the problem by creating N MDPs with 1 D actions. Does the proposed method would work in such a scenario? A discussion is needed. Significance While the proposed method seems a reasonable approach to handle the argmax problem, it still requires training multiple networks for Q^i (i 1,..N) for Q^L, which is a limitation. These limit the understand and hence significance.<BRK>The MDP within an MDP approach is quite similar to the Pazis and Lagoudakis MDP decomposition for the same problem (work which is appropriately cited, but maybe too briefly compared against). In other words, it strikes me as merely being P&L plus networks, dampening my enthusiasm. Experiments do demonstrate improved effectiveness in the chosen domains, and the authors do a nice job of illustrating the range of performance by their approach (which has low variance in some domains, but high variance in others). Because of the clarity of the paper, the effectiveness of the approach, and the high quality experiments, I encourage acceptance.
Reject. rating score: 5. rating score: 5. rating score: 5. <BRK>This paper presents an algorithm for clustering using DNNs. The second step also shrinks the number of targets over time to achieve clustering. 2.The delete and copy step also introduces randomness, and since the algorithm removes targets over time, it is not clear if the algorithm consistently optimizes one objective throughout.<BRK>5) It is not clear how to set the number of clusters. Is it data dependent? 4) The experimental results are a bit less satisfactory:a) It is known that unsupervised clustering methods can achieve 0.97 accuracy for MNIST. The proposed method can jointly learn latent features and the cluster assignments.<BRK>This paper proposes a neural clustering model following the "Noise as Target" technique. However, it is unclear to me why it is necessary to have the optimal matching here and why the simple nearest target would not work. Some other issues regarding quantitative results:  In Table 1, there are 152 clusters for 10 d latent space after convergence, but there are 61 clusters for 10 d latent space in Table 2 for the same MNIST dataset. Would NATAC k need a different number of clusters than the one from NATAC? The number of centroids learned from NATAC may not be good for k means clustering.
Accept (Poster). rating score: 7. rating score: 6. rating score: 6. <BRK>Pros:  The proposed method leads to state of the art results . Cons:  The local reprarametrisation trick it self is not new and applying it to a multinomial distribution (with one repetition) instead of a Gaussian is straight forward, but its application for learning discrete networks is to my best knowledge novel and interesting.<BRK>This paper introduces the LR Net, which uses the reparametrization trick inspired by a similar component in VAE. From the experiments, we can see that the proposed method is effective. Since you are sampling the pre activations instead of weights, I guess this approach is also able to reduce training time complexity by an order.<BRK>I would thus suggest that the authors update the paper accordingly. This paper proposed a relatively simple idea for training networks with discrete weights that seems to work in practice. The authors showed benefits compared to a continuous relaxation baseline.
Accept (Poster). rating score: 8. rating score: 7. rating score: 6. <BRK>Most of the modelling ideas already exists, but this paper show how they can be applied as a strong summarization model. The approach obtains strong results on the CNN/Daily Mail and NYT datasets. RL results are reported with only the best performing attention setup for each dataset. It is claimed that readability correlates with perplexity, so it would be interesting to see perplexity results for the models. The lack of correlation between automatic and human evaluation raises interesting questions about the evaluation of abstractive summarization that should be investigated further in future work. This is a strong paper that presents a significant improvement in document summarization.<BRK>This is a very clearly written paper, and a pleasure to read. While the new mechanisms are simple variants of what is taken from existing work, the entire combination is well tested in the experiments. ROUGE results are reported for the full hybrid RL+ML model, as well as various versions that drop each of the new components (RL training; intra attention). The best method finally outperforms the lea 3d baseline for summarization. What makes this paper more compelling is that they compared against a recent extractive method (Durret et al., 2016), and the fact that they also performed human readability and relevance assessments to demonstrate that their ML+RL model doesn t merely over optimize on ROUGE.<BRK>The paper is generally well written and the intuition is very clear. It is a good incremental research, but the downside of this paper is lack of innovations since most of the methods proposed in this paper are not new to us.
Reject. rating score: 4. rating score: 4. rating score: 6. <BRK>This paper introduces a conditional variant of the model defined in the Neural Statistician (https://arxiv.org/abs/1606.02185). The generative model defines the process that produces the dataset. This corresponds to a mixture of Neural Statisicians. In the experiments they only consider training the model with the context selection variable and the data variables observed.<BRK>This paper proposes a model for learning to generate data conditional on attributes. The most serious problems are the extensive discussion of the fully unsupervised variant (rather than the semisupervised variant that is evaluated), poor use of examples when describing the model, nonstandard terminology (“concepts” and “context” are extremely vague terms that are not defined precisely) and discussions to vaguely related work that does not clarify but rather obscures what is going on in the paper.<BRK>This also touches on the main criticism of the paper: While the model technically makes sense and is cleanly described and derived,  the empirical evaluation is on the weak side and the rich properties of the model are not really shown off. The authors propose an extension to the Neural Statistician which can model contexts with multiple partially overlapping features. A particularly interesting aspect of this model is the fact that it can learn these context c as features conditioned on meta context a, which leads to a disentangled representation. The experiments do not really convey how well this all will work in practice.
Accept (Poster). rating score: 7. rating score: 7. rating score: 6. <BRK>This paper studies the critical points of shallow and deep linear networks. While a result has been stated for single hidden ReLU networks. This paper is mostly about linear networks.<BRK>Authors of this paper provided full characterization of the analytical forms of the critical points for the square loss function of three types of neural networks: shallow linear networks, deep linear networks and shallow ReLU nonlinear networks. The paper is well organized and well written.<BRK>The equivalence of global minima and local minima for linear networks is not surprising based on existing works e.g.Hardt & Ma (2017) and Kawaguchi (2016). Pros:1.This manuscript provides the sufficient and necessary characterization of critical points for deep networks.
Reject. rating score: 3. rating score: 3. rating score: 5. <BRK>Should the hyperparameters be tuned separately for each generative model being evaluated? The sensitivity of the evaluation metric defined in equation 2 to the choice of hyperparameters of the classifier and the metric itself (e.g., alpha) is not evaluated. Is it possible that a different choice of hyperparameters can change the model ranking?<BRK>Despite the the idea of exploiting generative models to perform data augmentation is interesting, using it as an evaluation metric does not constitute an innovative enough contribution. 3.The authors should clarify if the method is specifically designed for GANs and VAEs. The mode collapse issue is never discussed elsewhere in the paper. In fact, the limit of training with a fixed dataset is that the model ‘sees’ the data multiple times across epochs with the risk of memorizing.<BRK>The paper proposes a technique for analysis of generative models. ), and compare to existing metrics (e.g."visual inspection" and "average log likelihood"). + I think the technique is particularly valuable verify that samples are capturing variety of modes in the data. To establish a new comparison method the paper needs to demonstrate it on relevant tasks (e.g.image generation?
Reject. rating score: 4. rating score: 5. rating score: 5. <BRK>The overall novelty of the work is limited though. It seems like most of the results are coming from the end to end learning.<BRK>One could work on the two separately; here, the authors propose sharing some of the weights to try and exploit/identify common features between the two datasets. The empirical results here are interesting but not particularly striking; the most salient feature perhaps is that the architectures and training algorithms are perhaps a bit simpler but the overall improvements over existing methods are not too exciting.<BRK>The paper focuses on learning common features from multiple domains data in a unsupervised and supervised learning scheme. Setting this as a general multi task learning, the idea consists in jointly learning autoecnoders, one for each domain, for the multiples domain data in such a way that parts of the parameters of the domain autoencoder are shared. Results on transfer are the most interesting ones actually but do not seem to improve so much over baselines. Experimental show some interesting results.
Reject. rating score: 4. rating score: 4. rating score: 4. <BRK>Unfortunately, this paper feels to be hastily written and can only be read when accompanied with several references as key parts (CLR) are not described and thus the work can not be reproduced from the paper. In this setting, a typical schedule with a decreasing learning rate would be a bad choice as the change of curvature must be corrected as well. However, this is not a general problem in neural network training and might not be generalizable to other datasets or architectures as the authors acknowledge. Most of the plots shown are actually not converged and convergence in test accuracy is not the same as convergence in training loss, which is not shown at all. Pros:  Many experiments which try to study the effectCons: The described phenomenon seems to depend strongly on the problem surface and might never be encountered on any problem aside of Cifar 10  Only single runs are shown, considering the noise on those the results might not be reproducible.<BRK>In this paper, the authors analyze training of residual networks using large cyclic learning rates (CLR). However, both these effects are only shown on a specific dataset, architecture, learning algorithm and hyper parameter setting. Some specific comments by sections:2. The authors also provide sufficient intuition for super convergence. Figs 1 and 5 illustrate only one cycle (one increase and one decrease) of CLR. However, the fact that the results only applies to CIFAR 10 dataset and could not be observed for ImageNet or other architectures is disappointing and heavily takes away from the significance of this work.<BRK>This paper discusses the phenomenon of a fast convergence rate for training resnet with cyclical learning rates under a few particular setting. It tries to provide an explanation for the phenomenon and a procedure to test when it happens. However, I don t find the paper of high significance or the proposed method solid for publication at ICLR. The paper is based on the cyclical learning rates proposed by Smith (2015, 2017). The explanation of the cause of "super convergence" from the perspective of  transversing the loss function topology in section 3 is rather illustrative at the best without convincing support of arguments. I feel most content of this paper (section 3, 4, 5) is observational results, and there is lack of solid analysis or discussion behind these observations.
Accept (Poster). rating score: 7. rating score: 6. rating score: 6. <BRK>This paper revisits the generative adversarial network guided cost learning (GAN GCL)  algorithm presented last year. Instead, they propose to learn a generative model wherein actions are sampled as a function of states. To avoid overfitting the expert s demonstrations (by mimicking the actions directly instead of learning a reward that can be generalized to different dynamics), the authors propose to learn rewards that depend only on states, and not on actions. The authors argue formally that this is necessary to disentangle the reward function from the dynamics.<BRK>This can be shown to be a form of GAN, obtained by using a specific discriminator [Finn et al.(2016a)].If the discriminator directly works with trajectories tau, the result would be GAN GCL. The paper, however, argues that the advantage function is “entangled” with the dynamics, and this is undesirable. EVALUATION:This is an interesting paper with good empirical results. * I am not convinced about the argument of Appendix C that shows that AIRL recovers reward up to constants. It is suggested that since the only items on both sides of the equation on top of p. 13 depend on s’ are h* and V, they should be equal.<BRK>It also argues that a key property to learning generalizable reward functions is for them to depend on state, but not state action or state action state. It uses this property to produce "disentangled rewards", demonstrating that they transfer well to the same task under different transition dynamics. The need for "state only" rewards is a useful insight and is covered fairly well in the paper. "which can effectively recover disentangle the goals"  > "which can effectively disentangle the goals"? "also consider learning cost function with"  > "also consider learning cost functions with"? are not robust the environment changes"  > " are not robust to environment changes"? "AIRLperforms"  > "AIRL performs". Figure 2: The blue and green colors look very similar to me. I d recommend reordering the legend to match the order of the lines (random on the bottom) to make it easier to interpret.
Accept (Poster). rating score: 7. rating score: 7. rating score: 6. <BRK>This paper presents a simple model based RL approach, and shows that with a few small tweaks to more "typical" model based procedures, the methods can substantially outperform model free methods on continuous control tasks. In particular, the authors show that by 1) using an ensemble of models instead of a single models, 2) using TRPO to optimize the policy based upon these models (rather that analytical gradients), and 3) using the model ensemble to validate when to stop policy optimization, then a simple model based approach actually can outperform model free methods. Overall, I think this is a nice paper, and worth accepting.<BRK>For example: Expressiveness of the models? Etc...This is not necessarily required for the paper but would be interesting. While the idea to use an ensemble of deep neural networks to estimate their uncertainty is not new, I haven t seen it yet in this context. The problem the authors tackle, namely learning a deep neural network model for model based RL, is important and relevant.<BRK>The authors combine an ensemble of DNNs as model for the dynamics with TRPO. The paper is well written and the experiments indicate good results. Can the authors comment on the time complexity of the proposed methods compared to the baselines? However, idea of using ensembles in the context of (model based) RL  is not novel, and it comes at the cost of time complexity.
Reject. rating score: 4. rating score: 5. rating score: 5. <BRK>The paper extended the Adam optimization algorithm to preserve the update direction. Instead of using the un centered variance of individual weights, the proposed method adapts the learning rate for the incoming weights to a hidden unit jointly using the L2 norm of the gradient vector. The authors also combined the proposed method with a few existing deep learning tricks in the paper. All those tricks that, ie. Do we expect this modification to fail in any circumstances? The experiments on CIFAR dataset and one CNN architecture do not provide enough evidence to show the proposed method work well in general.<BRK>Thus, ND Adam is more robust to improper initialization, and vanishing or exploding gradients." If the magnitude of each update does not depend on the magnitude of the gradient, then the algorithm heavily depends on the learning rate. First, the original WRN paper and many other papers with ResNets used weight decay of 0.0005 and not 0.001 or 0.002 as used for SGD in this paper. The difference between WRN 22 7.5 and WRN 28 10 is unlikely to be significant, the former might have about only 2 times less parameters which should barely change the final validation errors. I note that the use of WRN 22 7.5 is unlikely to be due to the used hardware because later in paper the authors refer to WRN 34 7.5.<BRK>This paper proposes a variant of ADAM optimization algorithm that normalizes the weights of each hidden unit. Pros:   The idea of optimizing the direction while ignoring the magnitude is interesting and make sense. Cons:  In the abstract, authors claim that the proposed method has good optimization performance of ADAM and good generalization performance of SGD. Such a method could be helpful if one can get to the same level of generalization faster (less number of epochs). The paper is not coherent. In particular, direction preserving ADAM and batch normalized softmax trick are completely orthogonal ideas. My own experience and several empirical works have suggested that weight decay does not improve generalization significantly.
Reject. rating score: 4. rating score: 5. rating score: 7. <BRK>This paper presents an analysis of an agent trained to follow linguistic commands in a 3D environment. The behaviour of the agent is analyzed by means of a set of "psycholinguistic" experiments probing what it learned, and by inspection of its visual component through an attentional mechanism. Conversely, when it is exposed to colors only, it will have a color bias. How is this interesting or surprising? The crucial question, here, would be whether, when an agent is trained in a naturalistic environment (i.e., where distributions of colors, shapes and other properties reflect those encountered by biological agents), it would show a human like shape bias. This, however, is not addressed in the paper. 4.2 The problem of learning negationI found this experiment very interesting. Perhaps, the authors could be more explicit about the usage of negation here. The meaning of commands containing negation are, I think, conjunctions of the form "pick something and do not pick X" (as opposed to the more natural "do not pick X"). 4.4 Processing and representation differencesThere is virtually no discussion of what makes the naturalistic setup naturalistic, and thus it s not clear which conclusions we should derive from the corresponding experiments. What is novel here? Also, since introducing attention changes the architecture, shouldn t the paper report the learning behaviour of the attention augmented network? I think the description uses "length" when "dimensional(ity)" is meant. This should be discussed in the paper, as it raises concerns about the linguistic interest of the controlled language that was used. Table 3: indicates is: indicates if<BRK>The authors used situated versions of human language learning tasks as simulation environments to test a CNN + LSTM deep learning network. Developing methods that enable humans to understand how deep learning models solve problems is an important problem for many reasons (e.g., usability of models for science, ethical concerns) that has captured the interest of a wide range of researchers. I was impressed by the range of phenomena they tackled and their analyses were informative in understanding the behavior of deep learning models I found the analogy persuasive in theory, but I was not convinced that the current manuscript really demonstrates its value. In particular, I did not see the value of situating their model in a grounded environment. One analysis that would have helped convince me is a comparison to an equivalent non grounded deep learning model (e.g., a CNN trained to make equivalent classifications), and show how this would not help us understand human behavior. If it does not, it could illustrate the efficacy of using situated environments. But, it also could mean that their technique acts differently for equivalent situated and non situated models. In this case though, what would we learn about the more general non situated case then? It does not seem like we would learn much, which would defeat the purpose of the technique. Alternatively, if the equivalent non situated model does show the phenomena, then using the situated version would not be useful because the model acts equivalently in both cases. Although combining LSTM with CNN via a “mixing” module was interesting, it added another layer of complexity that made it more difficult to assess what the results meant. This left me less convinced of the usefulness of their paradigm.<BRK>This paper presents an analysis of the properties of agents who learn grounded language through reinforcement learning in a simple environment that combines verbal instruction with visual information. The analyses are motivated by results from cognitive and developmental psychology, exploring questions such as whether agents develop biases for shape/color, the difficulty of learning negation, the impact of curriculum format, and how representations at different levels of abstraction are acquired. The extent to which it provides us with insight into human cognition depends on the degree to which we believe the structure of the agent and the task have a correspondence to the human case, which is ultimately probably quite limited. Nonetheless the paper takes on an ambitious goal of relating questions in machine learning in cognitive science and does a reasonably good job of analyzing the results. The results on word learning biases are not particularly surprising given previous work in this area, much of which has used similar neural network models. In figure 2 and the associated analyses, why were 20 shape terms used rather than 8 to parallel the other cases? It seems like there is a strong basic color bias. This seems like one of the most novel findings in the paper and is worth highlighting. I found these human results hard to reconcile with the results from the models. I also found it hard to understand why colors were hard to learn given the bias towards colors shown earlier in the paper. 5.The section on layerwise attention claims to give a “computational level” explanation, but this is a misleading term to use — it is not a computational level explanation in the sense introduced by David Marr which is the standard use of this term in cognitive science. The explanation of layerwise attention could be clearer.
Accept (Poster). rating score: 7. rating score: 6. rating score: 5. <BRK>I found this to be an interesting approach to hierarchical multitask learning, augmenting a previous approach with several steps leading to increased autonomy, an essential agent for any learning agent. The process of subtask discovery is done via non negative matrix factorization, whereby the matrix of desirability functions, determined by the solution of the LMDPs with exponentiated reward.<BRK>Then, your formulation for discovery subtasks seems to assume that Z is given. In that sense, this paper makes a reasonable contribution to that goal for multitask LMDPs.<BRK>I would also like to point out that this other paper is in a more complete form as it clears the issues (1) and (2) I raised above. This statement seems problematic. (2) If I understand correctly, the row dimension of Z is equal to the size of the state space, so the algorithm can only be applied to tabular problem as is.
Invite to Workshop Track. rating score: 6. rating score: 5. rating score: 5. <BRK>However, with ta more general recognition problem conducted with the CIFAR 10 database, the use of the proposed method improves both the error and the computational time, when compared with AT and Virtual Adversarial Train. Summary:The paper propose a method for generating adversarial examples in image recognition problems. The improvement in the error results in the db CIFAR 10 is good enough to see merit in the proposal approach.<BRK>What is the substantial novelty of this current approach? The performance is not as good as  (and seems significantly worse than) AT and VAT on MNIST. On CIFAR10, VMT all is only comparable with VAT. Although VMT is faster than VAT, it seems not a significant advantage since is not faster in a magnitude. The writing need to be significantly improved. Is Figure 3 the regularization on the training or testing set?<BRK>The paper is well formalized and the idea is interesting. The regularization approach is novel compared to the methods of the literature. Main concerns: 1)	The experimental validation of the proposed approach is not consistent:The description of the baseline method is not detailed in the paper. 3)	The positioning of the proposed approach is not so clear:  As mentioned above, your method is a tradeoff between accuracy and running time. Indeed, you clearly mentioned that the methods of the literature lacks interpretability.
Reject. rating score: 5. rating score: 5. rating score: 5. <BRK>The paper studies different methods for defining hypergraph embeddings, i.e.defining vectorial representations of the set of hyperedges of a given hypergraph. This is interesting in its own. Also run times are compared and the results are expected.<BRK>Further, the experiments only present several models this paper described. The idea of studying embedding of a hypergraph is interesting and novel, and the results show that several different kinds of methods can all provide meaningful results for realistic applications.<BRK>It presents a collection of methods for solving this problem and most of these methods are only adaptation of existing techniques to the hypergraph setting.
Accept (Poster). rating score: 6. rating score: 5. rating score: 5. <BRK>The proposed method fraternal dropout is a stochastic alternative of the expectation linear dropout method, where part of the objective is for the dropout mask to have low variance. So all the results should be compared with ELD. I feel the ideas interesting and valuable especially in light of strong empirical results, but the authors should do more to clarify what is actually happening. This work is very closely related to expectation linear dropout, except that you are now actually minimizing the variance: 1/2E[ ||f(s)   f(s )|| ] is used instead of E [ ||f(s)   f_bar|| ].<BRK>The paper proposes “fraternal dropout”, which passes the same input twice through a model with different dropout masks. The text is well written and easy to follow. I have only two concerns. The second is that of the experimental evaluation. They authors write that a full hyper parameter search was not conducted in the fear of having a more thorough evaluation than the base lines, erroneously reporting superior results. To me, this is not an acceptable answer. IMHO, the evaluation should be thorough for both the base lines and the proposed method.<BRK>The authors present Fraternal dropout as an improvement over Expectation linear dropout (ELD) in terms of convergence and demonstrate the utility of Fraternal dropout on a number of tasks and datasets. The paper addresses this issue by trying to reduce the gap. It could be made clearer why the proposed regularization would make the aforementioned gap smaller. Intuitively, the bias of the deterministic approximation (compared to the MC eval) should also play a role. A possibility is that MC and deterministic evaluations meet halfway and with fraternal dropout MC eval is worse than without. Wouldn t f(X_t) be preferrable to p^t(z_t)? The results on PTB and Wikitext 2 are really good. It is somewhat unfortunate that due to lack of computational resources the comparisons are made at a single hyperparameter setting.
Accept (Poster). rating score: 7. rating score: 6. rating score: 6. <BRK>Do you assume that the entrance is always at the lowest floor? The paper builds on a simple but useful idea and is able to develop it into a basic method for the goal.<BRK>The authors motivate the problem of floor level estimation and tackle it with a RNN. The results are good. The models the authors compare to are well chosen.<BRK>Update: Based on the discussions and the revisions, I have improved my rating. The typo of accuracy given by the authors is somewhat worrying, given that the result is repeated several times in the paper. Currently the results would be impossible to reproduce.
Reject. rating score: 2. rating score: 3. rating score: 4. <BRK>I am sorry to say that I have learned very little from this paper, and that in my view it does not make for a very compelling ICLR read. I briefly list some of these issues. 1) The model brings no novelty, or to put it bluntly, it is rather simplistic.<BRK>The details on the experiments are also scarce. They likely result from preliminary experiments, in that case it should be said. The main concern is that the experiments could be greatly improved.<BRK>With such fine grained supervision knowledge, it is also unfair to compare to other cross lingual methods that use much less auxiliary information. The authors should provide a review and comparison to related methods. The approach section is too short to provide a clear presentation of the model.
Reject. rating score: 4. rating score: 5. rating score: 6. <BRK>Therefore, also the paper is well written,   it lacks novelty,  its topic does not perfectly fit topics of interest for ICLR,So, I do not recommend this paper to be published. The paper considers structural time series model with seasonal component and stochastic trend, which allow for change points and structural breaks.<BRK>In the end of the page 2, the last panel is the residual, not the spikes. However, in classic formulation for change point or anomaly detection, usually there is also a mean shift other than the variance change. I believe that this kind of mean shift is more efficient to model the structure of change point. My main concern is with the novelty.<BRK>The paper introduces a Bayesian model for timeseries with anomaly and change points besides regular trend and seasonality. The performance is evaluated and compared against state of the art methods on three data sets: 1) synthetic data obtained from the generative Bayesian model itself; 2) well log data; 3) internet traffic data. On the methodological side, this appears to be a solid and significant contribution, although I am not sure how well it is aligned with the scope of ICLR.
Accept (Poster). rating score: 8. rating score: 7. rating score: 7. <BRK>Summary:The paper presents three different methods of training a low precision student network from a teacher network using knowledge distillation. Review:The paper is well written. The experiments are clear and the three different schemes provide good analytical insights. Using scheme B  and C student model with low precision could achieve accuracy close to teacher while compressing the model.<BRK>The paper aims at improving the accuracy of a low precision network based on knowledge distillation from a full precision network. Instead of distillation from a pre trained network, the paper proposes to train both teacher and student network jointly. I found the paper interesting but the contribution seems quite limited. Pros:1.The paper is well written and easy to read.<BRK>The authors investigate knowledge distillation as a way to learn low precision networks. For scheme A, the authors show the performance of the student network under many low precision regimes and different depths of teacher networks.
Accept (Poster). rating score: 8. rating score: 7. rating score: 6. <BRK>This would allow the readers to further understand why the proposed alignment approach performs better than e.g.Deep Coral. Comparisons with the state of the art approaches show that the proposed marginally improves the results. The paper is well written and easy to understand.<BRK>Strengths   The paper is clearly written and effectively makes a simple claim that geodesic distance minimization is better aligned to final performance than euclidean distance minimization between source and target. Geodesic flow kernel for unsupervised domain adaptation. Domain adaptation for object recognition: An unsupervised approach.<BRK>This paper improves the correlation alignment approach to domain adaptation from two aspects.
Reject. rating score: 5. rating score: 6. rating score: 7. <BRK>The paper is clearly written, with a good coverage of previous relevant literature. The contribution itself is slightly incremental, as several different parameterization of orthogonal or almost orthogonal weight matrices for RNN have been introduced. Therefore, the paper must show that this new method performs better in some way compared with previous methods. Pros:1.New, relatively simple method for learning orthogonal weight matrices for RNN2. Quite good results on several relevant tasks.<BRK>This paper suggests an RNN reparametrization of the recurrent weights with a skew symmetric matrix using Cayley transform to keep the recurrent weight matrix orthogonal. They suggest that they reparametrization leads to superior performance compare to other forms of Unitary Recurrent Networks. I think the paper is well written. Authors have discussed previous works adequately and provided enough insight and motivation about the proposed method. I appreciate if authors can provide more results in these settings.<BRK>This manuscript introduce a scheme for learning the recurrent parameter matrix in a neural network that uses the Cayley transform and a scaling weight matrix. This scheme leads to good performance on sequential data tasks and requires fewer parameters than other techniquesComments:  It’s not clear to me how D is determined for each test. For the copying and adding problem test cases, it might be useful to clarify or cite something clarifying that the failure mode RNNs run into with temporal ordering problems is an exploding gradient, rather than any other pathological training condition, just to make it clear why these experiments are relevant. Is this test loss the cross entropy? The plots in figure 1 and 2 have different colors to represent the same set of techniques. However, there is no example for restricted capacity uRNN with 69k parameters to show that the performance of restricted capacity uRNN doesn t also increase similarly with more parameters. I don’t understand the significance of each test and why the relative performance of the techniques vary from one to the other.
Accept (Poster). rating score: 8. rating score: 7. rating score: 6. <BRK>In this paper, the authors propose a new output layer for deep networks allowing training on logged contextual bandit feedback. The paper is well written.<BRK>While this is not a brand new problem, the important and relevant contribution that the authors make is to do this using policies that can be learnt via neural networks. The main contributions of the authors is to design an output layer that allows training on logged bandit feedback data.<BRK>This paper proposes a new output layer in neural networks, which allows them to use logged contextual bandit feedback for training. The paper is well written and well structured.
Reject. rating score: 4. rating score: 4. rating score: 4. <BRK>This paper proposes to represent nodes in graphs by time series. This is an interesting idea but the results presented in the paper are very preliminary. In Section 5.1, I did not understand the construction of the graph.<BRK>The experimental evaluation was somewhat limited and that is the biggest problem from a practical standpoint. As far as I understood, the authors are doing essentially reverse directed graphical model learning. The recovery algorithm is essentially previous work (but the application to graph recovery is new).<BRK>The paper proposes GRAPH2SEQ that represents graphs as infinite time series of vectors, one foreach vertex of the graph and in an invertible representation of a graph. By not having the restriction of representation to a fixed dimension, the authors claims their proposed method is much more scalable. I find the experiments to be hugely disappointing.
Invite to Workshop Track. rating score: 7. rating score: 6. rating score: 6. <BRK>2.The title of this paper is weird. And the paper showed the combination of the three performs the best in terms of precision@K and PCC@K, and also with good scalability. I am not fully sure why the proposed predictor model is able to win over LSTM.<BRK>Authors describe a procedure of building their production recommender system from scratch, begining with formulating the recommendation problem, label data formation, model construction and learning. They use several different evaluation techniques to show how successful their model is (offline metrics, A/B test results, etc.)<BRK>There s some slight concern that maybe this paper would be better for the industry track of some conference, given that it s focused on an empirical evaluation rather than really making much of a methodological contribution. The main finding of the paper is that a relatively simple method works for recommendation, compared to other methods based on neural networks that have been recently proposed. This contribution is not bad for an empirical paper. The paper proposes a new neural network based method for recommendation.
Accept (Poster). rating score: 8. rating score: 7. rating score: 4. <BRK>Overall, this work is providing useful experimental insights, clearly motivatingfurther study. Some of the used metrics can detect mode collapse. As a purely empirical study, it poses more new and open questions on GANoptimization than it is able to answer; providing theoretical answers isdeferred to future studies. This is not necessarily a bad thing, since theextensive experiments (both "toy" and "real") are well designed, convincing andcomprehensible.<BRK>3) the penalization strategies introduced for ``non standard GAN  with specific motivations, may also apply successfully to the ``standard GAN , improving robustness, thereby helping to set hyperparameters. Overall, I believe that the paper provides enough material to substantiate these claims, even if the message could be better delivered. In particular, the writing is sometimes ambiguous (e.g.in Section 2.3, the reader who did not follow the recent developments on the subject on arXiv will have difficulties to rebuild the cross references between authors, acronyms and formulae). The answers to the critiques referenced in the  paper are convincing, though I must admit that I don t know how crucial it is to answer these critics, since it is difficult to assess wether they reached or will reach a large audience.<BRK>Quality: The authors study non saturating GANs and the effect of two penalized gradient approaches. The authors consider a number of thought experiments to demonstrate their observations and validate these on real data experiments. Clarity: The paper is well written and clear. I would suggest keeping the main results in the main body and move extended results to an appendix. Originality: The authors demonstrate experimentally that there is a benefit of using non saturating GANs. Significance: The problems the authors consider is worth exploring further.
Invite to Workshop Track. rating score: 7. rating score: 6. rating score: 3. <BRK>In this paper, the authors present a novel way to look at a neural network such that each neuron (node) in the network is an agent working to optimize its reward. For example, if a classification task is formulated as reinforcement learning where the ultimate reward depends on the batch likelihood, the presented formulation (called Adaptive DropConnect in this context) does better on standard datasets when compared with a strong baseline. The idea proposed in the paper is quite interesting, but the presentation is severely lacking. In a work that relies heavily on precise mathematical formulation, there are several instances when the details are not addressed leading to ample confusion making it hard to fully comprehend how the idea works. For example, in section 5.1, notations are presented and defined much later or not at all (g_{jit} and d_{it}). Overall, I think the idea presented in the paper has merit, but without a thorough rewriting of the mathematical sections, it is difficult to fully comprehend its potential and applications.<BRK>This paper proposed a novel framework Neuron as an Agent (NaaA) for training neural networks to perform various machine learning tasks, including classification (supervised learning) and sequential decision making (reinforcement learning). The NaaA framework is based on the idea of treating all neural network units as self interested agents and optimizes the neural network as a multi agent RL problem. The motivation of the work is not very clear. It seems to me this is a very general claim. Does it have anything to do with existing issues in DRL? I also have some concerns regarding the claim that “We confirm that optimization with the framework of NaaA leads to better performance of RL”. Since there are only two baselines are compared to the proposed method, this claim seems too general to be true. It is not clear to why the authors mention that “negative result that the return decreases if we naively consider units as agents”. The introduction of GAN is very abrupt.<BRK>While this is an interesting idea on its surface, the paper suffers from many problems in clarity, motivation, and technical presentation. The major problem with this paper is its clarity. As far as I can tell, this paper has nothing to do with GANs. There is also a general lack of attention to detail. It is never explained how this reward is allocated even in the authors’ own experiments. The authors state that all units playing NOOP is an equilibrium. There are many ways to create rational incentives for neurons in a neural net. The neuroscientific motivation is not very convincing to me, considering that ultimately these neurons have to hold an auction. Is it just a different way to train a NN? Detailed Comments:“In the of NaaA”  > remove “of”?
Accept (Poster). rating score: 8. rating score: 7. rating score: 5. <BRK>The authors perform a comprehensive validation of LSTM based word and character language models, establishing that recent claims that other structures can consistently outperform the older stacked LSTM architecture result from failure to fully explore the hyperparameter space. The paper is clearly motivated and authoritative in its conclusions but it s somewhat lacking in detailed model or experiment descriptions. The description of the model is ambiguous on at least two points. The plot in Figure 2 is clear and persuasive, but for reproducibility purposes it would also be nice to see an example set of strong hyperparameters in a table. There has been some confusion in the literature as to which of these parameterizations is better or more standard; simply justifying the choice of parameterization a little more would also help.<BRK>The submitted manuscript describes an exercise in performance comparison for neural language models under standardization of the hyperparameter tuning and model selection strategies and costs. I have a favourable impression of this paper but would hope another reviewer is more familiar with the specific application domain than I am.<BRK>The authors did extensive tuning of the parameters for several recurrent neural architectures. However the corpus the authors choose are quite small, the variance of the estimate will be quite high, I suspect whether the same conclusions could be drawn. It would be more convincing if there are experiments on the billion word corpus or other larger datasets, or at least on a corpus with 50 million tokens. This will use significant resources and is much more difficult, but it s also really valuable, because it s much more close to real world usage of language models.
Accept (Poster). rating score: 7. rating score: 7. rating score: 6. rating score: 5. <BRK>This paper proposes the idea of having an agent learning a policy that resets the agent s state to one of the states drawn from the distribution of starting states. Ideally we should have confidence intervals in all the plots in the paper. This is a very elegant and general idea, where the value function learned in the reset task also encodes some measure of safety in the environment.<BRK>The paper solves the problem of how to do autonomous resets, which is an important problem in real world RL. The method is novel, the explanation is clear, and has good experimental results. Pros:1.The approach is simple, solves a task of practical importance, and performs well in the experiments. Cons:1.The method is evaluated only for 3 tasks, which are all in simulation, and on no real world tasks.<BRK>This paper proposes a good way of doing this:  learn a policy for resetting at the same time as learning a policy for solving the problem. It says that the reset policy is designed to achieve a distribution of final states that is equivalent to a starting distribution on the problem. All this aside, this seems like a fairly small but well considered and executed piece of work. I m rating it as marginally above threshold, but I indeed find it very close to the threshold.<BRK>(This delayed review is based on the deadline version of the paper.) This idea (both parts) is interesting and potentially very useful, particularly in physical domains where reset is expensive and exploration is risky. 8.The paper is not explicit about the learning dynamics of the reset policy. 2.It s not clear whether the authors suggest that the ability to reset is a good notion of safety, or just a proxy to such a notion.
Reject. rating score: 4. rating score: 5. rating score: 7. <BRK>This knowledge is "injected" into the learning process by augmenting the loss function with a symbolic loss term that, in addition to the traditional loss, increases the probability of legal states (which also includes incorrect, yet legal, predictions). Overall the idea is interesting, but the paper does not seem ready for publication. The  idea of semantic loss function is appealing and is nicely motivated in the paper, however the practical aspects of how it can be applied are extremely vague and hard to understand. Specifically, the authors define it over all assignments to the output variables that satisfy the constraints. For any non trivial prediction problem, this would be at least computationally challenging. This has been addressed in the literature (e.g., [1], [2]) it s not clear why the authors don t compare to these models, or even attempt any meaningful evaluation.<BRK>This is a nice idea, and certainly relevant. The authors clearly describe their problem, and overall the paper is well presented. Well, why not just open the paper with Definition 1, and try to justify this definition on the basis of its properties. Also it is frustrating to see some axioms in the main text, and some axioms in the appendix (why this division?). After presenting the loss function, the authors consider some applications. They are nicely presented; overall the gains are promising but not that great when compared to the state of the art   they suggest that the proposed semantic loss makes sense. By reading it, I feel that the best way to handle the unlabeled data would be to add a direct penalty term forcing the unlabeled points to receive a label. Page 6: "a mor methodological"... should it be "a more methodical"?<BRK>The axioms for the semantic loss function where defined but there seemed to be a lack of a clear algorithm provided showing the pipeline implementation of the semantic loss function. Output constraints for the semantic loss function are represented with one hot encoding, prefer  ence rankings, and paths in a grid. These three different output constraints are designed to explore different learning purposes. The paper would benefit by including a more concrete algorithm describing the flow of data through a given neural net to the semantic loss function, as well as the process by which the semantic loss function constrains the data based on propositional logic, but in general this complaint is more nit picking. Given that, the semantic loss function did prove to be a seemingly simple approach to improving semi supervised classification tasks. • Experiments which clearly show the benefit of using the semantic loss function. • In depth description of the definitions, axioms, and propositions of the semantic loss function. NEGATIVES I was not clear if the logical constraints are to be instantiated before learning, i.e.they are defined by hand prior to being implemented in the neural network. This is not a true negative, but an observation on the effectiveness of the function. A few typos in the paper.
Accept (Poster). rating score: 8. rating score: 6. rating score: 5. <BRK>This is an interesting paper investigating a novel neural TTS strategy that can generate speech signals by sampling voices in the wild. Compared to other state of the art systems like wavenet, deep voice and tacotron, the proposed approach here is claimed to be simpler and relatively easy to deploy in practice. Globally this is a good piece of work with solid performance. However, I have some (minor) concerns. 1.Although the authors claim that there is no RNNs involved in the architectural design of the system,  it seems to me that  the working memory with a shifting buffer which takes the previous output as one of its inputs is a network with recurrence. It would be interesting to know.<BRK>Then the output is created by buffer and speaker ID with another fully connected neural network. In experiments, authors try single speaker TTS and multi speaker TTS along with speaker identification (ID), and show that the proposed approach outperforms baselines, namely, Tacotron and Char2wav. I like the idea in the paper but it has some limitations as described below:Pros:1. 2.Using shifting buffer memory looks interesting and novel. 3.The proposed approach outperforms baselines in several tasks, and the ability to fit to a novel speaker is nice. But there are some issues as well (see Cons.) Cons:1.Writing is okay but could be improved. However, it was not compared to. To demonstrate the efficiency of the proposed model, it would be great to have the numbers of parameters for the proposed model and baseline models. 2.I was not so clear about how to fit a new speaker and adding more detail would be good.<BRK>This paper present the application of the memory buffer concept to speech synthesis, and additionally learns a "speaker vector" that makes the system adaptive and work reasonably well on "in the wild" speech data. This is a relevant problem, and a novel solution, but synthesis is a wicked problem to evaluate, so I am not sure if ICLR is the best venue for this paper. I see two competing goals:  If the focus is on showing that the presented approach outperforms other approaches under given conditions, a different task would be better (for example recognition, or some sort of trajectory reconstruction)  If the focus is on showing that the system outperforms other synthesis systems, then a speech oriented venue might be best (and it is unfortunate that optimized hyper parameters for the other systems are not available for a fair comparsion)  If fair comparisons with the other appraoches cannot be made, my sense is that the multi speaker (post training fitting) option is really the most interesting and novel contribution here, which could be discussed in mroe detailStill, the approach is creative and interesting and deserves to be presented. The authors will know the relevant data sets better than I do, maybe they can simply extend the discussion to show that this is what happens.
Accept (Poster). rating score: 6. rating score: 6. rating score: 6. <BRK>What would the authors propose to mitigate this issue? The two main claims of the paper, namely that discretization doesn t hurt performance on natural test examples and that better robustness (in the author s experimental setup) is achieved through the discretized encoding, are properly backed up by the experiments. Yet, the applicability of the method in practice is still to be demonstrated. The threshold effects might imply that small perturbations of the input (in the l_infty sense) will not have a large effect on their discritized version, but it may also go the other way: an opponent might be able to greatly change the discretized input without drastically changing the input.<BRK>The direct relationship between robustness to adversarial examples and intrinsic dimensionality is well known (paper by Fawzi.). It is well written overall, and the experiments support the claims of the authors. This work has a crucial limitation: scalability. The proposed method scales the input space dimension linearly with the number of discretization steps. What would be the impact of the hyper parameter k in such configuration?<BRK>This paper studies input discretization and white box attacks on it to make deep networks robust to adversarial examples. Robustness to adversarial examples for thermometer encoding is demonstrated through experiments. The empirical fact that thermometer encoding is more robust to adversarial examples than one hot encoding,is interesting. * Deep networks with thermometer encoded inputs empirically have higher accuracy on adversarial examples.
Accept (Poster). rating score: 8. rating score: 7. rating score: 6. <BRK>SUMMARYThe paper proposes to apply GrOWL regularization to the tensors of parameters between each pair of layers. The groups are composed of all coefficients associated to inputs coming from the same neuron in the previous layer. The proposed algorithm is a simple proximal gradient algorithm using the proximal operator of the GrOWL norm. Is the same threshold chosen for GrOWL and the Lasso?<BRK>Negative points:  The method is sold as inducing a clustering, but actually, the clustering is a separate step, and the choice of clustering algorithm might well have an influence on the results. Applied an efficient proximal gradient algorithm to train the model. Minor point:  p.5, in the definition of prox_Q(epsilon), the subscript for the argmin should be nu, not theta.<BRK>This paper proposes to apply a group ordered weighted l1 (GrOWL) regularization term to promote sparsity and parameter sharing in training deep neural networks and hence compress the model to a light version. What’s the benefit of the compression and retraining the trained neural network?
Reject. rating score: 4. rating score: 5. rating score: 6. <BRK>Instead these are emotion words that a person chooses to broadcast along with an associated announcement. In the PCA result, it is not "clear" that the first axis represents valence, as "sad" has a slight positive on this scale and "sad" is one of the emotions most clearly associated with negative valence. This analysis does not seem very different from Twitter analysis, because although Tumblr posts are allowed to be longer than Twitter posts, the authors truncate the posts to 50 characters. Additionally, the images do not seem to add very much to the classification. The authors algorithm also seems to be essentially a combination of two other, previously published algorithms.<BRK>The paper presents a multi modal CNN model for sentiment analysis that combines images and text. The model is trained on a new dataset collected from Tumblr. Weaknesses:  A deeper analysis of previous work on the combination of image and text for sentiment analysis (both datasets and methods) and its relation with the presented work is necessary. The study is limited to just one dataset.<BRK>For text processing the authors use a standard LSTM taking as input GLOVE vectors of words in a sentence. For visual information, authors use a pretrained CNN (with fine tuning). Experimental results are reported in a self generated data set. The contribution from the RL perspective is limited, in the sense that the authors simply applied standard models to predict a bunch of labels (in this case, emotion labels). Still, I think the contribution in that part is a: sentiment psychologically inspired analysis of the Thumbrl data set. I think the author s statement on that this study leads to a more plausible psychological model of emotion is not well founded (they also mention to learn to recognize the latent emotional state).
Reject. rating score: 4. rating score: 6. rating score: 8. <BRK>Summary: This paper proposes a new graph convolution architecture, based on Cayley transform of the matrix. Overall, this leads me to conclude that the paper presents insufficient justification to conclude that Cayley filters offer a significant advantage over existing work. The authors propose to compute them using Jacobi iteration (using the diagonal as a preconditioner), and present experimental results. The comparison to Chebyshev filters  (small degree polynomials in the Chebyshev basis) at several places is unconvincing.<BRK>Unlike other popular variants, it is not strictly supported on a small graph neighborhood, but the paper proves an exponential decay property on the norm of a filtered vertex indicator function. The paper argues that Cayley filters allow better spectral localization than Chebyshev filters. The paper is clear and well written. The proposed method seems of interest, although I find the experimental section only partly convincing. Is this a fair evaluation of ChebNet which may possibly perform better with larger filter orders? The paper could provide some insights as to why ChebNet is unable to work with unnormalized Laplacians while CayleyNet is (and why the ChebNet performance seems to get worse and worse as r increases?).<BRK>This paper is on construction graph CNN using spectral techniques. The originality of this work is the use of Cayley polynomials to compute spectral filters on graphs, related to the work of Defferrard et al.(2016) and Monto et al.(2017) where Chebyshev filters were used. Theoretical and experimental results show the relevance of the Cayley polynomials as filters for graph CNN. We recommend the authors to talk about some future work.
Accept (Poster). rating score: 8. rating score: 7. rating score: 7. <BRK>Also, in the paper it is shown how the matrix multiplications of the quantized model can be speeded up using 64 bits operation in CPU. The observation that the optimal binary code can be computed with a BST is simple and elegant.<BRK>Significance: I m not familiar with the quantization literature, so I ll let more knowledgeable reviewers evaluate this point. Correctness: The paper is technically correct. It would be nice to have those memory and speed gains for training as well.<BRK>The paper is relatively easy to read, and the technique is clearly explained. Also, the online quantization of activations doesn t seem to be factored into the speedup calculations, and no benchmarks are provided demonstrating how fast the quantization is (unless I m missing something). Experiments are conducted in an RNN context for some language modeling tasks.
Reject. rating score: 4. rating score: 5. rating score: 6. <BRK>Unfortunately, I m not sure what specific contributions come out, and the paper seems to meander in derivations and remarks that I didn t understand what the point was. Or given this abstraction, what new perspectives or analysis is offered? The authors propose an objective whose Lagrangian dual admits a variety of modern objectives from variational auto encoders and generative adversarial networks.<BRK>The stated usefulness of the method unfortunately do not answer my worry about the significance. Before rebuttal This paper proposes an optimization problem whose Lagrangian duals contain many existing objective functions for generative models. The paper has interesting elements and the results are original. After Eq.7, please add a proof (may be in the Appendix). It is not that straightforward to see this.<BRK>The paper presented a unifying framework for many existing generative modelling techniques, by first considering constrained optimisation problem of mutual information, then addressing the problem using Lagrange multipliers. In this perspective, the proposed framework might be useful, but as noted in the original review, the presentation is not clear, and it s not convincing to me that the MI framework is indeed useful in the sense I described above.
Reject. rating score: 3. rating score: 4. rating score: 7. <BRK>The authors propose to use 2D CNNs for graph classification by transforming graphs to an image like representation from its node embedding. The approach uses node2vec to obtain a node embedding, which is then compacted using PCA and turned into a stack of discretized histograms. In my opinion there are several weak points:1) The approach to obtain the image like representation is not well motivated. In summary: Since the technical contribution is limited, the approach needs to be justified by an authoritative experimental comparison. This is not yet achieved with the results presented in the submitted paper.<BRK>The paper introduces a method for learning graph representations (i.e., vector representations for graphs). It seems that the methods outperforms existing methods for learning graph representations. The problem with the approach is that it is very ad hoc. Since the methods is so ad hoc (node2vec  > PCA  > discretized density map  > CNN architecure) and since a theoretical understanding of why the approach works is missing, it is especially important to compare your method more thoroughly to simpler methods. The experimental results are also not explained thoroughly enough.<BRK>The paper presents a novel representation of graphs as multi channel image like structures. he resulting multi channel image like structures are then feed into vanilla 2D CNN. The papers is well written and clear, and proposes an interesting idea of representing graphs as multi channel image like structures. Furthermore, the authors perform experiments with real graph datasets from the social science domain and a comparison with the SoA method both graph kernels and deep learning architectures. The proposed algorithm in 3 out of 5 datasets, two of theme with statistical significant.
Reject. rating score: 5. rating score: 5. rating score: 6. <BRK>That is, the authors learn all the previous layersby finding point estimates. Clarity:The paper is clearly written. In this paper there is no generative model for the data and thedata obtained is not actually sampled from the model. I am also concerned about the hyper parameter tuning for the baselines. This will have the effect of tuning the width of theirposterior approximation which is directly related to the amount of explorationperformed by Thompson sampling. The use of Thompson sampling for efficientexploration in deep Q learning is also not new since it has been proposed byLipton et al.2016.The main contribution of the paper is to combine these twomethods (equations 6 10) and evaluate the results in the large scale setting ofATARI games, showing that it works in practice. The authors by contrast, perform exact Bayesian inference, butonly on the last layer of their neural network. It would be very useful to knowwhether the exact linear Gaussian model in the last layer proposed by theauthors has advantages with respect to a variational approximation on all thenetwork weights.<BRK>This paper proposed "Bayesian Deep Q Network" as an approach for exploration via Thompson sampling in deep RL. This algorithm maintains a Bayesian posterior over the last layer of the neural network and uses that as an approximate measure of uncertainty. The agent then samples from this posterior for an approximate Thompson sampling. The paper is mostly clear and well written. The experimental results are impressive in their outperformance. This paper spends a lot of time re deriving Bayesian linear regression in a really standard way... and without much discussion of how/why this method is an approximation (it is) especially when used with deep nets. Overall, I like this paper and the approach of extending TS style algorithms to Deep RL by just taking the final layer of the neural network. However, it also feels like there are some issues with the baselines + being a bit more clear about the approximations / position relative to other algorithms for approximate TS would be a better approach. Similarly *some* of the results for Bootstrapped DQN (2016) on Atari are presented without bootstrapping (pure ensemble) but this is very far from an essential part of the algorithm!<BRK>The authors propose a new algorithm for exploration in Deep RL. They apply Bayesian linear regression, given the last layer of a DQN network as features, to estimate the Q function for each action. I generally liked the paper and the approach, here are some more detailed comments. It’s not immediately clear what the semantics of this posterior are then. But the proposed algorithm samples every T_sample steps, did you find this beneficial to wait longer before resampling? It would be useful to comment on that aspect. The method is evaluated on 6 Atari games (How were the games selected?Do they have exploration challenges?) On these games versus (their implementation of) DDQN, the results seem encouraging. But it would be good to know whether the approach works well across games and is competitive against other stronger baselines. As is done in the baselines usually.
Invite to Workshop Track. rating score: 8. rating score: 5. rating score: 4. <BRK>In this paper, the authors argue for the use of convolutional architectures as a general purpose tool for sequence modeling. The paper is clearly written and easy to follow, does a good job of presenting both the advantages and disadvantages of the proposed method, and convincingly makes the point that convolutional architectures should at least be considered for any sequence modeling task; they are indeed still often overlooked, in spite of some strong performances in language modeling and translation in recent works.<BRK>The authors claim that convolutional networks should be considered as possible replacements of recurrent neural networks as the default choice for solving sequential modelling problems. The experiments seem sound and the information in both the paper and the appendix seem to allow for replication. The paper is well structured and written. The ideas in the paper are not novel and neither do the authors claim that they are. + Many experiments and tasks. + Well written and clear.<BRK>This paper argues that convolutional networks should be the defaultapproach for sequence modeling. The paper is nicely done and rather easy to understand. In order to support the original hypothesis,I think that a much larger and more diverse set of experiments should havebeen considered.
Reject. rating score: 2. rating score: 5. rating score: 5. <BRK>But as mentioned by the authors, this is not new either. But they are all known.<BRK>This paper uses a well known variational representation of the relative entropy (the so called Donsker Varadhan formula) to derive an expression for the Bellman error with entropy regularization in terms of a certain log partition function. This is stated in Equation (13) in the paper. The same applies to contraction results for the "softmax" Bellman operator   these results are not novel at all, see, e.g., D. Hernandez Hernandez and S. I. Marcus, “Risk sensitive control of Markov processes in countable state space,” Systems and Control Letters, vol.<BRK>Originality: The results presented are well known and there is no clear contribution algorithmic wise to the field of RL.
Accept (Poster). rating score: 7. rating score: 6. rating score: 5. <BRK>The motivation behind it is that in GANs interpolating between sampled points is common in the process of generating examples but the use of a normal prior results in samples that fall in low probability mass regions. In general, the proposed work is very interesting and the idea is neat. The paper is well presented and I want to underline the importance of this.<BRK>While the contribution of using a standard family of distributions in a standard implicit model setup is limited, the paper does make interesting observations, analyses and an attempt to correct the interpolation issue. The paper is clearly written and presents the theory and experimental results nicely.<BRK>The authors discuss a direct Gamma sampling method for the interpolated samples in GANs, and show the improvements over usual normal sampling for CelebA, MNIST, CIFAR and SVHN datasets. Thus I think this paper is below the acceptance threshold.
Accept (Oral). rating score: 9. rating score: 8. rating score: 8. <BRK>The paper presents three contributions: 1) it shows that the proof of convergence Adam is wrong; 2) it presents adversarial and stochastic examples on which Adam converges to the worst possible solution (i.e.there is no hope to just fix Adam s proof); 3) it proposes a variant of Adam called AMSGrad that fixes the problems in the original proof and seems to have good empirical properties. Hence, at least for a certain setting of its parameters, RMSProp will converge. See, for example, Zhang, ICML 04 where linear convergence is proved in a neighbourhood of the optimal solution for strongly convex problems. Overall, I strongly suggest to accept this paper. also x_2 and z_2?<BRK>This paper examines the very popular and useful ADAM optimization algorithm, and locates a mistake in its proof of convergence (for convex problems). Not only that, the authors also show a specific toy convex problem on which ADAM fails to converge. The experimental part is limited, as you state "preliminary", which is a unfortunate for a work with possibly an important practical implication. A simple pseudo code in the appendix would be welcome. It is a bit confusing.<BRK>Moreover, it gives a simple 1 dimensional counterexample withlinear losses on which Adam does not converge. The problem with Adam is that the "learning rate" matricesV_t^{1/2}/alpha_t are not monotonically decreasing. It is then shown that AMSGrad does satisfyessentially the same convergence bound as the one previously claimed forAdam. Given the popularity of Adam, I consider this paper to make a veryinteresting observation. The proof of Theorem 3 assumes there are no projections, so this  should be stated as part of its conditions. This should be stated in the statement of the Theorem.
Reject. rating score: 4. rating score: 4. rating score: 6. <BRK>In summary, I think the paper needs very careful editing for grammar and language and, more importantly, it needs solid experiments before it’s ready for publication. In light of this, the fact that SFNN is given extra epochs in Figure 4 does not mean much.<BRK># Conclusion:In conclusion, while interesting, for me the paper is not yet ready for publication. I would recommend this work for a workshop presentation at this stage.<BRK>How is this achieved in practice? The idea is interesting and to my knowledge novel. Experiments are carefully designed and presented in details, however assessing the impact of the proposed new objective is not straightforward.
Accept (Poster). rating score: 7. rating score: 7. rating score: 6. <BRK>Using this type of a network, authors introduce a notion of intrinsic dimension of optimization problems   it is minimal dimension of a subset, for which random subset neural net already reaches best (or comparable) performance. They then demonstrate that the intrinsic dimension for the same problem stays the same when different architectures are chosen. This does not always hold for CNNsModel with smaller intrinsic dimension is suggested to be better .<BRK>This paper proposes an empirical measure of the intrinsic dimensionality of a neural network problem. * The authors then go on to propose that compression of the network be achieved by random projection to a subspace of dimensionality greater than or equal to the intrinsic dimension.<BRK>While deep learning usually involves estimating a large number of variable, this paper suggests to reduce its number by assuming that these variable lie in a low dimensional subspace. Simulations show the promise of the proposed method. The method is clearly written and the idea looks original.
Reject. rating score: 2. rating score: 2. rating score: 3. <BRK>From my perspective, the work is very immature and seems away from current state of the art on object detection, both in the vocabulary, performance or challenges.<BRK>nan<BRK>nan
Accept (Poster). rating score: 7. rating score: 7. rating score: 6. <BRK>This paper proposes a method for parameter space noise in exploration. In some domains this can be a much better approach and this is supported by experimentation. This is clearly a very practical and extensible idea... the authors present good results on a whole suite of tasks. The paper is clear and well written, it has a narrative and the plots/experiments tend to back this up. However, there are also a few things to be cautious of... and some of them serious:  At many points in the paper the claims are quite overstated. I think the claim/motivation for this example in the bootstrapped DQN paper is more along the lines of "deep exploration" and you should be clear that your parameter noise does *not* address this issue. I can t really support the conclusion "RL with parameter noise exploration learns more efficiently than both RL and evolutionary strategies individually".<BRK>In recent years there have been many notable successes in deep reinforcement learning. However, in many tasks, particularly sparse reward tasks, exploration remains a difficult problem. For off policy algorithms it is common to explore by adding noise to the policy action in action space, while on policy algorithms are often regularized in the action space to encourage exploration. This work introduces a simple, computationally straightforward approach to exploring by perturbing the parameters (similar to exploration in some evolutionary algorithms) of policies parametrized with deep neural nets. This work is well written and cites previous work appropriately. The authors provide a significant set of experiments using their method on several different RL algorithms in both continuous and discrete cases, and find it generally improves performance, particularly for sparse rewards. It would be helpful if the authors are able to make their paper reproducible by releasing the code on publication. Deep reinforcement learning that matters.<BRK>This paper explores the idea of adding parameter space noise in service of exploration. The paper is very well written and quite clear. Parameter noise does better in some Atari + Mujoco domains, but shows little difference in most domains. Overall, I think parameter space noise is a worthy technique to have analyzed and this paper does a good job doing just that. However, I don’t expect this technique to make a large splash in the Deep RL community, mainly because simply adding noise to the parameter space doesn’t really gain you much more than policies that are biased towards particular actions. A non trivial amount of work has been done to find a sensible way of adding noise to parameter space of a deep network and defining the specific distance metrics and thresholds for (dual headed) DQN, DDPG, and TRPO.
Invite to Workshop Track. rating score: 6. rating score: 6. rating score: 6. <BRK>Performance of baseline models and humans show that it is a challenging task and more advanced models are required to solve this task. — The only advantages mentioned in the paper of using a synthetic dataset for this task are having greater control over task’s complexity and enabling auxiliary supervision signals, but none of them are shown in this paper, so it’s not clear if they are needed or useful. Or in short, what new challenges are being introduced by FigureQA and how should researchers go about solving them on a high level? — Are the model accuracies in Table 3 on the same subset as humans or on the complete test set? Can the authors please report both separately?<BRK>Summary:The paper introduces a new dataset (FigureQA) of question answering on figures (line plots, bar graphs, pie charts). The images are synthetic and the questions are templated. The proposed task is a useful task for building intelligent AI agents. 4.The baselines experimented with in the paper make sense. The motivation behind the proposed task needs to be better elaborated. 5.It is not clear why did the authors devise a new metric for smoothness and not use existing metrics?<BRK>This paper introduces a new dataset called FigureQA. This is definitely a novel area that requires the machine not only understand the corpus, but also the scientific figure associated with the figure. [Weaknesses]1: There are no novel algorithms associated with this dataset. 3: Since the generated question is very templated and less variational, a traditional hand crafted approach may perform much better compared to the end to end approach. Taking all these into account, I suggest accepting this paper if the authors could provide more justification on the question side of the proposed dataset.
Accept (Poster). rating score: 7. rating score: 7. rating score: 3. <BRK>The distinguishing aspect of the neural programmer interpreter is that it learns a generic core (which in the variant of the paper corresponds to an interpreter of the programming language) and programs for concrete tasks simultaneously. For instance, they separate out the evaluation of the detector from the LSTM used for the core. The authors describe two ways of training their variant of the neural programmer interpreter. I found the new architecture of the neural programmer interpreter very interesting. It is carefully crafted so as to support expressive combinators without making the learning more difficult. I am generally positive about accepting this paper to ICLR 18. I have three complaints, though. First, the paper uses 14 pages well over 8 pages, the recommended limit. Third, the authors claim universality of the approach. The* p9: As I mentioned, I suggest you to make clear that the claim about universality is mostly based on intuition, not on theorem.<BRK>QualityThe paper is very interesting and clearly motivated. I had a lot of questions while reading:What is the purpose of detectors? It is not clear what is being detected. In the original NPI, it is assumed that the caller knows which encoder is needed for each program. SignificanceThe paper will be significant to people interested in NPI related models and neural program induction generally, but on the other hand, there is currently not yet a “killer application” to this line of work. The experiments appear to show significant new capabilities of CNPI compared to NPI and RNPI in terms of better generalization and universality, as well as being trainable by reinforcement learning. Detector training is decoupled from core and memory training, so that perfect generalization does not have to be re verified after learning new behaviors.<BRK>The paper is interesting to read and gives valuable insights. However, the paper clearly breaks the submission guidelines. The paper is far too long, 14 pages (+refs and appendix, in total 19 pages), while the page limit is 8 pages (+refs and appendix). I can not foresee how the authors should be able to squeeze to content into 8 pages. The paper is more suitable for a journal, where page limit is less of an issue.
Accept (Poster). rating score: 8. rating score: 7. rating score: 5. <BRK>The corresponding slow convergence rate explains the phenomenon that the predictor can continue to improve even when the training loss is already small. The result of this paper can inspire the study of the implicit bias introduced by gradient descent variants or other optimization methods, such as coordinate descent. The proposed assumptions are reasonable, but it seems to limit to the loss function with exponential tail. However, there are some places can be improved in this paper. For example, in Lemma 1, results (3) and (4) can be combined together. It is better for the authors to use another section to illustrate experimental settings instead of writing them in the caption of Figure 3.1.<BRK>This paper shows that indeed, log loss (and some other similar losses), minimised with gradient descent, leads to convergence (in the above sense) to the max margin solution. On one hand it is an interesting property of model we train in practice, and on the other   provides nice link between two separate learning theories. It would be beneficial for the clarity if authors define what they mean by convergence (normalised weight vector, angle, whichever path seems most natural) as early in the paper as possible.<BRK>The overall discussion of the paper is well written, but on a moredetailed level the paper gives an unpolished impression, and has manytechnical issues. Although I suspect that most (or even all) of theseissues can be resolved, they interfere with checking the correctness ofthe results. Technical Issues:The statement of Lemma 5 has a trivial part and for the other part theproof is incorrect: Let x_u   ||nabla L(w(u))||^2. In particular, if the support vectors    do not span the space, because all data lie in the same    lower dimensional hyperplane, then this is not the case. In the proof sketch (p.3):    Why does the fact that the limit is dominated by gradients that are    a linear combination of support vectors imply that w_infinity will    also be a non negative linear combination of support vectors? "converges to some limit".
Accept (Poster). rating score: 7. rating score: 7. rating score: 6. <BRK>Summary of paper:The paper proposes a unique network architecture that can learn divide and conquer strategies to solve algorithmic tasks. Review:The paper is clearly written.<BRK>This paper studies problems that can be solved using a dynamic programming approach and proposes a neural network architecture called Divide and Conquer Networks (DCN) to solve such problems.<BRK>This paper proposes to add new inductive bias to neural network architecture   namely a divide and conquer strategy know from algorithmics.
Reject. rating score: 5. rating score: 5. rating score: 7. <BRK>This paper describes a method for computing representations for out of vocabulary words, e.g.based on their spelling or dictionary definitions. The main difference from previous approaches is that the model is that the embeddings are trained end to end for a specific task, rather than trying to produce generically useful embeddings. The method leads to better performance than using no external resources, but not as high performance as using Glove embeddings. The paper is clearly written, and has useful ablation experiments. I would be slightly surprised if no previous work has used external resources for training word representations using an end task loss, but I don’t know the area well enough to make specific suggestions   I’m a little skeptical about how often this method would really be useful in practice. It seems like the proposed method is quite generic, so why not apply it to a stronger baseline?<BRK>This paper examines ways of producing word embeddings for rare words on demand. The strength of this paper is that it both gives a more systematic framework for and builds on existing ideas (character based models, using dictionary definitions) to implement them as part of a model trained on the end task. The contribution is clear but not huge. The basic method easily fits within 3 pages, and while the presentation of the experiments would need to be much briefer, this seems quite possible. However, they point out that at least for matching tasks like QA and NLI that one can do better by assigning random vectors on the fly to unknown words. I guess that is because of the nature of NLI, but it isn t 100% clear why NLI benefits so much more than QA from definitional knowledge.<BRK>This paper illustrates a method to compute produce word embeddings on the fly for rare words, using a pragmatic combination of existing ideas:* Backing off to a separate decoder for rare words a la Luong and Manning (https://arxiv.org/pdf/1604.00788.pdf, should be cited, though the idea might be older). * Using character level models a la Ling et al.* Using dictionary embeddings a la Hill et al.None of these ideas are new before but I haven’t seen them combined in this way before. This is a very practical idea, well explained with a thorough set of experiments across three different tasks. The paper is not surprising but this seems like an effective technique for people who want to build effective systems with whatever data they’ve got.
Reject. rating score: 5. rating score: 5. rating score: 8. <BRK>The authors propose a different type of GAN the Dudley GAN that is related to the Dudley metric. One would have to show that the NN is dense in Dudley unit ball w.r.t.L_inf norm, but this sort of misnaming had started with the "Wasserstein" GAN. The main idea [and its variants] looks solid, but with the plethora of GANs in the literature now, after reading I m still left wondering why this GAN is significantly better than others [BEGAN, WGAN, etc.].<BRK>However, after reading through the manuscript, it is not clear to me what are the real contributions made in this paper. I also failed to find any rigorous results on generalization bounds. In this case, I cannot recommend the acceptance of this paper.<BRK>Ensuring Lipschitz condition in neural nets is essential of stablizing GANs. This paper proposes two contraint based optimzation to ensure the Lips condtions , and these proposed approaches maintain suffcient capacity, as well as expressiveness of the network. A simple theoritical result is given by emprical risk minimization. Here I am concerned with the following two questions:(1) How to parameterize the function space of f_w or h_w, since they are both multivariate and capacity of the network will bereduced if the used way of parametering functions is adopted inappropriatily. Besides, the parameter $\gamma$ appears in the discriminator, which contradicts its role on the contraint of functions space.
Accept (Poster). rating score: 7. rating score: 7. rating score: 5. <BRK>In this paper, the authors trains a large number of MNIST classifier networks with differing attributes (batch size, activation function, no.layers etc.) and then utilises the inputs and outputs of these networks to predict said attributes successfully.<BRK>The basic idea is to train a neural network to predict various hyperparameters of a classifier from input output pairs for that classifier (kennen o approach). That would make the results much more interesting, and may even turn out to be useful for other problems, such as hyperparameter optimization.<BRK>It is really hard to say what a supervised learning meta model approach such as the one presented in this work have to say about that case. The paper attempts to study model meta parameter inference e.g.model architecture, optimization, etc using a supervised learning approach. I also find the question interesting and the fact that it works on this relatively broad set of meta parameters and under a rigorous train/test split intriguing. Is it the other attributes ?
Reject. rating score: 2. rating score: 4. rating score: 4. <BRK>2) The experiments are restricted to a single dataset   MNIST. For instance, architectures of networks are not provided.<BRK>The method uses an InfoGAN to learn the distribution of filters.<BRK>The claim is that the found input represents the non linear transformations that the layer is invariant to. It seems that the generated images are not actually plausible images at all and so not many conclusions can be drawn from this method.
Reject. rating score: 2. rating score: 4. rating score: 6. <BRK>Paragraph vectors are also parallelizable so it s not obvious that this method would be superior to it. The paper in the introduction also states that doc2vec is trained using localized contexts (5 to 10 words) and never sees the whole document. If this was the case then paragraph vectors wouldn t work when representing a whole document, which it already does as can be seen in table 2. The paper also fails to compare with the significant amount of existing literature on state of the art document embeddings. Many of these are likely to be faster than the method described in the paper. Chen, M. Efficient vector representation for documents through corruption.<BRK>This paper uses CNNs to build document embeddings. The main advantage over other methods is that CNNs are very fast. This is especially true when the main contribution of the work is a network architecture. I m a bit confused by section 3.1 on language modelling. This paper proposed both a model and a training objective, and I would have liked to see some attempt to disentangle their effect. Overall I do not see a substantial contribution from this paper. The main claims seem to be that CNNs are fast, and can be used for NLP, neither of which are new.<BRK>The approach uses a CNN architecture, distinguishing it from the majority of prior efforts on this problem, which have tended to use RNNs. Overall, the model presented is relatively simple (a good thing, in my view) and it indeed seems fast. Furthermore, the work is presented relatively clearly. That said, my main concerns regarding this paper are that: (1) there s not much new here, and, (2) the experimental setup may be flawed, in that it would seem model hyperparams were tuned for the proposed approach but not for the baselines; I elaborate on these concerns below. How big of an effect does the stochastic sampling of document indices have on the learned embeddings? Yet this seems to be one of the main arguments the authors make in favor of the model (in contrast to RNN based methods). This claim should either by made more precise or removed.
Invite to Workshop Track. rating score: 6. rating score: 5. rating score: 5. <BRK>In the paper titled "Faster Discovery of Neural Architectures by Searching for Paths in a Large Model", the authors proposed an efficient algorithm which can be used for efficient (less resources and time) and faster architecture design for neural networks. The motivation of the new algorithm is by sharing parameters across child models in the searching of archtecture. Although the methodology within the paper appears to be incremental over previous NAS method, the efficiency got improved quite significantly.<BRK>In NAS, the practitioners have to retrain for every new architecture in the search process, but in ENAS this problem is avoided by sharing parameters and using discrete masks. The authors present two ENAS models: one for CNNs, and another for RNNs. Initially it seems like the controller can choose any of B operations in a fixed number of layers along with choosing to turn on or off ay pair of skip connections. However, in practice we see that the search space for modeling both skip connections and choosing convolutional sizes is too large, so the authors use only one restriction to reduce the size of the state space. This is a limitation, as the model space is not as flexible as one would desire in a discovery task. Although finding an appropriate architecture for skip connections is an important task, finding an efficient method to structure RNN cells seems like a significantly more important goal. Overall, the paper is well written, and it brings up an important idea: that parameter sharing is important for discovery tasks so we can avoid re training for every new architecture in the search process. It is also impressive how much faster their model performs on tasks without sacrificing much performance. The main limitation is that the best architectures as currently described are less about discovery and more about human input   finding a more efficient search path would be an important next step.<BRK>The method is relatively efficient, since it searches in a space of similar architectures, and uses weights sharing between the tested models to avoid optimization of each model from scratch. And: if each layer is connected to all its previous layers by skip connections, what remains to be learned w.r.t the model structure? Isn’t the pattern of skip connection the thing we would like to learn? ENAS does not reveal a new surprising architectures, and it seems that instead of searching in the large space it suggests, one can just tune a 1 2 parameters  (for the image network, it is the number of maps in a layer). We already know the answers to these search problems (denser skip connection pattern works better, more functions types  in a layer in parallel do better, the number of maps should be adjusted to the complexity and data size to avoid overfit). I hence believe the strong empirical results of ENAS are a property of the search space (the architecture used) and not of the search algorithm. this should be reported in some detail. And are there different matrices used/trained for each mask embedding (one for 1*1 conv, one for 3*3 conv, etc..)? The relevant baseline here is a model with 64 or 96 maps on each block, each layer.Such a model is likely to do as well as the ENAS model, and can be obtained easily with slight parameter tuning of a single parameter. The found configuration was not compared to these baselines. Comparison of the results to an architecture with all skip connections, and with a single skip connection per layer is required to estimate if useful structure is being learnt.
Accept (Poster). rating score: 6. rating score: 6. rating score: 6. <BRK>The experiments are carried out on ImageNet and are seriously conducted. The paper proposes a modification to adversarial training. In the end, while the work presented in the paper found its use in the recent NIPS competition on defending against adversarial examples, it is still unclear whether this kind of defence would make a difference in critical applications. One advantage of the method is that it is extremely simple.<BRK>(These methods may extend to other machine learning models and domains as well, but that s beyond the scope of this paper.) The proposed heuristics seem effective in practice, but they re somewhat ad hoc and there is no analysis of how these heuristics might or might not be vulnerable to future attacks. This yields more effective adversarial examples, both for attacking models and for training, because it relies less on the local gradient. R+Step LL can be viewed as a 2 step attack: a random step followed by a gradient step.<BRK>This paper proposes ensemble adversarial training, in which adversarial examples crafted on other static pre trained models are used in the training phase. Their method makes deep networks robust to black box attacks, which was empirically demonstrated. This is an empirical paper. The ideas are simple and not surprising but seem reasonable and practically useful. [Weak points]* no theoretical guarantee for proposed methods. * Robustness of their ensemble adversarial training depends on what pre trained models and attacks are used in the training phase.
Accept (Poster). rating score: 7. rating score: 7. rating score: 4. <BRK>Also developing GAN approaches for discrete variables is an important and unsolved problem. My understanding of the paper is that:1. the paper proposes a density ratio estimator via the f gan approach;2. the paper proposes a training criterion that matches the generator s distribution to a self normalised importance sampling (SIS) estimation of the data distribution;3. in order to reduce the variance of the REINFORCE gradient, the paper seeks out to do matching between conditionals instead.<BRK>Original Review  Summary of the paper:The paper presents a method based on importance sampling and reinforcement learning to learn discrete generators in the GAN framework. The generator is trained to minimize the KL distance between the  discrete generator q_{\theta}(x|z), and the importance weight discrete real distribution estimator w(x|z)q(\theta|z).<BRK>The paper introduces a new method for training GANs with discrete data. To this end, the output of the discriminator is interpreted as importance weight and REINFORCE like updates are used to train the generator.
Accept (Poster). rating score: 7. rating score: 7. rating score: 6. <BRK>Originality: The solution proposed is a novel one based on a dual memory system inspired by the memory storage mechanism in brain.<BRK>I quite liked the revival of the dual memory system ideas and the cognitive (neuro) science inspiration.<BRK>This paper addresses the problem of incremental class learning with brain inspired memory system.
Reject. rating score: 4. rating score: 4. rating score: 6. <BRK>2) Novelty/Significance I think the novelty of this paper is perhaps marginal. Experiments are provided on UCI datasets and demonstrate some promise. The discussion is missing any analysis of the results.<BRK>However, the experimental setting of this paper is biased. Pros:The paper is well written and easy to follow. Cons:1 The novelty of this paper is limited.<BRK>The main issues of the paper are the motivation and the experiments. The paper is overall well written and I have only 1 question about the clarity: there is a very short Sec.2.4 saying that "So far, we developed and applied our proposed CPD UML under input feature spaces.
Accept (Poster). rating score: 7. rating score: 6. rating score: 5. <BRK>It is a well written, very clear paper, and it has a good understanding of the literature, and does not overstate the results. The experiments are serious, and done using standard state of the art tools and architectures. Overall, it is an interesting idea, and due to the current focus on neural nets, it is of interest even though it is not yet providing substantial improvements. It s nice to have it very clear, since "gradient step" doesn t make it clear what the stepsize is, and if this is done in a "Jacob like" or "Gauss Seidel like" fashion. It would be nice to expand this.<BRK>The experiments demonstrate that the proposed algorithm are competitive with standard backpropagation and potentially faster if code is optimized further. Summary of the review: The paper is well written, clear, tackles an interesting problem. Review:Using an implicit step leads to a descent step in a direction which is different than the gradient step. Based on the experiment, the step in the implicit direction seems to decrease faster the objective, but the paper does not make an attempt to explain why. Is it because the method can be understood as some form of block coordinate Newton with momentum? It would be nice to have an even informal explanation.<BRK>In Fig 3.The full batch loss of Adam+ProxProp is higher than Adam+BackProp regarding time, which is different from Fig.2.Also, the figure shows that the performance of Adam+BackProp is worst than Adam+ProxProp even though the training loss of Adam+BackProp is smaller that of Adam+ProxProp. Then to make it fast, the implicit step is approximated using conjugate gradient method because the step is solving a quadratic problem. The theoretical result of the ProxProp considers the full batch, and it can not be easily extended to the stochastic variant (mini batch).
Invite to Workshop Track. rating score: 6. rating score: 4. rating score: 4. <BRK>No reasonable fixed kernel is likely to yield good results on a harder image modeling problem, but that is a slightly different message than the one this paragraph conveys. It would be interesting to replicate the analysis of Danihelka et al.(2017) on the Thin 8 dataset. Overall, I like the argument here, and think that it is a useful framework for thinking about these things. These are unlikely to lead to a practical learning algorithm, but could be mentioned in Table 1.<BRK>Most of their theoretical results  seems to be already existing in literature (Liu, Arora,  Arjovsky) in some form of other and it is claimed that this paper put these result in perspective in an attempt to provide a more principled view of the nature and usefulness of adversarial divergences, in comparison to traditional divergences. However, it seems to me that the paper is limited both in theoretical novelty and practical usefulness of these results. Approximation and convergence properties of generative adversarial learning.<BRK>This paper introduces a family of "parametric adversarial divergences" and argue that they have advantages over other divergences in generative modelling, specially for structured outputs. I think that presenting a very all encompassing formulation without a strong foundation does not add value. So, given the lack of strong foundation for the formulation, "parametric adversarial divergences" feel more like estimators of other divergences than a relevant new family. * The experiments are few and too specific, specially given that the paper presents a very general framework. The first experiment just shows that Wasserstein GANs don t perform well in an specific dataset and use that to validate a point about those GANs not being good for high dimensions due to their sample complexity. In summary, I like the authors idea to explore the restriction of the function class of dual representations to produce useful in practice divergences, but the paper feels a bit middle of the road. The theory is not strong and the experiments don t necessary support the intuitive claims made in the paper.
Reject. rating score: 4. rating score: 5. rating score: 7. <BRK>The method part is clear and well written. The results are fine when the idea is applied to the BiDAF model, but are not very well on the DrQA model. Together with the above concerns, it makes me doubt the motivation of this work on reading comprehension. The example work and its previous work also accelerated LSTM by several times without significant performance drop on some RC models (including DrQA). Considering that the DrQA is a better system on both SQuAD and TriviaQA, the speedup on DrQA is thus more important. (4) It seems that this paper was finished in a rush. (5) I do not quite understand the reason for the big performance drop on DrQA.<BRK>This paper proposes a convnet based neural network architecture for reading comprehension and demonstrates reasonably good performance on SQuAD and TriviaQA with a great speed up. I understand the motivation that ConvNet has a great advantage of easing parallelization and thus is worth exploring. However, I think the proposed architecture in this paper is less motivated. According to Table 4, dilation is really not worth that much and GLU seems to be significantly better than ReLU, but why? The architecture search (Table 3 and Figure 4) seems to quite arbitrary. I  would like to see more careful architecture search and ablation studies. Also, why is Conv DrQA significantly worse than DrQA while Conv BiDAF can be comparable to BiDAF?<BRK>The paper proposes a simple dilated convolutional network as drop in replacements for recurrent networks in reading comprehension tasks. The first advantage of the proposed model is short response time due to parallelism of non sequential output generation, proved by experiments on the SQuAD dataset. The paper should stress on this a bit more. The paper also lacks discussion with other models that use dilated convolution in different ways, such as WaveNet[1]. Therefore I recommend acceptance for it.
Accept (Poster). rating score: 7. rating score: 7. rating score: 6. <BRK>This paper describes a new model architecture for machine reasoning. Thelargest is that this paper offers very little in the way of analysis. The modelis structurally quite similar to a stacked attention network or a particularfixed arrangement of attentive N2NMN modules, and it s not at all clear based onthe limited set of experimental results where the improvements are actuallycoming from. Itseems like once all the pieces are in place it should be very easy to getnumbers on VQA or even a more interesting synthetic dataset like NLVR. Based on a sibling comment, it seems that there may also be some problems withthe comparison to FiLM, and I would like to see this addressed. But I will become much more enthusiastic about if if theauthors can provide results on other datasets (even if they re notstate of the art!) as well as evidence for the following:1. 2.Do these induce reasonable attentions over regions of the image? In addition to examples, here I think there are some useful qualitativemeasures.<BRK>The complete model consists of an input unit, a sequence of the proposed Memory, Attention and Composition (MAC) cell, and an output unit. Experiments on CLEVR dataset shows that the proposed model outperforms previous models. Strengths: — The idea of building a compositional model for visual reasoning and visual question answering makes a lot of sense, and, I think, is the correct direction to go forward in these fields. — It is not clear which part of the proposed model leads to how much improvement in performance. is needed to justify if the model is actually doing what the authors think it should do. — Why is it necessary to use both question and memory information to answer the question even when the question was already used to compute the memory information? I would think that including the question information helps in learning the language priors in the dataset. Have the authors looked at some qualitative examples where the model which only uses memory information gives an incorrect answer but adding the question information results in a correct answer? — Details such as using Glove word embeddings are important and can affect the performance of models significantly. Therefore, they should be clearly mentioned in the main paper while comparing with other models which do not use them. These should be reported in this section. What is there in the architecture of the proposed model which provides this ability? If it is the Glove vectors, it should be clearly mentioned since any other model using Glove vectors should have this ability. Can the authors show some qualitative examples for such cases?<BRK>This paper proposes a recurrent neural network for visual question answering. The proposed model shows the state of the art performance on CLEVR and CLEVR Humans dataset, which are standard benchmarks for visual reasoning problem. The proposed model in this paper is designed with reasonable motivations and shows strong experimental results in terms of overall accuracy and the data efficiency. Specifically, they proposed three different ways to update the memory (simple update, self attention and memory gate), but it is not clear which method is used in the end. To isolate the strength of the proposed reasoning module, I ask to provide experiments without pretrained word vectors. Lack of experimental justification of the design choicesThe proposed recurrent unit contains various design choices such as separation of three different units (control unit, read unit and memory unit), attention based input processing and different memory updates stem from different motivations. However, these design choices are not justified well because there is neither ablation study nor visualization of internal states.
Reject. rating score: 4. rating score: 4. rating score: 5. rating score: 7. <BRK>Despite some promising results, I found some issues with the paper. The main one is that the connection between conditional entropy and the proposed variance regularizer seems tenuous. Perhaps this is a bug in the reference? * The final form of the regularization makes it look like a more principled alternative to batchnorm. It would have been nice if the authors more directly compared SHADE to BN.<BRK>Summary:The paper presents an information theoretic regularizer for deep learningalgorithms. The discussion on how this method differs from the information bottleneck is  odd, as the bottleneck is usually minimising the encoding mutual informationI(X;Y) minus the decoding mutual information I(Y;C). The presentation of the Zlatent variable used to simplify the calculation of the entropy H(Y|C) is confusing and needs revision, but otherwise the paper is interesting. The discussion in Appendix Cdoesn t mention how the Z values are generated.<BRK>Since the class conditional distributions are unknown, a surrogate model Q_{Y|C} is used. The authors present some experimental results as well. However, this approach has a number of serious flaws. Moreover, why is it a good idea to _minimize_ I(C;Y) in the first place? One could start with that instead without changing the essence of the approach, but then the magic words "Shannon decay" would have to disappear altogether, and the proposed method would lose all of its appeal.<BRK>the paper adapts the information bottleneck method where a problem has invariance in its structure. specifically, the constraint on the mutual information is changes to one on the conditional  entropy. this is a nice and intuitive idea.
Accept (Poster). rating score: 7. rating score: 7. rating score: 6. <BRK>The research is in principle very important and interesting. I think the paper (not only the Intro) could be a bit condensed to more concentrate on the actual contribution. Also, it is written in the Conclusions (and in other places): "[..] we propose a new intrinsically Motivated goal exploration strategy....". This is not really true. There is nothing new with the intrinsically motivated selection of goals here, just that they are in another space. The paper is in principle interesting. I understand this because in phase 1 the robot would not move, but this connects to the next point:  The representation learning is only a preprocessing step requiring a magic first phase.<BRK>This paper introduces a representation learning step in the Intrinsically Motivated Exploration Process (IMGEP)  framework. Why not using architecture adapted to images such as CNN ? 2) The representation stage R seems to be learned at the beginning of the algorithm and then fixed.<BRK>Summary:The authors aim to overcome one of the central limitations of intrinsically motivated goal exploration algorithms by learning a representation without relying on a "designer" to manually specify the space of possible goals. This work is significant as it would allow one to learn a policy in complex environments even in the absence of a such a designer or even a clear notion of what would constitute a "good" distribution of goal states. It would improve the clarity of the paper. There are too many metrics and too few conclusions for this paper.
Reject. rating score: 5. rating score: 6. rating score: 7. <BRK>This paper empirically investigates the differences realized by using compositional functions over word embeddings as compared to directly operating the word embeddings. That is, the authors seek to explore the advantages afforded by RNN/CNN based models that induce intermediate semantic representations of texts, as opposed to simpler (parameter free) approaches to composing these, like addition. In sum, I think this is exploration is interesting, and suggests that we should perhaps experiment more regularly with simple aggregation methods like SWEM. In my view, then, this work does constitute a contribution, albeit a modest one. I do think the general notion of attempting to simplify models until performance begins to degrade is a fruitful path to explore, as models continue to increase in complexity despite compelling evidence that this is always needed. However, it is not clear that we should expect there to be a consistent result to this question across all NLP tasks. + The results are marginally surprising, insofar as I would have expected the CNN/RNN (particularly the former) to dominate the simpler aggregation approaches, and this does not seem borne out by the data. Weaknesses   There are a number of important limitations here, many of which the authors themselves note, which mitigate the implications of the reported results. *** Update based on author response *** I have read the authors response and thank them for the additional details. I appreciated the additional details regarding FOFE, which as the authors themselves note in their response is essentially a generalization of SWEM. Overall, the response has not changed my opinion on this paper: I think this (exploring simple representations and baselines) is an important direction in NLP, but feel that the paper would greatly benefit from additional work.<BRK>This paper extensively compares simple word embedding based models (SWEMs) to RNN/CNN based models on a suite of NLP tasks. While there is not much contribution in terms of technical novelty, I think this is an interesting paper that sheds new lights on limitations of existing methods for learning sentence and document representations. Do the authors have any sense on whether this is because of the difficulty in training an RNN/CNN model for long documents or whether compositions are not necessary since there are multiple predictive independent cues in a long text? It would be useful to include a linear classification model that takes the word embeddings as an input in the comparison (SWEM learned).<BRK>This paper presents a very thorough empirical exploration of the qualities and limitations of very simple word embedding based models. In many of the 9 evaluation tasks, this approach is found to match or outperform single layer CNNs or RNNs. The varied findings are very clearly presented and helpfully summarized, and for each task setting the authors perform an insightful analysis. Besides concluding from the presented results that these SWEMs should be considered a strong baseline in future work, one might also conclude that we need more challenging datasets! Minor things:  It wasn t entirely clear how the text matching tasks are encoded. ** Update **Thanks for addressing my questions in the author response. After following the other discussion thread about the novelty claims, I believe I didn t weigh that aspect strongly enough in my original rating, so I m revising it. I remain of the opinion that this paper offers a useful systematic comparison that goes sufficiently beyond the focus of the two related papers mentioned in that thread (fasttext and Parikh s).
Invite to Workshop Track. rating score: 5. rating score: 5. rating score: 4. <BRK>* On optimisation paths, the choice of PCA directions is wise compared to random projections, and results are nice as plotted. There is however a phenomenon I would have liked to be discussed, the fact that the leading eigenvector captures so much variability, which perhaps signals that optimisation happens in a very low dimensional subspace for the experiments carried, and could be useful for optimisation algorithms (you trade dimension d for a much smaller "effective" d , you only have to figure out a generating system for this subspace and carry out optimisation inside). I would suppose that flatness tends to increase the variability captured by leading eigenvectors ?<BRK>This paper provides visualizations of different deep network loss surfaces using 2D contour plots, both at minima and along optimization trajectories. Once these details are done correctly, the experiments support the relatively well accepted hypothesis that flat minima generalize better. The visualizations are interesting and provide some general intuition, but they don t yield any clear novel insights that could be used in practice. Also, several parts of the paper spend too much time on describing other work or on implementation details which could be moved to the appendix.<BRK>One of the main distinctions is using filter wise normalization, but it is somehow trivial. In experiments, no comparisons against existing works is performed (at least on toy/controlled environments). Some findings in this submission indeed look interesting, but it is not clear if those results are something difficult to find with other existing standard ways, or even how reliable they are since the effectiveness has not been evaluated. In figure 2, why do we have solutions at 0 for small batch size and 1 for large batch size case? (why should they be different?)
Reject. rating score: 2. rating score: 2. rating score: 3. <BRK>The algorithm, iterative temporal differencing, is introduced in a figure   there is no formal description. The paper over uses acronyms; sentences like “In this figure, VBP, VBP with FBA, and ITD using FBA for VBP…” are painful to read.<BRK>The paper is incomplete and nowhere near finished, it should have been withdrawn.<BRK>  This paper is not well written and incomplete.
Reject. rating score: 3. rating score: 4. rating score: 4. <BRK>I think in general this paper lacks novelty and it shouldn t be surprising that activations from all layers should be more representative than one single layer representation.<BRK>Doing this, it shows improvements over using a single layer for 9 target image classification datasets including object, scene, texture, material, and animals.<BRK>Paper is badly written and the problem it tries to solve is not clearly stated.
Reject. rating score: 6. rating score: 6. rating score: 6. <BRK>This paper connects Entropy SGD with PAC Bayes learning. It shows that maximizing the local entropy during the execution of Entropy SGD essentially minimize a PAC Bayes bound on the risk of the Gibbs posterior. The paper then proposes to use a differentially private prior to get a valid PAC Bayes bound with SGLD. Experiments on MNIST shows such algorithm does generalize better. However, I m not sure if the ideas and techniques used to solve the problem are novel enough. It would be better if the presentation of the paper is improved. Section 5 about previous work on differentially private posterior sampling and stability could follow other preliminaries in Section 2.<BRK>1) I would like to ask for the clarification regarding the generalization guarantees. The original Entropy SGD paper shows improved generalization over SGD using uniform stability, however the analysis of the authors rely on an unrealistic assumption regarding the eigenvalues of the Hessian (they are assumed to be away from zero, which is not true at least at local minima of interest). Please, list them all in one place in the paper and discuss in details.<BRK>Brief summary:    Assume any neural net model with weights w. Assume a prior P on the weights. The authors leave this hole unsolved. The authors do some experiments with MNIST to demonstrate that their bounds are not trivial. Although technically the paper is not very deep, leveraging existing results (with strong assumptions) to show generalization properties of entropy SGD is good. e) The paper is unclear in many places. However I am willing to upgrade to 7 if the authors can provide sound arguments to my above concerns.
Reject. rating score: 4. rating score: 5. rating score: 6. <BRK>Overall, I think there are vast scopes for improvement in presentation and comparisons with other methods, and hence find the paper not yet ready for publication. I had to work this out from the number of parameters (HL+1)^P in section 2.2. Regarding the experiments, I’m sceptical as to whether a grid search over hyperparameters for TLSTM vs grid search over the same hyperparameters for (M)LSTM provides a fair comparison. and that tuning them is important to prevent overfitting. I’m also curious as to how TLSTM compares to hierarchical RNN approaches for modelling long term dependencies.<BRK>Overall the paper tries to tackle an important problem, which is good. I would say the difference is not significant. The technique used in this paper is tensor trains, which is also proposed previously. A more important issue is the time cost.<BRK>This work addresses an important and outstanding problem: accurate long term forecasting using deep recurrent networks. One weakness with the experiments is that it is not clear that they were fair to RNN or LSTM, for example, in terms of giving them the same computation as the TT RNNs.
Reject. rating score: 4. rating score: 5. rating score: 6. <BRK>This goal is to achieve a similar effect to that of natural gradient, but with lighter computation. However, the contribution of the paper itself needs to be strengthened in both the theory and empirical sides. However, in the latter, the optimization objective is the f itself (sup E[f_1] E[f_2]) but in your scheme you propose adding the regularization term (which can be added to any objective function, and then the whole form loses its connection to Wasserstrin metric).<BRK>I have read comments and rebuttal   i do not have the luxury of time to read in depth the revision. It seems that the authors have made an effort to accommodate reviewers  comments. KL divergence is used as a similarity measure between two distributions. Clarity:The paper needs major revision w.r.t.presenting and highlighting the new main points. regularization for training of simple models, such as neural networks.<BRK>It would be of particular interest to highlight connections to algorithm regularly applied to neural network training. I do believe that L_2 distance is more natural in function space than in parameter space, but I am missing a strict argument for this in the paper.
Reject. rating score: 4. rating score: 6. rating score: 6. <BRK>I have two major concerns on the paper. Lastly, I also found a significant effort is also desired to improve the writing. The following reference also needs to be discussed in the context of using SG MCMC in RNN. the justification on the retraining phase is weak.<BRK>Therefore, the reason why retraining improves the performance in all cases is not clear to me. The approach is based on the recent scalable MCMC methods, namely the stochastic gradient Langevin dynamics. The main contributions of the paper, namely using SG MCMC methods within deep learning, and then increasing the computational efficiency by group sparsity+pruning are valuable and can have a significant impact in the domain.<BRK>Empirical evidence is convincing on the utility of the approach. The model averaging seems to have a smaller contribution. Due to that, it seems that the nature of the contribution needs to be clarified compared to the large literature on sparsifying neural networks, and the introductory comments of the paper should be rewritten to reflect that reality.
Reject. rating score: 3. rating score: 3. rating score: 3. <BRK>  The paper is fairly written and it is clear what is being done  There is not much novelty in the paper; it combines known techniques and is a systems paper, so I   would judge the contributions mainly in terms of the empirical results and messsage conveyed (see  third point)  The paper builds on a  previous paper (ICCV Workshops, https://arxiv.org/pdf/1707.06923.pdf),  however, there is non trivial overlap between the two papers, e.g.Fig.1 seems to be almost the  same figure from that paper, Sec 2.1 from the previous paper is largely copied       The message from the empirical validation is also not novel, in the ICCVW paper it was shown that  the combination of different modalities etc.<BRK>However, at the current stage the submission is not ready for publication. At its current form this paper is unfit for submission.<BRK>It is, however, not clear from the paper how the original dataset was divided into subsets.
Reject. rating score: 3. rating score: 5. rating score: 8. <BRK>  Paper summaryThe paper proposes a GAN training method for improving the training stability. Visual generation results from the proposed method with comparison to those generated by the DCGAN were used as the main experimental validation for the merit of the proposed method. Without comparing to the GMAN work, we do not know whether the benefit is from using multiple discriminators proposed in the GMAN work or from using the random low dimensional projections proposed in this paper. It does not compare to other approaches for stabilizing GAN training such as WGAN or LSGAN. The main results shown in the paper are generating 64x64 human face images, which is not impressive.<BRK>The paper proposes to stabilize GAN training by using an ensemble of discriminators, each workin on a random projection of the input data, to provide the training signal for the generator model. In Theorem A.2 the authors upperbound this residual as a function of the smoothness and support of the distributions as well as the projections presented to the discriminators. Did the authors do or considered any frequency analysis of the ensemble of random projection? However i’m not completely convinced that the “marginal” convergence proof holds for the relative low number of discriminators possible to use in practice. Also several other methods have recently been proposed to improve stability of GANs, however no experimental comparisons is made with these methods (WGAN, EGAN, LSGAN etc.)<BRK>The paper proposes a new approach to GAN training whereby they train one generator against an ensemble of discriminative that each receive a randomly projected version of the data. The authors show that this approach provides stable gradients to train the generator. Although the idea to train an ensemble of learning machines is not new, see e,.g. The results are quite convincing that the proposed method is useful in practice,It would be interesting to know if weighting the discriminators, or discarding the unlucky random projections as it was done in [1] would have potential in this context? Random projection ensemble classification.
Accept (Poster). rating score: 9. rating score: 6. rating score: 3. <BRK>This paper proposes to apply the obverter technique of Batali (1998) to a multi agent communication game. The main novelty with respect to Batali s orginal work is that the agents in this paper have to communicate about images represented at the raw pixel level. The paper presents an extensive analysis of the patterns learnt by the agents, in particular in terms of how compositional they are. The writeup is somewhat confusing, and in particular the reader has to constantly refer to the supplementary materials to make sense of the models and experiments. At least the model architecture could be discussed in more detail in the main text. Some more detailed comments:It would be good if a native speaker could edit the paper for language and style (I only annotated English problems in the abstract, since there were just too many of them). Section 2.2 is very confusing, since the obverter technique has not been introduced, yet, so I kept wondering why you were only describing the listener architecture. The obverter technique should be described in more detail in the main text. Also, it looks like they are developing some notion of "inflectional class" (Aronoff 1994) (the color groups), which is also intriguing. However, I did not understand rules such as: "remove two as and add one a"... isn t that the same as: "add one a"?<BRK>Employ “obverter” technique, showing that it can be an alternative approach comparing to RL 3. The authors provided various experiments to showcase their approachCons:1. 3.This paper lack original technical contribution from themselves. Readers would be curious how this approach scales to a more complex problem. In conclusion, this paper does not have a major flaw. Generating language based only on raw image pixels is not difficult. Emergence of grounded compositional language in multi agent populations. Computational simulations of the emergence of grammar. Approaches to the evolution of language: Social and cognitive bases, 405:426, 1998.<BRK>This paper presents a technique for training a two agent system to play a simple reference game involving recognition of synthetic images of a single object. But seriously, please, talk to a linguist. In each round of training, one agent is selected to be the speaker and the other to be the listener. This fact doesn t seem to be addressed. While Table 3 is suggestive, this paper has many serious problems. If the emergence of compositionality is sensitive to vocab size & message length, experiments demonstrating this sensitivity belong front and center in the paper. But the paper is not ready to be published. However, the paper doesn t even attempt to define what is meant by compositionality until the penultimate page, where it asserts that the ability to "accurately describe an object [...] not seen before" is "one of the marks of compositional language". This is very bad. I would gently remind the authors that NLP research did not begin with deep learning, and that there might be slightly earlier evidence for their claim.
Reject. rating score: 5. rating score: 5. rating score: 6. <BRK>The model is then trained with CelebA under different parameters settings and results are analyzed. Quality and significance:This is quite a technical paper, written in a very compressed form and is a bit hard to follow. Clarity:I would say this is one of the weak points of the paper   the paper is not well motivated and the results are not clearly presented.<BRK>It appears to me that the novelty of the paper is limited, in that the main approach is built on the existing BEGAN framework with certain modifications.<BRK>Summary: The paper extends the the recently proposed Boundary Equilibrium Generative Adversarial Networks (BEGANs), with the hope of generating images which are more realistic. Based on its incremental nature and weak experiments, I m on the margin with regards to its acceptance. My suspicion is reinforced by the fact that the experimental section is extremely weak.
Accept (Poster). rating score: 8. rating score: 7. rating score: 4. <BRK>The paper tackles the problem of training predictive state recurrent neural networks (PSRNN), which uses large kernel ridge regression (KRR) problems as a subprimitive, and makes two main contributions:  the suggestion to use orthogonal random features (ORFs) in lieu of standard random fourier features (RFFs) to reduce the size of the KRR problems  a novel analysis of the risk of KRR using ORFs which shows that the risk of ORFs is no larger than that of using RFFsThe contribution to the practice of PSRNNs seems significant (to my non expert eyes): when back propagation through time is used, using ORFs to do the two stage KRR training needed visibly outperforms using standard RFMs to do the KRR. The proof should be broken up into several manageable lemmas instead of its current monolithic and taxing form.<BRK>In particular, authors provide theoretical guarantee and show that the model using orthogonal features has a smaller upper bound on the failure probability regarding the empirical risk than the model using unstructured randomness. Question: 	What is the cost of constructing orthogonal random features compared to RF?<BRK>» « a predictive state is defined as… , where…  is a vector of features of future observations and ...  is a vector offeatures of historical observations. (Note: not by the next k observationsthemselves.) The paper comes with some experiments in order to empirically demonstrate the superiority  orthogonal random features over RFF. I found it that the contribution of the paper is very limited. Some points have been clarified but other still raise issues.
Reject. rating score: 4. rating score: 4. rating score: 4. <BRK>The authors deal with the problem of implicit ordering in a dataset and the challenge of recovering it, i.e.when given a random dataset with no explicit ordering in the samples, the model is able to recover an ordering. Do the authors mean that the GMN model does not explicitly assume any ordering in the observed dataset? Is the model able of recovering the order?<BRK>Given an unordered dataset, the authors maximize its likelihood under the model by alternating gradient ascent steps on the parameters of the network and greedy reordering of the dataset. What does one do with this order? I am not convinced that the proposed method is well tuned for the task. If so.why?3) The experiments on dataset ordering are not convincing. There are no quantitative results, just a few examples (and more in the supplement). 4) The authors call their method distance metric free.<BRK>The paper is about learning the order of an unordered data sample via learning a Markov chain. Most of the ordering results are qualitative, it would be nice if a dataset with a ground truth ordering can be obtained and we have some quantitative measure. However, this paper is well written and interesting. The paper is well written, and experiments are carefully performed.
Accept (Poster). rating score: 8. rating score: 7. rating score: 5. <BRK>A better explanation of the intuition would help other readers. The experiments were extensive and show that this is a solid new method for trying out for any adaptation problem. This also shows how to better utilize task models associated with GANs and domain adversarial training, as used eg. 3 paragraph 2 should be much clearer, it was hard to understand. it s each node within a layer associated with dropout (unless you have dropout on every layer in the network). It also wasn t clear to me whether C_1 and C_2 are always different. Why is that?<BRK>For example, training a classifier using simulated rendered images with labels, to work on real images. Learning discriminative features for the target domain is a fundamental problem for unsupervised domain adaptation. ADR achieves this goal by encouraging the learned features to be robust to the dropout noise applied to the classifier. The loss function (Equations 2 5) proposed in the paper does not prevent the occurrence of this counter example. The experimental evaluation seems solid for domain adaptation. Overall the performance of the proposed method is quite well done and the results are encouraging, despite the lack of theoretical foundations for this method.<BRK>(Cons)1.By biggest concern is that the authors avoid comparing the method to the most recent state of the art approaches in unsupervised domain adaptation and yet claims "achieved state of the art results on three datasets." Well, which architecture was it? The text says "(ENT) obtained by modifying (Springenberg 2015)".
Reject. rating score: 4. rating score: 5. rating score: 7. <BRK>They show performance improve over existing studies on WikiSQL dataset. It is not sure if the approach is generally applicable to other sequence to sql workloads. Due to its simple structure, the problem of sequence to sql translation over WikiSQL is actually simplified as a parameter selection problem for a fixed template. 2.The "order matters" motivation is not very convincing. That could ensure the orders in the SQL results are always consistent. 4.They do not compare against state of the art solution on column and expression selection. In (Yin, et al., IJCAI 2016), for example, representations over the columns are learned to generate better column selection. The quality of the paper could be much enhanced, if the authors deepen their studies on this direction.<BRK>Pros:  good problem, NL2SQL is an important task given how dominant SQL is  incorporating a grammar ("sketch") is a sensible improvement. DOI: https://doi.org/10.3115/1075812.1075823  In particular, the assumption that every token in the SQL statement is either an SQL keyword or appears in the natural language statement is rather atypical and unrealistic. The use of a grammar in the context of semantic parsing is not novel; see this tutorial for many pointers:http://yoavartzi.com/tutorial/  As far as I can tell, the set prediction is essentially predicted each element independently, without taking into account any dependencies. Cons:  The dataset used makes very strong simplification assumptions.<BRK>The authors present a neural architecture for the WikiSQL task. The approach can be largely seen as graphical model tailored towards the constrained definition of SQL queries in WikiSQL. The model makes strong independence assumptions, and only includes interactions between structures where necessary, which reduces the model complexity while alleviating the "order matters" problem. An attention mechanism over the columns is used to model the interaction between columns and the op or value in a soft differentiable manner.
Accept (Poster). rating score: 8. rating score: 8. rating score: 5. <BRK>The first personality learns to generate goals for other personality for which the second agent is just barely capable much in the same way a teacher always pushes just past the frontier of a student’s ability. The second personality attempts to achieve the objectives set by the first as well as achieve the original RL task. However, the paper only briefly mentions this corpus of work. The authors should consider additional experiments on the same domains of this prior work to contrast performance. Questions:Do the plots track the combined iterations that both Alice and Bob are in control of the environment or just for Bob?<BRK>In this paper, the authors describe a new formulation for exploring the environment in an unsupervised way to aid a specific task later. Overall, I think the paper presents a novel and unique idea that would be interesting to the wider research community. However, the paper presents a novel way to frame the problem, and shows promising results on several tasks. Furthermore, the crux of the proposal and simple and elegant yet leading to some very interesting results.<BRK>This paper proposes an interesting model of self play where one agent learns to propose tasks that are easy for her but difficult for an opponent. The idea is certainly elegant and clearly described. Given the existence of similar forms of self play the key issue with paper I see is that there is no strong self play baseline in the experimental evaluation. This suggests that the adversarial setting is quite brittle. I also find that the paper is a little light on the technical side.
Accept (Poster). rating score: 7. rating score: 6. rating score: 6. <BRK>As claimed by the authors, MemoryGAN is aimed at addressing two problems of GAN training: 1) difficult to model the structural discontinuity between disparate classes in the latent space; 2) catastrophic forgetting problem during the training of discriminator about the past synthesized samples by the generator. 4.The paper should be compared with InfoGAN (Chen et al.2016), and the authors should explain the differences between two models in the related work. [Summary]This paper proposed a new model called MemoryGAN for image generation. The arguments that MemoryGAN could solve the two infamous problem make sense. This paper is well written. 2.The paper presents a novel method called MemoryGAN for GAN training. Through memory network, MemoryGAN can explicitly learn the data distribution of real images and fake images. On CIFAR 10, it is ~5.35. I am wondering what makes such a big difference between the reported numbers in this paper and other papers? I did not understand why the memory size is such large. Take CIFAR 10 as the example, its training set contains 50k images.<BRK>MemoryGAN is proposed to handle structural discontinuity (avoid unrealistic samples) for the generator, and the forgetting behavior of the discriminator. Take MNIST for example, It can be seen that the DCGAN has to (1) transit among digits in different classes, while MemoryGAN only (2) transit among digits in the same class. A better experiment is to fix four digits from different class at first, find their corresponding latent codes, do interpolation, and propagate back to sample space to visualize results. Also, the current illustration also indicates that the generated samples by MemoryGAN is not diverse. It seems the memory mechanism can bring major computational overhead, is it possible to provide the comparison on running time? If the MemoryGAN can truly deal with structural discontinuity, the results on generating a wide range of different images for ImageNet may endow the paper with higher impact. The authors should consider to make their code reproducible and public.<BRK>The memory provides extra information for both the generation and the discrimination, compared with vanilla GANs. Also note that the high Inception Score cannot show the generalization ability as well because memorizing the training data will obtain the highest score. I know it s hard to evaluate a GAN model but I think the authors can at least show the nearest neighbors in the training dataset and the training data that maximizes the activation of the corresponding memory slot together with the generated samples to see the difference. Besides, personally speaking, Figure 1 is not so fair because a MemoryGAN only shows a very small local subspace near by a training data while the vanilla GAN shows a large subspace, making the quality of the generation different. The MemoryGAN also has failure samples in the whole latent space as shown in Figure 4. Overall, I think this paper is interesting but currently it does not reach the acceptance threshold.
Reject. rating score: 4. rating score: 5. rating score: 5. <BRK>The authors propose a method for graph classification by combining graph kernels and CNNs. Therefore, I cannot recommend the paper for acceptance. * The weights refer to the RKHS and filters are not easily interpretable. * The approach is similar in spirit to Niepert, Ahmed, Kutzkov, ICML 2016 and thus incremental.<BRK>This paper proposes a graph classification method by integrating three techniques, community detection, graph kernels, and CNNs. First, the authors say that the graph kernel + SVM approach has a drawback due to two independent processes of graph representation and learning. However, it is known that the performance of the WL kernel depends on the parameter and it should be tuned by cross validation. Thus the current comparison is not fair. In addition to the above point, how are parameters for GR and RW?<BRK>The paper presents a method of using convolution neural networks for classifying arbitrary graphs. Although the proposed algorithm seems to outperform on 7 out of 10 datasets, the performances are really close to the best SoA algorithm. Is there any statistical significance over the gain in the performances? Moreover, the method makes an strong assumption that the graph is strongly characterized by one of its patches, ie its subgraph communities, which might not be the case in arbitrary graph structures, thus limiting their method. Finally, it is not also clear to me the what are the communities reported in Table 2 for the  bioinformatics datasets.
Accept (Poster). rating score: 6. rating score: 5. rating score: 5. <BRK>Competitive with state of the art external implementations  Significant empirical advantage over TRPO. Clarity The paper is well written and clear. The paper leverages a novel method in determining the coefficient of relative entropy. Some ablation studies (e.g., on entropy regularization and relative entropy) and sensitivity analysis on parameters (e.g.\alpha and update frequency on \phi) would be helpful.<BRK>I am not an expert in this area. The paper largely follows the work of Nachum et al 2017. It would be much clearer if a detailed description of the algorithmic procedure is given.<BRK>This paper presents a policy gradient method that employs entropy regularization and entropy constraint at the same time. The entropy regularization on action probability is to encourage the exploration of the policy, while the entropy constraint is to stabilize the gradient. In experiments, why the off policy version of TRPO is not compared. What is the exploration strategy in the experiments? Thus it would be more interesting to see experiments on more complex benchmark problems like humanoids.
Reject. rating score: 4. rating score: 4. rating score: 5. <BRK>I have to say that I do not have all the background of this paper, and the paper is not written very clearly. I think the major contribution of the paper is represented in a very vague way.<BRK>I really enjoyed reading this paper and stopped a few time to write down new ideas it brought up. Well written and very clear, but somewhat lacking in the experimental or theoretical results. It is really hard to judge from these results. We can say that more GVFs are better, and that the compositional GVFs add to the ability to lower RMSE. This was not obvious to me. But, I think there is not quite enough here.<BRK>But it would be interesting to know *what* it is learning. While I am deeply sympathetic to the utility and difficulty of the discovery problem in this sort of state space modeling, this paper ultimately felt a bit weak. On the positive side, I felt that it was well written. The work is well localized in the literature, and answers most questions one would naturally have.
Accept (Poster). rating score: 7. rating score: 7. rating score: 4. <BRK>It further explains why binarization is able to preserve the model performance by analyzing the weight activation dot product with "Dot Product Proportionality Property." It also proposes "Generalized Binarization Transformation" for the first layer of a neural network. Many empirical and theoretical proofs are given, as well as some practical tricks that could be useful for diagnosing binary nets in the future. Cons:* it seems that there are quite some typos in the paper, for example:    1. I think the length of the proof won t matter a lot since it is already in the appendix, but it makes the reader a lot easier to understand it.<BRK>This paper investigates numerically and theoretically the reasons behind the empirical success of binarized neural networks. (2) Except the first layer, the dot product of weights*activations in each layer is highly correlated with the dot product of (binarized weights)*activations in each layer. This is claimed to entail that the continuous weights of the binarized neural net approximate the continuous weights of a non binarized neural net trained in the same manner. The second observation is much less clear to me. However, I’m not sure if I see why this is true: in a binarized neural net, u also changes, since the previous layers are also binarized. The third observation seems less useful to me. %%% After Author s response %%%a.<BRK>This paper presents three observations to understand binary network in Courbariaux, Hubara et al.(2016).My main concerns are on the usage of the given observations. Indeed, Courbariaux, Hubara et al.(2016) is a good and pioneered work on the binary network. However, as the authors mentioned, there are more recent works which give better performance than this one. For example, we can use +1, 0,  1 to approximate the weights. I wish to see how they can be used to improve binary networks.
Accept (Poster). rating score: 6. rating score: 6. rating score: 6. <BRK>The problem and the proposed solution is well motivated. However, there are some elements of the manuscript that are hard to follow and need further clarification/information. These need to definitely be addressed before this paper can be accepted. Page 1: It is a little hard to follow the motivation against existing methods.<BRK>How would this scale to larger datasets? Additionally the paper implies that this style of quantization has benefits for compute in addition to memory savings. I have a number of questions and concerns about the proposed approach. First, at a high level, there are many details that aren t clear from the text.<BRK>I have read the responses to the concerns raised by all reviewers. Also, it is difficult to see the benefits in terms of memory/accuracy compromise since not all competing quantization techniques are compared for all the datasets. This paper addresses a very relevant topic, because in limited resources there is a constrain in memory and computational power, which can be tackled by quantizing the weights of the network.
Reject. rating score: 5. rating score: 6. rating score: 6. <BRK>By doing this, it can take advantage of common information between related tasks and improve the generalization of target tasks. 2.The target task utilized in this paper is too simple, which only detects 5 facial landmarks. It is hard to say this proposed work can still work when facing more challenging tasks, for example, 60+ facial landmarks prediction.<BRK>This paper proposes a multi pathway neural network for facial landmark detection with multitask learning. The fused features are added to the task specific pathway using a residual connection (the input of the residual connection are the concatenation of the task specific features and the fuse features). Providing results in the average error can make the experiments more comprehensive.<BRK>In it s current form I would say the experiment section and large scale experiments are two places where the paper falls short. The few places where the paper needs improvement are:1. It would be great if Authors provide more experiments beyond Faces to test the universality of the proposed approach.
Reject. rating score: 2. rating score: 3. rating score: 3. <BRK>The idea as stated in the abstract and introduction may well be worth pursuing, but not on the evidence provided by the rest of the manuscript.<BRK>The authors claimed that they used techniques in [6] in which I am not an expert for this. That said, I would welcome for the authors feedback and see if I have misunderstood something. Although the author claim the novelty as adding noise to the discriminator, it seems to me that at least for the RBF case it just does the following:1. write down MMD as an integral probability metric (IPM)2. say the test function, which originally should be in an RKHS, will be approximated using random feature approximations.<BRK>The most severe issue is lacking novelty. It is a straightforward combination of existing work, therefore, the contribution of this work is rare. For example, using random features to approximate the kernel function does not bring extra stochasticity. Finally, the experiments are promising. However, to be more convincing, more benchmarks, e.g., cifar10/100 and CelebA, are needed.
Reject. rating score: 3. rating score: 3. rating score: 4. <BRK>The results in this paper do not provide strong support for using the proposed method.<BRK>However, the paper has two main problems:  The results are not convincing. The idea of the paper is good but the novelty is limited.<BRK>How would this scale to more? The paper is generally well written and easy to read. On the negative side: ultimately, the algorithm doesn t seem to work all that well.
Accept (Poster). rating score: 9. rating score: 7. rating score: 6. <BRK>This paper studies the problem of learning one hidden layer neural networks and is a theory paper. This paper establishes an interesting connection between least squares population loss and Hermite polynomials. Following from this connection authors propose a new loss function. For instance, is designing alternative loss function useful in practice? In summary, I recommend acceptance. The paper seems rushed to me so authors should polish up the paper and fix typos.<BRK>This paper proposes a tensor factorization type method for learning one hidden layer neural network. The most interesting part is the Hermite polynomial expansion of the activation function. They further redesign a new formulation for the tensor decomposition problem, and show that the new formulation enjoys the nice strict saddle properties as shown in Ge et al.2015.At last, they also establish the sample complexity for recovery. To make the paper accessible to the readers, they could provide more intuitions in the first 9 pages. There are also some typos: For example, the dimension of a is inconsistent. On Page 8, P(B) should be a degree 4 polynomial of B.<BRK>The paper still needs some work on clarity, and authors defer the changes to the next version (but as I understood, they did no changes for this paper as of now), which is a bit frustrating. [   END OF REVISION  ]This paper concerns with addressing the issue of SGD not converging to the optimal parameters on one hidden layer network for a particular type of data and label (gaussian features, label generated using a particular function that should be learnable with neural net). Authors demonstrate empirically that this particular learning problem is hard for SGD with l2 loss (due to apparently bad local optima) and suggest two ways of addressing it, on top of the known way of dealing with this problem (which is overparameterization). First is to use a new activation function, the second is by designing a new objective function that has only global optima and which can be efficiently learnt with SGDOverall the paper is well written. I do find interesting the formulation of population risk in terms of tensor decomposition, this is insightfulMy issues with the paper are as follows:  The loss function designed seems overly complicated.
Reject. rating score: 3. rating score: 4. rating score: 5. <BRK>The key contribution of the paper is a new method for nonlinear dimensionality reduction. Finally, the experimental results are somewhat uninspiring.<BRK>Figure 3, contrary to text, does not provide a visualisation to the sampling mechanism. Can you provide a citation for this? In figure 4a, x axis should be "number of landmarks".<BRK>The running times are not in favor of the proposed method. What is the take home message of the paper? * As a tool for unsupervised learning or exploratory data visualization, DR can hardly benefit from a parametric approach. The motivation in the end of page 3 seems to be computational only.
Reject. rating score: 5. rating score: 6. rating score: 6. <BRK>This paper focuses on using RNNs to generate straightline computer programs (ie.code strings) using reinforcement learning. 2.They argue that they don t need to separate train and test, but I think it is important to be sure that the generated programs work on test cases that are not a part of the reward function. al.2017.In Liang et.<BRK>This paper presents an algorithm called Priority Queue Training (PQT) forprogram synthesis using an RNN where the RNN is trained in presence of a reward signal over the desired program outputs. How many iterations are needed for PQT to come up with the target programs? How do the numbers in Table 3 look like when the NPE is 5M and when NPE is larger say 100M? There are many learnt programs in Table 4 that do not seem to generalize to new inputs. For every benchmark, it would be good to separately construct a set of held out test cases (possibly of larger lengths) to evaluate the generalization correctness of the learnt programs. How would the numbers in Table 3 look with such a correctness criterion of evaluating on a held out set?<BRK>This paper introduces a method for regularizing the REINFORCE algorithm by keeping around a small set of known high quality samples as part of the sample set when performing stochastic gradient estimation. 500 million evaluations is a lot. While the paper does demonstrate that PQT is helpful on this very particular task, it makes very little effort to investigate *why* it is helpful, or whether it will usefully generalize to other domains. In the related work, a paper by Nachum et al is criticized for providing a sequence of machine instructions, rather than code in a language.
Reject. rating score: 4. rating score: 6. rating score: 9. <BRK>The experiment show that the proposed change speeds convergence and improves the results by about 1 BLEU point. Clarity:The language is clear, but the main contribution could be better explained. Originality:The proposed change is a small extension to the Neural Transformer model. Significance:Rather small, the proposed addition adds little modeling power to the network and its advantage may vanish with more data/different learning rate schedule. Pros and cons:+ the proposed approach is a simple way to improve the performance of multihead attentional models. it is not clear from the paper how the proposed extension works: does it regularize the model or dies it increase its capacity?<BRK>While the results are an improvement over the baseline Transformer, my main concern with this paper is that the improved results are because of extensive hyperparameter tuning. Design choices like having a separate learning rate schedule for the alpha and kappa parameters, and needing to freeze them at the end of training stoke this concern.<BRK>This paper describes an extension to the recently introduced Transformer networks which shows better convergence properties and also improves results on standard machine translation benchmarks. It would have been good to also add a motivation for doing this (for example, this idea can be interpreted as having a variable number of attention heads which can be blended in and out with a single learned parameter, hence making it easier to use the parameters where they are needed).
Invite to Workshop Track. rating score: 7. rating score: 6. rating score: 5. <BRK>I did not verify the proofs in the appendix. The paper is well written and well presented, and the limitations of the approach, as well as its advantages over previous work, are clearly explained. They show this for a piecewise linear activation function, and input drawn from a standard Normal distribution. The approach in this paper is to study a standard MNN with one hidden layer.<BRK>Previous works needed a wide hidden layer (d1 > N). However, it comes at a cost of losing sharpness in the theoretical results. What about deep neural networks? 3.Can you relate the results to other more recent works like: https://arxiv.org/pdf/1707.04926.pdf. 4.Piecewise linear and positively homogeneous (https://arxiv.org/pdf/1506.07540.pdf) activation seem to be important assumption of the paper. How can you guarantee that it is a local minimum and not a saddle point?<BRK>The authors consider networks with single hidden layer. The main result is that volume of suboptimal local minima exponentially decreases in comparison to global minima. Logistic loss would have made a more compelling story. In this sense, this paper has novel technical contribution compared to prior literature. Finally, while introduction discusses the "last two layers", I don t see a technical result proving that the results extends to the last two layers of a deeper network. At least one of the assumptions require Gaussian data and the input to the last two layers will not be Gaussian even if all previous layers are fixed.
Accept (Poster). rating score: 7. rating score: 6. rating score: 6. <BRK>This makes this paper non clear as a stand alone document, and is a problem for a paper which is mostly based on experiments and their results: the main networks used are not described. The method is tested for generation of images on MNist and part of LSUM data and for creation of text examples which are adversarial in some sense to inference and translation sentences. It is shown that the distance between adversarial example and the original example in the latent space is proportional to the accuracy of the classifier inspected.<BRK>The authors of the paper propose a framework to generate natural adversarial examples by searching adversaries in a latent space of dense and continuous data representation (instead of in the original input data space). This makes me feel that there is large room to further advance the paper. The authors empirically evaluate their method in both image and text domains and claim that the corresponding generated adversaries are natural (legible, grammatical, and semantically similar to the input). Generally, I think that the paper is written well (except some issues listed at the end).<BRK>any remedy for this problem? What is the main benefit of the proposed mechanism compared to the existing ones? (c) The application of the search algorithm in case of imbalanced classes could be something that require further investigation.
Accept (Poster). rating score: 7. rating score: 7. rating score: 6. <BRK>########## UPDATED AFTER AUTHOR RESPONSE ##########Thanks for the good revision and response that addressed most of my concerns. I am bumping up my score. The basic idea of DIP VAE is to enforce the aggregated posterior q(z)   E_x [q(z | x)] to be close to an identity matrix as implied by the commonly chosen standard normal prior p(z). Following the derivation in Hoffman & Johnson, DIP VAE is basically adding a regularization parameter to the KL(q(z) | p(z)) term in standard ELBO. I think this interpretation is complementary to (and in my opinion, more clear than) the one that’s described in the paper. If so, what exactly are you trying to convey in Figure 2?<BRK>The authors propose to augment the standard VAE ELBO objective with an extra term that minimises the covariance between the latents. Unlike the original beta VAE objective which implicitly minimises such covariance individually for each observation x, the DIP VAE objective does so while marginalising over x. I am willing to increase my score for the paper if the authors can address my points. For example, the qualitative evaluation of the latent traversals is almost impossible due to the tiny scale of Table 5 (shouldn t this be a Figure rather than a Table?)<BRK>The reason for switching to the moment matching scheme seems not well motivated here without showing explicitly that Eq (4) has problems. In practice, the authors decide that the objective they want to optimize is unwieldy and resort to moment matching of covariances of q(z) and p(z) via gradient descent. In the probabilistic sense, regularizers can be seen as structural and prior assumptions on variables. All in all I find this paper interesting but would hope that a more careful technical justification and derivation of the model would be presented given that it seems to not be an empirically overwhelming change.
Reject. rating score: 3. rating score: 4. rating score: 7. <BRK>The algorithm here can be seen as SVG(0) with a particular parametrization of the policy. The empirical comparison is also hampered by only comparing with DDPG, there are numerous stochastic policy algorithms that have been compared on these environments. Additionally, the DDPG performance here is lower for several environments than the results reported in Henderson et al.2017 (cited in the paper, table 2 here, table 3 Henderson) which should be explained. While this particular parametrization may provide some benefits, the lack of engagement with relevant prior work and other stochastic baselines significant limits the impact of this work and makes assessing its significance difficult. (2015).Learning continuous control policies by stochastic value gradients.<BRK>The main claim being argued in the paper is that the proposed stochastic policy has better final performance on average than a deterministic policy, but the only practical difference seems to be a slightly more structured approach to exploration. This paper describes an approach to stochastic control using RL that extends DDPG with a stochastic policy. I find the general idea of the work compelling, but the particular approach is rather poor. I also think that better learning is not the only redeeming aspect of a stochastic policy.<BRK>Authors show improved performance on a large number of standard continuous control environment (openAI gym and TORCS). The paper is well written, and the idea seems to work perhaps surprisingly well. It would be interesting to investigate the behavior of the algorithm in a toy environment (perhaps a simple 2d navigation  task with distinct  paths  with same cost) where the number of distinct basins of optimality is know for various states, and investigate in more details how diversity is maintained (perhaps as a function of M).
Reject. rating score: 4. rating score: 6. rating score: 7. <BRK>3.Experimental results for classification are not convincing enough. This is an interesting result, and useful in its own right. I would recommend the authors to rerun these experiments but truncate the iterations early enough. If DANTE has better reconstruction performance than SGD with fewer iterations then that would be a positive result.<BRK>I think the paper is borderline, leaning towards accept. As a result, the optimization problem over the single hidden layer can be optimized efficiently using the algorithm of Hazan et al 2015. And note that the authors show that the proposed algorithm performs comparably to SGD, but not significantly better. This paper presents an algorithm for training deep neural networks.<BRK>In this paper an alternating optimization approach is explored for training Auto Encoders (AEs). It will be interesting if the authors can provide an analysis/relate to some theory related to alternating minimization of bi quasi convex objectives. The extension to muti layer AEs makes sense and seems to works quite well in practice. Showing that generalized ReLU is SLQC with a polynomial dependence on the domain is interesting.
Accept (Poster). rating score: 6. rating score: 6. rating score: 6. rating score: 5. <BRK>The paper proposed a copula based modification to an existing deep variational information bottleneck model, such that the marginals of the variables of interest (x, y) are decoupled from the DVIB latent variable model, allowing the latent space to be more compact when compared to the non modified version. How about t_j in equation (5)? al.2014 and applying it to the existing DVIB model.<BRK>Namely, the authors adjust deep variational bottle neck to conform to invariance properties (by making latent variable space to depend on copula only)   they name this model a  copula extension to dvib. They then go on to explore the sparsity of the latent spaceMy main issues with this paper are experiments: The proposed approach is tested only on 2 datasets (one synthetic, one real but tiny   2K instances) and some of the plots (like Figure 5) are not convincing to me.<BRK>If MI is invariant to monotone transformations and information curves are determined by MIs, why “transformations basically makes information curve arbitrary”? ***************Updates: ***************The authors have clarified some questions that I had and further demonstrated the benefits of copula transform with new experiments in the revised paper. The new results are quite informative and addressed some of the concerns raised by me and other reviewers. The paper suffers from lack of clarity at several places and the experimental results are convincing but not strong enough.<BRK>This paper identifies and proposes a fix for a shortcoming of the Deep Information Bottleneck approach, namely that the induced representation is not invariant to monotonic transform of the marginal distributions (as opposed to the mutual information on which it is based). The authors address this shortcoming by applying the DIB to a transformation of the data, obtained by a copula transform. There are also many missing details in the experimental section: how were the number of “active” components selected ?
Reject. rating score: 3. rating score: 4. rating score: 6. <BRK>Authors propose a greedy scheme to select a subset of (highly correlated) spectral features in a classification task. The selection criterion used is the average magnitude with which this feature contributes to the activation of a next layer perceptron. Pro:   Method works well on a single data set and solves the problem  Paper is clearly written   Good use of standard tricks Con:   Little noveltyThis paper could be a good fit for an applied conference such as the International Symposium on Biomedical Imaging.<BRK>In this paper, the authors proposed a framework to classify cells and implement cell segmentation based on the deep learning techniques. The authors seem to apply some well define methods to realize a new task. The authors are expected to demonstrate the results by improving these advanced models. In general, this is an interesting paper, but would be more fit to MICCAI or ISBI.<BRK>I think that this work can (1) be a lot clearer as to the novelty and (2) have a much bigger impact if this literature is addressed. In particular, there are a number of methods of supervised and unsupervised feature extraction used for classification purposes (e.g.endmember extraction or dictionary learning). The comparison with the random forests is nice, but that is not a standard method. Putting the presented work in context for these other methods would help make there results more general, and hopefully increase the applicability to more general HSI data (thus increasing significance).
Accept (Poster). rating score: 7. rating score: 7. rating score: 7. <BRK>The authors relate this idea to a recent dynamical systems view of ResNets in which residual blocks are viewed as taking steps in an Euler discretization of a certain differential equation. Moreover the paper is clearly written. Experimental results are decent — there are clear speedups to be had based on the authors  experiments.<BRK>This is a very well written paper, the authors propose a method for speeding up the training time of Residual Networks based on the dynamical system view interpretation of ResNets. The citation (E, 2017) seems to be wrong, could the authors check it? Table 3 and 4.<BRK>This paper interprets deep residual network as a dynamic system, and proposes a novel training algorithm to train it in a constructive way. The paper is interesting and easy to follow. I have several comments:1.
Reject. rating score: 4. rating score: 5. rating score: 5. <BRK>Especially with natural images,  the spacial location and the scale should be critical. However, the presentation of the method is not clear, the experiment is not sufficient, and the paper is not polished. Learning a meaningful representation is needed in general. The presentation of the model is not clear. This also leads to unclarity of the text presentation of the model, for example, section 3.2. Firstly, only one toy dataset is used for experimental evaluations.<BRK>similar?Did the model used for drawing Fig.7 converge? The paper seems to have weaknesses pertaining to the approach taken, clarity of presentation and comparison to baselines which mean that the paper does not seem to meet the acceptance threshold for ICLR. The paper does propose to use an image only encoder but that is intended in general as a modeling choice to explain statistics which are not captured by the attributes (in this case location and orientation as explained in the Introduction of the paper). * The claim that the paper works with natural language should be toned down and clarified. 3.What does “partial descriptions” mean? This seems like an important baseline to report for the image caption ranking task.<BRK>The proposed Generative Entity Networks jointly generates the natural language descriptions and images from scratch. Strengths:Simultaneous text and image generation is an interesting research topic that is relevant for the community. The paper is well written, the model is formulated with no errors (although it could use some more detail) and supported by illustrations (although there are some issues with the illustrations detailed below). Figure 2 is complex and confusing due to the lack of proper explanation in the text. However, confirmation of this intuition is needed since this is a central claim of the paper.
Reject. rating score: 4. rating score: 4. rating score: 4. <BRK>Without at least a working should be easy example like this, and with the rest of the paper s technical contribution so small, I just don t think this paper is ready for ICLR. The first, univariate, experiment shows that the scheme is at least plausible.<BRK>This paper addresses the problem of sample selection bias in MMD GANs. Are there any theoretical guarantees that this estimate will convergence to the true MMD? Also, the experiment results are too weak to make any justified conclusion. In brief, sample selection bias is generally a challenging problem in science, statistics, and machine learning, so the topic of this paper is interesting.<BRK>But, there are still more zeros by a factor of around 2. It would be good to see a comparison with other approaches for handling class imbalance. I would have thought that the right column, where the thinning function is used to correct for the class imbalance, would then have approximately equal numbers of zeros and ones in the generative samples.
Reject. rating score: 4. rating score: 5. rating score: 6. <BRK>The paper presents a method for iteratively pruning redundant weights in deep networks. The results shown in Table 2 do not indicate much difference in terms of number of parameters between the proposed method and that of Han etal. As a result, the impact of the proposed method seems quite limited. The paper in the title and abstract refers to segmentation as the main area of focus. However, there does not seem to be much related to it except an experiment on the CityScapes dataset.<BRK>This paper inherits the framework proposed by Han[1]. Here are some issues to be paid attention to:1. The overall pipeline including the last two stage looks quite similar to Han[1]. Though different initialization methods are tested in this paper, final conclusion does not change. Whether these number works for each layer or total network should be clarified. Deep compression: Compressing deep neural networks with pruning, trained quantization and huffman coding.<BRK>significance: this paper seems significant. PROS  a new approach to sparsifying that considers different thresholds for each layer  a systematic, empirical method to obtain optimal sparsity levels for a given neural network on a task. It would benefit the experimental section to use another dataset than MNIST (e.g.CIFAR 10) for the image recognition experiment. Although possibly inevitable, it would be valuable to discuss whether or not this approach can be refined. If there is a simple correlation between sparsity and accuracy, that might be faster; if there isn t (which would be believable given the complexity of neural nets), it would be valuable to confirm this with an experiment. Have you tried other pruning methods than thresholding to decide on the optimal sparsity in each layer?
Reject. rating score: 2. rating score: 5. rating score: 5. <BRK>But all the representations considered in the article seem to be applicable to functions in L^2(\R) only (like in Theorem 1.4 and Theorem 2.2), and not to sequences (x_n)_{n\in\N}. (4) I had understood in the introduction that the authors would explain how to define a (good) deep representation for data of the form (x_n)_{n\in\N}, where each x_n would be the value of a time series at instant t_n, with the t_n non uniformly spaced.<BRK>However, I am not familiar with the state of the art models on the data sets used in the paper. It seems to me that the representation gets more unstable as alpha decreases, and the lower bound will be zero when ReLU is used.<BRK>Pros:  combination of wavelets & CNNCons:  lack of motivationI am not sure to understand the motivation of good reconstruction/homeomorphism w.r.t.the numerical setting or combination with a CNN. Definition 1.3: with this definition, "the framing constants" are not unique, so it should be "some framing constants"There is a critical assumption to have an inverse, which is its stability. The conditioning is the good quantity to consider. Section 2:The figure of the Theorem 2 is not really clear and could be improved. My guess is that it is because this basis sparsify the input signal, but it would require some additional experiments, in particular to understand how the NN uses it.
Reject. rating score: 5. rating score: 5. rating score: 5. <BRK>This work proposes an approach for transcription factor binding site prediction using a multi label classification formulation. It is a very interesting problem and application and the approach is interesting. Novelty:The method is quite similar to matching networks (Vinyals, 2016) with a few changes in the matching approach. Impact:In its current form the paper seems to be most relevant to the computational biology / TFBS community. This greatly limits the impact of the work. Clarity:The paper can benefit from more clarity in the technical aspects.<BRK>The authors of this manuscript proposed a model called PMN based on previous works for the classification of transcription factor binding. The model itself is an incremental work, but the application is novel. 1.It is unclear how the prototype of a TF is learned. ), some items are duplicated, ... My major and minor concerns are not fully well addressed in the revised paper.<BRK>SummaryThis paper proposes a prototype matching network (PMN) to model transcription factor (TF) binding motifs and TF TF interactions for large scale transcription factor binding site prediction task. The final output is a sigmoid of the final hidden state concatenated with the read vector.
Accept (Poster). rating score: 7. rating score: 7. rating score: 7. <BRK>Later, the paper describes their idea and extension in details and reports comprehensive experiment results of a number of hypotheses. The research questions seems straightforward, but it is good to see those experiments review some interesting points. Do we have other datasets that can be used?<BRK>I would have liked to see analysis on the training process such as a plot of reward (or baseline adjusted reward) over training iterations. Overall I think this is an interesting and well designed work; however, some details are missing that I think would make for a stronger submission (see weaknesses). Strengths:   Generally well written with the Results and Analysis section appearing especially thought out and nicely presented.<BRK>The setup in the paper for learning representations is different to many other approaches in the area, using to agents that communicate over descriptions of objects using different modalities. The experimental setup is interesting in that it allows comparing approaches in learning an effective representation. For reproducibility and comparisons, this availability would be essential. The paper says is the training procedure is described in Appendix A, but as far as I see that contains the table of notations.
Reject. rating score: 5. rating score: 5. rating score: 6. <BRK>This paper proposes a model for adding background knowledge to natural language understanding tasks. The authors show this leads to some improvement on multiple tasks like question answering and natural language inference (they do not obtain state of the art but improve over a base model, which is fine in my opinion). Seems a bit marginal to me. So to conclude, the paper is well written, clear, and has nice results and analysis.<BRK>p.7 In the additional NLI results, it is interesting and valuable to note that the lemmatization and knowledge help much more when amounts of data (and the covarying dimensionality of the word vectors) is much smaller, but the fact that the ideas of this paper have quite little (or even negative) effects when run on the full data with full word vectors on top of the ESIM model again draws into question whether enough value is being achieved from the world knowledge. Minor notes:   The paper was very well written/edited. The only real copyediting I noticed was in the conclusion: and be used ➔ and can be used; that rely on ➔ that relies on.<BRK>The quality of this paper is good. Arrows seem to have some different meanings. The proposed model is not very innovative but works fine for the DQA task. As ESIM is one of the key components in the experiments, you should briefly introduce ESIM and explain how you incorporated with your vector representations into ESIM.
Accept (Poster). rating score: 8. rating score: 5. rating score: 4. <BRK>This fits into the recent line of work on implicit planning where forms of models are learned to be useful for a prediction/planning task. Same comment in Atari, but there it’s not really obvious that the proposed architecture is helping. I thought the paper was clear and well motivated.<BRK> This was an interesting read. I feel that there is a mismatch between intuition of what a model could do (based on the structure of the architecture) versus what a model does. When the system is learned end to end, the structure does not impose the behaviour of the model, and is up to the authors of the paper to prove that the trained model does anything similar to expanding a tree.<BRK>Comparison to VPN on Atari is not much convincing. Since the authors took the numbers from [Oh et al.] However, the particular method (TreeQN) proposed in this paper introduces a mismatch in the model learning as mentioned above. and [Schulman et al.].
Reject. rating score: 4. rating score: 6. rating score: 6. <BRK>These regions also have a correspondence with Activation Patterns, the active/inactive pattern of neurons over the entire network. This paper improves on the upper bound given by [2] and the lower bound given by [1]. (The improvement on Zaslavsky s theorem is interesting.) It is therefore hard to be entirely convinced by the empirical conclusions that more linear regions is better. [1] On the number of linear regions of Deep Neural Networks, 2014, Montufar, Pascanu, Cho, Bengio[2] On the expressive power of deep neural networks, 2017, Raghu, Poole, Kleinberg, Ganguli, Sohl Dickstein<BRK>This paper investigates the complexity of neural networks with piecewise linear activations by studying the number of linear regions of the representable functions. The improved upper bound given in Theorem 1 appeared in SampTA 2017   Mathematics of deep learning ``Notes on the number of linear regions of deep neural networks  by Montufar. The improved lower bound given in Theorem 6 is very modest but neat. These give an interesting picture, consistent with the theoretical bounds, and showing the behaviour during training. Here it would be interesting to run more experiments to see how the number of regions might relate to the quality of the trained hypotheses.<BRK>This is quite an interesting paper. For example a discussion of how practical is the algorithm proposed for exact counting of linear regions would be nice. Could the authors prioritize clarification to that point ! Otherwise one will have to scroll up and down all the time to understand the proof.
Reject. rating score: 4. rating score: 4. rating score: 6. <BRK>This paper proposes a method for explaining the classification mistakes of neural networks. For a misclassified image, gradient descent is used to find the minimal change to the input image so that it will be correctly classified. My understanding is that the proposed method does not explain why a classifier makes mistakes. Instead, it is about: what can be added/removed from the input image so that it can be correctly classified.<BRK>They evaluate their method on CelebA and MNIST datasets. Some of these yield interpretations that are semantically meaningful and some of these do not yield semantically meaningful interpretations. 2) The paper reports results on 2 datasets, out of which on 1 of them it does not perform well and gets stuck in a local minima therefore implying that it is not able to capture the diversity in the data well.<BRK>This single example is interesting but not sufficient to illustrate the success of this method. The examples from CelebA are interesting but inconclusive. For example, why should adding blue to the glasses fix the misclassification. Regarding epsilon, it is unclear what a small euclidean distance for epsilon is without more examples. The fact that perturbations are made in the latent space, and that this perturbation gets reflected in particular areas in the reconstructed image, is the most interesting part of this work.
Reject. rating score: 3. rating score: 4. rating score: 5. <BRK>What s the evidence *for* the theory? This is fine as far as it goes, but the paper refs Basri & Jacobs 2016 multiple times as if it says anything relevant about this paper: Basri & Jacobs is specifically about the ability of deep nets to fit data that falls on (actual, mathematical) manifolds. This postulate is sloppy and speculative. For instance, taken in its strong form, if believe the postulate, then a good model:1. For instance, I spent awhile trying to decide whether the authors assumed common classifiers are "good" (according to the postulate) or whether this paper was about a way to *make* classifiers good (I eventually decided the former). The experiments are weak.<BRK>In particular, for an adversarial example that is distinguishable from the points on the manifold and assigned a low confidence by the model, is projected back onto the designated manifold such that the model assigns it a high confidence value. The authors claim that the two objective functions proposed in this manuscript provide such a projection onto the desired manifold and assign high confidence for these adversarial points. More importantly, the experiments are very preliminary and there is much more room to deliver more comprehensive and compelling experiments.<BRK>1) SummaryThis paper proposes a new approach to defending against adversarial attacks based on the manifold assumption of natural data. The authors show that adversarial attack techniques can be with their algorithm for attack prevention. 2) Pros:+ Novel/interesting way of defending against adversarial attacks by taking advantage of the manifold assumption. The experimental section could improve a little bit in terms of baselines and test examples as previously mentioned, and also the authors may give some comments on if there is a simple way to make their algorithm not depend on assumptions of the learned embeddings.
Accept (Poster). rating score: 6. rating score: 6. rating score: 4. <BRK>Summary   This paper mainly focuses on a counting problem in visual question answering (VQA) using attention mechanism. The architecture of the proposed model looks natural and all components seem to have clear contribution to the model. The proposed model can be easily applied to any VQA model using soft attention. The paper is well written and the contribution is clear. Comments   It is not clear if the value of count "c" is same with the final answer in counting questions.<BRK>The proposed method improves the baseline by 5% on counting questions. The paper doesn’t compare experiment numbers with (Chattopadhyay et al., 2017). This is similar to a density map approach and the problem is that the model doesn’t develop a notion of instance. Compared to (Zhou et al., 2017), the proposed model does not improve much on the counting questions. Since the authors have mentioned in the related work, it would also be more convincing if they show experimental results on CLConclusion:  I feel that the motivation is good, but the proposed model is too hand crafted. Also, key experiments are missing: 1) NMS baseline 2) Comparison with VQA counting work  (Chattopadhyay et al., 2017). Therefore I recommend reject. ICLR 2017.<BRK>This paper tackles the object counting problem in visual question answering. The experimental results on counting are promising. Although counting is important in VQA, the method is solving a very specific problem which cannot be generalized to other representation learning problems. I thus believe the overall contribution is not sufficient for ICLR. Pros:1.Well written paper with clear presentation of the method. Other comments and questions:1. 2.Can the author provide analysis on scalability the proposed method? 3.Could the authors provide more insights on why the structured attention (etc) did not significantly improve the result?
Reject. rating score: 3. rating score: 4. rating score: 5. <BRK>Make SVM great again with Siamese kernel for few shot learning ** PAPER SUMMARY **The author proposes to combine siamase networks with an SVM for pair classification. ** REVIEW SUMMARY **The paper is readable but it could be more fluent. So you use a binary SVM, not one versus rest. Some of the results are better than your approach. This dataset offers a clearer experimental setup than your TIMIT setting and has abundant published baseline results.<BRK>Summary: The paper proposes to pre train a deep neural network to learn a similarity function and use the features obtained by this pre trained network as input to an SVM model. The SVM is trained for the final classification task at hand using the last layer features of the deep network. First, I did not see any novel idea presented in this paper.<BRK>The writing also needs to be improved. This paper presents an algorithm for few shot learning. I think the idea of representation learning using a somewhat artificial task makes sense in this setting. 1.I am not very familiar with the literature of few shot learning. 3.Relatively minor: The writing of this paper is readable, but could be improved.
Accept (Poster). rating score: 9. rating score: 7. rating score: 6. <BRK>This paper presents a fully differentiable neural architecture for mapping and path planning for navigation in previously unseen environments, assuming near perfect* relative localization provided by velocity. The model is more general than the cognitive maps (Gupta et al, 2017) and builds on the NTM/DNC or related architectures (Graves et al, 2014, 2016, Rae et al, 2017) thanks to the 2D spatial structure of the associative memory.<BRK>The short experiment in Appendix E seems to try and answer this question, but it s results are anecdotal at best. The paper introduces a new memory mechanism specifically tailored for agent navigation in 2D environments. I m concerned whether the proposed agent is actually employing a navigation strategy, as seems to be suggested, or is simply a good agent architecture for this task (e.g.for optimization reasons).<BRK>The proposed memory architecture is new. # Novelty and SignificanceThe proposed idea is novel in general. designed the memory specifically for predicting free space. On the other hand, the proposed method is also specific to navigation tasks in 2D or 3D environment, which is hard to apply to more general memory related tasks in non spatial environments. But, it is still interesting to see that the ego centric neural memory works well on challenging tasks in a 3D environment.
Reject. rating score: 4. rating score: 4. rating score: 5. <BRK>The paper presents a method to use non linear combination of context vectors for learning vector representation of words. I feel word vectors should definitely be tested on similarity tasks (if not analogy). I think the experimental section is weak.<BRK>This paper presents another variant on neural language models used to learn word embeddings. In keeping with the formulation of Mikolov et al, the model learned is a set of independent binary classifiers, one per word. The quantitative likelihood based evaluation can easily be gamed by making all classifiers output numbers which are close to 1. 2.The qualitative similarity based evaluation notes, correctly, that the standard metric of dot product / cosine between word embeddings does not work in the case of networks, and instead measures similarity by looking at the similarity of the predictions of the networks. While this approach is interesting, the baseline models were evaluated using the plain dot product.<BRK>A more sensible experiment would be to actually plug in the entire pretrained word nets into an external model and see how much they help. EDIT: It s usually the case that even if the number of parameters is the same, extra nonlinearity results in better data fitting (e.g., Berg Kirkpatrick et al, 2010), it s still not unexpected. Another contribution of the paper is a new form of regularization by tying a subset of layers between different N_x. The paper extends SGNS as follows.
Reject. rating score: 4. rating score: 5. rating score: 7. <BRK>A grid world and tele kinetically operated block stacking task is used to demonstrate the idea  This framework is exactly the same as semi MDPs (Precup, Sutton) and its several generalizations to function approximators as cited in the paper. The authors claim that the novelty is in using the framework for test generalization. I do not believe that the experiments alone demonstrate anything substantially new about semi MDPs even within the deep RL setup.<BRK>The proposed method is indeed technically sound and have some distinctions to other existing methods in literature, however, the novelty of this work does not seem to be significant as I will elaborate more. The authors mentioned that the proposed method can be placed into the framework of option.<BRK>I suspect that the AP would still have better generalization capabilities, but it is hard to know without seeing the results. While the paper in general is written very clearly, it would be helpful to the reader to include an algorithm for the AP. The high level task plan is not learned, but is computed using Dijkstra s algorithm. It is difficult to say how much of an improvement this paper is on top of other related hierarchical RL works as there are no comparisons made.
Reject. rating score: 4. rating score: 5. rating score: 6. <BRK>There doesn t appear to be a definition of the L1 penalty this paper compares against and it s unclear why this is a reasonable baseline.<BRK>If it is L_1 norm on the output coefficients the comparison is misleading. The original dimensionality of the data is 4, and only a linear relation is introduced.<BRK>The idea is simple and it seems to work for the presented examples. Any comment on that?.
Accept (Poster). rating score: 7. rating score: 7. rating score: 7. <BRK>The paper was very well written, and mostly clear, making it easy to follow. However, the authors clearly outline the tricks they had to do to achieve good performance on multiple domain adaptation tasks: confidence thresholding, particular data augmentation, and a loss to deal with imbalanced target datasets, all of which seem like good tricks of the trade for future work. Pros:* Winning entry to the VISDA 2017 visual domain adaptation challenge competition. Did you want to say that the data distributions are different? How does this make the task different. Having source and target come in different minibatches is purely an implementation decision. (although not a big deal with your acknowledgements)<BRK>No clear explanation is provided for why this may be a good thing to try. The authors also use other techniques like data augmentation to enhance their algorithms. The experimental results in the paper are quite nice. They apply the methodology to various standard vision datasets with noticeable improvements/gains and in one case by including additional tricks manage to better than other methods for VISDA 2017 domain adaptation challenge.<BRK>This paper presents a domain adaptation algorithm based on the self ensembling method proposed by [Tarvainen & Valpola, 2017]. Pros:+ The paper is well written and easy to read+ The proposed method is a natural extension of the mean teacher semi supervised learning model by [Tarvainen & Valpola, 2017]+ The model achieves state of the art results on a range of visual domain adaptation benchmarks (including top performance in the VisDA17 challenge)Cons:  The model is tailored to the image domain as it makes heavy use of the data augmentation. Related to the previous point, the final VisDA17 model seems to be engineered too heavily to work well on a particular dataset. I’m not sure if it provides many interesting insights for the scientific community at large. I appreciate the improvements that were made to the paper but still feel that this work a bit too engineering heavy, and the title does not fully reflect what s going on in the full pipeline.
Reject. rating score: 4. rating score: 4. rating score: 6. <BRK>This paper proposes a method to generate adversarial examples for text classification problems. To preserve correct grammar, they only change words that don t significantly change the probability of the sentence under a language model. The approach seems incremental and very similar to existing work such as Papernot et. al and a comparison to adversarial training for text classification in Miyato et.<BRK>The paper shows that neural networks are sensitive to adversarial perturbation for a set of NLP text classifications. Furthermore to test the validity of their adversarial examples, the authors show the following:1. The authors talk about "syntactic" similarity but then propose a language model constraint. The adversarial model should be tuned on the validation set, and then the same model should be used to generate test set examples. Even the related work in NLP that is cited e.g.Jia and Liang 2017 is obfuscated in the last page.<BRK>Nice overview of adversarial techniques in natural language classification. The paper introduces the problem of adversarial perturbations, how they are constructed and demonstrate what effect they can have on a machine learning models. The authors study several real world adversarial examples, such as spam filtering, sentiment analysis and fake news and use these examples to test several popular classification models in context of adversarial perturbations.
Accept (Poster). rating score: 8. rating score: 7. rating score: 7. <BRK>This paper proposes a skim RNN, which skims unimportant inputs with a small RNN while normally processes important inputs with a standard RNN for fast inference. One of the most difficult problems of this approach (non differentiable) is elegantly solved by employing gumbel softmax 	The effectiveness (mainly inference speed improvement with CPU) is validated by various experiments. No comments about computational costs during training with GPU (it would not increase the computational cost so much, but gumbel softmax may require more iterations). Section 4.2, “We also observe that the F1 score of Skim LSTM is more stable across different configurations and computational cost.”: This seems to be very interesting phenomena. Is there some discussion of why skim LSTM is more stable?<BRK>Summary: The paper proposes a learnable skimming mechanism for RNN. The paper finds that with the proposed skimming method, they achieve a significant reduction in terms of FLOPS. Unlike (Yu et al., 2017), which skips the text, here the model decides between small and large RNN. Pros:  Models that dynamically decide the amount of computation make intuitive sense and are of general interests. Cons:  Each model component is not novel. Questions:  Although it is out of the scope for this paper to achieve GPU level speedup, I am curious to know some numbers on GPU speedup.<BRK>In this sense, it would be more beneficial if the skimming happens over consecutive tokens (focus on a region, not on an individual token). * One advantage of proposed idea claimed against the skip RNN is that the Skim RNN can generate the same length of output sequence given input sequence. * One obvious way to reduce the computational complexity of RNN is to reduce the size of the hidden state. So that readers can check benefits of the skim RNN against skip RNN and small sized RNN.
Reject. rating score: 3. rating score: 5. rating score: 7. <BRK>naívely: the correct spelling would be naïvely or naively. The overall structure of the paper is good. The control theory literature has dealt with nonlinear SSMs for decades and there is recent work in the machine learning community on nonlinear SSMs, e.g.Gaussian Process SSMs. The authors claim that this marginalization "allows the SSL to have non Markovian state transition".<BRK>The text seems to indicate they are not. For inference the authors propose to make use of Monte Carlo expectation maximization. The model proposed seems to be a special case of previously proposed models that are mentioned in the 2nd paragraph of the related works section, and e.g.the Maddison et al.(2017) paper. Perhaps the authors can elaborate on this?<BRK>Monte Carlo EM does not in general have convergence guarantees of “standard” EM (i.e.each step is not guaranteed to monotonically improve the lower bound). This might be fine! A known issue when running an iterated conditional SMC algorithm like this is that path degeneracy can make it very difficult for the PG kernel to mix well over the early time steps in the LSTM.
Accept (Poster). rating score: 7. rating score: 7. rating score: 7. <BRK>The paper seems to be significant since it integrates PGM inference with deep models. The motivation of the paper, and the description of its contribution as compared to existing methods can be improved. I was not very sure as to why the proposed method is more general than existing approaches. Regarding the experiments, as mentioned in the paper the evaluation is performed on two fairly small scale datasets. the approach shows that the proposed methods converge faster than existing methods.<BRK>This paper presents a variational inference algorithm for models that containdeep neural network components and probabilistic graphical model (PGM)components. The authors propose a new variational inference algorithm that handles modelswith deep neural networks and PGM components. However, it appears that theauthors rely heavily on the work of (Khan & Lin, 2017) that actually providesthe algorithm. Finally, I found the experimental evaluation to not thoroughly demonstrate theadvantages and disadvantages of the proposed algorithm. For instance, is the (Johnson, et al., 2016) algorithmsuffering from the implicit gradient? What is a powerful model? The phrases "first term of the inference network" are not clear.<BRK>The authors adapts stochastic natural gradient methods for variational inference with structured inference networks. The variational approximation proposed is similar to SVAE by Jonhson et al.(2016), but rather than directly using the global variable theta in the local approximation for x the authors propose to optimize a separate variational parameter. The authors then extends and adapts the natural gradient method by Khan & Lin (2017) to optimize all the variational parameters. In the experiments the authors generally show improved convergence over SVAE. Or do you simply mean that computation of this is exact using regular message passing/Kalman filter/forward backward? There has recently been interest in using inference networks as part of more flexible variational approximations for structured models.
Invite to Workshop Track. rating score: 7. rating score: 6. rating score: 4. <BRK>The integral notation is not meaningful: you can’t sample something in the subscript the way you would in an expectation. For two existing algorithms (MAML, RL2) it proposes a modification of the metaloss that encourages more exploration in the first (couple of) test episodes. The approach is a reasonable one, the proposed methods seem to work, the (toy) domains are appropriate, and the paper is well rounded with background, motivation and a lot of auxiliary results.<BRK>Figures 5, 6, 9: Wouldn t it be better to also use log scale on the x axis for consistent comparison with curves in Krazy World experiments ? 3.It could be very interesting to benchmark also in Mujoco environments, such as modified Ant Maze. Overall, the idea proposed in this paper is interesting. I agree with the authors that a good learner should be able to generalize to new tasks with very few trials compared with learning each task from scratch. How does the approach of the authors relate to the following approaches?<BRK>There are many strong statements e.g., : "which indicates that at the very least the meta learning is able to do system identification correctly. ">> none of the results support such a claim. This paper needs substantial revision. The first batch of results in on a new task introduced by this paper. Usually one performance a search for the best setting and then compares the results. The results on the first task are not clear. In fig5 rl2 gets the best final performance do you have a hypothesis as to why? I am not sure what to conclude from this graph.
Accept (Poster). rating score: 7. rating score: 6. rating score: 6. <BRK>This paper proposes an extremely simple methodology to improve the network s performance by adding extra random perturbations (resizing/padding) at evaluation time. The main merit of the paper is to study this simple but efficient baseline method extensively and shows how adversarial attacks can be mitigated by some extent. Cons of the paper: there is not much novel insight or really exciting new ideas presented. Pros: It gives a convincing very simple baseline and the evaluation of all subsequent results on defending against adversaries will need to incorporate this simple defense method in addition to any future proposed defenses, since it is very easy to implement and evaluate and seems to improve the defense capabilities of the network to a significant degree. So I assume that this paper will be influential in the future just by the virtue of its easy applicability and effectiveness.<BRK>The paper basically propose keep using the typical data augmentation transformations done during training also in evaluation time, to prevent adversarial attacks. Some of the pros of the proposed tricks is that it doesn t require re training existing models, although as the authors pointed out re training for adversarial images is necessary to obtain good results. How would this method work with variable size images? Section 4.6.2 seems to indicate that 1 pixel padding or just resizing 1 pixel is enough to get most of the benefit, please provide an analysis of how results improve as the padding or size increase.<BRK>The authors propose a simple defense against adversarial attacks, which is to add randomization in the input of the CNNs. In particular, it was not clear whether the defense model was trained with the input randomization layers? Also, in Tables 1 6, how was the target model trained? How do the training procedures of target vs. defense model differ? In those tables, what is the testing procedure for the target model and how does it compare to the defense model? is missing but is very useful. While the experiments show that the randomization layers mitigate the effect of randomization attacks, it s not clear whether the effectiveness of this very simple approach is heavily biased towards the published ways of generating adversarial attacks and the particular problem (i.e.classification). The form of attacks studied in the paper is that of additive noise.
Reject. rating score: 4. rating score: 4. rating score: 6. <BRK>This paper proposes a method to automatically tuning the momentum parameter in momentum SGD methods, which achieves better results and fast convergence speed than state of the art Adam algorithm. Does it have anything to do with the paper as I didn t see it in the remaining text? There are a lot of undefined notation. For example, what does the *convergence rate* mean (what is the measurement for convergence)?<BRK>The paper explores momentum SGD and an adaptive version of momentum SGD which the authors name YF (Yellow Fin). They compare YF to hand tuned momentumSGD and to Adam in several deep learning applications. There are two major problems with this approach: First: Exploring 1 dim functions is indeed a nice way to get some intuition. This is since $\kappa  1$ for 1 dim problems, and therefore accelerated GD does not have any benefits over non accelerated GD in this case. And as I have mentioned, the design of the algorithm is inspired by the analysis of 1 dim quadratic functions.<BRK>Marking my review as "educated guess" since i didn t have time for a detailed review]The paper proposes an algorithm to tune the momentum and learning rate for SGD. I have personally tried the algorithm when the paper came out and can vouch for the empirical results presented here.
Reject. rating score: 5. rating score: 6. rating score: 8. <BRK>Summary: This work proposes a way to create 3D objects to fool the classification of their pictures from different view points by a neural network. Using an inception v3 net, they create adversarial attacks on a subset of the imagenet validation set transformed by translations, lightening conditions, rotations, and scalings among others, and observe a drop of the classifier accuracy performance from 70% to less than 1%. See for instance Cisse et al.Houdini: fooling Deep Structured Prediction Models, NIPS 2017 for a recent list of related work in this research area. It would be nice to include insights to improve neural nets to become less sensitive to these attacks. How about min, max ?<BRK>The authors present a method to enable robust generation of adversarial visualinputs for image classification. They adapt an existing method for deriving adversarial examples to act under aprojection space (effectively a latent variable model) which is defined througha transformations distribution. They demonstrate the effectiveness of their approach in the 2D and 3D(simulated and real) domains. The paper is clear to follow and the objective employed appears to be sound. And on a related note, how were the number of sampled   transformations chosen? For now, I think thesubmission is good for a weak accept –  if the authors address my concerns, and/orcorrect my potential misunderstanding of the issues, I d be happy to upgrade myreview to an accept.<BRK>The paper proposes a method to synthesize adversarial examples that remain robust to different 2D and 3D perturbations. The experimental results and video showing that the perturbation is effective for different camera angles, lighting conditions and background is quite impressive. This work convincingly shows that adversarial examples are a real world problem for production deep learning systems rather than something that is only academically interesting. However, the authors claim that standard techniques require complete control and careful setups (e.g.in the camera case) is quite misleading, especially with regards to the work by Kurakin et. al.This paper also seems to have some problems of its own (for example the turtle is at relatively the same distance from the camera in all the examples, I expect the perturbation wouldn t work well if it was far enough away that the camera could not resolve the HD texture of the turtle).
Accept (Poster). rating score: 7. rating score: 7. rating score: 6. <BRK>Also, although I found their work to be interesting after finishing the paper, I was initially confused by how the authors frame their work and where the paper was heading. Specifically:  I think Figure 3 in the appendix should be moved to the main text, to help understand the behavior of the analytical solution.<BRK>They show that a CNN can essentially encode a BFS, so theoretically a CNN should be able to solve the problem. The pictorial explanation for how the CNN can mimic BFS is interesting but I got a little lost in the 3 cases on page 4. I thought this could use a little more clarity. I thought this was an impressive paper that looked at theoretical properties of CNNs. Also, in the experiments, the authors mention multiple attempt with the same settings   are these experiments differentiated only by their initialization?<BRK>The paper is hard to evaluate. The actual setup seems somewhat arbitrary, but the method of analysing the failure modes is interesting. If we trust the authors, then the paper seems good because it is fairly unusual.
Accept (Poster). rating score: 8. rating score: 6. rating score: 6. <BRK>Because of this, I m willing to excuse the fact that the paper is only moderately novel, in light of the impressive reported results. I d like to see more discussion of how you performed your evaluation on the downstream tasks. The idea is reasonable, the topic is important, and the results are quite strong.<BRK>However, the balance between performance and computational complexity is not investigated, and I think such an analysis would add significant value to the paper. As such, the novelty of this paper rests in the specific modeling choices and the significance hinges on the good empirical results. For this reason, I believe it is important that additional details regarding the specific architecture and training details be included in the paper.<BRK>I appreciate the effort and think it has improved the paper. The proposed novelty is simple and intuitive, which I think is a strength of the method.
Reject. rating score: 3. rating score: 3. rating score: 4. <BRK>Studying the relation between predictive coding and deep learning makes sense, but I do not come to the same (strong) conclusions as the author(s) by considering the experimental results   and I do not see evidence for a sophisticated latent representation learned by the network. The network learns unsupervised using a predictive coding setup. Then images generated using the latent representations inferred on these images, on translated images, and on images of other objects are shown.<BRK>The paper attempts to extend the predictive coding model to a multilayer network. The math is developed for a learning rule, and some demonstrations are shown for reconstructions of CIFAR 10 images. It could also use better theoretical motivation   i.e., what sorts of representations do you expect to emerge in higher layers? That the model can reconstruct images per se is not particularly interesting.<BRK>It is unclear how the approach improves on the original predictive coding formulation of Rao and Ballard, who also use a hierarchy of transformations. No insight is provided about the kinds of filters that are learned. The paper should be checked for typos. OriginalityThere exist alternative deep predictive coding models such as https://arxiv.org/abs/1605.08104. ProsRelevant attempt to develop new predictive coding architecturesConsUnclear what is gained compared to existing work.
Accept (Poster). rating score: 8. rating score: 5. rating score: 5. <BRK>This paper proposes a device placement algorithm to place operations of tensorflow on devices. 2.The experiments are solid to demonstrate this method works very well. Cons:1.It is not very clear in the paper whether the training happens for each model yielding separate agents, or a shared agent is trained and used for all kinds of models. The latter would be more exciting.<BRK>In addition, it would be better to have some analysis on the end to end runtime efficiency and the effectiveness of the placements. [1] Mirhoseini A, Pham H, Le Q V, et al.Device Placement Optimization with Reinforcement Learning[J]. arXiv preprint arXiv:1706.04972, 2017. https://arxiv.org/pdf/1706.04972.pdf<BRK>The paper seems clear enough and original enough. Pros:  Jointly optimizing forming of groups and placing these seems to have merit  Experiments show improvements over placement by human "experts"  Targets an important problemCons:  Related work seems inadequately referenced. The methods are not well motivated. Also, please provide some reasoning behind your hypothesis which seems to be that while costs may be dynamic, optimally forming groups and placing them is learn able. ".... Device Placement" seems to suggest that one is placing devices when in fact, the operators are being placed.
Accept (Poster). rating score: 7. rating score: 7. rating score: 5. <BRK>Then it is easier for the readers to see the differences in the results from the two methods. The paper also extends GMV into a conditional generative model that takes an input image and generates different views of the object in the input image. Experiments are conducted on four different datasets to show the generative ability of the proposed method. Positives:  The proposed method is novel in disentangling the content and the view of objects in a GAN and training the GAN with pairs of objects.<BRK>This is an interesting problem setup, but not novel as such and unfortunately the paper does not do a very good job in putting it into the right context. One would expect to see more discussion on this, given the importance of this property as motivation for the method. A particularly interesting question would be whether the proposed model actually is a direct GAN based extension of IBFA, and if not then how does it differ. The results look aesthetically more pleasing than the baselines, but the reader does not learn much about how the method actually behaves in practice; when does it break down, how sensitive it is to various choices (network structure, learning algorithm, amount of data,  how well the content and view can be disentangled from each other, etc.). Revision of the review in light of the author response:The authors have adequately addressed my main remarks, and while doing so have improved both the positioning of the paper amongst relevant literature and the somewhat limited empirical comparisons.<BRK>That raises the question of how useful it is to have such a separation between content and views; for some datasets their diversity can be a bottleneck for this partition, making the interpretation of views difficult. The content can be seen as an intrinsic instantiation of the class that is independent of certain types of variation (eg viewpoint), and a view is the observation of the object under a particular variation. This paper firstly proposes a GAN architecture that aim at decomposing the underlying distribution of a particular class into "content" and "view". It seems that it depends heavily on the content.
Accept (Poster). rating score: 8. rating score: 7. rating score: 6. <BRK>This is a very interesting submission that takes an interesting angle on clinical time series modeling, namely, actively choosing when to measure while simultaneously attempting to impute missing measurements and predict outcomes of interest. Here are some potential areas for improvement:  The structure of the paper is a bit weird.<BRK>This paper proposes a novel method to solve the problem of active sensing from a new angle (Essentially, the active sensing is a kind of method that decides when (or where) to take new measurements and what measurements we should conduct at that time or (place)).<BRK>This paper presents a new approach to determining what to measure and when to measure it, using a novel deep learning architecture. The problem addressed is important and timely and advances here may have an impact on many application areas outside medicine.
Reject. rating score: 2. rating score: 4. rating score: 4. <BRK> This paper proposes a new type of RNN architectures called Dense RNNs. Unfortunately, this paper is hard to read, it is difficult to understand the intention of the authors. The experiment is only done on PTB dataset, and the reported numbers are not that promising either. This paper tries to combine three different features from previous works, and unfortunately, it is not so well conducted.<BRK>The theory is very hand wavy, the connections to the previous attempts to come up with related properties of the recurrent models should be cited. They report results on PTB character level language modelling task. The discussion about the skip connections is very related to the results in [2]. Overall, I think this paper is rushed and not ready for the publication.<BRK>The authors propose an RNN that combines temporal shortcut connections from [Soltani & Jang, 2016] and Gated Recurrent Attention [Chung, 2014]. However, their justification about the novelty and efficacy of the model is not well demonstrated in the paper. To conclude, this paper is an incremental work with limited contributions. Lack of referencing to previous works.
Reject. rating score: 2. rating score: 3. rating score: 4. <BRK>A majority of the references included in the reference section lack some or all of the required meta data. The originality and significance of the work reported in this paper are difficult to comprehend. Distilling the knowledge in a neural network, 2015.<BRK>Unfortunately, however, the body of the paper disappoints, as it has no real technical content or contribution. The paper also needs a spelling, grammar, typesetting, and writing check.<BRK>Issues of how to price the new data are discussed in a high level, abstract way, and arguments against retrieving the new data for free or encrypting it are presented. This makes the technical contribution rather shallow.
Reject. rating score: 4. rating score: 6. rating score: 7. <BRK>This paper proposes a WGAN formulation for generating graphs based on random walks. Unfortunately, the likelihood of the sampled graphs is not explicitly evaluated. First, wrt claim (i) the problem of generating "sibling" graphs is ill posed. Statistical graph models are typically designed to generate a probability distribution over all graphs with N nodes and, as such, are evaluated based on how well they model that distribution.<BRK>The paper proposes a GAN model to generate graphs with non trivial properties. This is possibly one of the best papers on graph generation using GANs currently in the literature. Learn to generate graphs is a key task in drug discovery, relational learning, and knowledge discovery. What are the conditions under which the method is likely to perform well? Would it be able to generate an expander graph? g)	Clearly, with a large T (number of RW steps), the RW is not modeling just a single community.<BRK>The authors proposed a generative model of random walks on graphs. Are there existing generative models based on walk paths? The choice of early stopping is a very interesting problem especially for the EO creitenrion. The draft is well written with convincing experiments.
Accept (Poster). rating score: 7. rating score: 7. rating score: 6. <BRK>Overall: Authors casted discrete structure generation as a planning task and they used Q learning + RNNs to solve for an optimal policy to generate valid sequences. The Q function is supported by a sequence model for state representation. Are there other simpler baseline approaches to compare against the proposed method? The lack of a baseline approach for comparison makes it hard to judge both results on Python Expressions and SMILES. But the lack of baseline results make it hard to judge significance of the work.<BRK>The authors use a recurrent neural network to build generative models of sequences in domains where the vast majority of sequences is invalid. The basic idea, outlined in Eq.2, is moderately straightforward: at each step, use an approximation of the Q function for subsequences of the appropriate length to pick a valid extension. I wish the paper had more detailed arguments and discussions. I question the appropriateness of Eq.2 as a target. For Python strings:   Should we view the fact that high values of tau give a validity of 1.0 as indicative that the domain s constraints are fairly easy to learn? It would be better to see a comparison to a strong non NN baseline. In particular, it might be that it s quite an easy example (compared to the SMILES) example. For SMILES, it seems like the Bayesian active learning technique is not by itself sufficient to create a good model? It is interesting that in the solubility domain the active model outperforms, but it would be nice to see more discussion / explanation.<BRK>SUMMARY:This work is about learning the validity of a sequences in specific application domains like SMILES strings for chemical compounds. In particular, the main emphasis is on predicting if a prefix sequence could possibly be extended to a complete valid sequence. In other words, one tries to predict if there exists a valid suffix sequence, and based on these predictions, the goal is to train a generative model that always produces valid sequences. EVALUATION:CLARITY & NOVELTY: In principle, the paper is easy to read. Or is it the way how you augment the dataset?
Accept (Poster). rating score: 6. rating score: 6. rating score: 5. <BRK>The paper presents a deep Poisson model where the last layer is the vector of word counts generated by a vector Poisson. From there the vectors are all Gammas with matrix vector parameterizations in a typical deep setup. While the model is reasonable, the purpose was not clear to me. If only the last layer generates a document, then what use is the deep structure? For example, learning hierarchical topics as in Figure 4 doesn t seem so useful here since only the last layer matters. The experiments are otherwise thorough and convincing that quantitative performance is improved over previous attempts at the problem.<BRK>The authors develop a hybrid amortized variational inference MCMC inference framework for deep latent Dirichlet allocation. They amortize inference at the observation level using a Weibull approximation. The structure of the inference network mimics the MCMC sampler for this model. Finally they use MCMC to infer the parameters shared across data. However, this approach doesn t use that directly. Was the structure of the inference network proposed here crucial? 5) How much like a Weibull do you expect the posterior to be? This seems unclear.<BRK>The authors propose a hybrid Bayesian inference approach for deep topic models that integrates stochastic gradient MCMC for global parameters and Weibull based multilayer variational autoencoders (VAEs) for local parameters. iii) The proposed approach is certainly faster at test time, however, it is not clear to me in which settings such speed (compared to Gibbs) would be needed, provided the unsupervised nature of the task at hand.
Reject. rating score: 2. rating score: 3. rating score: 6. <BRK>SUMMARY The model is an ANN whose units have the absolute value function abs as their activation function (in place of ReLU, sigmoid, etc.). The generalization accuracy in classification on 42k image MNIST is 97.4%. No baselines are given. The paper talks of a training set and a "dev" set, but no test set, and generalization performance is given for the dev set rather than a test set. * The work seems to be at too preliminary a stage to warrant acceptance at ICLR.<BRK>The paper proposes using the absolute value activation function in (what seems to be) an autoencoder architecture with an additional supervised learning term in the objective function that encourages the bottleneck layer representation to be discriminative. A few examples of reconstructed images and classification performance are reported for the MNIST dataset. It is not clear from the paper how a network of bidirectional neurons is different from an autoencoder.<BRK>This paper introduces a reversible network with absolute value used asthe activation function. I think there are a lot of novel and interesting ideas in this paperthough they have not been fully explored. (The use of the links for providing code and visualizations (when active) is a nice feature of this paper). Also, did you compare to using the leaky ReLU activation function  That would be interesting as it also doesn t have any areas of zeroslope? I am also curious, how does accuracy on digit classification differwhen trained only to optimize the forward error? The MNIST site referenced lists 60,000 training data and test data of10,000.
Accept (Poster). rating score: 8. rating score: 7. rating score: 7. <BRK>The network is demonstrated to learn compositional object representations which group together pixels, optimizing a predictive coding objective. As you always take the first frame of the 4 frame stacks in the data set, do the objects deform at all?<BRK>This was a pretty interesting read. What is the difference to Battaglia et al.that this should highlight?<BRK>These are moderately novel contributions and there are only minor weaknesses, so this is a clear accept. This representation aligns with the game s goal.
Accept (Poster). rating score: 7. rating score: 6. rating score: 6. <BRK>This paper provides an overview of the Deep Voice 3 text to speech system. It describes the system in a fair amount of detail and discusses some trade offs w.r.t.audio quality and computational constraints. I wish there would be more of that kind of thing in the paper. The comparison of attention mechanisms is also useful.<BRK>The paper presents a speech synthesis system based on convolution neural networks. The paper also introduces a attention model and can be used with various waveform synthesis methods. The paper is clearly written and easy to follow. * The footnote 2 on page 3 looks important enough to be part of the main text.<BRK>This paper discusses a text to speech system which is based on a convolutional attentive seq2seq architecture. Was the evaluation of each attention mechanism done blindly? The original wavenet paper describes an autoregressive model for waveform generation. Beyond that, I think more detailed description of the system would be necessary in order to reimplement it suitably (another important potential takeaway for a "system" paper).
Reject. rating score: 3. rating score: 5. rating score: 5. <BRK>While standard CNN like architectures are fine with the layer parallel updating process typically used in standard tools, for recurrent networks and also for networks with connections that skip layers, different update orders may be more natural, but no GPU accelerated toolboxes exist that support this. The authors provide such a toolbox, statestream, written Theano. For example, a number of times there are phrases like "previously mentioned", which is ugly. There are no results in the paper that demonstrate a case where it is useful to apply fully parallel updates.<BRK>This paper introduces a new toolbox for deep neural networks learning and evaluation. The paper presents a single example in which either the accuracy and the training time are not reported. While I understand that the main result of this work is the toolbox itself, more examples and results would improve the clarity and the implications for such paradigm switch. Another concern comes from the choice to use Theano as back end, since it s known that it is going to be discontinued.<BRK>In this paper, the authors present an open source toolbox to explore layerwise parallel deep neural networks. They offer an interesting and detailed comparison of the temporal progression of layerwise parallel and layerwise sequential networks, and differences that can emerge in the results of these two computation strategies. While the open source toolbox introduced in this paper can be an excellent resource for the community interested in exploring these networks, the present submission offers relatively few results actually using these networks in practice.
Reject. rating score: 4. rating score: 6. rating score: 6. <BRK>Summary: The authors observe that the success of GANs can be attributed to two factors; leveraging the inductive bias of deep CNNs and the adversarial training protocol. The design choices are well motivated in Chapter 2 which makes the main idea easy to grasp. Weaknesses:A relevant model is Generative Moment Matching Network (GMMN) which can also be thought of as a “discriminator less GAN”. Another relevant model is Variational Autoencoders (VAE) which also learns the data distribution through a learnable latent representation by minimizing a reconstruction loss. The paper would be more convincing if it provided a comparison with VAE. The evaluation protocol is quite weak: CelebA images are 128x128 while LSUN images are 64x64. This is a strong claim based on one example, moreover the evidence of this claim is not as obvious (based on the figure) to the reader.<BRK>In this paper, the authors propose a new architecture for generative neural networks. Overall, I think this paper is useful. The images generated by the model are not (qualitatively and in my opinion) as high quality as extremely recent work on GANs, but do appear to be better than those produced by DCGANs. More importantly than the images produced, however, is the novel training procedure. For all of their positive attributes, the adversarial training procedure for GANs is well known to be fairly difficult to deal with. Is it fair to assume from this that the initialization of z during training matters?<BRK>The paper is well written and easy to follow. I find the results very interesting. In particular the paper shows that many properties of GAN (or generative) models (e.g.interpolation, feature arithmetic) are a in great deal result of the inductive bias of deep CNN’s and can be obtained with simple reconstruction losses. Samples are quite good but inferior to GANs, but still impressive for the simplicity of the model. Naturally, the paper would make a much stronger claim showing good results on different datasets. The authors mentioned that the current method can recover all the solutions that could be found by an autoencoder and reach some others. It seems natural to include VAE baselines (using both of the losses in this work). Conversely, the proposed method can be thought as a form of autoencoder. It would be informative to also shows reconstruction and interpolation results for a set of ‘held out’ images.
Reject. rating score: 5. rating score: 6. rating score: 6. <BRK>The papers proposes a recurrent neural network based model to learn the temporal evolution of a probability density function. The paper is quite dense and quite difficult to follow, also due to the complex notation used by the authors. More sophisticated should have been employed.<BRK>Unclear if this would work at all in higher dimensional time series. In Fig 6c, the 95pct CI looks more or less constant over time. Is there an explanation for that? Clarity: The paper is well written. The presentations of the ideas are pretty clear. It would be excellent if the authors can extend this to higher dimensional time series.<BRK>The work would be stronger if the authors can extend this to higher dimensional time series. Clarity: The paper is well written. Some notations in the LSTM section could be better explained for readers who are unfamiliar with LSTMs. I think it s correct but may not scale well with dimensions.
Reject. rating score: 3. rating score: 3. rating score: 4. <BRK>This paper proposes to use self training strategies for using unlabeled data in GAN. The proposed methods should be evaluated with the state of the art semi supervised deep learning methods, such as those mentioned in related work section. Cons:* The novelty and technical contribution is low.<BRK>This paper presents a self training scheme for GANs and tests it on image (NIST) data. The idea makes a lot of sense though, so it would be great to expand on these preliminary results and explore the use of GANs in semi supervised learning in a more thorough manner.<BRK>The paper presents to combine self learning and GAN. Secondly, the compared methods are too few and do not include many state of the art SSL methods like graph based approaches. 2.The paper claims that ‘when paired with deep, semi supervised learning has had a few success’. 3.The layout of the paper could be improved. 4.Overall technically the proposed approach is a bit straightforward and does not bring too much novelty.
Reject. rating score: 5. rating score: 6. rating score: 6. <BRK>[Main comments]* I would advice the authors to explain in more details in the introwhat s new compared to Li & Malik (2016) and Andrychowicz et al.(2016).It took me until section 3.5 to figure it out. * Please clarify what are the hyper parameters of your meta training algorithm  and how you chose them. "Learningwhat to learn" does not mean anything. I understand that the authors wanted tohave "what", "which" and "how" sections but this is not clear at all. I think it would be useful to define it moreprecisely early on. How do you  explain that it converges to a an objective value that is so much worse?<BRK>This work is an extended version of [1], aiming to address the high dimensional problem. Strengths:The proposed method has achieved a better convergence rate in different tasks than all other hand engineered algorithms. The proposed method has better robustess in different tasks and different batch size setting. Since the batch size is small in each experiment, it is hard to compare convergence rate within one epoch. 2.In Figure 1b, L2LBGDBGD converges to a lower objective value, while the other figures are difficult to compare, the convergence value should be reported in all experiments. Sufficient experiment results show that the proposed method has better convergence rate than [1]. But comparing to [1], this paper has limited contribution. [1]: Ke Li and Jitendra Malik.<BRK>Summary of the paper The paper derives a scheme for learning optimization algorithm for high dimensional stochastic problems as the one involved in shallow neural nets training. A hatch of how to learn the overall process is presented. Finally experimental evaluations on synthetic or real datasets are conducted to show the effectiveness of the approach. It would be desirable to sum up the overall procedure in an algorithm. How do they impact for instance changing the time range in the definition of $\Phi$) in the performance of the meta learner? Which guarantees do we have that the algorithm will not diverge as L2LBGDBGD does? How long should be the training to ensure a good and stable convergence of the method?
Accept (Poster). rating score: 8. rating score: 7. rating score: 5. <BRK>This paper is a thorough investigation of various “class aware” GAN architectures. This analysis motivates the paper’s proposed extension, called Activation Maximization. A discussion throughout the paper involves dealing with the issue of mode collapse   a problem plaguing standard GAN variants. The paper additionally performs a thorough investigation of the inception score and proposes a new metric the AM score. As a reader, I found this paper to be thorough, honest, and thoughtful.<BRK>+ Significance:   Authors show that in quantitative measures, AM GAN is better than existing GANs on CIFAR 10 / TinyImageNet. + Pros:  The paper properly compares and discusses the connection between AM GAN and class conditional GANs in the literature (AC GAN, LabelGAN)  The experiments are thorough  Relation to activation maximization in neural visualization is also properly mentioned  The authors publish code and honestly share that they could not reproduce AC GAN s results and thus using to its best variant AC GAN* that they come up with. Overall, this is a good paper with thorough experiments supporting their findings regarding AM GAN and Inception score! The analysis of Inception score is sound.<BRK>The authors describe a new version of a generative adversarial network (GAN) for generating images that is heavily related to class conditional GAN s. The authors highlight several additional results on evaluation metrics and demonstrate some favorable analyses using their new proposed GAN. However, the authors quote an MS SSIM for various GAN models in Table 3. MS SSIM is a measure of image similarity between a pair of images. Hence, I do not see how the inception score captures this property.
Accept (Poster). rating score: 7. rating score: 7. rating score: 6. <BRK>This paper revisits an interesting and important trick to automatically adapt the stepsize. They consider the stepsize as a parameter to be optimized and apply stochastic gradient update for the stepsize. Although the theoretical analysis is weak that theorem 1 does not reveal the main reason for the benefits of such trick, considering their performance, I vote for acceptance. Please include this part for self containing. 3, As the authors claimed, the Maclaurin et.al.<BRK>The authors consider a method (which they trace back to 1998, but may have a longer history) of learning the learning rate of a first order algorithm at the same time as the underlying model is being optimized, using a stochastic multiplicative update. The authors highlight theoretical convergence guarantees as an important future work item, and the lack of them here (aside from Theorem 5.1, which just shows asymptotic convergence if the learning rates become sufficiently small) is a weakness, but not, I think, a critical one.<BRK>Please do not use color as the only cue to identify a curve. However, it is not clear why the method is tested only on a single data set: MNIST. Despite its limited novelty, this is a very interesting and potentially impactful paper. I like in particular the detailed discussion of related work, which includes some frequently overlooked precursors of modern methods.
Reject. rating score: 5. rating score: 5. rating score: 7. <BRK>_________________________________________________________________________________________________________This paper proposes to perform Adversarially Learned Inference (ALI) in a layer wise manner. The idea is interesting, and the authors did a good job to describe high level idea, and demonstrate one advantage of hierarchy: providing different levels reconstructions. (2) Could the authors provide the pseudocode procedure of the proposed algorithm? The reconstruction issues have first been highlighted and theoretically analyzed in ALICE [*], and some remedy has been proposed to alleviate the issue. It would be better to reflect the non identifiability issues raised by ALICE in Introduction, rather than hiding it in Future Work as "Although recent work designed to improve the stability of training in ALI does show some promise (Chunyuan Li, 2017), more work is needed on this front."<BRK>The paper was improved significantly but still lacks novelty. Also, I would suggest the authors study the modified prior with marginal statistics and other means to understand not just  that  their model performs better with the extra degree of freedom but also  how  exactly it does it. The only evaluation is sampling from z1 and z2 for reconstruction which shows that some structure is learned in z2 and the attribute classification task. However, more statistical understanding of the distributions of the extra layers/capacity of the model would be interesting. ******The authors propose a hierarchical GAN setup, called HALI, where they can learn multiple sets of latent variables. In conclusion, this paper needs to flesh out its contributions on the empirical side and position its exact contributions accordingly and improve the attribution.<BRK>The paper incorporated hierarchical representation of complex, reichly structured data to extend the Adversarially Learned Inference (Dumoulin et al.2016) to achieve hierarchical generative model. We provide theoretical arguments for why HALI’s adversarial game should be sufficient to minimize the reconstruction cost and show empirical evidence supporting this perspective. The authors also noted that the introduction of a hierarchy of latent variables can add to the difficulties in the training. Overall, the paper is well written. How does the training time scale with the levels of the hierarchical structure? 3.It seems that in the experimental results, $L$ is at most 2.
Accept (Oral). rating score: 8. rating score: 7. rating score: 7. <BRK>Overall the paper is a clearly written, well described report of several experiments. It shows convincingly that standard NMT models completely break down on both natural "noise" and various types of input perturbations. This study clearly addresses an important issue in NMT and will be of interest to many in the NLP community. The outcome is not entirely surprising (noise hurts and training and the right kind of noise helps) but the impact may be. Or introducing noise "on line" as part of the training? [Response read   thanks]I agree with authors that this paper is suitable for ICLR, although it will clearly be of interest to ACL/MT minded folks.<BRK>What I like about this paper is that:1) The experiments are very carefully designed and thorough. 2) This problem might actually matter. But as the paper shows, it’s easy to make NMT robust to this kind of noise, and Google (and other NMT providers) could do this tomorrow. So this paper could have real world impact. So solving the problem of natural noise is not so simple… it’s a *real* problem. So these methods could be applied in the real world.<BRK>This is a thorough exploration of a mostly under studied problem. The paper is well written and easy to follow. Finally, this paper doesn’t seem to be a perfect fit for ICLR, as it is mostly experimental with few technical contributions that are likely to be impactful; it feels like it might be more at home and have greater impact in a *ACL conference. First of all, errors learned from the noisy data sources are constrained to exist within a word. This tilts the comparison in favour of architectures that retain word boundaries (such as the charCNN system here), while those systems may struggle with other sources of errors such as missing spaces between words. Section 7.2 on the richness of natural noise is extremely interesting, but maybe less so to an ICLR audience.
Accept (Poster). rating score: 8. rating score: 7. rating score: 6. <BRK>In some cases the results are similar to pix to pix (also in the numerical evaluation) but the method allows for one to many image generation, which is a important contribution.<BRK>This paper proposes a compositional nearest neighbors approach to image synthesis, including results on several conditional image generation datasets. Pros:  Simple approach based on nearest neighbors, likely easier to train compared to GANs.<BRK>This paper presents a pixel matching based approach to synthesizing RGB images from input edge or normal maps. The paper mentions making predictions from “incomplete” input several times, but in all experiments, the input is an edge map, normal map, or low resolution image.
Reject. rating score: 4. rating score: 5. rating score: 5. <BRK>2.The whole standpoint of the paper is quite vague and not very convincing. In section 2, the authors introduce angle bias and suggest its effect in MLPs that with random weights, showing that different samples may result in similar output in the second and deeper layers. For the 100 layer MLP, it s very hard to train a simple MLP and the training/testing accuracy is very low for all the methods. In Figure (7), LCW seems to avoid gradient vanishing but introduces gradient exploding problem.<BRK>On the theoretical side, the linearly constrained weights are only shown to work for a very special case. The proposed solution to angle bias is to place a linear constraint such that the sum of the weight becomes zero. Although this does not rule out angle bias in general, it does so for the very special case where the expected value of the random vector is a vector consisting of a common value. Overall, this paper introduces an interesting phenomenon that is worth studying to gain insights into how to train deep neural networks, but the results are rather preliminary both on theory and experiments.<BRK>The authors introduce the concept of angle bias (angle between a weight vector w and input vector x)  by which the resultant pre activation (wx) is biased if ||x|| is non zero or ||w|| is non zero (theorm 2 from the article). While BN explicitly makes the activation zero mean LCW seems to achieve it through constraint on the weight features. Though it is shown to be computationally less expensive LCW seems to work in only specific cases unlike BN.
Reject. rating score: 3. rating score: 4. rating score: 7. <BRK>I suspect that in this case, the results would be very similar. The idea of functional PCA is to view \x as a function is some appropriate Hilbert space, and expands it in some appropriate basis. Unfortunately, beyond rather obvious approximation results, the paper does not get major mileage out of this idea.<BRK>Specifically, the authors propose to extend deep neural networks to the case where hidden layers can be infinite dimensional. This is interesting. This paper fails to consider properly the work in its FDA context. The paper does not clearly provide information about how the functional nature and the infinite dimensional can be handled in practice. 3) Some parts of the paper are hard to read.<BRK>This paper extends the framework of neural networks for finite dimension to the case of infinite dimension setting, called deep function machines. This theory seems to be interesting and might have further potential in applications.
Accept (Poster). rating score: 8. rating score: 7. rating score: 6. <BRK>In this paper, the authors present a novel formulation for learning the optimal architecture of a neural network in a multi task learning framework. The idea is very interesting and the paper is well written.<BRK>Why couldn t these techniques scale to large numbers of routing decisions and task? How could the proposed network in this paper scale? The paper is well organized and the goal of the paper is valuable. REVISEDThank you for adding the comparisons with other work and re writing of the paper for clarity.<BRK>It is proposed to learn the modules with standard back propagation and the controller with reinforcement learning techniques, mostly tabular. Instead routing depends on the task and depth only. In addition it is not clear in this case what network architecture is used for computation of the policy (PG) or valkue (Q learning), and how exactly they are optimized.
Reject. rating score: 4. rating score: 5. rating score: 6. <BRK>The paper addresses a real problem which is meaningful, and provides interesting insights, but it is more of an extension.<BRK>The paper tries to maintain the accuracy of 2bits network, while uses possibly less than 2bits weights. 2.Indeed, AlexNet is a good seedbed to test binary methods. However, it is more interesting and important to test on more advanced networks. 3.The paper wants to find a good trade off on speed and accuracy. The authors have plotted such trade off on space v.s. Is such improvement really important?<BRK>This paper suggests a method for varying the degree of quantization in a neural network during the forward propagation phase. 3.The review of the literature is inaccurate. Also, it is not clear if the scale factors introduced by XNOR Net indeed allowed "a significant improvement over previous work" in ImageNet (e.g., see DoReFA and Hubara et al.who got similar results using binarized weigths and activations on ImageNet without scale factors). Therefore, it is not clear that the proposed methods improve over previous approaches.
Accept (Poster). rating score: 8. rating score: 7. rating score: 6. rating score: 6. <BRK>The authors try to bring in two seemingly different areas and tryto leverage the results in one for another. First authors show that the equivalence of the function realized(intensor form, given in earlier work) by a ConvAC andthe function used to model n body quantum system. This is also validatedexperimentally. Although I do not see major results at this moment, this work can beof great significance. The attempt to bring in two areashave to be appreciated. The paper is lucidly written, comprehensively covering thepreliminaries. I thoroughly enjoyed reading it, and I think thepaper and the work would be of great contribution to the community. (There are some typos  (preform  > perform ))<BRK>A simple example is given in the experiment where the wider layers can be either early in the the neural network or at the later stages; demonstrating that one does better than the other in a certain regime. In summary, this paper is of high theoretical interest and has potential for future applications. The connection has potential to yield fruitful new results, however, the potential is not manifested (yet) in the paper.<BRK>This paper draws an interesting connection between deep neural networks and theories of quantum entanglement. They demonstrated how their theory can help designing neural network architectures on the MNIST dataset. I think the theoretical findings are novel and may contribute to the important problem on understanding neural networks theoretically. I am not familiar with the theory for quantum entanglement though.<BRK>This is not a major concern. The ultimate goal for this work, if I understands correctly, is a provide a theoretical explanation to the design of deep neural architectures. The paper is well written (above most submissions, top 10%) with clear clarity. But the experiments and the conclusion derived from the analysis make the paper not solid to me and I am quite skeptical about its actual effectiveness. The reason as to why authors use ConvAC is that it more resembles the tensor operations introduced in the paper. Based on the derived rule of thumb, the most important layer in MNIST should be layer 3 or 4 (log 10). are seriously needed. I know the paper starts with building the connection from physics to deep learning and it is natural to solve the width design issue alone.
Accept (Poster). rating score: 8. rating score: 7. rating score: 6. <BRK>The paper introduces a non volume preserving generalization of HMC whose transitions are determined by a set of neural network functions. This is admittedly hard to quantify objectively, but just an order of magnitude would be helpful for orientation. The experimental section proves the usefulness of the method on a range of relevant test cases; in addition, an application to a latent variable model is provided sec5.2. inside zeta_2 and zeta_3, do you not mean $m^t" and $\bar{m}^t$ ?<BRK>I think this paper has a good and strong point: this work points out the difficulties in choosing properly the parameters in a HMC method (such as the step and the number of iterations in the leapfrog integrator, for instance). I suggest to compare the proposed method to other efficient methods that do not use the gradient information (in some cases as multimodal posteriors, the use of the gradient information can be counter productive for sampling purposes), such as Multiple Try Metropolis (MTM) schemesL. Martino, J. An adaptive Metropolis algorithm. (4) and (5) are quite complicated; I think a running toy example can help the interested reader.<BRK>The paper proposed a generalized HMC by modifying the leapfrog integrator using neural networks to make the sampler to converge and mix quickly. Mixing is one of the most challenge problems for a MCMC sampler, particularly when there are many modes in a distribution. It showed that the proposed method could mix between the modes in the posterior. Although the method could mix well when applied to those particular experiments, it lacks theoretical justifications why the method could mix well.
Reject. rating score: 4. rating score: 4. rating score: 5. <BRK>Significance:  It is not clear how these results may be applied in practice or open new directions for future theoretical work. Clarity:  The main claims/results in the paper are not stated very clearly, and the authors are not clear about what the contributions of the paper are or why they are useful.<BRK>Unfortunately though, at this point I feel that the theoretical results, which constitute the majority of the paper, are of limited novelty and/or significance. This is not actually true, and portends problematic claims later in the paper.<BRK>The proof of this statement seems relatively straightforward and appears to be correct. Second, the authors argue that the loss function for their simple network has poor local minima. Overall, I found the main idea of the paper relatively straightforward, but the presentation is a bit awkward in places.
Reject. rating score: 5. rating score: 5. rating score: 6. <BRK>The authors present two autoregressive models for sampling action probabilities from a factorized discrete action space. On a multi agent gridworld task and a multi agent multi armed bandit task, the proposed method seems to benefit from their lower variance entropy estimator for exploration bonus. It also seems like a simple, obvious baseline is missing from their experiments   simply independently outputting D independent softmaxes from the policy network. A direct comparison against one of the related methods in the discussion section would help better contextualize the paper as well.<BRK>Originality, novelty and Significance:The paper claims that the approach is novel in the context of policy gradient and Deep RL. The paper under review  should acknowledge this prior work and discuss the similarities and the differences.<BRK>The entropy derivation are more interesting   and the smoothed entropy technique is as far as I know, novel. The experiments are well done, though on simple toy environments. The combined use of the chain rule and RNNs (LSTM or not) to induce correlations in multi dimensional outputs is well know (sequence to sequence networks, pixelRNN, etc.) and the extension to RL presents no difficulties, if it is not already known.
Accept (Poster). rating score: 8. rating score: 7. rating score: 7. <BRK>This paper describes an approach to train a neural machine translation system without parallel data. This is very nice work, and I have very little to criticize. But the aspect I like most about this paper is the experimental analysis. Considering that this is a big, complicated system, it is crucial that the authors included both an ablation experiment to see which pieces were most important, and an experiment that indicates the amount of labeled data that would be required to achieve the same results with a supervised system.<BRK>Although the resulting performance is only about half that of a more traditional model, the fact that this is possible at all is remarkable. It would be interesting to check how the method behaves on really comparable corpora where its advantage would be much clearer. Misc comments:"domain" seems to be used interchangeably with "language". This is unfortunate as "domain" has another, specific meaning in NLP in general and SMT in partiular.<BRK>The results are interesting (but perhaps less interesting than what is hinted in the abstract). 1) In the abstract the authors mention that they achieve a BLEU score of 32.8 but omit the fact that this is only on Multi30K dataset and not on the more standard WMT datasets. This is just a conjecture and it is very hard to measure it. I would like the authors to comment on this (of course, I am not asking you to compare with  the other paper but I am just saying that I have read contradicting observations   one which seems intuitive and the other does not). The caption of one of the figure mentions that there were 10M sentences. 9) There are some missing citations (already pointed by others in the forum) . +++++++++++++++++++++++I have noted the clarifications posted by the authors.
Accept (Poster). rating score: 6. rating score: 6. rating score: 6. <BRK>This paper proposes an idea to do faster RNN inference via skip RNN state updates. Also besides the number of updates reported in table, I think the wall clock time for inference should also be reported, to demonstrate what the paper is trying to claim.<BRK>After going over the additional experiments in the appendix, and I find the three results shown in the main paper seem cherry picked, and it will be good to include more NLP tasks. Most of the experiments in the main paper are on toy tasks with small LSTMs.<BRK>The number of state updates that the model learns to use can be controlled with an auxiliary loss function.
Accept (Poster). rating score: 7. rating score: 6. rating score: 5. <BRK>While the contribution is explored in all its technical complexity, fundamentally this algorithm exploits policies for selfish and prosocial strategies to determine expected rewards in a training phase. The work is contrasted to tit for tat approaches that require complete observability and operate based on expected future rewards. Comments:The findings suggest the effectiveness of that approach. While performing worse than the amTFT approach and only working well for larger number of iterations, the outcome based evaluation shows benefits. This should be explained. Another point relates to the fishing game. What is the bases for these parameter choices? Overall, the paper is well written and explores the technical details of the presented approach.<BRK>The authors could, therefore, provide more detail in relating the contribution to these papers and other relevant past work and existing algorithms. As this is an important component of the proof of the main result, the paper would benefit from an explanation of this step? The paper is also clearly written and the theoretical result is accompanied by some supporting experiments. There are also a number of items that could be added that I believe would strengthen the contribution and novelty, in particular:Some highly relevant references on (prosocial) reward shaping in social dilemmas are missing, such as Babes, Munoz de cote and Littman, 2008 and for the (iterated) prisoner s dilemma; Vassiliades and Christodoulou, 2010 which all provide important background material on the subject.<BRK>The idea is to learn to cooperate (think prisoner s dilemma) but in more complex domains. In this paper, they design an apporach to learn to cooperate in a more complex game, like a hybrid pong meets prisoner s dilemma game. This is fun but I did not find it particularly surprising from a game theoretic or from a deep learning point of view. It is perhaps interesting that one can make deep learning learn to cooperate with imperfect information, but one could have illustrated the game theory equally well with other techniques. In the PPD the only (jointly) winning move is not to play. However, a fullycooperative agent can be exploited by a defector.
Reject. rating score: 4. rating score: 4. rating score: 6. <BRK>Breaking down the problem into an AIR style sequential detection task and a program induction is certainly a reasonable thing to do.<BRK>Overall, the paper is interesting and the proposed method seems reasonable. The method consists of two components.<BRK>The use of the likelihood P_{\theta}[T | I] as a proposal, as the paper also acknowledges, is also unconventional. Inference Compilation and Universal Probabilistic Programming.
Accept (Poster). rating score: 7. rating score: 6. rating score: 5. <BRK>Summary:The paper proposes a mixture of  generators to train GANs. Originality: Using multiple generators for GAN training has been proposed in many previous work that are cited in the paper, the difference in this paper is in weight tying between generators of the mixture, the first layer is kept free for each generator. General review:  when only the first layer is free between generators, I think it is not suitable to talk about multiple generators, but rather it is just a multimodal prior on the z, in this case z is a mixture of Gaussians with learned covariances (the weights of the first layer). see if this also helps? would be good to have some experiments and  see how much we loose for example in term of inception scores, between tied and untied weights of generators.<BRK>MGAN aims to overcome model collapsing problem by mixture generators. Compare to traditional GAN, there is a classifier added to minimax formulation. In training, MGAN is optimized towards minimizing the Jensen Shannon Divergence between mixture distributions from generator and data distribution. The author also present that using MGAN to achive state of art results. The paper is easy to follow. Seems there still no principle to choose correct number of generators but try different setting. 2.Parameter sharing seems is a trick in MGAN model. Could you provide experiment results w/o parameter sharing.<BRK>The present manuscript attempts to address the problem of mode collapse in GANs using a constrained mixture distribution for the generator, and an auxiliary classifier which predicts the source mixture component, plus a loss term which encourages diversity amongst components. All told the proposed method is quite incremental, as mixture GANs/multi generators have been done before. The Inception scores are good but it s widely known now that Inception scores are a deeply flawed measure, and presenting it as the only quantitative measure in a manuscript which makes strong claims about mode collapse unfortunately will not suffice. I ve raised my score to a 5. The mixing proportions are fixed to the uniform distribution, and therefore this method also makes the unrealistic assumption that modes are equiprobable and require an equal amount of modeling capacity. This seems quite dubious. Of course parameter sharing leverages common information. It s fine to include it but indicate it as such.
Accept (Oral). rating score: 9. rating score: 9. rating score: 9. <BRK>In this very good paper, the objective is to perform robust learning: to minimize not only the risk under some distribution P_0, but also against the worst case distribution in a ball around P_0. Since the min max problem is intractable in general, what is actually studied here is a relaxation of the problem: it is possible to give a non convex dual formulation of the problem. If the duality parameter is large enough, the functions become convex given that the initial losses are smooth. What follows are certifiable bounds for the risk for robust  learning and stochastic optimization over a ball of distributions. Experiments show that this performs as expected, and gives a good intuition for the reasons why this occurs: separation lines are  pushed away  from samples, and a margin seems to be increased with this procedure.<BRK>This paper applies recently developed ideas in the literature of robust optimization, in particular distributionally robust optimization with Wasserstein metric, and showed that under this framework for smooth loss functions when not too much robustness is requested, then the resulting optimization problem is of the same difficulty level as the original one (where the adversarial attack is not concerned). Although it only holds when light robustness are imposed, but in practice, this seems to be more of the case than say large deviation/adversary exists. As adversarial training is an important topic for deep learning, I feel this work may lead to promising principled ways for adversarial training.<BRK>This paper proposes a principled methodology to induce distributional robustness in trained neural nets with the purpose of mitigating the impact of adversarial examples. The idea is to train the model to perform well not only with respect to the unknown population distribution, but to perform well on the worst case distribution in some ball around the population distribution. In particular, the authors adopt the Wasserstein distance to define the ambiguity sets. This allows them to use strong duality results from the literature on distributionally robust optimization and express the empirical minimax problem as a regularized ERM with a different cost. The theoretical results in the paper are supported by experiments. Overall, this is a very well written paper that creatively combines a number of interesting ideas to address an important problem.
Reject. rating score: 4. rating score: 6. rating score: 8. <BRK>Summary:This paper proposes to use a GAN to generate goals to implement a form of curriculum learning. What are the goals that the non uniform baselines predict? The authors claim that this model can discover all "goals" in the environment and their  difficulty , which can be measured by the success rate / reward of the policy. Hence the goal network could learn a form of curriculum, where a goal is  good  if it is a state that the policy can reach after a (small) improvement of the current policy. This entire experiment should be explained much more clearly (+image). It is not clear how this method compares qualitatively vs baselines (differences in goals etc). Hence using a manually defined set of goals should work quite well. It would be better to include such a baseline as well. The entire method is quite complicated (e.g.training GANs can be highly unstable). In that case, to discover a good solution, the generated goals should focus on one alternative and hence the GAN should have a unimodal distribution. How do you force the GAN in a principled way to focus on one goal in this case? Are goals overlapping or non overlapping subsets of the state space?<BRK>They use a generator network to propose tasks for the agent accomplish. In addition, the proposed method is also shown to be able to solve tasks with sparse rewards without the need manually modify reward functions. The technical contributions seem sound, however I find it is slightly difficult to fully digest the whole paper without getting the insight from each individual piece and there are some important details missing, as I will elaborate more below. The author should provide more insight for this contribution. One of the assumption is “A policy trained on a sufficient number of goals in some area of the goal space will learn to interpolate to other goals within that area”. 5.For the plots in Figure 2, is there any explanation for the large variance for Goal GAN? Given that the state space is continuous, 10 runs seems not enough.<BRK>In general I find this to be a good paper and vote for acceptance. The paper is well written and easy to follow. Besides that I have not much to say except one point I would like to discuss:In 4.2 I am not fully convinced of using an adversial model for goal generation. This imposes another layer of possible instability. Furthermore, did the authors do experiments with simpler models? I am now more convinced in the quality of the proposed work, and have updated my review score accordingly.
Invite to Workshop Track. rating score: 5. rating score: 5. rating score: 4. <BRK>The results on road network graph is not a strong support for the Seq2Seq model application.<BRK>are they close to optimal? dense parts? Additionally, the experiments are not particularly insightful. Why was this done? How were these chosen? The target for training is very unclear.<BRK>However, due to a number of missing references, unclear description of the method, and limited experimental results I feel that the paper is not ready for publication in its current form. The method is not described in sufficient detail.
Accept (Oral). rating score: 8. rating score: 8. rating score: 8. <BRK>This paper presents a novel application of machine learning using Graph NN s on ASTs to identify incorrect variable usage and predict variable names in context. The paper is to be commended for the following aspects:1) Detailed description of GGNNs and their comparison to LSTMs2) The inclusion of ablation studies to strengthen the analysis of the proposed technique3) Validation on real world software data4) The performance of the technique is reasonable enough to actually be used. 2) How does the approach generalize to other languages? (Presumably well, but something to consider for future work.)<BRK>The paper is generally well written, easy to read and understand, and the results are compelling. Because the tasks are not widely explored in the literature, it could be difficult to know how crucial exploiting graphically structured information is, so the authors performed several ablation studies to analyze  this out. Those results show that as structural information is removed, the GGNN s performance diminishes, as expected. As a demonstration of the usefulness of their approach, the authors ran their model on an unnamed open source project and claimed to find several bugs, at least one of which potentially reduced memory performance. Overall the work is important, original, well executed, and should open new directions for deep learning in program analysis.<BRK>The evaluation is extensive and mostly very good. Nice ablation studies. The model (GGNN) is not particularly novel, but I m not much bothered by that. I agree with your pair of sentences in the conclusion: "Although source code is well understood and studied within other disciplines such as programming language research, it is a relatively new domain for deep learning. I d like to see work in this area encouraged.
Accept (Poster). rating score: 9. rating score: 7. rating score: 7. <BRK>This is a new direction of attacks that opens a whole new dimension of things to consider. It is hard to evaluate this paper since it opens a new direction but the authors do a good job using numerous datasets, CAM attention visualization and also additional materials with high res attacks.<BRK>This paper explores a new way of generating adversarial examples by slightly morphing the image to get misclassified by the model.<BRK>I also don t think FGSM and OPT are this bad in Fig.4.Are the authors sure that if more regularization are used these 2 methods no longer fool the corresponding classifiers? I would likely upgrade to a 7 if those concerns are addressed.
Reject. rating score: 5. rating score: 5. rating score: 5. <BRK>This paper produces word embedding tensors where the third order gives covariate information, via venue or author. It is hard to believe that meaningful results are achieved using such a small dataset with random initialization. If the rank is > 1000 I wonder how meaningful it actually is. I think this is not sufficient. Overall, I believe the idea is nice, and the initial analysis is good, but I think the evaluation, especially against other methods, needs to be stronger. The addition of one or two tables with either a standard task against reported results or created tasks against downloadable contextual / tensor embeddings would be enough for me to change my vote.<BRK>The authors present a method for learning word embeddings from related groups of data. An experimental comparison is needed. Also, why highlight the problem of authorship attribution of Shakespear s work in the introduction, if that problem is not addressed later on? In the model section, the paragraphs "notation" and "objective function and discussion" are clear. In the third paragraph, starting with "Therefore we consider a natural extension of this model, ..." it is unclear which model the authors are referring to. In the last paragraph, beginning with "Note that this is essentially saying...", I don t agree with the argument that the "base embeddings" decompose into independent topics. The clustering by weight (4.1.)<BRK>Section 5.1:  I don t agree with the authors that the topics in Table 3 are interpretable. The authors demonstratethe method on a corpus of books by various authors and on a corpus of subreddits. Some of the experimental resultsdo a good job of demonstrating the advantages of the models. However, some of theexperiments are not obvious that the model is really doing a good job. Additionally, the functionalform chosen for f() in the objective was chosen to match previous work but with noexplanation as to why that s a reasonable form to choose. There is a terminology problem in this section.
Reject. rating score: 4. rating score: 5. rating score: 5. <BRK>This paper introduce a times series prediction model that works in two phases. First learns a deterministic mapping from x to y. And then train another net to predict future frames given the input and residual error from the first network. And does sampling for novel inputs by sampling the residual error collected from the training set. There are many baselines missing.<BRK>The paper proposes a model for prediction under uncertainty where the separate out deterministic component prediction and uncertain component prediction. This low dim latent space is first predicted from the residual of the (deterministic prediction   groundtruth), and then the low dim encoding goes into a network that predicts a corrected image. If the baselines are stronger and this point is more convincing, I am happy to raise my rating of the paper. [1] http://openaccess.thecvf.com/content_ICCV_2017/papers/Marwah_Attentive_Semantic_Video_ICCV_2017_paper.pdf<BRK>I don t fully understand the rationale for the experiments: I cannot speak to the reasons for the GAN s failure (GANs are not easy to train and this seems to be reflected in the results); the newly proposed model seems to improve with samples simply because the evaluation seems to reward the best sample. Comments:The model proposes to learn a conditional stochastic deep model by training an output noise model on the input x_i and the residual y_i   g(x_i). However, I don t think the authors explore the idea well enough   they simply appear to propose a non parametric way of learning the stochastic model (sampling from the training data z_i s) and do not compare to reasonable alternative approaches. If the contribution of the paper is the "output stochastic" noise model, I think it is worth experimenting with the design options one has with such a model. The experiments range over 4 video datasets. The new model "EEN" is compared to a deterministic model and conditional GAN.
Reject. rating score: 4. rating score: 5. rating score: 6. <BRK>This paper presents a variant of auto encoder that relaxes the decoder targets to be neighbors of a data point. Different from original auto encoder, where data point x and the decoder output \hat{x} are forced to be close, the neighbor encoder encourage the decoder output to be similar to the neighbors of the input data point. The authors conduct experiments on several real but relative small scale data sets, and demonstrate the improvements of learned latent representations by using neighbors as targets. Major issues:There are some unaddressed theoretical questions. The data sets used in the experiments  are relatively small and simple, larger scale experiments should be conducted. It is possible other variants improve the basic auto encoder in similar ways. Some results are not very well explained. Some notations are confusing and need to be improved.<BRK>This paper describes a generalization of autoencoders that are trained to reconstruct a close neighbor of its input, instead of merely the input itself. As the authors recognize, there is a long history of research on variants of autoencoders. At the very least, I would expect a comparison with denoising autoencoders, since they are similar if one thinks of the use of neighbors as a structured form of noise added to the input. Finally, I think results would be more impressive and likely to have impact if the authors used datasets that are more commonly used for representation learning, so that a direct performance comparison can be made with previously published results. CIFAR 10 and SVHN would be good alternatives.<BRK>denotes the neighbor(s) of x) is introduced. The underlying idea is interesting, as such, each and every degree of freedom do not synthesize itself similar to the auto encoder setting, but rather synthesize a neighbor, or k neighbors. The authors argue that this form of unsupervised learning is more powerful compared to the standard auto encoder setting, and some preliminary experimental proof is also provided. However, I would argue that this is not a completely abstract   unsupervised representation learning setting since defining what is "a neighbor" and what is "not a neighbor" requires quite a bit of domain knowledge. For instance, in section 4.3, the 40 dimensional feature vector space is used to define neighbors in. All in all, I do like the idea as a concept but I am wary about its applicability to real data where defining a good neighborhood metric might be a major challenge of its own.
Reject. rating score: 5. rating score: 5. rating score: 6. <BRK>This paper is well written and easy to follow. The authors propose pixel deconvolutional layers for convolutional neural networks. More comments and further exploration on this results should be done. Is it because of the residual connection? Or other component of DeepLab? I also think the experiments on VAE are not conclusive.<BRK>Major Weaknesses:  The main weakness of this paper lies in its weak experiments. Authors experimented with two randomly chosen connectivities which is not enough to understand what type of connectivities work best. This is important as this forms the main contribution of the paper. Also, several quantitative results seem incomplete. A quick look at PascalVOC results indicate that DeepLab ResNet has IoU of over 79 on this dataset, but the reported numbers in this paper are only around 73 IoU. There is no mention of IoU for base DeepLab ResNet model and the standard DeepLab+CRF technique. And, there are no quantitative results on image generation.<BRK>By sequentially applying a series of decomposed convolutions, the proposed method explicitly enforces the model to consider the relation between pixels thus effectively improve the deconvolution network with an increased computational cost to some extent. For example, it is well known that simple bilinear interpolation optionally followed by convolutions effectively removes checkboard artifact to some extent, and bilinear additive upsampling proposed in Wonja et al., 2017 also demonstrated its effectiveness as an alternative for deconvolution. Comparisons against these approaches would make the paper stronger. Wonja et al, The Devil is in the Decoder, In BMVC, 2017
Reject. rating score: 5. rating score: 5. rating score: 9. <BRK>Experiments on digit recognition and semantic segmentation verify the effectiveness of the proposed method. As below, I highlight a few points and the authors are referred to the comments by Cedric Nugteren for more suggestions. The technical contribution seems like only marginal innovative. The experiments adapting from MNIST to SVHN would be really interesting, given that the MNIST source domain is not as visually rich as the SVHN target. This paper applies the cycle consistent GAN to domain adaptation. Nonetheless, I do not weigh this point too much given that the experiments are very extensive.<BRK>I believe the authors should have at least done an ablation study to see if the cycle consistency loss truly makes a difference on top of these works that would be the biggest selling point of this paper. Pros:* CycleGANs for domain adaptation! * the authors claim that Bousmalis et al, Liu & Tuzel and Shrivastava et al ahve only been shown to work for small image sizes. Although I agree that is not an ideal validation, I m not sure if it s equivalent or not the authors  validation setting, as they don t describe what that is. As this is the main novelty, I believe this should be in the forefront of the experimental evaluation.<BRK>This is achieved by leveraging the feature and semantic losses to achieve a more realistic image reconstruction. The experiments show that including these additional losses is critical for improving the models performance. The paper is very well written and technical details are well described and motivated. It would be good to identify the cases where the model fails and comment on those. What are the bounds of the domain discrepancy in this case?
Reject. rating score: 6. rating score: 6. rating score: 6. <BRK>The first contribution of the paper is a novel PAC Bayesian risk bound. This risk bound serves as an objective function for multi task machine learning. But since there is no empirical evaluation in this setting, I suggest to adjust the title of the paper. The novel risk bound of the paper is an extension of the bound from [Pentina & Lampert, ICML 2014]. The extension seems to be quite significant. Unlike the bound of [Pentina & Lampert, ICML 2014], the new bound allows to re use many different PAC Bayesian complexity terms that were published previously. But I was less convinced by the empirical experiments. Since the paper improves the risk bound of [Pentina & Lampert, ICML 2014], I expected to see an empirical comparison of LAP and optimization  algorithm from the latter paper. The experiment with multi task learning over MNIST dataset looks interesting, but it is still a toy experiment.<BRK>This is not a big issue, as I accept that the authors are mainly interested in the learning strategy promoted by the bound. * Secondly, and more importantly, I doubt that the uaw of the meta posterior as a distribution over priors for each task is valid. I would like to see the results obtained by another method using five tasks. Equation 4: Please explicitly define m in this context (size of the learning sample drawn from tau). UPDATE  I increased my score after author s rebuttal.<BRK>The author extends existing PAC Bayes bounds to multi task learning, to allow the prior to be adapted across different tasks. Results are evaluated on a toy dataset and a synthetically modified version of MNIST. While this paper is well written and addresses an important topic, there are a few points to be discussed:* Experimental results are really week. I went in appendix to try to see how the proof could be adapted but it’s definitively not as well written as the rest of the paper. In this case it did not. Lifelong learning implies some continual learning and addressing the catastrophic forgetting issues. I would recommend against overuse of the lifelong learning term.
Reject. rating score: 5. rating score: 6. rating score: 6. <BRK>The paper contains several grammatical mistakes, and in my opinion could explain things more clearly, particularly when arguing that the algorithm 2 will converge. Additionally, at some times the authors seem to indicate that the trust region method should be good at escaping from narrow basins (as opposed to avoiding them in the first place), see for example the left plot of figure 4. I don t see why this is true the quadratic approximation would be likely to capture the narrow basin only. This skepticism aside, the experiments in figure 2 do clearly show that, while the proposed approach doesn t converge nearly as quickly as SGD in terms of training loss, it does ultimately find a solution that generalizes better, as long as both SGD and TR use the same batch size (but I don t see why they should be using the same batch size). Section 4.3 (Figure 3) contain a very nice experiment that I think directly explores this issue, and seems to show that SGD with a batch size of 64 generalizes better than TR at any of the considered batch sizes (but not as well as the proposed TR+SGD hybrid). Lemma 1: This looks correct to me, but are these the KKT conditions, which I understand to be first order optimality conditions (these are second order)?<BRK>**I am happy to see some good responses from the authors to my questions. Summary: A new stochastic method based on trust region (TR) is proposed. The main algorithm has not been properly developed; there is too much focus on the convergence aspects of the inner iterations, for which there are many good algorithms already in the optimization literature. There are no good explanations for why the method yields better generalization. Overall, TR seems like an interesting idea, but it has neither been carefully expanded or investigated. We postulate this is because SGD is easy to overfit training data and “stick” to a solution that has a high loss in testing data, especially with the large batch case as the inherent noise cannot push the iterate out of loss valley while our TR method can." [But note here that mini batch SGD is not a closed chapter. 2.TR seems to lose generalization more gracefully than SGD when batch size is increased. are too strong; no evidence is given for this. The writing of the paper needs a lot of improvement. Algorithm 1 is also stated from that thinking and it is a well known optimization algorithm. Trust region methods are generally batch methods. I suggest TR be replaced by "Stochastic TR" everywhere.<BRK>It shows better generation errors by trust region methods than SGD in different tasks, despite slower running time, and the authors speculate that trust region method can escape sharp minima and converge to wide minima and they illustrated that through some hybrid experiment. The paper is organized well. The results are interesting but not quite convincing. At best, this provides a data point in an area that has received attention, but the lack of precision about sharp and wide makes it difficult to know what the more general conclusions are. It is my understanding that Pytorch support higher order derivative both for ReLu and Max pooling. Hence, it is not an explanation for not using ReLu and Max pooling. In section 4.3, the authors claimed that numerical diffentiation only hurts 1 percent error for second derivative. Please provide numerical support. 5.The setting of numerical experiments is not clear, e.g.value of N1 and N2. 5.It s not clear whether this is a theoretical paper or an empirical paper. For example, there is a lot of math, but in Section 4.5 the authors seem to hedge and say "We give an intuitive explanation ... and leave the rigorous analysis to future works."
Reject. rating score: 4. rating score: 5. rating score: 5. <BRK>This paper introduces a skip connection based design of fully connected networks, which is loosely based on learning latent variable tree structure learning via mutual information criteria. Also in terms of experiments, there is not enough exploration of simpler sparse learning methods such as heavy regularization of the weights. As far as I understand, the methods for learning backbone structure and the skip path are performed independently, i.e.there is no end to end training of the structure and parameters of the layers.<BRK>There is a vast literature on structure learning for constructing neural networks (topologies, layers, learning rates, etc.) in an automatic fashion. I am a bit surprised that you have not discussed it in the paper not to mention provided a baseline to compare your method to. Also, without knowing intricate details about each of 17 tasks you mentioned it is really hard to make any judgement as to how significant is improvement coming from your approach.<BRK>Below are some suggestions for improving the paper:Can you enumerate the paper’s contributions and specify the scope of this work? Why is the paper focused on these specific contributions? On the other hand, to distinguish this work from the baselines you mention that this work is the first to apply the method to supervised learning problems. However, it does not adequately motivate the skip path connections or applications of the method to supervised tasks. Or is this work focused on improving the performance of existing methods? Regarding the experimental results, is there any insight on why the dense networks are falling short?
Reject. rating score: 4. rating score: 5. rating score: 5. <BRK>This paper mostly extends Vinyals et al, 2015 paper ("Order Matters") on how to represent sets as input and/or output of a deep architecture. The decoder, on the other hand, is different and relies on a loss that is based on an heuristic to find the current best order (based on an ordering, or mapping W, found using the Gale Shapely algorithm). In particular, the authors have not answered my questionsabout differences with the prior art, and have not provided results onreal data. As far as I understood, the set encoder is the same as the one in "Order Matters".<BRK>SummaryThis paper proposes an autoencoder for sets. The decoder generates the output sequentially and the generated sequenceis matched to the best matching ordering of the target output set. Experiments are done on synthetic datasets to demonstrate properties of thelearned representation. The analysis of how the decoder generates data is insightful. This suggests that while the model is able to representoverall shape, it has a hard time remembering individual elements of the set.<BRK>Summary:This paper proposes an encoder decoder framework for learning latent representations of sets of elements. Comparisons to this baseline will reveal the contribution of the stable matching procedure in the whole  framework of  the set autoencoder for learning representations. 2.The experiments are only evaluated on synthetic datasets, and applications of the set autoencoder to real world applications or scientific problems will make this work more interesting and significant. A strong set autoencoder baseline will be, the encoder employs the neural attention mechanism proposed in (Vinyals et al., ICLR 2016), but the decoder just uses a standard LSTM as in a seq2seq framework.
Reject. rating score: 4. rating score: 5. rating score: 7. <BRK>As the authors stated, an attack that is designed to fool one network does not necessarily fool the other networks in the same way. However, robustness of ensembles to the white box attacks that are generated from the ensemble is still low for FGS. Note that in this case, it is recommended to compare the amount of distortion (perturbation) with different number of iterations in order to indicate the effectiveness of the ensembles to white box BIM attacks. Overall, I found the paper as having several methodological flaws in the experimental part, and rather light in terms of novel ideas. Moreover, experimental setup using a lot of space for comparing results on standard datasets (i.e., MNIST and CIFAR10), even with long presentation of these datasets.<BRK>The defense is evaluated on MNIST and CIFAR10 ans shows reasonable performance against FGSM and BIM. As far as I know, this paper is however the first to empirically study the robustness of ensembles against adversarial examples. Limited analysis of the results. Ensembling neural networks is very costly in terms of training. For this reason, I do not recommend this paper for acceptance.<BRK>In this paper the authors explore their use to increase the robustness of neural networks to adversarial examples. Different ensembles of 10 neural networks are considered. These include techniques such as bagging or injecting noise in the training data. The results obtained show that ensemble methods can sometimes significantly improve the robustness against adversarial examples. The paper is clearly written. I have read the updated version of the paper.
Accept (Poster). rating score: 7. rating score: 6. rating score: 6. <BRK>****Summary:The authors propose an experimental framework and metrics for the quantitative evaluation of disentangling representations. 4.For a paper introducing a formal experimental framework and metrics or evaluation I find that the paper is light on experiments and evaluation.<BRK>The authors consider the metrics for evaluating disentangled representations. A second limitation of the work is the reliance on a "true" set of disentangled factors.<BRK>*Quality* The problem addressed is surely relevant in general terms. Finally, by also taking into account the positive evaluation provided by the fellow reviewers, the rating of the paper has been risen towards acceptance.
Reject. rating score: 5. rating score: 5. rating score: 5. <BRK>Learning deep parsimonious representations. NIPS.Pros:(1) The paper is clearly written. But as pointed out by authors, improvement of generalization performance is not the main focus. Overall, I think the paper is technically sound. I would like to hear authors’ feedback on the issues I raised.<BRK>1.SummaryThe authors of the paper compare the learning of representations in DNNs with Shannons channel coding theory, which deals with reliably sending information through channels. In the reviewers opinion, there is no theoretical connection between DNNs and channel theory. The authors conclude that it is unclear which statistical properties of representations are generally helpful when being strengthened. 2.RemarksShannons channel coding theory was used by the authors to derive regularizers, that manipulate certain statistical properties of representations learned by DNNs.<BRK>All that is derived is that it may be a good idea to penalise large variations in the representation   within class variations, in particular. The uncertainties are over different runs of the algorithm and not over different partitions of the data into training and test sets.
Reject. rating score: 5. rating score: 5. rating score: 7. <BRK>I think it is slightly unfair to say that you are comparing with Xie & Tu, 2015 and Huang et al., 2017 just because they use the CONSTANT weighing schemes. 3.High level technicalI have a few concerns:  Why does AANN+LINEAR nearly match the accuracy of EANN+SIEVE near 3e9 FLOPS in Figure 4b but EANN+LINEAR does not in Figure 4a? Shouldn t EANN+LINEAR be strictly better than AANN+LINEAR? 4. Review summaryUltimately because the model itself resembles previous cascade models, the selected weighings have little justification, and there isn t a comparison with another anytime method, I think this paper isn t yet ready for acceptance at ICLR.<BRK>I do not fully agree with the explanation given for the “alternating weights”. There are few baselines compared in the result section. In addition, the proposed method underperforms the MSDNet (Huang et al., 2017) on ILSVRC2012. The EANN is similar to the method used by Adaptive Networks (Bolukbasi et al., 2017), and the baseline “Ensemble of ResNets (varying depth)” in the MSDNet paper.<BRK>To achieve that, the model includes auxiliary predictions which can make early predictions. In the experiments, test error of the proposed model was shown to be comparable to the optimal one at each time budget. It is an interesting idea to add auxiliary predictions to enable early predictions and the experimental results look promising as they are close to optimal at each time budget. The parallel training is not clear to me and it would be great to have more explanation on this with examples. 3.In the experiments, it would be great to add a few alternatives to be compared for anytime predictions.
Reject. rating score: 4. rating score: 4. rating score: 5. <BRK>SummaryThis paper proposes a penalized VAE training objection for the purpose of increasing the information between the data x and latent code z. What about the Beta VAE? The first is that I would like to see how well the mutual information is being estimated during training. The second reason is the paper’s weak experimental section. Questions I would like to see answered: How good is the MI estimate?<BRK>I really enjoyed reading the paper, the proposed approach is well motivated and clearly described. However, the experiments section is very weak. Further, the authors do not consider a more rigorous benchmark including additional datasets and state of the art modelling approaches for text.<BRK>Overall, the paper is well written. The problem that VAEs fail to learn a meaningful representation is a well known issue. This paper presents a simple, yet principled modification to the VAE objective to address this problem. (And this also adds an additional term to the gradient wrt \theta). I wonder if the authors have explored this idea.
Reject. rating score: 3. rating score: 4. rating score: 6. <BRK>This paper describes a method for improving a goal oriented dialogue system using selfplay. I found it very difficult to work out what the contribution of this paper is over previous work, particularly the Lewis et al.(2017) paper that they cite.<BRK>Summary: The paper proposes a self play model for goal oriented dialog generation, aiming to enforce a stronger coupling between the task reward and the language model. One iteration or one epoch of supervised training?<BRK>After all, reward seems to play a very important role for the proposed system. In fact in this paper even more dialogue modules are coupled. 500k of data is quite some. There are papers that are goal oriented and have many turns.
Accept (Poster). rating score: 7. rating score: 6. rating score: 6. <BRK>This paper gives an explicit proof when the function in question is a sparse polynomial, ie: a polynomial in n variables, which equals a sum J of monomials of degree at most c. In this setup, Theorem 4.3 says that a shallow network need at least ~ (1 + c/n)^n many neurons, while the optimal deep network (whose depth is optimized to approximate this particular input polynomial) needs at most  ~ J*n, that is, linear in the number of terms and the number of variables. The paper also has bounds for neural networks of a specified depth k (Theorem 5.1), and the authors conjecture this bound to be tight (Conjecture 5.2). Overall, I like the paper. Comments and clarifications:* What about the ability of a single neural network to approximate a class of functions (instead of a single p), where the topology is fixed but the network weights are allowed to vary? Could you comment on this problem?<BRK>Despite these criticisms, I contend that the significance of the problem, and the clean and understandable results in the paper make it a decent paper for ICLR. +ves:Explaining the power of depth in NNs is fundamental to an understanding of deep learning. and the proofs are clearly written. The theorems provide exponential gaps for very simple polynomial functions.<BRK>It shows exponential gaps between the width of shallow and deep networks required for approximating a given sparse polynomial. By focusing on polynomials, the paper is able to use of a variety of tools (e.g.linear algebra) to investigate the representation question. The idea being to construct a superposition of Taylor approximations of the individual monomials. Also, in the discussion of Taylor series, it would be good to mention the point around which the series is developed, e.g.the origin. In page 1 ```existence proofs  without explicit constructions  This is not true, with numerous papers providing explicit constructions of functions that are representable by neural networks with specific types of activation functions.
Reject. rating score: 5. rating score: 5. rating score: 5. <BRK>The paper presents interesting algorithms for minimizing softmax with many classes. The objective function is a multi class classification problem (using softmax loss) and with linear model. It seems the implicit SGD approach is better in the experimental comparisons. I found the paper quite interesting, but meanwhile I have the following comments and questions:   As pointed out by the authors, the idea of this formulation and doubly SGD is not new. I think most first order methods instead of Newton will have linear converge (log(1/epsilon) time)? For example, implicit SGD has another iterative solver for each update. Therefore, the timing comparison is needed in this paper to justify that implicit SGD is faster. The presentation can be improved.<BRK>It would have been helpful if the author(s) could have made a formal statement. Since the main contributions are two algorithms for stable SGD it is not clear how one can formally say that they are stable. For this a formal problem statement is necessary. The title is also misleading in using the word "exact". I have understand it correct the proposed SGD method solves the optimization problem to an additive error. In summary the algorithms are novel variants of SGD but the associated claims of numerical stability and speed of convergence vis a vis existing methods are missing.<BRK>The key idea is to reformulate the problem as a convex minimization of a "double sum" structure via a simple conjugation trick. The main contributions of this paper are: "U max" idea (for numerical stability reasons) and an ""proposing an "implicit SGD" idea. I believe this was explained in the paper. I will stress though that the statement about Newton in the paper is not justified. Cubic regularisation is needed for global convergence. I believe the paper could warrant acceptance if all criticism raised by reviewer 2 is addressed. I apologise for short and late review: I got access to the paper only after the original review deadline.
Accept (Poster). rating score: 7. rating score: 7. rating score: 6. <BRK>This paper is about low precision training for ConvNets. It proposed a "dynamic fixed point" scheme that shares the exponent part for a tensor, and developed procedures to do NN computing with this format. The proposed method is shown to achieve matching performance against their FP32 counter parts with the same number of training iterations on several state of the art ConvNets architectures on Imagenet 1K. According to the paper, this is the first time such kind of performance are demonstrated for limited precision training.<BRK>The paper is written clearly and the English is fine. This paper describes an implementation of reduced precision deep learning using a 16 bit integer representation. In this work, the authors show that a careful implementation of mixed precision dynamic fixed point computation can achieve SOTA on 4 large networks on the ImageNET 1K datasets. Using a INT16 (as opposed to FP16) has the advantage of enabling the use of new SIMD mul acc instructions such as QVNNI16.<BRK>This work presents a CNN training setup that uses half precision implementation that can get 2X speedup for training. The work is clearly presented and the evaluations seem convincing. The presented implementations are competitive in terms of accuracy, when compared to the FP32 representation.
Invite to Workshop Track. rating score: 7. rating score: 5. rating score: 5. <BRK>The authors point out that the original "value iteration network” (Tamar 2016) did not handle non stationary dynamics models or variable size problems well and propose a new formulation to extend the model to this case which they call a value propagation network. The work is therefore original and significant. The authors Prop and MVProp are able to solve the grids with much higher reliability at the end of training and converge much faster. The M in MVProp in particular seems to be very useful in scaling up to the large grids. The authors also show that the algorithm handles non stationary dynamics in an avalanche task where obstacles can fall over time. These are given by the domain?<BRK>The paper introduces two alternatives to value iteration network (VIN) proposed by Tamar et al.VIN was proposed to tackle the task of learning to plan using as inputs a position and an image of the map of the environment. The approaches are evaluated in grid worlds with and without other agents. It seems this is not the case for VProp and MVProp, given the gradient updates provided in p.5. Is that correct? I think the writing needs to be improved on the following points:  The abstract doesn t fit well the content of the paper. For instance, "its variants" is confusing because there is only other variant to VProp. The authors should recall more thoroughly and precisely the work of Tamar et al., on which their work is based to make the paper more self contained, e.g., (1) is hardly understandable.<BRK>However, I just don t think the focus on 2D grid based navigation has sufficient interest and impact. It s true that the original VIN paper worked in a grid navigation domain, but they also had a domain with a fairly different structure;  I believe they used the gridworld because it was a convenient initial test case, but not because of its inherent value. So, making improvements to help solve grid worlds better is not so motivating. It may be possible to motivate and demonstrate the methods of this paper in other domains, however. The work on dynamic environments was an interesting step:  it would have been interesting to see how the "models" learned for the dynamic environments differed from those for static environments.
Accept (Poster). rating score: 7. rating score: 7. rating score: 6. <BRK>Quality:The paper appears to be correctClarity:the paper is clear, although more formalization would help sometimesOriginalityThe paper presents an analysis for unsupervised learning of mapping between 2 domains that is totally new as far as I know. A large set of experiments are provided to support the claims of the paper.<BRK>The paper addresses the problem of learning mappings between different domains without any supervision. GAN are sufficient to learn « semantic mappings » in an unsupervised way, if the considered networks are small enough2. The introduced notion is very simple and intuitive and is defined as  the depth of a network  which is necessary to  implement the considered function. The paper  comes with a series of experiments to empirically « demonstrate » the conjectures. The paper is well written. How does this affect the paper statements and results? and their spaces (D_A, D_B, D_Z) would greatly help the understanding. I thank them for that. My scores were changed accrodingly.<BRK>The paper shows that is possible to learn the between domains mapping using GAN only without a reconstruction (cyclic) loss. I did not get the time to go through the proofs, but they handle the fully connected case as far as I understand. Overall it is an interesting  but long paper, the claims are a bit strong for CNN and need further theoretical and experimental verification. Pros:Important and challenging topic to analyze and any progress on unsupervised learning is interesting. is this intuition correct?
Reject. rating score: 4. rating score: 5. rating score: 7. <BRK>The paper proposes an idea to integrate simulation based planning into a neural network. In my point of view, the efficiency of the proposed framework is also questionable. It looks like after training MCTSnet with a massive amount of data from another MCTS, MTCSnet algorithm as in Algorithm 2 will not do very much more planning yet. Is it the reason why in experiments M is always small and increasing it does not make a huge improvement?. This supervising step is similar to one previous work [1] and not mentioned in the paper. If M is large, the tree maintained is huge too. It might be not correct, then I am curious how this technical aspect is handled by MCTSnet. Other questions: + how the value network used in the MCTS in section 3.5 is constructed? + is the action distribution from the root memory p_{\theta}(a|s)? al.Deep Learning for Real Time Atari Game Play Using Offline Monte Carlo Tree Search Planning, NIPS 2014[2] Guez et.<BRK>From gold standard state action pairs, it learns each component of this architecture in order to predict similar actions. I enjoyed reading this paper. The main flaw of the paper is in its experiments. If I understand them correctly, the comparison is between a neural network that has been learned on 250,000 trajectories of 60 steps each where each step is decided by a ground truth close to optimal algorithm, say MCTS with 1000 rollouts (is this mentioned in the paper). I suspect that generating the training data and learning the model takes an enormous amount of CPU time, while 25 MCTS rollouts can probably be done in a second or two. Or one that uses the value function that was learned with classic search? I suspect that the O(N^2) complexity is very prohibitive and will not allow this to scale up? I m a bit worried about the idea of learning to trade off exploration and exploitation. This seems risky, and I suspect that UCB and more statistically principled approaches would be more robust in this regard? Are these Sokoban puzzles easy for classical AI techniques? It would be fair to discuss this. The last sentence of conclusions is too far reaching; there is really no evidence for that claim.<BRK>The authors introduce an approach for adding learning to search capability to Monte Carlo tree search. The computation of the network proceeds just like a simulation of MCTS, but using a simulation policy based on the memory vector to initialize the memory vector at the leaf. The proposed method allows each component of MCTS to be rich and learnable, and allows the joint training of the evaluation network, backup network, and simulation policy in optimizing the MCTS network. The paper is thorough and well explained. My only complaint is the evaluation is only done on one domain, Sokoban.
Accept (Poster). rating score: 7. rating score: 7. rating score: 6. <BRK>This paper consolidates and builds on recent work on adversarial examples and adversarial training for image classification. Its contributions:   Making the connection between adversarial training and robust optimization more explicit. (This comment does not affect my opinion about whether or not the paper should be accepted, and is merely a suggestion for the authors.) It would be very helpful to add some discussion of this.<BRK>The authors show empirically that PGD finds adversarial examples with very similar loss values on multiple runs. Many previous works on dealing with uncertain inputs in classification apply this minimax approach using robust optimization, e.g.: https://www2.eecs.berkeley.edu/Pubs/TechRpts/2003/CSD 03 1279.pdfhttp://www.jmlr.org/papers/volume13/ben tal12a/ben tal12a.pdfIn the case of convex uncertainty sets, many of these problems can be solved efficiently to a global minimum. Generalization bounds on the adversarial losses can also be proved. This could make the robustness of the network rather dependent on the specific implementation of PGD for the inner maximization problem.<BRK>This paper proposes to look at making neural networks resistant to adversarial loss through the framework of saddle point problems. They show that, on MNIST, a PGD adversary fits this framework and allows the authors to train very robust models. They also show encouraging results for robust CIFAR 10 models, but with still much room for improvement. The contributions in your appendix are interesting.
Invite to Workshop Track. rating score: 9. rating score: 6. rating score: 5. rating score: 3. <BRK>The paper is very clearly written, very clearly presented, addresses an important issue, and the results are solid. More info on this throughout the paper would be a valuable contribution. When does it not work? I guess the most accurate opposite would have been "The service is quick but not good"... )I really like the reverse perplexity measure. Also, it was interesting how that was found to be high on AAE due to mode collapse. p5, beginning of Section 6.1:  "to regularize the model produce"  > "to regularize the model to produce" ? "is quite high for the ARAE than in the case"  > quite a bit higher than? In the words of one of the paper s own examples: "It has a great atmosphere, with wonderful service."<BRK>the paper presents a way to encode discrete distributions which is a challenging problem. it ll interesting to see how the proposed approach gets around this issue. 2. the second question is related. it is unclear how the optimal distribution would look like with the latent variable gan. ideally, the discrete encoding be simply a discrete approximation of the continuous encoding. but optimization with two latent distributions and one discriminator can be hard. what we get in practice is pretty unclear.<BRK>The empirical work is somewhat compelling, though I am not an expert in thistask domain. It is not at all  clear to me why this would be the case. "One benefit of the ARAE framework is that it compresses the input to a  single code vector." It would be worth explaining, in a sentence, the approach in Shen et al for  those who are not familiar with it, seeing as it is used as a baseline. Shortly  thereafter we are told that the generator quickly learns to produce norm 1  outputs as evidence that it is matching the encoder s distribution, but this  is something that could have just as easily have been built in, and is a  trivial sort of "distribution matching"  In general, tables that report averages would do well to report error bars as  well. Spherical interpolation as recommended by White (2016) may  improve qualitative results.<BRK>The proposed method jointly trains an RNN encoder with a GAN to produce latent representations which are designed to better encode similarity in the discrete input space. A variety of experiments are conducted that demonstrate the efficacy of the proposed methodology. The generated textual samples look good and offer strong support for the model. However, I would have preferred to see more quantitative evaluation and less qualitative evaluation, but I understand that doing so is challenging in this domain. I will refrain from adding additional detailed commentary in this review because I am unable to judge this paper fairly with respect to other submissions owing to its large deviation from the suggested length limits. In fact, there are obvious places where the exposition is excessively verbose, and there are clear opportunities to reduce the length of the submission. While I fully understand that the length suggestions are not requirements, in my opinion this paper did not make an adequate effort to abide by these suggestions. As such, I find it difficult or impossible to judge this paper fairly relative to other submissions. I regrettably cannot recommend this paper for acceptance owing to these concerns.
Accept (Poster). rating score: 7. rating score: 7. rating score: 7. <BRK>The authors propose to use a generative model of images to detect and defend against adverarial examples. The design choice of PixelCNN, which allows for a greedy optimization seems reasonable in that setting. Thus, in reality, the experiments show that the pipeline generative model + classifier is robust against the strongest white box methods for this classifier, but on the other hand these methods do not transfer well to new models. At a high level, the message is that projecting the image into a high density region is sufficient to correct for a significant portions of the mistakes made on adversarial examples.<BRK>The paper describes the creative application of a density estimation model to clean up adversarial examples before applying and image model (for classification, in this setup). The proposed method is very intuitive, but might be expensive if a naive implementation of PixelCNN is used for the cleaning. The approach is novel. Also, it might even be trained on a different dataset potentially. The con is that the proposed methodology still does not solve the problem of adversarial examples completely.<BRK>That said, it would be interesting to see the *training* data on this plot as well to see if there are any systematic shifts that might make the distribution of adversarial examples less discernible. I would like to see individual examples of (a) adversarial perturbation for a given image and (b) PixelDefend perturbation for that adversarial image. It is not clear from the figure if the  clean  (nor the other data for that matter) is from the *training* or  *testing* data for the PixelCNN model. The authors demonstrate some success in restoring images that have been adversarially perturbed with this technique.
Reject. rating score: 4. rating score: 5. rating score: 7. <BRK>Visual concepts were introduced in Wang et al.2015, which are clustering centers of feature vectors in a lattice of a CNN. For a given image, its visual concept embedding is computed by thresholding the distances between feature vectors in the lattice of the image to the visual concepts. Experiments are conducted on the Mini ImageNet dataset and the PASCAL3D+ dataset for few shot learning. Negatives:  The novelty of the paper is limited. The two baseline methods for few shot learning provide limited insights in solving the few shot learning problem. The paper uses a hard thresholding  in the visual concept embedding. It would be interesting to see the performance of other strategies in computing the embedding, such as directly using the distances without thresholding.<BRK>Therefore, I rate the current manuscript as a reject. This is crucial in order to follow / judge the rest of the paper. Still I give it a try. The VCs are introduced for few shot classification, unclear how this is different from "previous few shot methods" (sect 5). What is the influence of the layer on the performance? The used Visual Concepts (VCs) were already introduced by other works (Wangt 15), and is not a novelty. Also the authors refer to another paper (about using VCs for detection) which is also under submission (somewhere).<BRK>The paper adds few operations after the pipeline for obtaining visual concepts from CNN as proposed by Wang et al.(2015).This latter paper showed how to extract from a CNN some clustered representations of the features of the internal layers of the network, working on a large training dataset. The results a are convincing, even if they are not state of the art in all the trials. Few comments:The authors in the abstract should revise their claims, too strong with respect to a literature field which has done many advancements on the cnn interpretation (see all the literature of Andrea Vedaldi) and the literature on zero shot learning, transfer learning, domain adaptation and fine tuning in general.
Invite to Workshop Track. rating score: 7. rating score: 6. rating score: 5. <BRK>I do think the paper would benefit from experimental results, but agree with the authors that the theoretical results are non trivial and interesting on their own merit. The paper presents a theoretical analysis of depth in RNNs (technically a variant called RACs) i.e.stacking RNNs on top of one another, so that h_t^l (i.e.hidden state at time t and layer l is a function of h_t^{l 1} and h_{t 1}^{l})The work is inspired by previous results for feed forward nets and CNNs. However, what is unique to RNNs is their ability to model long term dependencies across time. Therefore sep_{S,E}(y) is the rank of a particular matricization of y (with respect to the partition S,E). Why is Theorem 1 not a function of L? What makes this more challenging? I feel if comparing L 2 vs L 3 is hard, the authors should be more up front about that in the introduction/abstract. (2) I think it would have been stronger if the authors would have provided some empirical results validating their claims.<BRK>The paper proposes to use the start end rank to measure the long term dependency in RNNs. The theory part seems to be technical enough and interesting, though I haven t checked all the details. The main concern with the paper is that I am not sure whether the RAC studied by the paper is realistic enough for practice. The paper will be much stronger if it has some experiments along this line.<BRK>This paper investigates an effect of time dependencies in a specific type of RNN. The idea is important and this paper seems sound. However, I am not sure that the main result (Theorem 1) explains an effect of depth sufficiently. I think that this paper should quantify the effect of an increase of $L$. Sub commentNumerical experiments for calculating the separation rank is necessary to provide evidence of the main result. Only a simple example will make this paper more convincing.
Accept (Poster). rating score: 6. rating score: 6. rating score: 5. <BRK>Although using tensors to capture high order interaction and performing dimension reduction over that are both not novel, the paper explores them for NLI. The ablation experiments in Table 5 give a good level of details to help observe different components  effectiveness. With regard to MultiNLI, since the previous results (e.g., those in Table 2) did not use cross sentence attention and had to represent a premise or a hypothesis as a *fixed length* vector, is it fair to compare DIIN with them? Note that the proposed DIIN model does represent a premise or a hypothesis by variable lengths (see interaction layer in Figure 1), and tensors provide some sorts of attention between them. Can this (Table 2) really shows the advantage of the proposed models? However, when a variable length representation is allowed (see Table 3 on SNLI), the advantage of the model is also not observed, with no improvement as a single model (compared with ESIM) and being almost same as previous models (e.g., model 18 in Table 3) in ensembling.<BRK>Thank you for this paper! Hence, there is not a direct analysis of what s inside the interaction tensors. This is the major limitation of the study. According to this analysis, DIIN seems to be a very good paraphrase detector and word aligner. It seems also that examples where rules are necessary are not correctly modeled by DIIN: this is shown by the poor result on Conditional and Active Passive.<BRK>This paper proposes Densely Interactive Inference Network to solve recognizing textual entailment via extracting a semantic feature from interaction tensor end to end. Their results show that this model has better performance than others. Encoding layer in section 3.2, there is no motivation why it needs to use fuse gate. The authors imply on page 2, end of paragraph 5,  that this is the first work that shows attention weight contains rich semantic and previous works are used attention merely as a medium for alignment. Referring to the some of the related works (cited in this paper), I am not sure this is a correct statement. The authors claim to introduce a new class of architectures for NLI and generability of for this problem.
Accept (Poster). rating score: 8. rating score: 7. rating score: 6. <BRK>The proposed method can reduce more than 90% memory consumption while keeping original model accuracy in both the sentiment analysis task and the machine translation tasks. The motivation is clear, the idea and approaches look suitable and the results clearly follow the motivation. I think it is better to clarify in the paper that the proposed method can reduce only the complexity of the input embedding layer. For example, the model does not guarantee to be able to convert resulting "indices" to actual words (i.e., there are multiple words that have completely same indices, such as rows 4 and 6 in Table 5), and also there is no trivial method to restore the original indices from the composite vector.<BRK>It may also be interesting to provide suitable theoretical analysis, e.g., relationships with the SVD of the embedding matrix. This paper is well written and easy to follow. The motivation is clear and the idea is simple and effective. The proposed method achieved compression rate 98% in a sentiment analysis task, and compression rate over 94% in machine translation tasks, without a performance loss.<BRK>This paper presents an interesting idea to word embeddings that it combines a few base vectors to generate new word embeddings. It also adopts an interesting multicodebook approach for encoding than binary embeddings. The paper presents the proposed approach to a few NLP problems and have shown that this is able to significant reduce the size, increase compression ratio, and still achieved good accuracy. The experiments are convincing and solid.
Accept (Poster). rating score: 7. rating score: 6. rating score: 4. <BRK>The paper proposes a novel architecture for capsule networks. This new representation comes with a novel iterative routing scheme, based on the EM algorithm. Capsule networks have recently gained attention from the community. Clarity could be improved in some parts of the paper (e.g.: Sec.1.1 may not be fully clear if the reader is not already familiar with (Sabour et al., 2017); the authors could give a better intuition about what is kept and what is discarded, and why, from that approach. Sec.2: the sentence "this is incorrect because the transformation matrix..." could be elaborated more. The authors could discuss in more detail why the approach does not show significant improvement on NORB with respect to the state of the art. The authors could provide more insights about why capsule gradients are smaller than CNN ones.<BRK>This paper proposes a new kind of capsules for CNN. The capsule contains a 4x4 pose matrix motivated by 3D geometric transformations describing the relationship between the viewer and the object (parts). An EM type of algorithm is used to compute the routing. The authors use the smallNORB dataset as an example. It would be more beneficial to know if this kind of capsules is limited to the motivation or is general. For example, the authors may consider reporting the results of the affNIST dataset where the digits undergo 2D affine transformations (in which case perhaps 3x3 pose matrices are enough?).<BRK>The paper describes another instantiation of "capsules" which attempt to learn part whole relationships and the geometric pose transformations between them. Results are presented on the smallNORB test set obtaining impressive performance. Although I like very much this overall approach, this particular paper is so opaquely written that it is difficult to understand exactly what was done and how the network works. It sounds like the main innovation here is using a 4x4 matrix for the pose parameters, and an iterative EM algorithm to find the correspondence between capsules (routing by agreement). I think the authors could do a much better job explaining this model, the rationale behind it, and how it works. Perhaps the most interesting and compelling result is Figure 2, which shows how ambiguity in object class assignment is resolved with each iteration. Although the results are impressive, if one can t understand how this was achieved it is hard to know what to make of it.
Reject. rating score: 2. rating score: 4. rating score: 5. <BRK>Aren t you trying to show that CR can find the "correct" learning rate? Also, what if the range of \sigma values that need to be considered is larger than the range of \alpha values? There are many issues. The scope of the experiments is limited because only a single network architecture is considered, and it is not a state of the art architecture (no convolution, no normalization mechanism, no skip connections). Also note that the low rank structure of deep gradients is well known and not a contribution of this paper. You say in the text that they were the least favorable for CR. You only demonstrate superior results when the weights are badly initialized.<BRK>[Main comments]* The authors made a really odd choice of notation, which made the equations hard to follow. * The function f that the authors differentiate is not even defined in the main manuscript! * The low rank structure they describe only holds for a single sample at a time. * Introducing cubic regularization seems interesting. The present paper has neither (the empirical evidence shown is very limited). It is an online algorithm that became popular in the DL community later on.<BRK>Comments: The idea of showing low rank structure which makes it possible to use second order information without approximations is interesting. This feedforward network with ReLU activation, output softmax and cross entropy loss is well known structure for neural networks. There is no gain on using CR with Adam as you mention in Discussion part of the paper. The author(s) should do more experiments to various dataset to be more convincing. And if possible, please also consider CNN even if you are not able to provide any theory.
Reject. rating score: 4. rating score: 4. rating score: 5. <BRK>2) None of the layers is degenerate. I have read the rebuttal and still not convinced. The claim that this paper is for unsupervised semantic segmentation is overblown. This was advertised in the paper, but the loss function is only on the final product G_K.<BRK>This paper proposes a neural network architecture around the idea of layered scene composition. The idea is interesting and different from established approaches to segmentation. This is far below the norm for semantic segmentation work in computer vision. An additional problem is that performance is not compared to any external prior work. The range of prior work on semantic segmentation is extensive.<BRK>Paper Strengths:+ The idea of the paper is interesting.
Invite to Workshop Track. rating score: 7. rating score: 5. rating score: 4. <BRK>The authors present a study of priors employed by humans in playing videogames   with a view to providing some direction for RL agents to be morehuman like in their behaviour. The results of the experiments, conducted using AMT participants, demonstratesthe existence of a taxonomy of features that affect the ability to completetasks in the game to varying degrees. The paper is clearly written, and the experiments follow a clean and coherentnarrative. I understand that at least one such experiment is shown in Figure 1 which   involves consistent semantics, but it would be quite interesting to see how   RL agents perform when this consistency is taken away. If not, is it the   total number of states/frames seen? Given recent advances in RL and ML that eschew all manner of structuredrepresentations, I believe this is a well timed reminder that being able totransfer know how from human behaviour to artificially intelligent ones.<BRK>Here without semantic priors I would hypothesize that human performance would fall quite far (whereas with semantics people would be able to figure it out quite well). Thus, I think the authors’ claim needs to be qualified quite a bit. I have some reservations about the execution of the experiments as well as some of the conclusions drawn. Paper Summary:For RL to play video games, it has to play many many many many times. The authors study, using experiments, what aspects of human priors are the important parts. However, I am not completely convinced by some of the experimental demonstrations. There are only 30 participants per condition and so it’s hard to tell whether the large differences in conditions are due to noise and what a stable ranking of conditions actually looks like. This would give at least some proxy to study the importance of priors about “how video games are generally constructed” rather than priors like “objects are special”. There is no real evidence here for the strong version of the claim. Thus, we could consider a weaker version of the claim: semantic priors are important but even in the absence of explicit semantic cues (note, this is different from having the wrong semantic cues as above) people can do a good job on the game.<BRK>This paper investigates human priors for playing video games. Considering a simple video game, where an agent receives a reward when she completes a game board, this paper starts by stating that: 	Firstly, the humans perform better than an RL agent to complete the game board. Unfortunately, I have other concerns about the method and the conclusions. For instance, masking where objects are or suppressing visual similarity between similar objects should also deteriorate the performance of a RL agent. So it cannot be concluded that the change of performances is due to human priors. The authors have to include RL agent in all their experiments to be able to dissociate what is due to human priors and what is due to the noise introduced in the game.
Accept (Poster). rating score: 9. rating score: 7. rating score: 4. <BRK>This paper proposes a simple modification to the standard alternating stochastic gradient method for GAN training, which stabilizes training, by adding a prediction step. This is a clever and useful idea, and the paper is very well written. I particularly liked the analogy with the damped harmonic oscillator. The experiments are well designed and provide clear evidence in favor of the usefulness of the proposed technique. I believe that the method proposed in this paper will have a significant impact in the area of GAN training.<BRK>This work proposes a framework for stabilizing adversarial nets using a prediction step. Then this prediction step is applied in many recent applications in training adversarial nets and compared with state of the art solvers. The better performance of this simple step is shown in most of the numerical experiments. Though this work applies one step from the convex optimization to solve a more complicated problem and obtain improved performance,  there is more work to be done.<BRK>NOTE:I m very willing to change my recommendation if I turn out to be wrong about the issues I m addressing and if certain parts of the experiments are fixed. This is not generally true for GANs. [1] contains an explanation of this. Finally, I m not totally sure you can show that simultaneous gradient descent won t converge as well under the assumptions you made. In fact, it seems like both evaluated Stacked GAN models get worse performance with the higher learning rate. b) I had a lot of issues with the GAN experiments already and I don t think the paper should be accepted unless those are addressed.
Accept (Poster). rating score: 7. rating score: 6. rating score: 6. <BRK>In this paper a modular architecture is proposed with the aim of separating environment specific (dynamics) knowledge and task specific knowledge into different modules.<BRK>I think reading the paper, it should be much clearer how the embedding is computed for Atari, and how this choice was made.<BRK>( If the goal state s’ was just the next state, then this would just be a dynamics model and this would be model based learning?So I assume there are multiple steps between s and s’?).
Reject. rating score: 3. rating score: 4. rating score: 6. <BRK>In terms of substance, the experiments don t really add much value in terms of general lessons. This is a great idea and could be a strong paper, but it s really hard to glean useful recommendations from this for several reasons:  The writing of the paper makes it hard to understand exactly what s being compared and evaluated. All in all this paper reads like a tech report but not a conference publication.<BRK>This paper presents three different techniques for model specialization, i.e.adapting a pretrained network to a more specific task and reduce its computational cost while maintaining the performance. There are some spelling errors, for instance in the beginning of section 4.1  Pruning does not seem to produce much speed up. The experimental part is difficult to read.<BRK>This will make the paper more valuable to the community. The speedups and accuracy gains of this paper are impressive.
Accept (Oral). rating score: 9. rating score: 8. rating score: 7. <BRK>Conclusion  I think this paper would be a very worthy contribution to ICLR. Learning to adapt on the basis of few observations is an important prerequisite for real world agents, and this paper presents a reasonable approach backed up by a suite of informative evaluations. The spider spider results in Fig.6 do not support the argument that meta learning is better than PPO tracking in the few shot regime.<BRK>This is a dense, rich, and impressive paper on rapid meta learning. In particular, it would be good to explicitly name the “meta loss” (currently the unnamed triple expectation in (3)).<BRK>This paper proposed a gradient based meta learning approach for continuous adaptation in nonstationary and adversarial environment. P10, “This suggests that it meta learned a particular…” This sentence need to be rewritten. P10, ELO is undefined
Accept (Oral). rating score: 8. rating score: 8. rating score: 10. <BRK>The key insight here is that solving an optimization problem on an MDP of a single agent is equivalent to solving the inference problem of the (population level) MFG. In general, details are missing about how the VAR and RNN were run. I have found the paper to be interesting, and, although I am not an expert in MFGs, novel and well articulated.<BRK>However, this claim is far from mainstream, and is in fact highly contested. At the same time, the scientific content of the work has critical conceptual flaws.<BRK>The paper proposes a novel approach on estimating the parameters  of Mean field games (MFG). I think that the general discussion about the collective behavior application should be more carefully presented and some better examples of applications should be easy to provide.
Reject. rating score: 4. rating score: 5. rating score: 7. <BRK>This paper proposes to automatically recognize domain names as malicious or benign by deep networks (convnets and RNNs) trained to directly classify the character sequence as such. ProsThe paper addresses an important application of deep networks, comparing the performance of a variety of different types of model architectures. The tested networks seem to perform reasonably well on the task. While this paper addresses an important problem, in its current form the novelty and analysis are limited and the paper has some presentation issues.<BRK>The baseline is random forests and feature engineering. This is clearly an application paper. No new method is being proposed, only existing methods are applied directly to the task. There is however a lack of technical novelty or insight in the models themselves. I think that the paper should be submitted to a journal or conference in the application domain where it would be a better fit.<BRK>A class of malware uses algorithms to automatically generate artificial domain names for various purposes, e.g.to generate large numbers of rendezvous points. A set of five deep learning architectures (both CNNs and RNNs) are compared empirical on the text classification task. The main conclusion is intuitive.
Reject. rating score: 3. rating score: 4. rating score: 4. <BRK>I also have concerns regarding the experiments. It starts with a theoretical analysis of memory capabilities in different RNN cells and goes on with experiments on POS tagging and dependency parsing. It is also very common to use several BiRNN layers, whereas the paper only uses one. As for the proposed DBRNN method, unfortunately, I was not able to understand it.<BRK>The other one (SLSTM II) looses the capability of forgetting as it seems. While the claim of the paper sounds very ambitious and good, the paper has several flaws. Of course, it is known that Simple Recurrent Networks (SRN) have a vanishing gradient problem. In your experiments it would be fair to compare to Cheng et al.2016I suggest the authors being more modest with the name of the memory cell as well as with the abstract (especially in the POS experiment, SLSTM is not superior)<BRK>While LSTM do have problem to learn very long term dependency, the model proposed in this paper is very inefficient, the number of parameters are depend on the the length of sequences. How to handle very long sequence and also, how to deal with different length? But I don t understand why the DBRNN can handle it. It essentially just a multitask learning function: L   L_f + L_b + L_fb where error signal backprop to different layer directly which is not new. The experimental results are weak. It compare with Seq2Seq model without attention. The other baseline for POS tag is from 1997.
Reject. rating score: 4. rating score: 5. rating score: 6. <BRK>The authors propose to employ provably minimal distance examples as a tool to evaluate the robustness of a trained network. This is demonstrated on a small scale network using the MNIST data set. I therefore find the authors  statement on page 3 disturbing: "... they are trained over a small set of inputs, and can then perform well, in general, on previously unseen inputs"   which seems false (with high probability over all possible worlds). All such examples shown in the paper are indeed within class examples that are misclassified. Adding these as adversarial examples could seriously degrade the accuracy.<BRK>Summary: The paper proposes a method to compute adversarial examples with minimum distance to the original inputs, and to use the method to do two things: Show how well heuristic methods do in finding "optimal/minimal" adversarial examples (how close the come to the minimal change that flips the label) and to assess how a method that is designed to make the model more robust to adversarial examples actually works. Pros:I like the idea and the proposed applications. * Algorithm 1 is essentially only a description of binary search, which should not be necessary. That’s also the conclusion two paragraphs below, no? Those were not created with the CW attack, but are related because CW was used to initialize the search.<BRK>The paper describes a method for generating so called ground truth adversarial examples: adversaries that have minimal (L1 or L_inf) distance to the training example used to generate them. The technique uses the recently developed reluplex, which can be used to verify certian properties of deep neural networks that use ReLU activations.
Reject. rating score: 3. rating score: 6. rating score: 6. <BRK>The paper proposes to use 2 d image representation techniques as a means of learning representations of graphs via their adjacency matrices. This can then be fed into a classifier. This is a little too unprincipled for my taste. I think the goal is to identify which of the graphs a subgraph belongs to? I m not sure how relevant this graph classification task is.<BRK>The paper proposed a subgraph image representation and validate it in image classification and transfer learning problems. It is very positive that the figures are very helpful for delivering the information. A novelty of this work seems to be transforming a graph into an image.<BRK>This paper views graph classification as image classification, and shows that the CNN model adapted from image net can be effectively adapted to the graph classification. The idea is interesting and the result looks promising, but I do not understand the intuition behind the success of analogizing graph with images.
Reject. rating score: 1. rating score: 2. rating score: 3. <BRK>There would have been ample space to provide the mathematical formulations. The paper has numerous flaws and is clearly below acceptance threshold for any scientific forum. 2.Complete lack of details for related work.<BRK>The authors propose a bootstrap based test for determining the number of latent dimensions to retain for linear dimensionality reduction (SVD/PCA). The paper addresses an important problem, but does not seem ready for publication:   The evaluation only uses simulated data. There is limited technical novelty.<BRK>The procedure is contrasted to some existing methods for determining the number of latent components and found to perform similarly to another procedure based on bootstrapping correlation matrices, the PA procedure. Cons:I find the paper poorly written and the methodology not sufficiently rooted in the existing literature. Thus, it seems the proposed procedure which is very similar in spirit to PA does not have much benefit over this procedure.
Accept (Poster). rating score: 8. rating score: 7. rating score: 5. <BRK>Overall I liked this paper: The authors provide a frank view on the current state of neural program synthesis, which I am inclined to agree with: (1) existing neural program synthesis has only ever worked on ‘trivial’ problems, and (2) training program synthesizers is hard, but providing execution traces in the training data is not a practical solution. I’m guessing that these short programs are just for illustration – could you provide an example execution trace for one of the programs in the larger test sets? However, the authors present the technique as a general approach to synthesizing complex programs. I feel that the authors need to either justify this bold assertion with least one additional example task or tone down their claims. The paper is reasonably clear.<BRK>This paper presents a reinforcement learning based approach to learn context freeparsers from pairs of input programs and their corresponding parse trees. The mainidea of the approach is to learn a neural controller that operates over a discretespace of programmatic actions such that the controller is able to produce thedesired parse trees for the input programs. Can the authors comment on the generality of the presented approach to some other program synthesis tasks? The results show thatthe proposed approach is able to achieve 100% generalization on test sets with programs upto 100x longer than the training programs, while baseline approaches such as seq2seq and stack LSTM do not generalize at all.<BRK>This paper proposes a method for learning parsers for context free languages. In comparison, existing approaches appear to achieve little to no generalization, especially when tested on longer examples than seen during training. Despite the thoroughness of the task and model descriptions, the proposed method is not well motivated. This is particularly problematic because the only empirical result reported is that it achieves 100% accuracy. Unless I am misunderstanding the experimental setup, this is not supported by the result, correct? The proposed method achieves perfect accuracy in every condition.
Accept (Poster). rating score: 7. rating score: 6. rating score: 3. <BRK>This paper proposes a new theoretically motivated method for combining reinforcement learning and imitation learning for acquiring policies that are as good as or superior to the expert.<BRK>This is not a deal breaker, and I would not consider this as a weakness of the work, but the paper should be more clear and upfront about this. (Theorem 5 in Appendix 3.B: Learning with a smaller horizon). After Rebuttal: Thank you for your answer. There are some other papers that have ideas similar to this work in relation to truncating the horizon.<BRK>This work proposes to use the value function V^e of some expert policy \pi^e in order to speed up learning of an RL agent which should eventually do better than the expert. I disagree with this claim. Generalization of the value function to other states is a hard problem in RL and is the topic of important research.
Accept (Poster). rating score: 8. rating score: 7. rating score: 5. <BRK>The paper considers the problem of training neural networks in mixed precision (MP), using both 16 bit floating point (FP16) and 32 bit floating point (FP32). Using these techniques allows the authors to match the results of traditional FP32 training on a wide variety of tasks without modifying any training hyperparameters.<BRK>The paper is missing results comparing training and testing speeds in all these models, to illustrate the benefits of using the proposed techniques. It would be very valuable to add the baseline wall time to the tables, together with the obtained wall time for training and testing using the proposed techniques.<BRK>The paper provides methods for training deep networks using half precision floating point numbers without losing model accuracy or changing the model hyper parameters. Positives  The experimental evaluation is fairly exhaustive on a large number of deep networks, tasks and datasets and the proposed training preserves the accuracy of all the tested networks at half the memory cost.
Reject. rating score: 4. rating score: 4. rating score: 5. rating score: 7. <BRK>I recommend "a"  > "a)" and "b"  > "b)". This concern was also raised on this forum prior to the revision. My original review had rating "4: Ok but not good enough   rejection". My original review is kept below for reference.<BRK>I think this paper requires significant work and it not suitable for publication in its current state.<BRK>The qualitative assessment is always interesting and it is completed with some label prediction task. Discussing this potential similarity would certainly broaden the readership of the paper.<BRK>This paper proposes tree vertex embeddings over hyperbolic space. However, graph embeddings are now a very well explored space, and this paper does not seem to mention or compare against other hyperbolic (or any noneuclidean) embedding techniques.
Reject. rating score: 4. rating score: 4. rating score: 6. <BRK>This paper presents HybridNet, a neural speech (and other audio) synthesis system (vocoder) that combines the popular and effective WaveNet model with an LSTM with the goal of offering a model with faster inference time audio generation. Low novelty of approach / impact assessment:The proposed model is based closely on WaveNet, an existing state of the art vocoder model.<BRK>This reduces significantly the generality of the proposed technique. Pros:  Attempting to solve the important problem of speeding up autoregressive generation. The absolute results are not that great (MOS ~3.8 is not close to the SOTA of 4.4   4.5)<BRK>TL;DR of paper: for sequential prediction, in order to scale up the model size without increasing inference time, use a model that predicts multiple timesteps at once. The idea is simple and intuitive.
Reject. rating score: 3. rating score: 4. rating score: 5. <BRK>As one of the listed contributions, the authors propose using policy gradient. Also, it seems the reward is not a function of the future actions, which further questions the need for a reinforcement learning formulation. The paper is written poorly.<BRK>The paper considers speech generation conditioned on an accent class. I do not see a reason for the policy gradients. People can then judge the quality of the speech synthesis.<BRK>This paper presents a method for generating speech audio in a particular accent. The proposed approach relies on a generative adversarial network (GAN), combined with a policy approach for joining together generated speech segments. Some of the approach details are unclear and the research is not motivated well. Why is accented modelling important?
Accept (Poster). rating score: 7. rating score: 6. rating score: 5. <BRK>Overall this is a good paper and I believe it should be accepted, given the authors are more clear on the details pointed above. Some of the hyperparameters used (alpha and specially rho) seem to be used very ad hoc.<BRK>The paper suggests normal pruning does not necessarily preserve the network function. As strong points, the paper is easy to follow and does a good review of existing methods. Then, the proposal is simple and easy to reproduce and leads to interesting results.<BRK>The quality and clarity of the paper can be improved in some sections.
Accept (Poster). rating score: 7. rating score: 6. rating score: 6. <BRK>The paper s main claim is that this model architecture enables stronggeneralization: it allows the model to succeed at the instruction following taskeven when given words it has only seen in QA contexts, and vice versa. Experiments show that on the navigation task, the proposed approach outperformsa variety of baselines under both a normal data condition and one requiringstrong generalization. I have a few questions about theevaluation, but most of my comments are about presentation. Given that this paper spends a lot of timemotivating the QA task as part of the training scenario, I was surprised not tosee it evaluated. This should be made moreclear in the paper.<BRK>Overall, I think the paper proposes an interesting environment and task that is of interest to the community in general. The modes and its evaluation are relevant and intuitions can be made use for evaluating other similar tasks (in 3D, say). Agents working in this setting therefore, learn the language of the "teacher" and efficiently ground words to their respective concepts in the environment. The paper introduces XWORLD, a 2D virtual environment with which an agent can constantly interact via navigation commands and question answering tasks.<BRK>[Overview]In this paper, the authors proposed a unified model for combining vision, language, and action. It is aimed at controlling an agent in a virtual environment to move to a specified location in a 2D map, and answer user s questions as well. As we know, in a realistic environment, all three components are necessary to complete a complex tasks which need the interactions with the physical environments. The authors should release the dataset to prompt the research in this area. To address this problem, the authors proposed a simple model to ground words on visual signals, which prove to outperform previous methods, such as CA, SAN, etc.
Reject. rating score: 4. rating score: 5. rating score: 7. <BRK>In Bayesian neural networks, a deterministic or parametric activation is typically used. In this work, activation functions are considered random functions with a GP prior and are inferred from data. Unnecessary complexityThe presentation of the paper is unnecessarily complex. Bayesian warped Gaussian processes. Although some of the derivations in Section 3.2.2 are a bit involved, most of the derivations up to that point (which is already in page 6) follow preexisting literature. Correctness of the approachCan the authors guarantee that the variational bound that they are introducing (as defined in eqs.(19) and (41)) is actually a variational bound? It seems to me that the approximations made to Q(A) to propagate the uncertainty are breaking the bounding guarantee. The authors should contrast their approach with [4] and discuss if and why that additional central limit theorem application is necessary. No experimentsThe use of a non parametric definition for the activation function should be contrasted with the use of a parametric one.<BRK>The paper addresses the problem of learning the form of the activation functions in neural networks. The authors propose to place Gaussian process (GP) priors on the functional form of each activation function (each associated with a hidden layer and unit) in the neural net. The inducing point approximation used here is very efficient since all GP functions depend on a scalar input (as any activation function!) Of course then inference involves approximating the finite posterior over inducing function values U and the paper make use of the standard Gaussian approximations. In general I like the idea and I believe that it can lead to a very useful model. However, I have found the current paper quite preliminary and incomplete. Your method seems to be connected with Deep GPs although there appear to be important differences as well. This is the technical part of your paper which is a non standard approximation.<BRK>This paper investigates probabilistic activation functions that can be structured in a manner similar to traditional neural networks whilst deriving an efficient implementation and training regime that allows them to scale to arbitrarily sized datasets. The extension of Gaussian Processes to Gaussian Process Neurons is reasonably straight forward, with the crux of the paper being the path taken to extend GPNs from intractable to tractable. These are temporary and are later made redundant. The resulting model is theoretically scalable to arbitrary datasets as the total model parameters are independent of the number of training samples. It is unfortunate but understandable that the GPN model experiments are confined to another paper.
Accept (Poster). rating score: 8. rating score: 6. rating score: 6. <BRK>The paper also provides an interesting analysis of various design issues. A strength based re ranker captures how often a candidate answer would be selected while a coverage based re ranker aims to estimate the coverage of the question by the supporting passages. This article is really well written and clearly describes the proposed scheme.<BRK>Although the idea seems incremental, the experimental results do seem solid. The paper is generally easy to follow, but in several places the presentation can be further improved. I feel this is a somewhat indirect and less effective design, if avoiding stop words is really the reason.<BRK>The paper is clear, although there are many English mistakes (that should be corrected). The proposed method aggregates answers from multiple passages in the context of QA. The analysis of the results is interesting and largely convincing, although a more dedicated error analysis or discussion of the limitation of the proposed approach would be welcome. The terminology should be corrected here.
Invite to Workshop Track. rating score: 6. rating score: 5. rating score: 4. <BRK>The authors propose a domain specific language two types of generators (random and RL based) together with a ranking function and evaluator. Also, the clarity of the text could be improved. The approach is tested in a number of tasks: Language modelling (PTB and wikipedia 2) and machine translation (3.In these work the authors tested a wide range of different RNNs3.<BRK>Authors evaluate their approach on  PTB/Wikitext 2 language modeling and Multi30k/IWSLT 16  machine translation. Few details can be clarified. Did you use this in the experiments? OriginalityThe idea of using DSL + ranking for architecture search seems novel. It would be nice to have other tasks that are commonly used as benchmark for RNN to see where this approach stand. In addition, authors propose both a DSL, a random and RL generator and a ranking function. In particular, did the authors compare the random search vs the RL based generator or the performances of the RL based generator when the ranking network is not used?<BRK>Generated by a RL agent. While the overall approach appears to generalize previous work, I see a few serious flaws in this work:Limited scopeAs far as I can tell this work only tries to come up with a design for a single RNN cell, and then claims that the optimality of the design will carry over to stacking of such modules, not to mention more complicated network designs. This leads one to wonder if the winning architecture has won only because of the specific parameters that were used in the evaluation. Indeed, the experiments in the paper show that a cell which was successful on one task isn’t necessary successful on a different one, which questions the competence of the scoring function / RL agent.
Reject. rating score: 4. rating score: 6. rating score: 6. <BRK>This paper discusses an application of survival analysis  in social networks. While the application area seems to be pertinent, the statistics as presented in this paper are suboptimal at best. There is no useful statistical setup described (what is random?etc etc), the interplay between censoring and end of life is left rather fuzzy, and mentioned clustering approaches are extensively studied in the statistical literature in so called frailty analysis. The setting is also covered in statistics in the extensive literature on repeated measurements  and even time series analysis. It s up to the authors discuss similarities and differences of results of the present approach and those areas. The numerical result is not assessing the different design decisions of the approach (why use a Kuyper loss?)<BRK>Pros:The paper is a nice read, clearly written, and its originality is well stated by the authors, “addressing the lifetime clustering problem without end of life signals for the first time”. The approach proposed in the manuscript is mainly based on a newly designed nonparametric loss function using the Kuiper statistic and uses a feed forward neural network to optimize the loss function. By analyzing a large scale social network dataset, it is shown that the proposed method performs better on average than the other two traditional models. Cons:       I think that the main drawback of the paper is that the structure of the neural network and the deep learning techniques used for optimizing the loss function are not explained in sufficient detail.<BRK>Authors provide an interesting loss function approach for clustering using a deep neural network. They optimize Kuiper based nonparametric loss and apply the approach on a large social network data set. However, the details of the deep learning approach are not well described. 3.Conclusion section is very brief and can be expanded by including a discussion on results comparison and  over fitting aspects in cross validation. Use of Kuiper based nonparametric loss should also be justified as there are other loss functions can be used under these settings.
Accept (Poster). rating score: 8. rating score: 7. rating score: 7. <BRK>Summary:This paper considers the problem of classifying linearly separable data with a two layer \alpha  Leaky ReLU network, in the over parametrized setting with 2k hidden units. The algorithm used for training is SGD which minimizes the hinge loss error over the training data. First result shows that the loss function does not have any sub optimal local minima. One confusing aspect in the paper is the optimization and generalization results hold for any global minima w* of the L_s(w).<BRK>This paper shows that on linearly separable data, SGD on a overparametrized network (one hidden layer, with leaky ReLU activations) can still lean a classifier that provably generalizes. Ideally, one would also want to see a result where overparametrization actually helps (in the main result the whole data can be learned by a linear classifier). The algorithm part seems to heavily rely on the linear separable assumption. The proof of the optimization part is very similar to the proof of perceptron algorithm, and really relies on linear separability.<BRK>It focuses on a setting with three crucial simplifications:  data is linearly separable  model is 1 hidden layer feed forward network with homogenous activations  **only input hidden layer weights** are trained, while the hidden output layer s weights are fixed to be (v, v, v, ..., v,  v,  v,  v, ...,  v) (in particular   (1,1,...,1, 1, 1,..., 1))While the last assumption does not limit the expressiveness of the model in any way, as homogenous activations have the property of f(ax) af(x) (for positive a) and so for any unconstrained model in the second layer, we can "propagate" its weights back into first layer and obtain functionally equivalent network. Consequently, while the results are very interesting, claiming their applicability to the deep models is (at this point) far fetched. I am happy to revisit my current rating given authors rephrase the paper so that the simplifications being made are clear both in abstract and in the text, and that (at least empirically) it does not affect learning in practice.
Accept (Poster). rating score: 7. rating score: 7. rating score: 6. <BRK>This idea is novel and interesting. While the first three tasks are smaller proof of concept, the last task could have been  more convincing if near state of the art methods were used. The authors use a Resnet 50 which is a  smaller and lesser performing model, they do mention that benefits are expected to be   complimentary to say larger model, but in general it becomes harder to improve strong models.<BRK>CelebA dataset could have been a good platform for testing different tasks, attribute classification and landmark detection. Notation and Typos:  Figure 2 is a bit confusing, how come the accuracy decreases with increasing number of training samples? Figure 4(a) and 5(b), the results shown on validation loss, how about testing error similar to Figure 6(a)?<BRK>My primary concern about this paper is the lack of interpretation on permuting the layers. The paper also needs more clarifications in the writing.
Accept (Poster). rating score: 8. rating score: 7. rating score: 7. rating score: 7. <BRK>Quality, clarity : Very well written, well motivated, convincing experiments and analysisOriginality: I think they framed the problem of domain robustness very well: how to obtain a "domain level embedding" which generalizes to unseen domains.<BRK>The idea is interesting and new. And the paper is well written.<BRK>The captions should provide more information about the main point of these figures. This should be clarified in the paper.<BRK>This paper proposed a domain generalization approach by domain dependent data augmentation. Experiments on four datasets verify the effectiveness of the proposed approach. Unified deep superviseddomain adaptation and generalization. + The experiments show that the proposed method outperforms two baselines.
Invite to Workshop Track. rating score: 6. rating score: 6. rating score: 4. <BRK>This paper considers a special deep learning model and shows that in expectation, there is only one unique local minimizer. As a result, a gradient descent algorithm converges to the unique solution. This works address a conjecture proposed by Tian (2017). While it is clearly written, my main concern is whether this model is significant enough. The assumptions K 2 and v1 v2 1 reduces the difficulty of the analysis, but it makes the model considerably simpler than any practical setting.<BRK>In this paper the authors studied the theoretical properties of manifold descent approaches in a standard regression problem, whose regressor is a simple neural network. Leveraged by two recent results in global optimization, they showed that with a simple two layer ReLU network with two hidden units, the problem with a standard MSE population loss function does not have spurious local minimum points. Based on the results by Lee et al, which shows that first order methods converge to local minimum solution (instead of saddle points), it can be concluded that the global minima of this problem can be found by any manifold descent techniques, including standard gradient descent methods. Contribution: As discussed in the literature review section, apart from previous results that studied the theoretical convergence properties for problems that involves a single hidden unit NN, this paper extends the convergence results to problems that involves NN with two hidden units. Without extra justifications, it seems that the theoretical result only holds for an artificial problem setting. While the ReLU activation is very common in NN architecture, without more motivations I am not sure what are the impacts of these results. General Comment:  The technical section is quite lengthy, and unfortunately I am not available to go over every single detail of the proofs.<BRK>Summary: The paper considers the problem of a single hidden layer neural network, with 2 RELU units (this is what I got from the paper   as I describe below, it was not clear at all the setting of the problem   if I m mistaken, I will also wait for the rest of the reviews to have a more complete picture of the problem). The techniques used depend on previous work. According to the authors, this paper extends(?) previous results on a NN with a single layer with a single unit. Originality: The paper heavily depends on the approach followed by Brutzkus and Globerson, 2017. Importance: Understanding the landscape (local vs global minima vs saddle points) is an important direction in order to further understand when and why deep neural networks work. The title is not clear whether the paper considers a two layer RELU network or a single layer with with two RELU units. In the abstract the authors state that it has to do with a two layer RELU network with two hidden units (per layer?in total?). It would be great to have more intuitions.
Reject. rating score: 4. rating score: 5. rating score: 8. <BRK>The authors propose an approach to dynamically adjust the feature map depth of a fully convolutional neural network. These are very different concepts. However, this work falls short of its promises. 1.The title is misleading.<BRK>Experiments are carried out on small scale datasets such as MNIST and CIFAR, as well as an exploratory run on ImageNet (AlexNet). Overall, I find the approach proposed in the paper interesting but a little bit thin in content. It would be much valuable to see ablation studies to show the effectiveness of such criterion: for example, simple cases one can think of is to model (1) a data distribution of known rank, (2) simple MLP/CNN models to show the cross layer relationships (e.g.sudden increase and decrease of the number of channels across layers will be penalized by c^l_{f^{l+1}, t}), etc. The experimentation section uses small scale datasets and as a result, it is relatively unclear how the proposed approach will perform on real world applications.<BRK>That said, it would be nice to demonstrate that the algorithm also works for other tasks than image classification. It is definitely surprising that a simple method like this ends up working this well. The fact that all parameters are reinitialised whenever any layer width changes seems odd at first, but I think it is sufficiently justified.
Accept (Poster). rating score: 7. rating score: 6. rating score: 6. <BRK>The idea is to learn a second (kind of) Q function, which could be called E function, which captures the value of exploration (E value). This provides a nice story for a simple (in a positive sense) approach to tackle the exploration exploitation tradeoff. The experimental results demonstrate this is a sufficient number of domains. To summarize, for an informed outsider such as the reviewer, the paper makes a simple but strong contribution to an important problem. Overall the paper is well writing and structured.<BRK>I like the idea a lot. On the theoretical side, can a bound be proven for this approach, even in the tabular case? My intuition is that such an approach ought to be effective, but I really want to see additional evidence. Given the availability of so many RL testbeds, I worry that it had been tried but failed. "The other alternative"  > "The alternative"? Perhaps select the paper that first used the term? ": I think you should cite http://research.cs.rutgers.edu/~nouri/papers/nips08mre.pdf , which also combines a kind of counter idea with function approximation to improve exploration.<BRK>The method presented in the paper trains a parallel "E value" MDP, with initial value of 1 for all state action pairs. This bonus term is shown to be equivalent counter based methods for finite MDPs when the discount factor of the E MDP is set to 0. There are also several computationally tractable approximations of Bayesian RL that can be used as baseline for empirical analysis. It would have also been nice to do some analysis on how the update rule in a function approximation case is affecting the bonus terms. Given the lack of any theory on this, an empirical analysis is certainly valuable. Please scale them at least by 200% and use a larger font for the legends.
Reject. rating score: 4. rating score: 5. rating score: 5. <BRK>Originality:As far as I can tell, the approach is quite original and the results proved are new. The paper is overall difficult to follow (at least for me) for different reasons. p5: « The goal of learning a nonlinear transform (2) is to estimate… »: not sure what you mean by « accurate approximation » in this sentence. First, with 17 pages + references + appendix, the length of the paper is way above the « strongly suggested limit of 8 pages ».<BRK>This paper proposes a method of learning sparse dictionary learning by introducing new types of priors. It is also presented the power of the proposed method in comparison with the existing methods in the literature. Overall, the paper deals with an important issue in dictionary learning and proposes a novel idea of utilizing a set of priors.<BRK>Overview:This paper proposes a method for learning representations using a “non linear transform”. In particular, the motivation and approach are not clear (sec.1.2), making it hard to understand the proposed method. The paper is 17 pages long (24 pages with the appendix), so I had to skim through some parts.
Accept (Poster). rating score: 7. rating score: 7. rating score: 3. <BRK>The paper correctly identifies an important problem with the way most deep generative models evaluate variance. I especially appreciate the interpolation and random walk experiments. These are hard to evaluate objectively, but the results to hint at the phenomena the authors describe when comparing Euclidean to Riemannian metrics in the latent space. Cons:1.The part of the paper proposing new variance estimators is ad hoc and is not experimented with rigorously, comparing it to other methods in terms of calibration for example. One could equally argue that the Euclidean distance on z is natural, and that this distance is then pushed forward by f to some induced distance over X.<BRK>In the paper the authors analyse the latent space generated by the variational autoencoder (VAE). They show that this latent space is imbued by a Riemannian metric and that this metric can be easily computed in terms of mean and variance functions of the corresponding VAE. In the experiments section the authors evaluate the quality and meaningfulness of the induced Riemannian metric. In section 4, it says proof for theorem 1 is in appendix B. In Fig.6, why was 7 NN used, instead of k means, to colour the background? I think that the result from the theorem 1 is very important, since the estimation of the Riemannian metric is usually very slow.<BRK>The paper investigates the geometry of deep generative models. The authors describe the geometric setting, how distances in the latent space can be interpreted with the non Euclidean geometry, and how interpolation, probability distributions and random walks can be constructed. While the paper makes a decent presentation of the geometry of the generative setting, it is not novel. It is well known that (under certain conditions) the mapping described by the generator function is a submanifold of the input space. The latent space geometry is nothing but the submanifold geometry the image f(Z) inherits from the Euclidean geometry of X. The latent space distances and geodesics corresponds to distances and geodesics on the submanifold f(Z) of X. I cannot identify a clear contribution or novelty in the paper which is the basis for my recommendation of rejection of the paper.
Accept (Poster). rating score: 7. rating score: 7. rating score: 6. <BRK>The authors present 3 architectures for learning representations of programs from execution traces. Assuming that the authors can address these clarity issues, I would in principle be happy for the paper to appear. They additionally use their embeddings to decrease the search time for the Sarfgen program repair system. This is a fairly strong paper.<BRK>Clarity: The paper is clearly written. Originality: This work doesn t seem that original from an algorithmic point of view since Reed & De Freitas (2015) and Cai et. al (2017) among others have considered using execution traces. However the application to program repair is novel (as far as I know). Significance: This work can be very useful for an educational platform though a limitation is the need for adding instrumentation print statements by hand.<BRK>This paper considers the task of learning program embeddings with neural networks with the ultimate goal of bug detection program repair in the context of students learning to program. For example, the Cai et al.paper from ICLR 2017 is not considered4) If I understand dependency reinforcement embedding correctly, a RNN is trained for every trace. The approach is validated using programming assignments from an online course, and compared against syntax based approaches as a baseline. The problem considered by the paper is interesting, though it s not clear from the paper that the approach is a substantial improvement over previous work.
Accept (Poster). rating score: 9. rating score: 7. rating score: 6. <BRK>This paper examines a distributed Deep RL system in which experiences, rather than gradients, are shared between the parallel workers and the centralized learner. The experiences are accumulated into a central replay memory and prioritized replay is used to update the policy based on the diverse experience accumulated by all the of the workers. It’s hard to take issue with a paper that has such overwhelmingly convincing experimental results. Pros:•	Well written and clear. •	It’s remarkable that Ape X preforms as well as it does given the simplicity of the algorithm. Cons:•	Hard to replicate experiments without the deep computational pockets of DeepMind.<BRK>Experiments on Atari game playing and two MuJoCo continuous control tasks show significant improvements in terms of training time and final performance compared to previous baselines. The core idea is pretty straightforward but the paper does a very good job at demonstrating that it works very well, when implemented efficiently over a large cluster (which is not trivial). There are essentially two more things I would have really liked to see in this paper (maybe for future work?):  Using all Rainbow components  Using multiple learners (with actors cycling between them for instance)Sharing your custom Tensorflow implementation of prioritized experience replay would also be a great bonus!<BRK>This paper proposes a distributed architecture for deep reinforcement learning at scale, specifically, focusing on adding parallelization in actor algorithm in Prioritized Experience Replay framework. It has a very nice introduction and literature review of Prioritized experience replay and also suggested to parallelize the actor algorithm by simply adding more actors to execute in parallel, so that the experience replay can obtain more data for the learner to sample and learn. While the strength of this paper is clearly the good writing as well as rigorous experimentation, the main concern I have with this paper is novelty. It is in my opinion a somewhat trivial extension of the previous work of Prioritized experience replay in literature; hence the challenge of the work is not quite clear.
Reject. rating score: 5. rating score: 6. rating score: 6. <BRK>However, one could say that this paper is, on careful examination, an application of reparameterisation gradient VI for a specific application. Nevertheless, the parameterisation of the conditional variational distribution q(\theta | \phi, (x, y)) using recognition model is interesting and could be useful in other models. However, the presentation in 4 is hard to follow. The first paragraph of page 5 uses q(\theta | \phi, (x, y)), but y is not known at test time. There are also many hyperparameters that need to be chosen   what would happen if these are optimised using the free energy? What is the computational complexity of BBB with posterior sharpening?<BRK>This paper proposes an interesting variational posterior approximation for the weights of an RNN. pros: I liked the posterior sharpening idea. There are many works on Bayesian neural networks but they never present an analysis of the uncertainty introduced in the weights. The experiments were well carried through. cons: Change the title! This was a claim in the paper and there was no theoretical justification or an empirical comparison of the gradient variance in the experiment section(2) how is the level of uncertainty related to performance? It would have been insightful to see effect of \sigma_0 on the performance rather than report the best result.<BRK>In particular, the authors propose a new framework to "sharpen" the posterior. In the experimental results, it seems clear that this approach is beneficial, but it s not clear as to why. It s completely feasible that these outputs would just be highly uncertain, and I m not sure how you can ascribe meaning to them. The authors should not compare to the uniform prior as a baseline for entropy. It s much more revealing to compare it to the empirical likelihoods of the words.
Reject. rating score: 4. rating score: 4. rating score: 5. <BRK>However, I feel the current presentation fails to provide adequate intuition and interpretation of the results. This is in a way a measure of the added representational power obtained from dense connections. In this paper, the authors present a theoretical analysis of the gain in representational capacity induced by additional inter layer connections.<BRK>The paper attempts to provide a theoretical justification for "DenseNet", a neural net architecture proposed by Huang et al.that contains connections between non successive layers. The gap between r and r  is rather small. While the goal is good, I find too many aspects of the paper that are confusing, at least to someone not an expert on ConvACs. first, the definition of growth rate is quite different from the paper of Huang et al.(here, the rate is defined as the number of forward layers a given layer is connected to, while Huang et al.define it as the number of  new features  that get generated in the current layer).<BRK>Authors derive thisform for the DenseNet variant of convolutional arithmetic circuits,and give bounds on the rank of the associated tensor and using thesebounds argue the expressive power of the network. The authors alsoclaim these bounds can help in practical guidelines while designingthe network. Looks like the results are mostly for interblock connections, for which the empirical results are not there. A lower bound would have beenhelpful. But partly dueto the poor presentation style, deviation from DenseNets and unclearnature of the practical usefulness of the results, the paper may notbe of contribution to the community at this stage.
Reject. rating score: 5. rating score: 5. rating score: 6. <BRK>The game theoretic approach to attacks with / defense against adversarial examples is an important direction of the security of deep learning and I appreciate the authors to initiate this kind of study. In case of deep learning, the convexity is not guaranteed and the resulting solutions do not have necessarily follow Lemma 1. It contains a game theoretic framework of adversarial examples/training, novel attack method, and many experimental results. In some cases, MNIST is too easy to consider the complex structure of deep architectures.<BRK>The authors describe a mechanism for defending against adversarial learning attacks on classifiers. They first consider the dynamics generated by the following procedure. The paper is well written and easy to follow. However, I found the empirical results to be a little underwhelming. How does it stack up against other defense approaches (e.g.https://arxiv.org/pdf/1705.09064.pdf)? With regards to the second part of the paper, using gradient methods to solve a minimax problem is not especially novel (i.e.Goodfellow et al.), thus I would liked to see more thorough experiments here as well. However, this is a bit unsatisfying.<BRK>This paper presents a sensitivity penalized loss (the loss of the classifier has an additional term in squart of the gradient of the classifier w.r.t.perturbations of the inputs), and a minimax (or maximin) driven algorithm to find attacks and defenses. + This paper presents a game formulation of learning based attacks and defense in the context of adversarial examples for neural networks, and empirical findings support its claims. + Prior work seem adequately cited and compared to, but I am not really knowledgeable in the adversarial attacks subdomain. The paper is hard to follow at times (and probably that dealing with the point above would help in this regard), e.g.Lemma 1 and experimental analysis.
Reject. rating score: 5. rating score: 5. rating score: 7. <BRK>The motivation in the appendix is very informal and no clear derivation is provided. This seems to contradict the theorem. The paper also does not compare to any other techniques that attempt to deal with catastrophic forgetting and/or the changing state distribution ([1,2]). No empirical results on the effect of the parameter are given.<BRK>The fear model is trained in parallel to predict catastrophic states. COMMENTSNot convinced about the fact that an agent forgets about catastrophic states. Because it does not experience it any more. Shouldn’t the agent stop learning at some point in time?<BRK>The paper studies catastrophic forgetting, which is an important aspect of deep reinforcement learning (RL). The theoretical results regarding the properties of the proposed algorithm are also relevant, and points out some of its benefits, though I do not view the results as particularly strong. The proposed method is novel, but perhaps the most interesting aspect of this paper is that they demonstrate that “DQNs  are susceptible to periodically repeating mistakes”.
Accept (Poster). rating score: 7. rating score: 7. rating score: 4. <BRK>This is an interesting direction. There is still much to understand about the relative strengths and limitations of model based and model free techniques, and how best to combine them, and this paper discusses a new way to address this problem. TDM is now parameterized with (state, action (goal) state, and the horizon tau). The results are better than models in some places.<BRK>I recommend that the authors find a way to keep the info in Section 4.3 (Dynamic Goal and Horizon Resampling) in the paper though, unless I missed where it was moved to. They also show the impact of planning horizon on performance, demonstrating a nice trade off.<BRK>On the positive side, there are certainly some interesting ideas here: the notion of goal conditioned value functions as proxies for a model, and as a means of merging model free and model based approaches is very really interesting, and hints at a deeper structure to goal conditioned value functions in general. To address the first point, although the paper stresses it to a very high degree, I can t help but feel that the connection that the claimed advance of "unifying model based and model free RL" is overstated.
Accept (Poster). rating score: 7. rating score: 6. rating score: 6. <BRK>The paper is well written. It may be straight forward but some small simulation/plots explaining this is important. It seems from Figure 3,4 that retain rates in lower layers are more closer to 1 and they decrease to 0.5 as depth increases. What does this mean in terms of network architecture? Again it appears p q 2 is the best, but need confidence intervals here to say anything substantial.<BRK>While it is great to see deep learning techniques inspired by learning theory, I think the paper makes too many leaps and the Rad story is ultimately unconvincing. It would be interesting if it also turns out to be a good regularizer, but the authors do not say why nor cite anything. 2) Why is it reasonable to go from a regularizer based on RC to a loose bound of Rad? There should be some analysis on how loose this bound is, and if this looseness matter at all. Afterall, it seems that the proposed method had several parameters that were turned, where the analogous parameters are not present in the competing methods. While the authors point out that one can use the mean, but that is more problematic for the gradient than for normal forward predictions.<BRK>(2) I can not follow the inequality (5). (3) I can also not see clearly the third equality in (9). Note that f^l is a vector valued function. Is it possible to further improve this dependency? (3) It would be better to give more details (e.g., page, section) in citing a book in the proof of Theorem 3.1Summary:The mathematical analysis in the present version is not rigorous. The authors should improve the mathematical analysis. After Rebuttal:Thank you for revising the paper.
Reject. rating score: 5. rating score: 7. rating score: 8. <BRK>as an additional ingredient the authors also propose "representation learning" by mapping x to some representation Phi(x). The goal is to learn the mapping Phi (and its inverse) and the weighting function simultaneously by optimizing the derived bound on the prediction performance. Pros:   The problem is relevant and also appears in similar form in domain adaptation and transfer learning. The derived bounds and procedures are interesting and nontrivial, even if there is some overlap with earlier work of Shalit et al.Cons:  I am not sure if ICLR is the optimal venue for this manuscript but will leave this decision to others. Especially the second half of page 5 is at times very hard to understand as it is so dense. The example does not provide much insight into the main ideas and when we would expect the procedure to work more generally.<BRK>According to Lemma 1 and its finitesample version in Theorem 1, the risk on the target domain can be upper boundedby the combination of 1) the re weighted empirical risk on the source domain; and 2) the distributional discrepancy between the re weighted source domain andthe target domain. These theoretical results justify the objective functionshown in Equation 8. Comments:1) This paper is well motivated. 3) I have some questions on the details. Is this a standard assumption in the literature? 4) Two drawbacks of previous methods motivate this work, including the bias ofrepresentation learning and the high variance of re weighting. 6) Besides IHDP, did the authors run experiments on other real world datasets, such as Jobs, Twins, etc?<BRK>This paper proposes a deep learning architecture for joint learning of feature representation, a target task mapping function, and a sample re weighting function. Specifically, the method tries to discover feature representations, which are invariance in different domains, by minimizing the re weighted empirical risk and distributional shift between designs. Overall, the paper is well written and organized with good description on the related work, research background, and theoretic proofs. The main contribution can be the idea of learning a sample re weighting function, which is highly important in domain shift.
Reject. rating score: 5. rating score: 6. rating score: 7. <BRK>Summary: The paper addresses the issue of training feed forward neural networks with memory constraints. The experiment is also limited to MNIST and fully connected neural networks.<BRK>There could be an interesting idea here, but the limitations and applicability of the proposed approach are not clear yet. More analysis should be done to clarify its potential. This is clear in Fig.1 and in the explanation in Sect. It should be stressed that the networks end up with this non conventional “tiled” architecture. And this problem extends throughout the paper.<BRK>The main problem with this method that there is undiscussed payment with current hardware architectures. There have been a series of works showing how to discretisize neural networks. This work, discretisize a NN incrementally. So, in theory this might be a good idea, but I think this idea is not out of the box method for implementation.
Accept (Poster). rating score: 7. rating score: 6. rating score: 5. <BRK>Summary This paper proposes a new model called SCAN (Symbol Concept Association Network) for hierarchical concept learning. Strengths The idea of concept learning considered here is novel and satisfying. * How specific is the HSV trick to this domain?<BRK>This paper introduces a VAE based model for translating between images and text. Overall, I am borderline on this paper, due to the limited experimental evaluation, but lean slightly towards acceptance. The idea of learning compositional representations inside of a VAE framework is very appealing. They test their model only on a simple, artificial dataset.<BRK>This paper proposed a novel neural net architecture that learns object concepts by combining a beta VAE and SCAN. In general, I think this paper is interesting. The experimental results are good and the model is compared with very recent baselines. However, there are in essence very straightforward extensions of VAE and beta VAE (this is based on the fact that beta VAE itself is a simple modification of VAE and the effect was discussed in a number of concurrent papers). The authors are using images as input, but the images are all synthetic, and further, they are all synthesized to have highly regular structure.
Reject. rating score: 2. rating score: 7. rating score: 7. <BRK>SUMMARY.The paper presents an extension of word2vec for structured features. The authors introduced a new compatibility function between features and, as in the skipgram approach, they propose a variation of negative sampling to deal with structured features. Bottom of page one: "a positive example is  semantic ", please, use another expression to describe observable examples,  semantic  does not make sense in this context. is it the embedding vector? Finally, the most striking flaw of this paper is the lack of references to previous works on word embeddings and feature representation, I would suggest the author check and compare themselves with previous work on this topic.<BRK>Summary:This paper proposes an approach to learn embeddings for structured datasets i.e.datasets which have heterogeneous set of features, as opposed to just words or just pixels. The paper compares against a Word2vec baseline that pools all the heterogeneous content learns just one set of embeddings. In both the tasks, Feat2vec leads to significant reduction in error compared to Word2vec. Comments:The paper is well written and addresses an important problem of learning word embeddings when there is inherent structure in the feature space. It is a very practically relevant problem. The novelty of the proposed approach seems limited in light of the related paper that is concurrently under review at ICLR2018, on which this paper heavily relies. Perhaps the authors should consider combining the two papers into one complete paper?<BRK>This paper provides a clean way of learning embeddings for structured features that can be discrete   indicating presence / absence of a certain quality. Further, these features can be structured i.e.a set of them are of the same  type . Therefore, this can be used as a multi label classifier by using two feature types   the input and the set of categories. This proposed scheme is evaluated on two datasets   movies and education in a retrieval setting. For example, I am aware of   http://manikvarma.org/downloads/XC/XMLRepository.html contains some interesting datasets which have a large number of discrete features and classes.
Reject. rating score: 4. rating score: 5. rating score: 6. <BRK>Summary: In this work, the authors propose a text reconstructing auto encoder which takes a sentence as the input sequence and an integrated text generator generates another version of the input text while a reconstructor determines how well this generated text reconstructs the original input sequence. Strengths:The proposed idea of generating text using summary sentences is new. The experiments are conducted on English and Chinese corpora, comparison with competitive baselines are provided. Similarly, if so this would mean that the authors use additional supervision than the compared models. In footnote 1, the authors mention “seq2seq2seq2” term which they do not explain anywhere in the text. Therefore, it is not easy for the reader to judge the behavior of the model well. Moreover, the results are not sufficiently discussed. As a general remark, although the idea presented in this paper is interesting, both in terms of writing and evaluation, this paper has not yet reached the maturity expected from an ICLR paper.<BRK>The general gist of this paper is to take the idea from "Language as a Latent Variable" by Miao et al., and then change it from a VAE to an adversarial autoencoder. The reconstruction cost is not precisely explained, though I assume it s a teacher forced conditional log likelihood (conditioned on the "summary" sequence). The description of baselines for REINFORCE is a bit strange   e.g., annealing a constant in the baseline may affect variance of the gradient estimator, but the estimator is still unbiased and shouldn t significantly impact exploration. The results look decent, but I would be more impressed if the authors could show some benefit relative to the supervised model, e.g.in a reasonable semisupervised setting. Overall, the paper covers an interesting topic but could use extra editing to clarify details of the model and training procedure, and could use some redesign of the experiments to minimize the number of arbitrary (or arbitrary seeming) decisions.<BRK>TL;DR of paper: Generating summaries by using summaries as an intermediate representation for autoencoding the document. An encoder reads in the document to condition the generator which outputs a summary. An additional GAN loss is used on the generator output to encourage the output to look like summaries   this procedure only requires unpaired summaries. The results are that this procedure improves upon the trivial baseline  but still significantly underperforms supervised training. The problem set up of unpaired summarization is not particularly compelling, since summaries are typically found paired with their original documents. Unsurprisingly, the proposed method requires a lot of twiddling to make it work since GANs, REINFORCE, and pretraining are necessary.
Reject. rating score: 3. rating score: 6. rating score: 7. <BRK>This paper presents a clustering method in latent space. The proposed method is tested on several image and text data sets. However, the work has a number of problems and unclear points. 1) There is no theoretical guarantee that RCC or DCC can give good clusterings. The second term in Eq.2 will pull z s closer but it can also wrongly place data points from different clusters nearby. This is not suitable for data sets such as images and time series. 3) Please elaborate "redesending M estimator" in Section 2. 4) The method requires many extra hyperparameters lambda, delta_1, delta_2. 6) The experimental results are not convincing. For MNIST its best accuracy is only 0.912. Existing methods for this data set have achieve 0.97 accuracy. For RCV1, [Ref2] gives 0.54, but here it is only 0.495. 8) It is unknown how to set the number of clusters in proposed method.<BRK>The only difference in (3) from RCC DR is the decoding part, which is replaced by autoencoder instead of linear transformation used in RCC DR. Hence, DCC is just a simple extension of RCC DR. It is unclear why the existing autoencoder solver can be used to solve (3) or (5). It is better to clarify the correctness of the optimization algorithm. As a result, delta_2 is a fixed threshold for graph construction, so it is indirectly related to the number of clusters generated. Many terms in the paper are not well explained. For example, in (1), theta are treated as parameters to optimize, but what is the theta used for? Why should this estimator be used? The results between Table 2 and Figure 1 might be interesting to investigate.<BRK>The authors proposed a new clustering algorithm named deep continuous clustering (DCC) that integrates autoencoder into continuous clustering. As a variant of  continuous clustering (RCC), DCC formed a global continuous objective for joint nonlinear dimensionality reduction and clustering. Extensive experiments on image and document datasets show the effectiveness of DCC. The idea of integrating autoencoder with continuous clustering is novel, and the optimization part is quite different. The trick used in the paper (sampling edges but not samples) looks interesting and seems to be effective. It is difficult to follow Eq.(6) and (7). 2.Compare DCC to RCC, the pros and cons are obvious. Since clustering is one unsupervised learning task. This will increase the usability of the proposed method. In general, the paper is interesting and proposed method seems to be promising. I would vote for accept if my concerns can be addressed.
Reject. rating score: 3. rating score: 3. rating score: 6. <BRK>This paper proposes a framework called  multi instance learning , in which a time series is treated as a  set  of observations, and label is assigned to the full set, rather than individual observations. Major issues with the paper:   Lack of reliable experiment section. A few of the relevant hyper parameters are tuned and some important hyper parameters (i.e.number of hidden states in the LSTMs, or optimization method and learning rate) are not tuned. Quality and Significance   Due to small size of the cohort and lack of additional dataset, it is difficult to reliably access quality of experiments.<BRK>This could also be due to the small dataset. As the authors are proposing a new MIL learning paradigm, I feel they should experiment on a number of MIL tasks, not limited to analyzing time series medical data.<BRK>The paper addresses the classification of medical time series data by formulating the problem as a multi instance learning (MIL) task, where there is an instance for each timestep of each time series, labels are observed at the time series level (i.e.for each bag), and the goal is to perform instance level and series level (i.e.bag level) prediction. A comparison to other deep learning MIL methods, i.e.those that do not exploit the time series nature of the problem, would be valuable. 2003.A two level learning method for generalized multi instance problems. The proposed time series MIL problem formulation makes sense.
Accept (Poster). rating score: 9. rating score: 8. rating score: 8. <BRK>While ICLR is not focused on neuroscientific studies, this paper clearly belongs here as it shows what representations develop in recurrent networks that are trained on spatial navigation. The paper mentions a metabolic cost that is not specified in the paper. This should be added.<BRK>I enjoyed reading the paper which is in general clearly written. This is not to say that demonstrating that these patterns can arise as a byproduct is not important, on the contrary. These are just two complementary lines of work. Why is this a reasonable starting point to study the emergence of grid cells? It might be obvious to the authors but it will not be to the ICLR audience.<BRK>They find many properties reminiscent of neurons in the mammalian entorhinal cortex (EC): grid cells, border cells, etc. When regularization of the network is not used during training, the trained RNNs no longer resemble the EC.
Reject. rating score: 4. rating score: 4. rating score: 5. <BRK>The covered framework is limited to regularization parameters. One motivation of the approach is to fix the slow convergence of the method in (Franceschi et al, 2017). The paper proposes a dynamical system including the dynamical update of $\theta$ and the update of the gradient $y$, derivative of $\theta$ w.r.t.to the hyper parameters.<BRK>Summary of paper:This work proposes an extension to an existing method (Franceschi 2017) to optimize regularization hyperparameters. Summary of review:This is an incremental change of an existing method. This is acceptable as long as the incremental change significantly improves results or the paper presents some convincing theoretical arguments.<BRK># Summary of paperThe paper proposes an algorithm for hyperparameter optimization that can be seen as an extension of Franceschi 2017 were some estimates are warm restarted to increase the stability of the method. Typically, the regularization depends on both hyperparameters and parameters. This is not correct.
Reject. rating score: 4. rating score: 5. rating score: 6. <BRK>Graph Laplacian preserves the information about connectivity of the vertices and filtering in the vertex domain can be done via polynomial filters in the Fourier domain. I think it can be viewed as an extension of the GCN (Kipf and Welling, 2017), where instead of an adjacency matrix with self connections (i.e.first degree polynomial), a higher degree graph polynomial filter is used on every layer (please correct me if this comparison is not accurate). I think that the graph polynomial filters can be summarized in much more succinct way and details deferred to the appendix for interested reader.<BRK>Experiments are performed on the usual reference benchmarks for the task and showsensible improvements with respect to the state of the art. Why is the proposed approach better than thepreviously published ones, and when is that there is an advantage in using it? The authors should report statistics overmultiple runs. I find the paper interesting but not very clearly written in some sections,for instance I would better explain what is the main contribution and devotesome more text to the motivation.<BRK>The authors propose a new CNN approach to graph classification that generalizes previous work. EDIT  After reading the publications mentioned by the other reviewers as well as the following related contributions* Network of Graph Convolutional Networks Trained on Random Walks (under review for ICLR 2018)* Graph Convolution: A High Order and Adaptive Approach, Zhenpeng Zhou, Xiaocheng Li (arXiv:1706.09916)I agree that the relation to previous work is not adequately outlined. * The paper is clearly written and well illustrated by figures and examples. The paper is easy to follow although it is on an adequate technical level.
Accept (Poster). rating score: 8. rating score: 6. rating score: 4. <BRK>They set up a Wasserstein GAN and train it on both synthetic and real multi neuron recordings, using data from the salamander retina. What stimulus did they use? This is insinuated but not stated. As far as I know, this would be the first peer reviewed application of GANs to multi neuron recordings of neural data (but see https://arxiv.org/abs/1707.04582 for an arxiv paper, not cited here  should be discussed at least). A refractory period?) Therefore, the paper should really be evaluate as an `application  paper, and be assessed in terms of i) how important the application is, ii) how clearly it is presented, and iii) how convincing the results are relative to state of the art. i) [Importance of problem, potential significance] Finding statistical models for modelling and simulating population spike trains is a topic which is extensively studied in computational neuroscience, predominantly using  model based approaches using MaxEnt models, GLMs or latent variable models. iii) [Quality, advance over previous methods] The authors discuss several methods for simulating spike trains in the introduction. In particular, the only ‘gain’ of the GAN over ME  models in the results comes from their ability of the GAN to match temporal statistics. 4A,B, the texts suggests that packets fire spontaneously with a given probability.<BRK>The paper applies the GAN framework to learn a generative model of spike trains. The generated spike trains are compared to traditional model fitting methods, showing comparable or superior ability to capture statistical properties of real population  activity. This seems like an interesting exercise, but it’s unclear what it contributes to our understanding of neural circuits in the brain. The advantage of structured models is that they potentially correspond to underlying mechanisms and can provide insight. The authors point to the superior ability to capture temporal structure, but this does not seem like a fundamental limitation of traditional approaches. The potential applicability of this approach is alluded to in this statement toward the end of the paper:“...be used to describe and interpret experimental results and discover the key units of neural information used for functions such as sensation and behavior.”It is left for the reader to connect the dots here and figure out how this might be done.<BRK>Summary:The paper proposes to use GANs for synthesizing realistic neural activity patterns. Learning generative models of neural population activity is an important problem in computational neuroscience. Unfortunately, I do not see any such insights and am therefore not sure about the value of the paper. Pros:  Using GANs to synthesize neural activity patterns is novel (to my knowledge). Using the critic to learn something about what are the crucial population activity patterns is an interesting idea and could be a valuable contribution. The only demonstrated advantage of the proposed approach over MaxEnt models is that it models temporal correlations. GANs have well known problems like mode collapse and low entropy of samples. The authors do not address this issue, neither qualitatively nor quantitatively, although both would be possible:  a) A quantitative approach would be to look at the entropy of the data, the MaxEnt model and the GAN samples. The novel idea of using the critic to learn something about the crucial population activity patterns is not fleshed out at all. In practice, the (unsorted) filters will look just as uninterpretable as the (unsorted) population activity they show. Unfortunately, the idea is not really fleshed out or studied in any detail. Poor comparison to state of the art. However, it is not clear that MaxEnt models are the state of the art.
Accept (Poster). rating score: 8. rating score: 6. rating score: 6. <BRK>The paper would greatly benefit from a deeper comparison over other techniques. The key idea is a smart evolution scheme. The implementation seems technically sound. It circumvents the traditional tradeoff between search space size and complexity of the found models.<BRK>This work fits well into a growing body of research concerning the encoding of network topologies and training of topology via evolution or RL. It may be more clear to change this phrase to “evolutionary methods” or similar. The work is still great, but this misleading statement in the beginning of the paper left the rest of the paper with a dishonest aftertaste. It seems fewer hyperparameters are needed to describe VGG 16, making this paper hardly an alternative to the "[common solution] to restrict the search space to reduce complexity and increase efficiency of architecture search."<BRK>The fundamental contribution of the article is the explicit use of compositionality in the definition of the search space. If that was indeed the case, there is a systematic asymmetry between the probability to add and remove an edge, making the former considerably more likely. Overall, the paper is well written, clear in its exposition and technically sound.
Reject. rating score: 4. rating score: 5. rating score: 6. <BRK>This paper proposed the combination of topic model and seq2seq conversational model. However, I m not sure that the empirical evaluation shows the really impressive results. In particular, the difference between LV S2S and LTCM seem to be trivial. Moreover, the details of human evaluation are not clear, e.g., the number of users and the meaning of each rating.<BRK>In addition, it could be even better if there are some analysis about topics extracted by this model. This paper also doesn t pay much attention to the existing work on topic driven conversational modeling.<BRK>My only concern about the paper is that is very incremental in nature   the authors combine two separate models into a relatively straight forward way. The results do are good and validate the approach, but the paper has little to offer beyond that.
Reject. rating score: 5. rating score: 5. rating score: 7. <BRK>This paper studies the approach of coreset for SVM. The coreset idea has been applied to SVM in existing work, but this paper uses a new theoretical framework. Definition 3: what randomness is the probability with respect to? Indeed one can do divide and conquer.<BRK>I think the experimental section is still a bit weak (given that there are several very competitive SVM algorithms that the paper didn t compare with). In fact, the experimental result on the running time is a bit weak. The notion of coreset was originally formulated in computational geometry by Agarwal et al.(see e.g., [A])Recently it has been extended to several clustering problems, linear algebra, and machine learning problems. The paper follows from now standard technique for constructing coreset.<BRK>As per the theory developed in these past works, sampling a subset of size proportional to the sum of sensitivities gives a coreset for the given problem. Since Coreset construction is being sold as a fast alternative to previous methods for training SVMs, it would have been nice to see the running time and cost comparison with other training methods that have been discussed in section 2. Importance sampling: This is based on the theory developed in Feldman and Langberg, 2011 (and some of the previous works such as Langberg and Schulman 2010, the reference of which is missing). One aspect in which the paper is lacking is the empirical analysis.
Reject. rating score: 4. rating score: 5. rating score: 5. <BRK>The merit of this approach is to decompose the learning of complex networks to that of small to large networks in a moderate way and it uses less computational costs. The experimental results are good. Comments: The idea of the paper is natural and interesting. The details are below. It is not a mistake, but, there is no explanation why the equality between (27) and (28) holds. Please add an explanation.<BRK>Disclaimer: I reviewed this paper for NIPS as well and many of comments made by reviewers at that time still apply to this version of the paper as well, although presentation has overall improved. Paper concludes with some experimental results. This indeed appears to an interesting insight about ResNet training. In particular, generalization bounds presented in this work are results taken from that paper (which authors admit).<BRK>Based on this formulation, the authors prove that the generalization error bound decays exponentially with respect to the number of residual blocks. Overall, the paper is well organized and easy to follow. My concerns are mainly on the proposed BoostResNet algorithm. 2.The claim that BoostResNet is memory efficient may not hold in practice. Is there any explanations?
Accept (Poster). rating score: 9. rating score: 5. rating score: 5. <BRK>This is a well written paper with good comparisons to a number of earlier approaches. Results with 2 bit activations and 4 bit weights seem to match baseline accuracy across the models listed in the paper. Pros:  Positive results with low precision (4 bit, 2 bit and even 1 bit)  Moving the state of the art in low precision forward  Strong potential impact, especially on constrained power environments (but not limited to them)  Uses same hyperparameters as original training, making the process of using this much simpler. How much does that impact the overall compute? Is there a certain width where 1 bit activation and weights would match the accuracy of the baseline model?<BRK>This paper presents an simple and interesting idea to improve the performance for neural nets. The idea is we can reduce the precision for activations and increase the number of filters, and is able to achieve better memory usage (reduced). The paper is aiming to solve a practical problem, and has done some solid research work to validate that. In particular, this paper has also presented a indepth study on AlexNet with very comprehensive results and has validated the usefulness of this approach. In addition, in their experiments, they have demonstrated pretty solid experimental results, on AlexNet and even deeper nets such as the state of the art Resnet. On the other side, the idea of this paper does not seem extremely interesting to me, especially many decisions are quite natural to me, and it looks more like a very empirical practical study. So the novelty is limited. So overall given limited novelty but the paper presents useful results, I would recommend borderline leaning towards reject.<BRK>The paper studies the effect of reduced precision weights and activations on the performance, memory and computation cost of deep networks and proposes a quantization scheme and wide filters to offset the accuracy lost due to the reduced precision. Positives  Using lower precision activations to save memory and compute seems new and widening the filter sizes seems to recover the accuracy lost due to the lower precision. Negatives  While the exhaustive analysis is extremely useful the overall technical contribution of the paper that of widening the networks is fairly small. The paper motivates the need for reduced precision weights from the perspective of saving memory footprint when using large batches. However, the results are more focused on compute cost. Also large batches are used mainly during training where memory is generally not a huge issue. Memory critical situations such as inference on mobile phones can be largely mitigated by using smaller batch sizes. It might help to emphasize the speed up in compute more in the contributions.
Invite to Workshop Track. rating score: 6. rating score: 6. rating score: 5. <BRK>This is mostly a “theory building” work. The result in Section 3 on local openness of matrix multiplication on its range (which is substantially motivated by Behrends 2017) may be of independent interest. I did not check the proof of this result in detail, but it appears to be correct. For the linear, deep case, the paper corrects imprecisions in the previous work (Lu + Kawaguchi). For deep nonlinear networks, the results require the “pyramidal” assumption that the dimensionality is nonincreasing with respect to layer and (more restrictively) the feature dimension in the first layer is larger than the number of input points. This seems to differ from typical practice, in the sense that it does not allow for wide intermediate layers. There are some imprecisions in the writing.<BRK>The paper covers a somewhat wide range of deep nets (from shallow with linear activation to deeper with non linear activation); it focuces only on feed forward neural networks. Originality: The paper provides similar results to previous work, while removing some of the assumptions made in previous work. Thus, I would say original. If that is the case, could the authors describe this a bit further?<BRK>The paper studies locally open maps, which preserve the local minima geometry. Comments:The locally open maps (Behrends 2017) is an interesting concept. However I am not convinced that the paper is able to show stronger results about the geometry of linear/neural networks. Further the claims all over the paper, comparing with the existing works. are over the top and not justified. The results are not a strict improvement over existing works. Results for linear networks are not an improvement over existing works. How do you use this to conclude locally openness of \mathcal{M}?
Invite to Workshop Track. rating score: 5. rating score: 4. rating score: 3. <BRK>The paper considers the synthetic problem setting of classifying two concentric high dimensional spheres and the worst case behavior of neural networks on this task, in the hope to gain insights about the vulnerability of deep networks to adversarial examples. The results are presented in  rather descriptive rather than a quantitative way. It is observed that words case and average case empirical error estimates diverge when the input is high dimensional.<BRK>However, the results reported here are not sufficiently significant for ICLR. The authors make a big deal throughout the paper about how close to training data the adversarial examples they can find on the data manifold are. They also stress in the Conclusions their Conjecture 5.1 that under some assumptions “the average distance to nearest error may decrease on the order of O(1 / d) as the input dimension grows large.” However, earlier they admitted that “Whether or not a similar conjecture holds for image manifolds is unclear and should be investigated in future work.” So, the practical significance of this conjecture is unclear.<BRK>It looks as if the paper was written in a hurry, and it shows in the writing. This paper is a list of experiments and observations, that are not coherent and does not give much insight into the topics of "adversarial examples". The only main messages are that on ONE synthetic dataset, random perturbation does not cause misclassification and targeted classification can cause misclassification. Is it related to any experiment?
Invite to Workshop Track. rating score: 5. rating score: 5. rating score: 5. <BRK>The paper makes the simple but important observation that (deep) reinforcement learning in alternating Markov games requires a min max formulation of the Bellman equation as well as careful attention to the way in which one alternates solving for both players  policies in a policy iteration setting. While some of the core algorithmic insights regarding Algorithms 3 & 4 in the paper stem from previous work (Condon, 1990; Hoffman & Karp, 1966), I was not actually aware of these previous results until I reviewed this paper. Because this paper is not proposing the best Hex player (i.e., the winning rate against Wolve never exceeds 0.5), I think it is quite reasonable to request the authors to compare AMCPG A and AMCPG B to standard REINFORCE variants on other games (they do not need to be as difficult as Hex). Finally, assuming that the results do generalize to other games, I am left wondering about the significance of the contribution. This is a useful contribution no doubt, but I am concerned with whether it meets the significance level that I am used to with accepted ICLR papers in previous years.<BRK>This paper introduces a variation over existing policy gradient methods for two players zero sum games, in which instead of using the outcome of a single policy network rollout as the return, they use the minimum outcome among a few rollouts either from the original position or where the first action from that position is selected uniformly among the top k policy outputs. The proposed method supposedly provides slightly stronger targets, due to the extra lookahead / rollouts. There is no comparison against state of the art methods like AlphaGo Zero which uses MCTS root move distribution and MCTS rollouts outcome to train policy and value network, even though the author do cite this work. There is also no comparison with Hexit which also trains policy net on MCTS move distribution, and was also applied to Hex. For example,  was it really necessary to introduce state transition probabilities p(s’, a, s) when all the experiments are done in the deterministic game of Hex ? It is conceivable that the vanilla reinforce would do just as well as the proposed method if the plots were aligned this way. It would also be good to know the asymptotic behavior. I would recommend to accept for a workshop paper but not sure about the main track.<BRK>This paper is outside of my area of expertise, so I ll just provide a light review:  the idea of assuming that the opponent will take the worst possible action is reasonable in widely used in classic search, so making value functions follow this intuition seems sensible,  but somehow I wonder if this is really novel? the results on Hex have some signal, but I don’t know how to calibrate them w.r.t.The state of the art on that game? A 40% win rate seems low, what do other published papers based on RL or search achieve?
Reject. rating score: 3. rating score: 4. rating score: 5. <BRK>Major* The novelty of the paper is not clear. Perhaps the advantage of diversity is in interpretability but that is hard to quantify and the authors did not put enough effort to do that; we only have small anecdotal results in section 4.3.<BRK>The paper studies a regularization method to promote sparsity and reduce the overlap among the supports of the weight vectors in the learned representations. The motivation of using this regularization is to enhance the interpretability of the learned representation and avoid overfitting of complex models. For example, in the image classification problem with ResNet.<BRK>I would suggest to move the derivations of Sec.3.1 into an appendix not to break the flow of the readers. The paper is overall clear and fairly well structured, but it suffers from several flaws, as next discussed. Is a "manual" thresholding applied thereafter?
Reject. rating score: 3. rating score: 4. rating score: 4. <BRK>First of all, the connection to multi label learning is very loose, and the majority of the paper deals with learning the latent variable model. Second, there is almost nothing new in the paper compared to Anandkumar et al 2012, except it uses probability as parameters but not expectations.<BRK>The main novelty in this paper is that it uses the label as a third view of a multi view model and make use of cross moments. Equation (7) and also (9) only works in the standard bag of words model that is also used in Anandkumar et al.(the same equations were also proved). The model between document and topic is very similar to previous pure topic models (see more discussions below), and because it is a pure topic, the label is just modeled by a conditional distribution. The reviewer feels this alone is not enough contribution.<BRK>The model is very close to latent Dirichlet allocation by Blei, et al.(2003), but differences are not discussed. If think the authors should do some more work before this paper can be published. The use of the whitening approach is not justified in their setting working with joint distributions of couples and triples and it has no statistical meaning.
Reject. rating score: 3. rating score: 4. rating score: 4. <BRK>without any support for these desiderata. Without this problem description and a good motivation, it is impossible to assess why such desiderata (which look awkward to me) are important. Could not follow this part, as such elements lack definition. Experiments are uninteresting and show same results as many other RL algorithms that have been proposed in the past. The paper should continue the experimental section making explicit comparisons with such related work.<BRK>My main problem with this paper is clarity and I am afraid that not everything might be technically correct. Let me just list my main concerns in the below. The definition of social dilemma, is unclear:"A social dilemma is a game where there are no cooperative policies which form equilibria. If not, there seems to be a contradiction? And this only holds for the tabular case.<BRK>From a game theoretic point of view, the paper begins with somewhat sloppy definitions followed by a theorem that is not very surprising. The paper follows with some fun experiments implementing these new game theory notions. Unfortunately, since the game theory was not particularly well motivated, I did not find the overall story compelling. We refer to this game as the PongPlayer’s Dilemma (PPD). However, a fullycooperative agent can be exploited by a defector. Thus, in the PPD the only (jointly) winningmove is not to play, but selfish agents are again tempted to defect and try to score points even thoughthis decreases total social reward.
Reject. rating score: 4. rating score: 5. rating score: 5. <BRK>This paper s main thesis is that automatic metrics like BLEU, ROUGE, or METEOR is suitable for task oriented natural language generation (NLG). The authors here show that the performance of various NN models as measured by automatic metrics like BLEU and METEOR is correlated with human eval. Overall, this paper presents a useful conclusion: use METEOR for evaluating task oriented NLG.<BRK>1) This paper conducts an empirical study of different unsupervised metrics  correlations in task oriented dialogue generation. The third paragraph in the introduction should include one use case about non task oriented dialogue system, such as chatbots. Maybe the dataset is too simple with limited options or the training/testing are very similar to each other, even the random could achieve very good performance in table 1 and 2. Even the random could achieve 0.8 (out of 1) in BLEU, this is a very high performance. v) BLEU usually correlates with human better when 4 or more references are provided. I suggest the authors include some dataset with 4 or more references instead of just 2 references.<BRK>The authors present a solid overview of unsupervised metrics for NLG, and perform a correlation analysis between these metrics and human evaluation scores on two task oriented dialog generation datasets using three LSTM based models. But the novel work in the paper is limited to the human evaluations collected and the correlation studies run, and the authors  efforts to analyze and extend these results fall short of what I d like to see in a conference paper.
Invite to Workshop Track. rating score: 8. rating score: 5. rating score: 4. <BRK>This was originally adapted from the SUNCG dataset, enhanced with the addition of a physics model and an API for interacting with the environment. The paper in general is well written, and the environment will be a useful addition to the community. Whilst the novelty of the two models is questionable (they are adaptations of existing models to the task),  they are a useful addition to enable a benchmark on the task. As a general comment it seems that this is a very strong (and unrealistic) reward signal, particularly for generalisation.<BRK>2.The authors provide a thorough analysis on the contribution of different feature types (Mask, Depth, RGB) towards the success rate of the goal task. The authors claim that the proposed environment allows for multiple applications and interactions, however from the description in section 3, the capacities of the simulator beyond navigation are unclear. The most important contributions with respect to SUNCG are:  An efficient renderer: an important aspect. This is not technically challenging. It seems to me that the continuous policy would be justified in this setting.<BRK>These methods are applied for the task of navigation. (2) There is not much novelty in the paper. The SUNCG dataset already exists. Adding a renderer to that is not a big deal. The paper proposes to use gated attention, which is not novel and it does not help much according to Figures 3b and 4b. This environment is not very different from existing platforms. Success rate is not enough for evaluation.
Reject. rating score: 5. rating score: 5. rating score: 6. <BRK>As a final point, I would say that while some of the criticisms could be addressed in a revision, the improvements seem relatively modest. Seems like visual and biological are distinct attributes but visual attention can be biological (or not, I guess) and it is not clear how biological the proposed approach is. Certainly no attempt is made by the authors to connect to biology. In general, the paper could be clearer.<BRK>This paper proposes a feed forward attention mechanism for fine grained image classification. 3) My main concern is the improvement gain is very small. Strength of this work:1) It is end to end trainable and doesn t require multiple stages, prediction can be done in single feedforward pass.<BRK>Introduction states that the method is simple and easy to understand. Why not much stronger constraint such as orthogonality over elements of M in equation 1? One may use a Sigmoid as well? Is it better to use soft max? Equation 9 is not entirely clear to me. Why not perform this experiment over all 5 datasets?
Accept (Poster). rating score: 8. rating score: 6. rating score: 6. <BRK>Summary of the reviews:Pros:•	A novel way to evaluate the quality of heterogeneous data sources. •	An interesting way to put the data quality measurement as a regularization term in the objective function. Cons:•	Since the data quality is a function of local variation, it is unclear about the advantage of the proposed data quality regularization versus using a simple moving average regularization or local smoothness regularization. •	It is unclear about the advantage of using the Multi layer graph convolutional networks versus two naïve settings for graph construction + simple LSTM, see detailed comments D1Detailed comments:D1: Compared to the proposed approaches, there are two alternative naïve ways: 1) Instead of construct the similarity graph with Gaussian kernel and associated each vertex with different types of time series, we can also construct one unified similarity graph that is a weighted combination of different types of data sources and then apply traditional LSTM; 2) During the GCN phases, one can apply type aware random walk as an extension to the deep walk that can only handle a single source of data. It is unclear about the advantage of using the Multi layer graph convolutional networks versus these two naïve settings. Either some discussions or empirical comparisons would be a plus.<BRK>The paper is an application of neural nets to data quality assessment. The authors introduce a new definition of data quality that relies on the notion of local variation defined in (Zhou and Schölkopf, 2004), and they extend it to multiple heterogenous data sources. The data quality function is learned using a GCN as defined in (Kipf and Welling, 2016). However the paper mostly provides a framework that relies on existing work. Maybe publish it in a data quality conference/journal. The application is very nice and complex, but I find that the experiments are a little bit too limited. 6) The notations w and W are used for different things, and that is slightly confusing. Usually one is used as a matrix notation of the other. 7) I would tend to associate data quality with how noisy observations are at a certain node, and not heterogeneity. It would be good to add some discussion on noise in the paper. 8) “A bridge node highly likely has a lower data quality level due to the heterogeneity”.<BRK>Paper reads better and comparison to Auto regression was added. This work presents a novel way of utilizing GCN and I believe it would be interesting to the community. In this regard, I have updated my rating. Results in Table 1 show that proposed method is capable of forecasting next hour temperature with about 0.45C mean absolute error. In this paper authors develop a notion of data quality as the function of local variation of the graph nodes. The concept of local variation only utilizes the signals of the neighboring vertices and GCN is used to take into account broader neighborhoods of the nodes. I liked the idea of using local variations of the graph signals as quality of the signal. GCN does so intuitively, but it is not clear what exactly is happening due to non linearities. It can more evidently capture the K hop neighborhood of a vertex. It is not easy to follow what information is used for training and testing. means.I would expect that forecasting of temperature tomorrow is solely performed based on today s and past information about temperature and other measurements. I would like to see comparison to some classical time series forecasting techniques, e.g.Gaussian Process regression and Auto regressive models. Also some references and comparisons are needed to state of the art weather forecasting techniques.
Invite to Workshop Track. rating score: 6. rating score: 6. rating score: 6. <BRK>The manuscript proposes to increase the norm of the last hidden layer to promote better classification accuracy. However, the motivation is a bit less convincing. How do we know it will not lead to overfitting? (2) Increasing the feature norm of mis classified examples will make gradient larger for self correction. As an example, if a negative example is already mis classified as a positive, and its current probability is very close to 1, then further increasing feature norm would make the probability even closer to 1, leading to saturation and smaller gradient. (3) Figure 1 shows that examples with larger feature norm tend to be predicted well. In addition, alpha is in the formula of upper bound, but what is the upper bound of alpha? The manuscript does comprehensive experiments to test the proposed method.<BRK>Pros:1.It provided theoretic analysis why larger feature norm is preferred in feature representation learning. 2.A new regularization method (feature incay) is proposed. Cons:It seems there is not much comparison between this proposed method and the concurrent work "COCO(Liu et al.(2017c))".<BRK>The reciprocal norm loss seems to be reasonable idea to improve the CNN learning based on the analyses. However, the presentation of this paper need to be largely improved. Therefore, the author used reciprocal norm loss which increases feature norm as shown in Figure 4. The author should refer all Figures and Tables. In the experimental results, “RN” is not defined. Table 5 is not referred in the main text. Following are further comments for presentation. It would be better put Figure 5 under Property3. However, in Table 5, it seems to be 261.
Accept (Poster). rating score: 6. rating score: 6. rating score: 6. <BRK>This paper proposes replacing the weights of the final classifier layer in a CNN with a fixed projection matrix.<BRK>The paper proposes to use a fixed weight matrix to replace the final linear projection in a deep neural network. This is difficult to get my head round. The idea is extremely simple and I like it conceptually.<BRK>Including a fixed Hadamard matrix for the classification layer is I believe new (although related to an existing literature on using structured matrices in neural networks). Original Review:The paper proposes fixing the classification layers of neural networks, replacing the traditional learned affine transformation with a fixed (e.g., Hadamard) matrix.
Reject. rating score: 4. rating score: 5. rating score: 5. <BRK>This paper proposes a new reading comprehension model for multi choice questions and the main motivation is that some options should be eliminated first to infer better passage/question representations. It is a well written paper, however, I am not very convinced by its motivation, the proposed model and the experimental results. First of all, the improvement is rather limited. I don’t see any convincing explanations here. Secondly, in terms of the development of reading comprehension models, I don’t see why we need to care about eliminating the irrelevant options. If the point is that the options can add useful information to induce better representations for passage/question, there should be some simple baselines in the middle that this paper should compare to. Some visualizations/motivational examples could be also useful to understand how some options are eliminated and how the document representation has been changed based on that.<BRK>This paper gives an elaboration on the Gated Attention Reader (GAR) adding gates based on answer elimination in multiple choice reading comprehension. This dataset has not attracted much attention and most work in reading comprehension has now moved to the SQUAD dataset for which there is an active leader board. I realize that SQUAD is not explicitly multiple choice and that this is a challenge for an answer elimination architecture. However, it seems that answer elimination might be applied to each choice of the initial position of a possible answer span. In any case, competing with an active leader board would be much more compelling.<BRK>In this paper, a model is built for reading comprehension with multiple choices. In order to show the usefulness of the elimination module, the model should be exactly built on the GAR with an additional elimination module (i.e.after removing the elimination module, the performance should be similar to GAR but not something significantly worse with a 42.58% accuracy). 2) A figure showing the model architecture and the corresponding QA process will better help the readers understand the proposed model. What’s the performance of only using $s_i$ for answer selection or replacing $x^L$ with $s_i$ in score function? (4), it would be better to use a vector as the input of softmax.
Accept (Poster). rating score: 7. rating score: 6. rating score: 5. <BRK>The main findings are that prosocial agents are able to learn to ground symbols using RL, but self interested agents are not. The work is interesting and clearly described, and I think this is an interesting setting for studying emergent communication.<BRK>The background and results are well contextualised with relevant related work. The same applies to Figure 4 (although less critical in this case). One of the motivating factors of using cheap talk is the exploitation of lying on the part of the agents. In summary, the paper presents an interesting approach to combine unsupervised learning with multiple communication channels to improve learning of preferences in a well established negotiation game.<BRK>The experimental setup is clear, although the length of the utterances and the number of symbols in them is not explicitly stated in the text (only the diagrams). Experiment 2 shows that by making the agents prosocial, they are able to learn to communicate on the linguistic channel to achieve pretty much optimal rewards, a very nice result. Why are the 10 turn games even included in this table? Why do self interested agents for 10 turns on the linguistic channel terminate early? Overall, the paper has some nice results and an interesting ideas but could do with some tightening up of the results to make it really good.
Reject. rating score: 3. rating score: 3. rating score: 4. <BRK>This is very likely an incorrect and unsatisfying assumption which does not take into account narrative, context etc. It would be good to clarify from the start that the model does need paired videos and text, and state exactly how much aligned data is needed. 3.The setup and ultimately the motivation in context of the setup is fairly artificial   the dataset does have images corresponding to each “dialog” so it is unclear why the associative model is needed in this case. 2.For a large part the paper talks about how visual instances are not available for textual phrases and then proceeds to assume access to aligned text and visual data.<BRK>Unfortunately, while the idea has merit, and I d like to see it pursued, the paper suffers from a fatal lack of validation/evaluation, which is very curious, given the amount of data that was collected, the fact that the authors have both a training and a test set, and that there are several natural ways such an evaluation might be performed. Is this worth talking about? How often is this associative vector closer to a confounding image vector than an appropriate one? The result is a model that can make use of the text visual associations without needing visual stimuli.<BRK>The contribution of the paper relies on the usage of visual information to enhance the performance of a dialogue system. I am not very familiar with this type of systems, but it is clear to me that the evaluation is biased and does not prove the working hypothesis of the authors.
Reject. rating score: 4. rating score: 5. rating score: 5. <BRK>It starts from observation that for batch learning with a fixed number of epochs, the accuracy drops when the batch size is too large. Assuming that the number or epochs and batch size are fixed, the contribution of the paper is a heuristic that assigns different learning late to each layer of a network depending on a ratio of the norms of weights and gradients in a layer. The experimental results show that the proposed heuristic helps AlexNet and ResNet end up in a larger accuracy on ImageNet data. Positives:  the proposed approach is intuitively justified  the experimental results are encouraging  Negatives:  the methodological contribution is minor  no attempt is made to theoretically justify the proposed heuristic  the method introduces one or two new hyperparameters and it is not clear from the experimental results what overhead is this adding to network training  the experiments are done only on a single data set, which is not sufficient to establish superiority of an approach  Suggestions:  consider using different abbreviation (LARS is used for least angle regression)<BRK>This paper provides an optimization approach for large batch training of CNN with layer wise adaptive learning rates. It starts from the observation that the ratio between the L2 norm of parameters and that of gradients on parameters variessignificantly in the optimization,  and then introduce a local learning rate to consider this observation for a more stable and efficient optimization. Experimental results show improvements compared with the state of the art algorithm. However, it seems that the authors also combine with LR "warm up" in your proposed method in the experimental part, e.g., Table 3. ii) There is one coefficient that is independent from layers and needs to be set manually in the proposed local learning rate. The authors do not have a detail explanation and experiments about it. Some parts are confusing, for example, the authors claim that they use initial LR 0.01, but in Table 1(a) it is 0.02.<BRK>This paper proposes a training algorithm based on Layer wise Adaptive Rate Scaling (LARS) to overcome the optimization difficulties for training with large batch size. The authors use a linear scaling and warm up scheme to train AlexNet on ImageNet. The presented method is interesting. My detailed comments are as follows. The authors propose a training algorithm based LARS with the adaptive learning rate for each layer, and train the AlexNet and ResNet 50 to a batch size of 16K. Weak points:The training algorithm does not overcome the optimization difficulties when the batch size becomes larger (e.g.32K), where the training becomes unstable, and the training based on LARS and warm up can’t improve the accuracy compared to the baselines. 3.How can the training algorithm based on LARS improve the generalization for the large batch? How to choose it?
Accept (Poster). rating score: 8. rating score: 6. rating score: 6. <BRK>The paper obtains a lower bound on the rank of the resultant grid tensors, and uses them to show that an exponentially large number of non overlapping ConvACs are required to approximate the grid tensor of an overlapping ConvACs. Assuming that the result carries over to ConvNets, I find this result to be very interesting.<BRK>What is the factor of augmentation? That this is the case should be made clear in the title and abstract. The paper makes a point that generalized tensor decompositions can be potentially applied to solve the more general case, but since it is left as future work, the paper should make it clear throughout.<BRK>The paper studies the expressive power provided by "overlap" in convolution layers of DNNs. That said, the results could be of interest to those thinking about minimizing the number of connections in ConvNets, as it gives some intuition about how much overlap might  suffice . I recommend weak accept. The key part of the proof is showing a lower bound on the rank for networks with overlap.
Reject. rating score: 4. rating score: 4. rating score: 7. <BRK>This paper proposed to learn a generative GAN model that generates the training data from the labels, given that only the black box mapping $f$ from data to label is available, as well as an aux dataset that might and might not overlap with the training set. It would be great if the author could show how the aux dataset is partitioned according to the function $f, and what is the representative sample from aux dataset that maximizes a given class label. Given the current version of the paper, it is not clear at all.<BRK>What is the result when the auxiliary dataset comes from a different kind of images? Clarity: The paper is well structured and written, but Sections 1 4 could be significantly shorter to leave more space to additional and more conclusive experiments. Significance: If additional simulations confirm the author’s claims, this work can represent a significant contribution to the forensic analysis of discriminative classifiers. It seems that generated images contain only parts of the auxiliary images related to the most discriminative features of the given classifier.<BRK>This paper considers a new problem : given a classifier f trained from D_tr and a set of auxillary samples from D_aux, find D_tr conditioned on label t*. It is unclear to me if the generated distribution in the experiments is similar to the original distribution D_tr given y   t^*, either from inception accuracy or from pictorial illustration. Since we have hold out the training data, perhaps we can measure the distance between the generated distribution and D_tr given y   t^* directly. 2.It would be great if we can provide experiments quantifying the utility of the auxillary examples.
Reject. rating score: 3. rating score: 4. rating score: 4. <BRK>The paper proposes a technique for exploiting prior knowledge to learn embedding representations for new words with minimal data. The problem is useful and very relevant to natural language applications, especially considering the widespread use of word embeddings within NLP systems. Such an analysis would be useful to understand the effectiveness of the overall approach. This is useful to understand how the test perplexity varies with #training examples for these individual settings.<BRK>However, the main evaluation only considers four words   "bonuses", "explained", "marketers", "strategist"   with no explanation of how these words were chosen. How can we use the "current" embedding for a word if it s never been seen before?<BRK>Their results show that learning from centroids of other words can outperform full training on the new words. ExplanationThe paper is well written, and the experiments are well explained. It is an interesting paper, and a research topic which is not well studied. The method seems to work well. The main contribution of this work is the evaluation section. Why only use the PTB language modeling task.
Accept (Poster). rating score: 7. rating score: 7. rating score: 7. <BRK>The paper proposes action dependent baselines for reducing variance in policy gradient, through the derivation based on Stein’s identity and control functionals.<BRK>Nevertheless, applying Stein s identity to estimating policy gradient is a novel approach in reinforcement learning community. To me, this approach is the right way of constructing control variates for estimating policy gradient. Overall this is a strong paper and I recommend to accept.<BRK>In this work, the authors suggest the use of control variate schemes for estimating gradient values, within a reinforcement learning  framework. The first one introducing Stein’s identity and the related comments that you need, and a second one, starting after Theorem 3.1, with title “Stein Control Variate”. The paper is interesting and well written. I suggest to divide Section 3.1 in two subsections.
Accept (Poster). rating score: 7. rating score: 6. rating score: 5. <BRK>The author give strong and convincing justifications based on the Lagrangian dual of the Bellman equation (although not new, introducing this as the justification for the architecture design is plausible). Alg 1 line 5:  closed form : there is no closed form in Eq(14). It is just an MC approximation. The algorithm requires that every step is crystal clear. 2.Also, there are several format error which may be due to compiling, e.g., line 2 of Abstract, Dual AC   (an extra space). There are many format errors like this throughout the paper. The author is suggested to do a careful format check. The current justification is reasonable but too brief.<BRK>The paper is well written, and the authors do an admirable job of motivating their primary contributions throughout the early portions of the paper. Turning to the experimental section, I think the authors did a good job of evaluating their approach with the ablation study and comparisons with PPO and TRPO. The difference in performance for Dual AC between Figure 1 and Figure 2b is significant, but the only difference seems to be a reduce batch size, is this right? Nonetheless, in recent work PPO and TRPO performance on the same set of tasks seem to be substantively different than what the authors get in their experiments. Part of this could be the notion of an "iteration", which was not clear to me how this corresponded to actual time steps. Most likely, to my mind, is that the parameterization used (discussed in the appendix) is improving TRPO and hurting PPO.<BRK>Empirically, how does changing the regularization parameter affect the performance in terms of reward maximization? Lastly, there are empirical experiments done to conclude the superior performance of Dual AC in contrast to other actor critic algorithms. Overall, this paper could be a significant algorithmic contribution, with the caveat for some clarifications on the theory and experiments. Page 2: In equation 5, there should not be a  ds  in the dual variable constraint Further, eta_v must be chosen to ensure that it does not affect (constrain) the optimal solution, which implies it might need to be very small. The transition from the second line of the proof to the third line is not clear. It would also be good to more clearly state on page 14 how you get the first inequality, for || V^* ||_{2,mu}^2.
Reject. rating score: 5. rating score: 5. rating score: 6. <BRK>I find this paper not suitable for ICLR. All the results are more or less direct applications of existing optimization techniques, and not provide fundamental new understandings of the learning REPRESENTATION.<BRK>The paper studies the global convergence for policy gradient methods for linear control problems.<BRK>2) The authors state in the abstract (and elsewhere): "... showing that (model free) policy gradient methods globally converge to the optimal solution ...". The authors show the convergence of the objective but not of the iterates sequence.
Reject. rating score: 6. rating score: 6. rating score: 7. <BRK>This work addresses the scenario of fine tuning a pre trained network for new data/tasks and empirically studies various regularization techniques. Relationship to prior work:Regularizing a target model against a source model is not a new idea. The authors miss key connections to A SVM [1] and PMT SVM [2]   two proposed transfer learning models applied to SVM weights, but otherwise very much the same as the proposed solution in this paper. Please at least label the test sets on each sub graph. 4) There seems to be some issue with the freezing experiment in Figure 2. The experiments are now clear and thorough enough to provide a convincing argument for using this regularization in deep nets.<BRK>The paper proposes an analysis on different adaptive regularization techniques for deep transfer learning. + The paper is easy to read and well organized+ The advantage of the proposed regularization against the more standard L2 regularization is clearly visible from the experiments  The idea per se is not new: there is a list of shallow learning methods for transfer learning based on the same L2 regularization choice[Cross Domain Video Concept Detection using Adaptive SVMs, ACM Multimedia 2007][Learning categories from few examples with multi model knowledge transfer, PAMI 2014][From n to n+ 1: Multiclass transfer incremental learning, CVPR 2013]I believe this literature should be discussed in the related work section  It is true that the L2 SP Fisher regularization was designed for life long learning cases with a fixed task, however, this solution seems to work quite well in the proposed experimental settings.<BRK>The paper addresses the problem of transfer learning in deep networks. A pretrained network on a large dataset exists, what is the best way to retrain the model on a new small dataset? Although the novelty of the paper is limited and have been shown for transfer learning with SVM classifiers prior to resurgence of deep learning, the reviewer is unable to find a prior work doing same regularization in deep networks.
Invite to Workshop Track. rating score: 6. rating score: 6. rating score: 5. <BRK>The authors introduce a sequential/recurrent model for generation of small graphs. The recurrent model takes the form of a graph neural network. Similar to RNN language models, new symbols (nodes/edges) are sampled from Bernoulli or categorical distributions which are parameterized by small fully connected neural networks conditioned on the last recurrent hidden state. The paper is very well written, nicely structured, provides extensive experimental evaluation, and examines an important problem that has so far not received much attention in the field. [1] speeds this process up by predicting multiple nodes and edges at once, whereas in this paper, such a multi step process is left for future work. A short discussion of this result would make the paper stronger. Typos in this sentence: “Lastly, when compared using the genaric graph generation decision sequence, the Graph architecture outperforms LSTM in NLL as well.”Overall I feel that this paper can be accepted with some revisions (as discussed above), as, even though it shares many similarities with previous work on a very related problem, it is well written, well presented and addresses an important problem.<BRK>The authors proposed a graph neural network based architecture for learning generative models of graphs. I support its acceptance. The draft does need some improvements and here is my suggestions. If space allowed, an example of different ordering leads to the same graph will also help. 2.More details on how node embedding vectors are initialized. Why is nodes at different stages with the same initialization problematic? 3.More details of how conditioning information is used, especially for the attention mechanism used later in parse tree generation. 4.The sequence ordering is important. While the draft avoids the issue theoretically, it does has interesting results in molecule generation experiment. 5.In Section 4.1, the choice of ER random graph as a baseline is too simplistic. It does not provide a meaningful comparison. A better generative model for cycles and trees could help.<BRK>A summary of all the hyperparameters should be given. Also, a summary of the hyperparameters used in the proposed system should be given. The paper contain many interesting contributions but, in my opinion, the model is too general and the focus should be given on some retricted classes of graphs. I am not convinced by the discussion on graph grammars in the second paragraph. * Section 5 should contain a discussion on complexity issues because it is not clear how the model can learn large graphs. The discussion on the difficulty of training shoud be emphasized and connected to the  missing  description of the model architecture and its hyperparameters. * Related work. I am not expert in the field but I think that there are recent references which could be cited for probablistic models of graphs.
Reject. rating score: 4. rating score: 4. rating score: 4. <BRK>The paper operates under the hypothesis that the rigidity of the convolution operator is responsible in part for the poor performance of GANs on diverse visual datasets. State of the art Inception scores are presented for the CIFAR 10 and STL 10 datasets. Wouldn t the arguments used to justify replacing regular convolutions in the generator with adaptive convolution blocks apply equally well to any other decoder based generative model, like a VAE, for instance? I find the paper lacking on the evaluation front.<BRK>The paper proposes to use Adaptive Convolution (Niklaus 2017) in the context of GANs. Experiments:  Experiments are very limited, only overfit to inception score. Inception score experiments as the only experiments of a paper are woefully inadequate. The inception score is computed using a pre trained imagenet model.<BRK>This manuscript proposes the use of "adaptive convolutions", previously proposed elsewhere, in GAN generators. While Inception scores were the only proposed metric available for a time, other metrics have now been introduced in the literature (AIS log likelihood bounds, MS SSIM, FID) and reporting Inception scores (with all of their problems) falls short for this reviewer. Not only is the quantitative analysis lacking but also absent is any qualitative analysis of what exactly these adaptive convolutions are learning, whether this additional modeling power is well used, etc.
Reject. rating score: 3. rating score: 5. rating score: 6. <BRK>If that is the case,the paper does not have any original contribution.<BRK>Novelty: This is in my opinion the weakest point of the paper.<BRK>In summary, I feel that while there are some issues with the paper, it presentsinteresting results and can be accepted. There is noclear trend in many of them and a lot of noise.
Invite to Workshop Track. rating score: 6. rating score: 5. rating score: 4. <BRK>This paper proposes a novel framework for ensuring fairness in the classification pipeline. To this end, this work explores models that learn to defer. The idea of learning to defer (as proposed in the paper) as a means to fairness is not only novel but also quite apt. While this work is conceptually quite novel and interesting, the technical novelty and contributions seem fairly minimal. While the ideas of learning to defer have already been studied in the context of classification models, this is the first contribution which leverages learning to defer strategy as a means to achieve fairness.<BRK>To conclude, although the application is of high interest and the numerical results encouraging, the methodological approach does not seem to be very novel. Edited :As noted by a fellow reviewer, the paper is a bit out of the scope of ICLR and may be more in line with other ML conferences. The results are promising and the applications are very important for the acceptance of ML approaches in the society. The proposed method is a classifier that is fair and works in collaboration with an unfair (but presumably accurate model).<BRK>I like the direction this paper is going in by combining fairness objectives with deferment criteria and learning. I find it quite interesting that the authors go beyond  classification with a reject option  to learning to defer, based on output predictions of the second decision maker. However, the authors do not make this aspect of the work very clear until Section 6. The distinction and contribution of this part is not made obvious in the abstract and introduction. Typically it is DM labels that are the only thing available for training and what cause the introduction of unfairness to begin with. The math and empirical results all seem to be correct and interesting.
Accept (Poster). rating score: 7. rating score: 7. rating score: 7. <BRK>The paper discusses the problem of optimizing neural networks with hard threshold and proposes a novel solution to it. Using these modifications the authors propose an algorithm (Algorithm 2 in appendix) to train such models efficiently. They show superiority of their algorithm over SSTE. The authors formulation of the problem as one of combinatorial optimization and proposing Algorithm 1 is also quite interesting. There are a few flaws/weaknesses in the paper though, making it somewhat lose. The authors start of by posing the problem as a clean combinatorial optimization problem and propose Algorithm 1.<BRK>This paper examines the problem of optimizing deep networks of hard threshold units. This is a significant topic with implications for quantization for computational efficiency, as well as for exploring the space of learning algorithms for deep networks. While none of the contributions are especially novel, the analysis is clear and well organized, and the authors do a nice job in connecting their analysis to other work.<BRK>The paper studies learning in deep neural networks with hard activation functions, e.g.step functions like sign(x). Arguably the most popular is straight through estimation (Hinton 2012, Bengio et al.2013), in which the activation functions are simply treated as identity functions during backpropagation. More recently, a new type of straight through estimation, saturated STE (Hubara et al., 2016) uses 1[|z|<1] as the derivative of sign(z). Deciding on these targets is formulated as a combinatorial optimization problem. This paper s ideas are very interesting, exploring an alternative training method to backpropagation that supports hard threshold activation functions.
Reject. rating score: 4. rating score: 4. rating score: 5. <BRK>The tasks for the NRAM are more demanding, but the results are also more mixed. Neural programmers are notoriously hard to tune, so it is hard to be sure if this difference could be eliminated with more tuning effort. Also, while it s hard to call the idea of using partial traces original, it s not been studied in this extent and setting before.<BRK>Developing a general formalism for neural computers which includes both the Neural Turing Machine (NTM) and the Neural Random Access Machine (NRAM), as well as a model for providing partial supervision to this general architecture. I found quite compelling the idea of exploring the use of additional supervision in neural architectures since oftentimes a user will know more about the problem at hand than just input output examples.<BRK>While they took inspiration from NTMs, their Neural Stack has not much resemblance with this architecture. It would be good to give a concrete example at this point. Adding supervision on execution traces in ∂NCM improves performance over NTM and NRAM which are trained end to end from input/output examples only. Citation style: sometimes citation should be in brackets, for example "(Graves et al.2016)" instead of "Graves et al.(2016)" in the first paragraph of the introduction.
Reject. rating score: 3. rating score: 7. rating score: 7. <BRK>This paper proposes a dynamic evaluation of recurrent neural network language models by updating model parameters with certain segment lengths. Pros.Simple adaptation scheme seems to work, and the paper also shows (marginal) improvement from a conventional method (neural cache RNNLM) Cons. The paper is not well written due to undefined variables/indexes, confused explanations, not clear explanations of the proposed method in abstract and introduction (see the comments below)  Although the perplexity is an important measure, it’s better to show the effectiveness of the proposed method with more practical tasks including machine translation and speech recognition. It’s better to explain it in more detail in the abstract. Abstract: It’s better to provide relative performance (comparison) of the numbers (perplexity and bits/char) from conventional methods. Section 2: Some variables are not explicitly introduced when they are appeared including i, n, g, and l  Section 3: same comment with the above for M. Also n is already used in Section 2 as a number of sequences. Why does the paper only provide examples for SGD and RMSprop?<BRK>The authors provide an improved implementation of the idea of dynamic evaluation, where the update of the parameters used in the last time step proposed in (Mikolov et al.2010) is replaced with a back propagation through the last few time steps, and uses  RMSprop rather than vanilla SGD. The method is applied to word level and character level language modeling where it yields some gains in perplexity. While the general idea is not novel, the implementation choices matter, and the authors provide one which appears to work well with recently proposed models. The paper s weakest part is the word level language modeling section. How sensitive are the final results to this choice? Comparing dynamic evaluation to neural cache models is a good idea, given how both depend en medium term history: (Grave et al.2017) provide results on the larger text8 and wiki103, it would be useful to see results for dynamic evaluation at least on the former.<BRK>This paper takes AWD LSTM, a recent, state of the art language model that was equipped with a Neural Cache, swaps the cache out for Dynamic Evaluation and improves the perplexities. Dynamic Evaluation was the baseline that was most obviously missing from the original Neural Cache paper (Grave, 2016) and from the AWD LSTM paper. In this sense, this work fills in a gap. Looking at the proposed update rule for Dynamic Evaluation though, the Global Prior seems to be an implementation of the Fast Weights idea. It would be great to explore that connection, or at least learn about how much the Global Prior helps. The sparse update idea feels very much an afterthought and so do the experiments with Spanish. All in all, this paper could be improved a lot but it is hard to argue with the strong results ...Update:  I m happy with how the authors have addressed these and other comments in revision 2 of the paper and I ve bumped the rating from 6 to 7.
Reject. rating score: 3. rating score: 4. rating score: 4. <BRK>The paper proposes a method which jointly learns the label embedding (in the form of class similarity) and a classification model. There are 5 terms in the proposed objective function. This is perhaps more important than improving the baseline method by a few point, especially given that the goal of this work is not to beat the state of the art.<BRK>The paper proposes to add an embedding layer for labels that constrains normal classifiers in order to find label representations that are semantically consistent. I believe this is true, but there is a lot of prior work on these, such as adding a temperature to the softmax, or using distillation, etc. Section 2.2 describes a way to compress the label embedding representation, but it is not clear if this is actually used in the experiments. Experiments on known datasets are interesting, but none of the results are competitive with current state of the art results (SOTA), despite what is said in Appending D. For instance, one can find SOTA results for CIFAR100 around 16% and for CIFAR10 around 3%.<BRK>This paper proposes a label embedding network method that learns label embeddings during the training process of deep networks. How to decide the test labels? The proposed approach is neither well motivated, nor well presented/justified. (3) The label embeddings are not directly used for the classification (H(y, z’_1)), but rather as auxiliary part of the objective.
Reject. rating score: 5. rating score: 5. rating score: 6. <BRK>This paper proposes an approximate method to construct Bayesian uncertainty estimates in networks trained with batch normalization. There is a lot going on in this paper. Overall, the reported functionality is nice, although the experimental results are difficult to intepret (despite laudable effort by the authors to make them intuitive). If so, how is this method specific to batch normalization? Maybe I’m showing my own lack of understanding here, but it’s worrying that the actual sampling technique is not explained anywhere. This relates to a larger point about the paper s main point: What, exactly, is the Bayesian interpretation of batch normalization proposed here? The argument in Section 3.3 seems rather weak to me. I wouldn t be surprised that there are many directions in the weight space of a trained DNN along which the posterior is dominated by the prior. It is presented well (modulo the above problems), and it makes some strong points. But I’m worried about the empirical evaluation, and the omission of crucial algorithmic details.<BRK>The stochasticity of the prediction comes from different minibatches of training data that were used to normalise the activity/pre activation values at each layer. This is justified by an argument that using batch norm is doing variational inference, so one should use the approximate posterior provided by batch norm at prediction time. It is not clear to me from the presentation what the q(w) density is   whether this is explicit (as in vanilla Gaussian VI or MC dropout), or implicit (the stochasticity on the activity h due to batch norm induces an equivalence q on w). From a Bayesian perspective, it is also not satisfying to ignore the regularisation term by an empirical heuristic provided in the batch norm paper [small \lambda]   what is the rationale of this? Can this be explained by comparing the variational free energy. The experiments also do not compare to modern variational inference methods using the reparameterisation trick with Gaussian variational approximations (see Blundell et al 2016) or richer variational families (see e.g.Louizos and Welling, 2016, 2017). *Clarity*The paper is in general well written and easy to understand.<BRK>The authors compare this approach toMonte Carlo dropout (another regularization technique which can also beconsidered to perform approximate Bayesian inference). Quality:I found the quality to be low in some aspects. First, the description of whatis the prior used by batch normalization in section 3.3 is unsatisfactory. The authors should highlight inbold face the results of the best performing method. The authors indicate that they do not need to compare to variational methodsbecause Gal and Ghahramani 2015 compare already to those methods. Clarity:The paper is clearly written and easy to follow and understand. How is this actually done in practice? Originality:The proposed contribution is original. This is the first time that a Bayesianinterpretation has been given to the batch normalization regularizationproposal. Batch normalization is a verypopular regularization technique and showing that it can be used to obtainestimates of uncertainty is relevant and significant.
Accept (Poster). rating score: 7. rating score: 7. rating score: 6. <BRK>This paper is about a new experimental technique for exploring different neural architectures. It is well written in general, numerical experiments demonstrate the framework and its capabilities as well as its limitations. A disadvantage of the approach may be that the search for architectures is random. It would be interesting to develop a framework where the search for the architecture is done with a framework where the updates to the architecture is done using a data driven approach. Nevertheless, there are so many different neural architectures in the literature and this paper is a step towards comparing various architectures efficiently.<BRK>This choice may be a reflection of preferring to highlight the experimental results over possible contributions to theory of neural nets. Pros   * Strong related work section that contextualizes this paper among current work* Very interesting idea to more efficiently find and train best architectures * Excellent and thought provoking discussions of middle steps and mediocre results on some experiments (i.e.last paragraph of section 4.1, and last paragraph of section 4.2)* Publicly available code Cons   * Some very strong experimental results contrasted with some mediocre results* The balance of the paper seems off, using more text on experiments than the contributions to theory. Review   The paper is very well written with clear examples and an excellent contextualization of the work among current work in the field. It is a shame that the authors chose to push their section on future work to the appendices.<BRK>This paper tackles the problem of finding an optimal architecture for deep neural nets . They propose to solve it by training an auxiliary HyperNet to generate the main model. The authors propose the so called "SMASH" algorithm that ranks the neural net architectures based on their validation error. It is not clear whether this is a new contribution of this paper or whether the authors merely adopt this idea. One final point, for the uninitiated reader  sections 3.1 and 3.2 could probably be written somewhat more lucidly for better access.
Reject. rating score: 1. rating score: 4. rating score: 4. <BRK>This paper attempts to analyze the gradient flow through a batchNorm ReLU ResNet and make suggestions for reducing gradient explosion. However, much more analysis, detail and care would be required to make this argument successfully. So is k the change in the receptive field size or the filter number of both? It is not a problem in itself to use a nonstandard architecture, but you do not discuss how your results would generalize to other ResNet architectures. You do not understand the work by Veit et al.You do not know how to interpret gradient variances. The vanishing gradient problem refers to the size of the gradient shrinking exponentially. See "DEEP INFORMATION PROPAGATION" by Schoenholz et al.from ICLR 2017 to learn more about how gradient explosion can arise. However, you seem to be arguing that it is a problem if the gradient scale does increases too little from one residual block to the next.<BRK>Intriguingly, the analysis suggests that using a shallower but wider resnet should provide competitive performance, which is supported by empirical evidence. This work should help elucidate the structure in the learning, and help to support efforts to improve both learning algorithms and the architecture. In particular, the assumption that all weights are independent is valid only at the first random iteration. The empirical support does provide evidence that the theory is reasonable. However, it is limited to a single dataset. Second, it is clear that shallow+wide networks may be better than deep+narrow networks, but it s not clear about how the width is evaluated and supported. I would encourage the authors to do more extensive experiments and evaluate the architecture further. Revision:Upon examining the comments of the other reviews, I have agreed with several of their points and it is necessary to increase the explanation of the mathematical points.<BRK>Summary:This paper analyzed the effect of batch normalization (BN) on gradient backpropagation in residual networks (ResNets). 3.The authors discussed the tradeoff between the depth and width of residual networks based on the analysis of BN. The motivation of the analysis on the effect of BN in residual network is not clear. So, how to transform the formulas in the original BN paper to the gradient w.r.t.a specific channel like Eqn. (4).More details should be provided. Please explain more on this point. However, it does not mean that the gradient w.r.t.the weights has zero mean and the gradient will introduce a distribution bias in the weights.
Accept (Poster). rating score: 7. rating score: 5. rating score: 3. <BRK>I have listed this review as a good for publication due to the novelty of ideas presented, but the accusation of misrepresentation below is a serious one and I would like to know the author s response. *Overview*This paper presents a method of generating both syntactically and semantically valid data from a variational autoencoder model using ideas inspired by compiler semantic checking. Using Bayesian Optimization, search over this space can yield decodings with targeted properties. The results presented are state of the art. As noted in the paper, the generation of syntactically and semantically valid data is still an open problem.<BRK>This paper proposes to include additional constraints into a VAE which generates discrete sequences, namely constraints enforcing both semantic and syntactic validity. This is an extension to the Grammar VAE of Kusner et. al, which includes syntactic constraints but not semantic ones. These semantic constraints are formalized in the form of an attribute grammar, which is provided in addition to the context free grammar. al.However, it isn t clear whether the comparison is appropriate: the authors report in the appendix that they use the kekulised version of the Zinc dataset of Kusner et. The baselines they compare against for CVAE and GVAE in Table 1 are taken directly from Kusner et.<BRK>The paper presents an approach for improving variational autoencoders for structured data that provide an output that is both syntactically valid and semantically reasonable. The idea presented seems to have merit , however, I found the presentation lacking. I didn t like that two types of experiment are now presented in parallel.
Reject. rating score: 3. rating score: 4. rating score: 5. <BRK>Is this embedding the DNN? In Eq.4 I would assume that $\phi(x)$ is the DNN and that it should be $\phi_\theta(x)$, because otherwise the equation does not make sense. I think the authors should explain a few things more clearly in order to make the paper foolproof.<BRK>This paper is a follow up work to the CCS 2017 paper on the GAN based attack on collaborative learning system where multiple users contribute their private and sensitive data to joint learning tasks. This can be easily done by including the representations in the embedding space in the parameters in GAN for learning.<BRK>This paper proposes a collaborative learning framework (CLF) that mitigates the GAN attack. 1.Some of the details of key sharing are not clear and would appear to be important for the scheme to work.
Accept (Poster). rating score: 8. rating score: 6. rating score: 4. <BRK>Why not consider ImageNet? Goyal et al reports that it took an hour for them to train ResNet on ImageNet with 256 GPUs, and authors may demonstrate it can be trained faster. This is a simple and sensible idea, and empirical experiments convincingly demonstrate the advantage of the method in large scale distributed training.<BRK>Although I am not an expert on this area, but this paper clearly explains their contribution and provides enough evidences to prove their results. Online distillation technique is introduced to accelerate traditional algorithms for large scale distributed NN training. Could the authors add more results on the CNN ?<BRK>The paper proposes an online distillation method, called co distillation, where the two different models are trained to match the predictions of other model in addition to minimizing its own loss. In this sense, it is also a limitation that the authors showing experiments where only two models are codistillated. The paper is clearly written and was easy to understand. But, because from Zhang s method, I don t see any significant difficulty in applying to large scale problems, I m not sure that this can be a significant contribution.
Reject. rating score: 3. rating score: 4. rating score: 5. <BRK>SummaryThis paper proposes a hybrid model (C+VAE) a variational autoencoder (VAE) composed with a differentiable decision tree (DDT) and an accompanying training scheme. Cons:  An indefensible flaw in the work is that the model is evaluated on only MNIST.<BRK>The paper tries to build an interpretable and accurate classifier via stacking a supervised VAE (SVAE) and a differentiable decision tree (DTT). It is most useful to interpret the state of the art classifier while the results of the proposed methods are far from the state of the art even on such simple MNIST dataset. The the previous methods evaluate log p(x) while the proposed method evaluates log p(x, y) which should be much lower as the proposed method potentially trains a separated model for each class of the x for evaluation.<BRK>This paper addresses a method of building an interpretable model for classification, where two key ingredients are (1) supervised variational autoencoder and (2) differentiable decision tree. Recently one important line of research is to build interpretable models which have more modeling capacity while maintaining interpretability, over existing models such as linear models or decision trees. However, I understand that there have been various work on probabilistic decision tree, Bayesian decision tree, and Mondrian tree. Regarding the supervised VAE, the term "supervised VAE" is misleading.
Accept (Poster). rating score: 7. rating score: 6. rating score: 5. <BRK>* Summary of paper: The paper addresses the problem of optimizing metrics in the context of retinal prosthetics: Their goal is to learn a metric which assumes spike patterns generated by the same stimulus to be more similar to each other than spike patterns generated by different stimuli. p.6 (Fig.4) Would be useful to state that column below 0 is the target. It would be valuable for them to comment on what additional challenges would arise by using the neural network instead, and whether they think they could be surmonted. The paper states this as a goal ("This measure should expand upon...), but then never does that  why not? It would be useful for the paper to comment on how they think that metric to be useful for retinal prosthetics.<BRK>In their paper, the authors propose to learn a metric between neural responses by either optimizing a quadratic form or a deep neural network. The pseudometric is optimized by positing that the distance between two neural responses to two repeats of the same stimulus should be smaller than the distance between responses to different stimuli. Second, I find the framework proposed by the authors interesting, but not clearly motivated from a neurobiological perspective, as the similarity between stimuli does not appear to play a role in the optimized loss function.<BRK>This would avoid any complications from sub optimal decoders, and be a much more direct test. Why not use the better neural network metric for the subsequent studies of image similarity, and retinal stimulation? Overall, I love the concepts in this paper. But I encourage the authors to consider following through on these suggestions to improve their paper: the paper s key idea is really good, and I think it s worth the effort to flesh that idea out more thoroughly.
Reject. rating score: 3. rating score: 4. rating score: 6. <BRK>The HolE paper discusses this for instance when relating the model to associative memory. However, I do not see how the proposed method is a more principled than previously proposed methods.<BRK>This paper tackles the task of learning embeddings of multi relational graphs using a neural network. The only difference comes from that the prediction function (softmax and not ranking for instance) and the loss used.<BRK>The paper is well written and provides sufficient background on the knowledge graph tasks. Was it always better than the original model? ?Why is a binary classifier for Q4 not part of the model? I am missing a comparison of the change in the prediction score.
Accept (Poster). rating score: 7. rating score: 7. rating score: 7. <BRK>Otherwise, the structure of the environment might support learning even when the reward delay would otherwise not. This is a well motivated and explained paper, in which a research agenda is clearly defined and evaluated carefully with the results reflected on thoughtfully and with intuition. Rather, the average achievable reward for an oracle (that knows whether health packs are) is fixed.<BRK>There is a really interesting experiment performed that suggests that this is the case due to finite horizon MC having an easier time with learning perceptual representations.<BRK>This is inline with TD lambda analysis in previous work. The contributions of the paper are in parts surprising and overall interesting.
Reject. rating score: 4. rating score: 4. rating score: 9. <BRK>Section 4, which is the main technical section of the paper, is quite full of lengthy descriptions that are a bit equivocal. Could you please further elaborate on this? "we restrict the posterior representation of the student model to **be close to that of the teacher** for the previous distributions** accumulated by the teacher. Otherwise, what are the solutions to that? Can they be estimated somehow (as a future work)?<BRK>The paper proposed a teacher student framework and a modified objective function to adapt VAE training to streaming data setting. The modified objective function is a little ad hoc, and it s unclear how to relate the overall objective function to Bayesian posterior inference (what exactly is the posterior that the encoder tries to approximate?). There is a term in the objective function that is synthetic data specific. What is the motivation/justification of choosing KL(Q_student||Q_teacher) as regularisation instead of the other way around? Without a streaming benchmark or a realistic motivating example in which the proposed scheme makes a significant difference, it s difficult to judge the contribution of this work. The student teacher framework by itself isn t novel. The modifications to the objective function appears to be novel as far as I am aware, but it doesn t require much special insights.<BRK>This paper attacks a different kind of problem, namely lifelong learning. This key aspect of the paper, besides the fact that it constitutes a very important problem, does also addes a strong element of freshness to the paper. The derivations are correct, while the experimental evaluation is diverse and convincing.
Reject. rating score: 3. rating score: 3. rating score: 3. rating score: 7. <BRK>This paper is concerned with video prediction, for use in robotic motion planning. The incremental architectural changes, different dataset and training are responsible for most of the other improvements.<BRK>1) SummaryThis paper proposes a flow based neural network architecture and adversarial training for multi step video prediction. 4) Conclusion:This paper seems to contain very minimal changes in comparison to the baseline by [1]. In ICLR, 2016.<BRK>In this paper a neural network based method for multi frame video prediction is proposed. This work is neither mentioned in the related work nor compared to. My main concerns with this paper are novelty, reproducibility and evaluation.<BRK>It is not the case that the adversarial loss was simply removed. It is in line with current trends in the research community and is a good fit for ICLR. The paper is well written, reasonably scholarly, and contains stimulating insights. It s not how the paper is written, though.
Accept (Poster). rating score: 9. rating score: 7. rating score: 4. <BRK>I think this paper presents an interesting take on feature pooling. I would have loved to see some experiments that prove/disprove this. Pros:  The nice thing about this method is that average pooling is in some sense a special case of this method, so we can see a clear connection. Final comments:  I like the idea and it seems novel it may lead to some promising research directions related to lossy pooling methods/channel aggregation.<BRK>The paper proposes "wavelet pooling" as an alternative for traditional subsampling methods, e.g.max/average/global pooling, etc., within convolutional neural networks. Moreover, it would increase the relevance of this work in the computer vision community. Overall the method is well presented and properly motivated.<BRK>The idea is tested on small scale datasets such as MNIST and CIFAR. Overall, a major issue of the paper is the linear nature of DWT. As a result, if my understanding is correct, this explains why the wavelet pooling is almost the same as average pooling in the experiments (other than MNIST).
Reject. rating score: 5. rating score: 5. rating score: 6. <BRK>The approach to training discrete weights NNs, which is variational inference, is more principled than previous works (but see below). Pros:The paper is well written and connections with the literature properly established. The title of the paper says "discrete valued NNs". This is not discussed.<BRK>The authors consider the problem of ultra low precision neural networks motivated by limited computation and bandwidth. Their approach first posits a Bayesian neural networka discrete prior on the weights followed by central limit approximations to efficiently approximate the likelihood. The paper is promising, but I have several questions:1) One major concern is that the experimental results are only on MNIST. Is the Bayesian optimization necessary?<BRK>Summary: The paper considers a Bayesian approach in order to infer the distribution over a discrete weight space, from which they derive hardware friendly low precision NNs. This is not within my areas of expertise.
Accept (Poster). rating score: 9. rating score: 6. rating score: 6. <BRK>The approach is basically to apply self attention at two levels, such that each level only has a small, fixed number of items, thereby limiting the memory requirement while having negligible impact on speed.<BRK>This paper introduces bi directional block self attention model (Bi BioSAN) as a general purpose encoder for sequence modeling tasks in NLP. The advantage of this model is that we can use little memory (as in RNNs) and enjoy the parallelizable computation as in (SANs), and achieve similar (or better) performance.<BRK>Pros: The paper proposes a “bi directional block self attention network (Bi BloSAN)” for sequence encoding, which inherits the advantages of multi head (Vaswani et al., 2017) and DiSAN (Shen et al., 2017) network but is claimed to be more memory efficient. The proposed model was tested on nine benchmarks  and achieve good efficiency memory trade off.
Reject. rating score: 4. rating score: 5. rating score: 6. <BRK>This paper proposes new idea of using controller modules for increment learning. I found the idea of using controller modules for increment learning interesting and have some practical use cases. I m curious to see some other multitask learning approach, e.g.branch out on the last few layers for different tasks and finetune the last few layers.<BRK>  Summary  The paper tackles the problem of task incremental learning using deep networks. Overall  The paper tackles an important problem, aims for important characteristics, and does extensive and various experiments.<BRK>(As an intermediate step, the performance could also be measured with the dataset classifier trained in the same way but used as a soft weighting, rather than the hard version rounding alpha to 0 or 1.) Overall, the paper is clear and the proposed method is sensible, novel, and evaluated reasonably thoroughly. A more systematic set of experiments could compare learning the proposed weightings on the first K layers of the network (for K {0, 1, …, N}) and learning independent weights for the latter N K layers, but I understand this would be a rather large experimental burden. (This is hinted at by the mention of fully convolutional networks.)
Accept (Poster). rating score: 9. rating score: 7. rating score: 6. <BRK>This should be the first work which introduces in the causal structure into the GAN, to solve the label dependency problem. The idea is interesting and insightful. 2) how the tune related weight of the different objective functions.<BRK>IMO, the structure of the paper can be improved. This means, it s "just" using NNs as a model class. (This would also clarify the paper s contribution.) Such a causal model allows us to not only sample from conditional observational distributions, but also from intervention distributions. Architecture (which the causal controller is part of) etc. many realizations   one sample (not samples), I think. Also, it is quite important for the paper, I think it should be in the main part. I like the idea of this paper. The paper has the potential of conveying the message of causality into the ICLR community and thereby trigger other ideas in that area. (1) What is an "informal" theorem?<BRK>The paper describes a way of combining a causal graph describing the dependency structure of labels with two conditional GAN architectures (causalGAN and causalBEGAN) that generate  images conditioning on the binary labels. Ideally, this type of approach should allow not only to generate images from an observational distribution of labels (e.g.P(Moustache 1)), but also from unseen interventional distributions (e.g.P(Male 0 | do(Moustache  1)). The (known) causal graph is only used to model the dependencies of the labels, which the authors call the “Causal Controller”. Overall, I think the paper proposes some interesting ideas, but it doesn’t explore them yet in detail. Moreover, I would be very curious about ways to better integrate causality and generative models, that don’t focus only on the label space.
Reject. rating score: 4. rating score: 4. rating score: 6. <BRK>This work proposes a dynamic weight update scheme that updates weights for different task losses during training time by making use of the loss ratios of different tasks. The term ’GradNorm’ seem to be not defined anywhere in the paper. There is no clear basis for the main equations 1 and 2.<BRK>The paper addresses an important problem in multitask learning. This is different from the sum of the original losses, which seems to be the one used to train the “equal weight” baseline. It looks like a loss, but Equation 2 seems to define it to be the difference between the gradient norm of a task and the average over all tasks. A more fair baseline is to directly use the evaluation metric as the training loss.<BRK>The paper proposes a method to train deep multi task networks using gradient normalization. 4.It would be useful to discuss the implementation of the method as well.
Accept (Poster). rating score: 7. rating score: 7. rating score: 5. <BRK>In this paper active learning meets a challenging multitask domain: reinforcement learning in diverse Atari 2600 games. Naive multitask learning with deep neural networks fails in many practical cases, as covered in the paper. The one concern I have is perhaps the choice of distinct of Atari games to multitask learn may be almost adversarial, since naive multitask learning struggles in this case; but in practice, the observed interference can appear even with less visually diverse inputs. Although performance is still reduced compared to single task learning in some cases, this paper delivers an important reference point for future work towards achieving generalist agents, which master diverse tasks and represent complementary behaviours compactly at scale. I wonder how efficient the approach would be on DM lab tasks, which have much more similar visual inputs, but optimal behaviours are still distinct.<BRK>Pros:  very promising results with an interesting active learning approach to multitask RL  a number of approaches developed for the basic idea  a variety of experiments, on challenging multiple task problems (up to 21 tasks/games)  paper is overall well written/clearCons:  Comparison only to a very basic baseline (i.e.uniform sampling)Couldn t comparisons be made, in some way, to other multitask work? Additional  comments:  The assumption of the availability of a target score goes againstthe motivation that one need not learn individual networks ..  authorssay instead one can use  published  scores, but that only assumessomeone else has done the work (and furthermore, published it!). The authors do have a section on eliminating the need by doubling anestimate for each task) which makes this work more acceptable (shownfor 6 tasks or MT1, compared to baseline uniform sampling). But is there a pattern? Can you motivate/discuss better why not providing the identity of a  game as an input is an advantage? what are the pros/cons?<BRK>The main contribution is to introduce active learning principles for sampling the sequential tasks in an online algorithm. Experimental results are given on different multi task instances. Also, most of the paper is devoted to the study of algorithms for which the expected target scores are known. This is a very strong assumption. In my opinion, the authors should have put the focus on the DU4AC algorithm which get rids of this assumption. * I do not see how the single output layer is defined. * As said in the general comments, in my opinion Section 6 should be developped and more experiments should be done with the DUA4C algorithm.
Reject. rating score: 5. rating score: 5. rating score: 8. <BRK>The authors propose to combine nonlinear bijective transformations and flexible density models for density estimation. Nonetheless, the comparison between models in flexible density models, change of variables transformations and combinations of both remain relevant.<BRK>This paper offers an extension to density estimation networks that makes them better able to learn dependencies between covariates of a distribution. Most results in the paper are comparisons of toy conditional models. The one Table that lists other work showed LAM and RAM to be comparable.<BRK>This paper is well constructed and written. It consists of a number of broad ideas regarding density estimation using transformations of autoregressive networks. Specifically, the authors examine models involving linear maps from past states (LAM) and recurrence relationships (RAM). The authors then explored a variety of transformations designed to increase the expressiveness of LAM and RAM.
Accept (Poster). rating score: 7. rating score: 6. rating score: 4. <BRK>This paper presents a new covariance function for Gaussian processes (GPs) that is equivalent to a Bayesian deep neural network with a Gaussian prior on the weights and an infinite width. The analysis of the phases in the hyperparameter space is interesting and insightful.<BRK>This paper leverages how deep Bayesian NNs, in the limit of infinite width, are Gaussian processes (GPs). None of these results beat state of the art deep NNs. The provided phase analysis and its relation to the depth of the network is also very interesting. Both are useful contributions as long as deep wide Bayesian NNs are concerned.<BRK>[*]: Generally, in reading the abstract and introduction, we get the impression that this work somehow allows us to use really deep and infinitely wide neural networks as Gaussian processes, and even without the pain of training these networks. Thus the novelty of this aspect of the paper is overstated. These works should be discussed in the text.
Accept (Poster). rating score: 7. rating score: 6. rating score: 6. <BRK>It makes several important contributions, including extending the previously published bounds by Telgarsky et al.to tighter bounds for the special case of ReLU DNNs, giving a construction for a family of hard functions whose affine pieces scale exponentially with the dimensionality of the inputs, and giving a procedure for searching for globally optimal solution of a 1 hidden layer ReLU DNN with linear output layer and convex loss.<BRK>In the last paragraph of Section 3 ``m   w^k 1  This is a very big first layer.<BRK>This paper presents several theoretical results regarding the expressiveness and learnability of ReLU activated deep neural networks. Result (3) is more interesting as it is a new result.
Reject. rating score: 5. rating score: 5. rating score: 7. <BRK>For quantization, it appears that the HD rotation is essential for CIFAR, but less important for the reddit data. It would be interesting to understand when HD works and why,  and perhaps make the paper more focused on this winning strategy, rather than including the low rank algo. The paper is mainly an empirical comparison of several approaches, rather than from theoretically motivated algorithms. "Scalable distributed dnn training using commodity gpu cloud computing."<BRK>This paper proposes a new learning method, called federated learning, to train a centralized model while training data remains distributed over a large number of clients each with unreliable and relatively slow network connections. Hence, the novelty of this paper is limited. The studied problem in this paper seems to be interesting, and with potential application in real settings like mobile phone based learning. The authors do not provide any analysis about what can be learned from this learning procedure.<BRK>The authors examine several techniques that lead to low communication updates during distributed training in the context of Federated learning (FL). The major contribution of this paper is their experimental section, where the authors show the effects of training with structured, or sketched updates, in terms of reduced communication cost, and the effect on the training accuracy.
Reject. rating score: 4. rating score: 4. rating score: 4. <BRK>More discussions and case studies are sorely needed.<BRK>It would be good to include more details of DCG to make the papers more complete and easier to read.<BRK>The problem definition and proposed approach needs to be made more precise.
Accept (Poster). rating score: 6. rating score: 6. rating score: 6. <BRK>Isn t that a problem? * the notation x^(i) (in the theorem and the proof notably) could be changed, for the ^(i) index refers to the depth of the layer in the rest of the notations, and is here surprisingly referring to a set of observations.<BRK>The lack of practical relevance combined with the not groundbreaking novelty of the result makes this paper less appealing. Most of the applications of deep learning benefit from strong structured priors that cannot be represented as a GP. This is properly acknowledged in the paper.<BRK>It would be nice to see some practical benefit for a predictive task actually demonstrated in the paper. The paper is well written, and the experiments are enlightening. It does add to our understanding of this body of work.
Reject. rating score: 4. rating score: 5. rating score: 5. <BRK>The paper is overall difficult to read. Finally (and as further discussed below), the paper does not sufficiently discuss related work.<BRK>Please, provide the definition of the ReLu function here. What is the "activation" refers to? Indeed, this is a privileged norm for support recovery. Idem for b_e and b_d.<BRK>It is my impression that a lot of conditions have to be satisfied for the recovery guarantee to be meaningful. It is also not clear why compressed sensing type recovery using a single ReLU or Sigmoid would be of interest: are their complexity benefits?
Reject. rating score: 4. rating score: 5. rating score: 5. <BRK>The paper proposes techniques for encouraging neural network representations to be more useful for clustering tasks. How are they calculated? It is not clear why this is a corollary of the example in the introduction (that should be moved to the method part).<BRK>This paper proposes two regularization terms to encourage learning disentangled representations. KL divergence is a natural similarity measure for probability distribution. I would like to hear authors’ feedback on the issues I raised. Additionally, I have the following questions:(1) I am curious how the proposed method compares to other competitors in terms of the original classification setting, e.g., 10 class classification accuracy on CIFAR10.<BRK>SummaryThis paper proposes two regularizers that are intended to make therepresentations learned in the penultimate layer of a classifier more conformingto inherent structure in the data, rather than just the class structure enforcedby the classifier. The model is not compared to simpler alternatives such as adding an  orthogonality regularization on the weights, i.e., computing W^TW and makingthe diagonals close to 1 and all other terms 0. This paper seems tobe about learning separable representations, whereas the title suggests that itis about disentangled ones. This topic has a wealth of previous work.
Reject. rating score: 2. rating score: 5. rating score: 6. <BRK>The paper is a pain to read. Most of the citation styles are off (i.e., without parentheses). It is ironic that the paper is proposing a model to generate grammatically correct sentences, while most of the sentences in the paper are not grammatically correct. The experimental numbers look skeptical. For example, 1/3 of the training results are worse than the test results in Table 1. The running times in Table 9 are also skeptical. Why are the Concorde models faster than unigrams and bigrams? Maybe this can be attributed to the difference in the size of the vocabulary, but why is the unigram model slower than the bigram model?<BRK>The proposed technique is a simple modification to the standard encoder decoder paradigm which makes it more efficient and better suited to this task. The morphological agreement task would be an interesting contribution of the paper, with wider potential. But one concern that I have is regarding the evaluation metrics used for it. Firstly, word accuracy rate doesn t seem appropriate, as it does not measure morphological agreement. Also, the word order of the normalized answer and/or context could be reversed (as is done in sequence to sequence translation models). Also, many experimental details are missing from the draft:  What are the sizes of the train/test sets derived from the OpenSubtitles database?<BRK>The key contributions of this paper are:(a) proposes to reduce the vocabulary size in large sequence to sequence mapping tasks (e.g., translation) by first mapping them into a "standard" form and then into their correct morphological form,(b) they achieve this by clever use of character LSTM encoder / decoder that sandwiches a bidirectional LSTM which captures context,(c) they demonstrate clear and substantial performance gains on the OpenSubtitle task, and(d) they demonstrate clear and substantial performance gains on a dialog question answer task. As an aside, the authors should correct the numbering of their Figures (there is no Figure 3) and provide better captions to the Tables so the results shown can easily understood at a glance. The only drawback of the paper is that this does not advance representation learning per se though a nice application of current models.
Reject. rating score: 4. rating score: 4. rating score: 6. <BRK>This is important to specify. However comparisons are lacking and the paper is not presented very scientifically.<BRK>I find that the description of the applied problem is quite interesting; in fact overall the paper is well written and very easy to follow. The references are not properly formatted; they should appear at (XXX YYY) but appear as XXX (YYY) in many cases, mixed with the main text.<BRK>The method uses a learnable character embedding to transform the data, but is an end to end approach.
Reject. rating score: 4. rating score: 6. rating score: 7. <BRK>Overall, I don t think this paper provides sufficiently novel or justified contributions compared to the baseline approach of Ulyanov and Lebedev.<BRK>Such a system is difficult to evaluate, but examples are presented where the style of one song is applied to the content of another. The paper does not completely define the mathematical formulation of the system, making it difficult to understand what is really going on. Changing the harmony is problematic as it can end up clashing with the generated melody or just change the listener s perception of which song it is. Cover songs are re performances of an existing (popular) song by another artist. In western music, adjacent notes are very different from one another and are usually not played in the same key, for example C and C#.<BRK>Summary This paper describes a method for style transfer in musical audio recordings. This does not necessitate or suggest a Mel spectrum, though dimensionality  reduction is probably beneficial. Originality The components of the model are not individually novel, but their combination and application are compelling. The approaches to evaluation, while still somewhat unclear, are interesting and original to the best of my knowledge, and could be useful for other practitioners in need of ways to evaluatestyle transfer in music.
Reject. rating score: 4. rating score: 5. rating score: 6. <BRK>The key observation is that it is possible to generate adversarial perturbations wherein the behavior of feature importance methods (e.g.simple gradient method (Simonyan et al, 2013), integrated gradient (Sundararajan et al, 2017), and DeepLIFT ( Shrikumar et al, 2016) ) have large variation while predicting same output. Thus the authors claim that one has to be careful about using feature importance maps. Pro:  The paper raises an interesting point about the stability of feature importance maps generated by gradient based schemes. The examples in the paper seem to be cherry picked to illustrate dramatic effects. The experimental protocol used does not provide enough information of the variability of the salience maps shown around small perturbations of adversarial inputs. The paper would benefit from more systematic experimentation and a better definition of what authors believe are important attributes of stability of human interpretability of neural net behavior.<BRK>The authors perturb input images and create explanations using different methods. Even though the image is inperceptibly different to a human observer, the authors observe large changes in the heatmaps visualizing the explanation maps. This is true even for random perturbations. For one, the model itself reacts to the perturbation, which can be measured by the (considarbly) increased class probability. The excellent Figure 2 supports this point. The explanation methods are expected to highlight highly important areas in an image, which have been targetet by these perturbation approaches. Further do targeted perturbations not show the fragility of explanation methods, but rather that the models actually find what is important to the model.<BRK>They systematically characterize the fragility of several widely used feature importance interpretation methods. How reliable are the interpretations? Understanding whether influence functions provide meaningful explanations is very important and challenging problem in medical imaging applications. The authors showed that across the test images, they were able to perturb the ordering of the training image influences. I am wondering how this will be used and evaluated in medical imaging setting.
Accept (Poster). rating score: 7. rating score: 7. rating score: 7. <BRK>In this work, the objective is to analyze the robustness of a neural network to any sort of attack. This is measured by naturally linking the robustness of the network to the local Lipschitz properties of the network function.<BRK>The method proposed in the paper already exists for classical function, they only transpose it to neural networks. Clarity The paper is clear and well written. "Estimation of the Lipschitz constant of a function." Indeed, this constant can be easier obtained if the gradient exists, but this is not a requirement. Otherwise, the theorem will work with the maximum over all Lipschitz constants but the theoretical result will be weakened. This is right.<BRK>The work claims a measure of robustness of networks that is attack agnostic. The following section (the part starting from 5.3) presents the key to the success of the proposed measure. This is an important problem and the paper attempts to tackle it in a computationally efficient way. It would be nice to be able to show that one can find corresponding attacks that are not too far away from the proposed score. I believe this is just a typo.
Accept (Poster). rating score: 8. rating score: 7. rating score: 7. <BRK>The paper also argues that the performance on the target domain can be further improved by a post hoc minimization of L_t using natural gradient descent (DIRT T) which ensures that the decision boundary changes incrementally and slowly.<BRK>The paper was a good contribution to domain adaptation. The experimental evaluation was very thorough and shows that VADA and DIRT T performs really well. I found the math to be a bit problematic. The latter also had MNIST/MNIST M experiments.<BRK>On the theoretical side, the discussion could be improved. Great empirical results. The theoretical discussion could be improved. Reference:Ben David and Urner.
Accept (Poster). rating score: 9. rating score: 6. rating score: 6. <BRK>This paper proposes a novel scalable method for incorporating uncertainty estimate in neural networks, in addition to existing methods using, for example, variational inference and expectation propagation. The novelty is in extending the Laplace approximation introduced in MacKay (1992) using a Kronecker factor approximation of the Hessian. The paper is well written and easy to follow. Cons: Although it is a predictive method, it s still worth discussing how this method relates to training. Botev et al.2017 reports they are slightly different in approximating the Gaussian Newton matrix.<BRK>This paper uses recent progress in the understanding and approximation of curvature matrices in neural networks to revisit a venerable area  that of Laplace approximations to neural network posteriors. The paper is generally well written.<BRK>This paper proposes a Laplace approximation to approximate the posterior distribution over the parameters of deep networks. The idea is interesting and the realization of the paper is good. The idea builds upon previous work in scalable Gauss Newton methods for optimization in deep networks, notably Botev et al., ICML 2017.
Accept (Poster). rating score: 7. rating score: 6. rating score: 5. <BRK>The paper introduces a neural network architecture to operate on graph structureddata named Graph Attention Networks. There is noexplicit usage of the graph beyond the selection of the local neighborhood.<BRK>State of the art results on three datasets. This paper has proposed a new method for classifying nodes of a graph. [The experiments are now more informative. Explaining how attention relates to previous body of work in embedding propagation and when it would be more powerful.<BRK>This is a paper about learning vector representations for the nodes of a graph. There are two  issues with the experiments. The authors provide a fair and almost comprehensive  discussion of state of the art approaches. Still, something to relate to.
Reject. rating score: 4. rating score: 4. rating score: 7. <BRK>However in the current approach the connection is simply not proven in the paper. Consequently for a strong empirical paper I would expect much more baselines, including these proposed by Krueger et al.Pros:  empirical improvements shown on two different classes of problems. without any explanation. I am assuming font are assigned randomly and they represent the same object. Did authors mean  an actual "degenerate Gaussian", which does not have a PDF?<BRK>This paper proposes an L2 norm regularization of the output of penultimate layer of the network. Authors show a good gain on two language modeling tasks, CIFAR 10, and CIFAR 100. The regularizer is based only on the other term.<BRK>The paper puts forward Activation Norm Penalty ("ANR", an L_2 type regularization on the activations), deriving it from the Information Bottleneck principle. When it is applied without dropout to image classification, shouldn t that be explained? Maybe dropping the determinant term also deserves some justification. Maybe discuss it and the differences (if any) in the related work section? The Information Bottleneck section doesn t feel like an integral part of the paper.
Accept (Poster). rating score: 7. rating score: 6. rating score: 5. <BRK>Later in the paper, you then have to state "...are now noted \xi" several times, which I found rather clumsy. p5: Why do you use option (b) for DQN and Dueling and option (a) for A3C? Why is it that, in deterministic environments, the network does not converge to a deterministic policy, which should be able to perform better? Why is it that the adequate level of noise changes depending on the environment? The best would be to need to reject nothing to the Appendix.<BRK>My second concern is that I find the title and overall discussion in the paper potentially misleading, by focusing only on the “exploration” part of the proposed algorithm(s). I have two main concerns about this submission. The first one is the absence of a comparison to the method from “Parameter space noise for exploration”, which shares similar key ideas (and was published in early June, so there was enough time to add this comparison by the ICLR deadline).<BRK>At the end of the paper a single experiment investigates the behavior of weights of noise during the learning. Unfortunately this experiment seems to be done in a hurry. 3 NoisyNet exhibits impressive experimental results in comparison to the usual exploration heuristics for to A3C, DQN and Dueling agents.
Reject. rating score: 5. rating score: 5. rating score: 8. <BRK>This work proposes Sparse Attentive Backtracking, an attention based approach to incorporating long range dependencies into RNNs. Through time, a “macrostate” of previous hidden states is accumulated. The proposed architecture is compared against LSTMs trained with both BPTT and truncated BPTT. Cons:  The proposed algorithm is compared against TBPTT but it is unclear the extent to which it is solving the same computational issues TBPTT is designed to solve. However, unlike TBPTT, activations for previous timesteps (even those far in the past) need to be maintained since gradients could flow backwards to them via the macrostate. Thus SAB seems to have higher memory requirements than TBPTT. The empirical results demonstrate that SAB performs slightly better than TBPTT for most tasks in terms of accuracy/CE, but there is no mention of comparing the memory requirements of each. Results demonstrating also whether SAB trains more quickly than the LSTM baselines would be helpful. Overall, the combination of recurrent skip connections and attention appears to be novel, but experimental comparisons to other skip connection RNN architectures are missing and thus it is not clear how this work is positioned relative to previous related work. 2014.[3] Chang, Shiyu, et al."Dilated recurrent neural networks." Therefore SAB should also be compared against other approaches that use skip connections, and not just BPTT / TBPTT, which operate on the standard LSTM. Thus to me the experiments are still lacking. However, I think the approach is quite interesting and as such I am revising my rating from 4 to 5.<BRK>The paper proposes sparse attentive backtracking, essentially an attention mechanism that performs truncated BPTT around a subset of the selected states. While a strong motivator for this work would be in allowing for higher efficiency on longer BPTT sequences, potentially capturing longer term dependencies, this aspect was not explored to this reviewer s understanding. Another broader question is whether longer term dependencies could be caught at all given the model doesn t feature "exploration" in the reinforcement learning sense, especially for non trivial longer term dependencies. For the experiments, I was looking for comparisons to attention over the "LSTM (full BPTT)" window. For sequential MNIST, a relatively small dataset, previous papers have LSTM models that achieve 98.2% test accuracy (Arjovsky et al, https://arxiv.org/abs/1511.06464) and the IRNN example included as part of the Keras framework achieves 93% out of the box. Noting similarities to the Transformer architecture and other similar architectures would also be useful. Both are using attention to minimize the length of a gradient s path, though in Transformers it eliminates the RNN entirely.<BRK>re.Introduction, page 2: Briefly explain here how SAB is different from regular Attention? There s not that much discussion of the proposed SAB compared to regular Attention, perhaps that could be expanded. Also, I suggest summarizing the experimental findings in the Conclusion.
Accept (Poster). rating score: 8. rating score: 7. rating score: 7. <BRK>The proposed simple change help with the dealing of discrete GAN. The benefit of increased stability by adding Jacobian norm regularization term to the discriminator s loss is nice. Similar for Equations 2 and 3.<BRK>The paper shows an application of GANs to deciphering text. They point out that using GANs for discrete distributions is a challenging problem since it can lead to uninformative discriminants. The new proposed algorithm, called CipherGAN, is then shown to be stable and achieve deciphering of substitution ciphers and Vigenere ciphers. Apart from that, the paper is well written and well motivated.<BRK>SUMMARYThe paper considers the problem of using cycle GANs to decipher text encrypted with historical ciphers. The model proposed is a variant of the cycle GAN in which in addition embeddings helping the Generator are learned for all the values of the discrete variables. REVIEWThe paper considers an interesting and fairly original problem and the overall discussion of ciphers is quite nice. This might be connected to the discussion at the top of page 5. In that case it helps. Is this what you mean?
Accept (Poster). rating score: 8. rating score: 7. rating score: 7. <BRK>The paper is very well written, easy to follow and substantiates its claims convincingly on variants of MNIST. It doesn’t live up to the promise of rotation and scale equivariance in 3D. If I understand it correctly, it’s simply a polar transformer in (x,y) with z maintained as a linear axis and assumed to be parallel to the axis of rotation.<BRK>This paper proposes a method to learn networks invariant to translation and equivariant to rotation and scale of arbitrary precision. The approach is evaluated on: 1) several variants of MNIST. This variant is of course very well suited to the proposed method, and a bit artificial. The objects can be rotated around the z axis, and the method is used to be equivariant to this rotation. It should at least be evaluated.<BRK>This paper presents a new convolutional network architecture that is invariant to global translations and equivariant to rotations and scaling. The method is combination of a spatial transformer module that predicts a focal point, around which a log polar transform is performed. Although we do not have data on this, I would guess that for more complex datasets like imagenet / ms coco, where a lot of variation can be reasonably well modelled by diffeomorphisms, this will result in degraded performance. It would not work if the heatmap is multimodal, e.g.when there are multiple instances in the same image or when there is a lot of clutter. The experimental results presented in this paper are quite good, but both MNIST and ModelNet40 seem like simple / toyish datasets.
Accept (Poster). rating score: 6. rating score: 6. rating score: 6. <BRK>Summary: The paper proposes Neumman optimizer, which makes some adjustments to the idealized Neumman algorithm to improve performance and stability in training. The question is that, with the given architectures and dataset, what algorithm should people consider to use between Neumann optimizer and Adam? If Neumann optimizer can surpass Adam on ImageNet, I think your algorithm will be widely used after being published. Do you think it would be interesting if you could compare the efficiency of Neumann optimizer with Adam?<BRK>In the actual performance, the paper presents both practical efficiency and better generalization error in different deep neural networks for image classification tasks, and the authors also show differences according to different settings, e.g., Batch Size, Regularization. 4.It said that the algorithm is hyperparameter free except for learning rate. However, from Table 3, there is no such phenomenon.<BRK>Given the fact that there is convergence analysis or similar theoretical results, I would expect to have much more numerical experiments. how sensitive is the algorithm for different choices of those parameters? Otherwise, the paper is clearly written.
Reject. rating score: 5. rating score: 5. rating score: 7. <BRK>This paper suggests a reparametrization of the transition matrix. The paper is well written and authors explain related work adequately. I have two comments on the experiment section:  Choice of experiments. For example, the adding problem, the copying problem and the permuted MNIST problem and language modeling are the common experiments in the context of RNNs. The plots suggest that the optimization has stopped earlier for some models. Figures are very hard to read because of small font.<BRK>The paper introduces SVD parameterization and uses it mostly for controlling the spectral norm of the RNN. b) the theoretical analysis might be misleading   clearly section 6.2 shouldn t have title ALL CRITICAL POINTS ARE GLOBAL MINIMUM because 0 is a critical point but it s not a global minimum. c) the paper should run some experiments on language applications where RNN is widely usedd) I might be wrong on this point, but it seems that the GPU utilization of the method would be very poor so that it s kind of impossible to scale to large datasets?<BRK>This paper proposed a new parametrization scheme for weight matrices in neural network based on the Householder  reflectors to solve the gradient vanishing and exploding problems in training. The experimental results also look promising. It would be nice if the authors can analyze the spectral properties of the saddle points in linear RNN (nonlinear is better but it s too difficult I believe).
Reject. rating score: 3. rating score: 4. rating score: 4. <BRK>My main concern with the paper is that the contribution is unclear, as the authors failed from my point of view in establishing the novely w.r.t.the state of the art regarding uncertainty in neural networks. The section 4 is also particularly hard to follow. So what is the selection score that is used      "Due to the fact that data noise is small given x"  > what does it mean since x is a couple ? Also I cannot understand the causal relation with the following of the sentence      Figure 4 (and the associated paragraph) is very difficult to understand (I couldn t extract any information from this)      Too many abreviations that complicate the reading      The throughput measure is not clear      Not enough justification about the architecture.<BRK>The paper adresses a very interesting question about the handling of the dynamics of a recommender systems at scale (here for linking to some articles). This is equivalent to separate a local estimation error from the noise. * None of the experiments is done on public data which lead to an impossible to reproduce paper* The proposed baselines are not really the state of the art (Factorization Machines, GBDT features,...) and the used loss is MSE which is strange in the context of CTR prediction (logistic loss would be a more natural choice)* I m not confident with the proposed surrogate metrics.<BRK>This paper presents a methodology to allow us to be able to measure uncertainty of the deep neural network predictions, and then apply explore exploit algorithms such as UCB to obtain better performance in online content recommendation systems. Otherwise it is a very simple problem to solve, as you can just simply assume it s a independent binomial model for each (t,c). My other concerns of this paper include:1. The method presented in this paper seems to be novel but lacks clarity unfortunately.
Reject. rating score: 4. rating score: 5. rating score: 6. <BRK>The putative contributions of the paper can be (a) Directly solving the target problem from raw sensory data without first solving an inversion problem(b) (Directly) solving the lung nodule detection problem using a DNN. The two component network, comprising of the reconstruction network and the nodule detection network, is trained end to end. It is shown that end to end training produces better results compared to a two step approach, where the reconstruction DNN was trained first and the detection DNN was trained on the reconstructed images. So, this clearly is not the main contribution of the paper.<BRK>It is interesting to see improvement in lesion detection when training end to end from raw sinogram data. Because such a mapping loses information, optimizing such a mapping jointly with the task should preserve more information that is relevant to the task. This should be like tracking the condition number of the Hessian of R(x).<BRK>The authors present an end to end training of a CNN architecture that combines CT image signal processing and image analysis. Time will tell whether a disease specific signal processing will be the future of medical image analysis, but   to the best of my knowledge   this is one of the first attempts to do this in CT image analysis, a field that is of significance both to researchers dealing with image reconstruction (denoising, etc.) and image analysis (lesion detection).
Invite to Workshop Track. rating score: 6. rating score: 6. rating score: 5. <BRK>The paper presents a method to compress deep network by weight sampling and channel sharing. The method combined with weight quantization provides 180x compression with a very small accuracy drop. The method is novel  and tested on multiple audio classification datasets and results show a good compression ratio with a negligible accuracy drop. The organization of the paper is good.<BRK>Although the results are interesting, I have a number of concerns about this work, which are listed below:1. The idea of tying weights in the neural network in order to compress the model is not entirely new. This makes the paper harder to understand, in my view. 2015.Compressing neural networks with the hashing trick.<BRK>The paper is clearly written and results seem compelling but on a pretty restricted domain which is not well known. This could have significance if it applies more generally. Is this just because it acts on audio and these filters are phase shifted? What happens with 2D convnets on more established datasets and with more established baselines? Overall I think this paper lacks the breadth of experiments, and to really understand the significance of this work more experiments in more established domains should be performed.
Reject. rating score: 4. rating score: 5. rating score: 7. <BRK>However, I believe the results are preliminary and the paper lacks an adequate explanation/hypothesis for the observed phenomenon either via a theoretical work or empirical experiments. Could we perform more extensive empirical study to substantiate the phenomenon here? 2014+ Clarity: The paper is easy to read. + Significance:While the results are interesting, the contribution is not significant as the paper misses an important explanation for the phenomenon. I m not sure what key insights can be taken away from this.<BRK>This paper presents an experimental study on the behavior of the units of neural networks. I found the paper unnecessarily longer than the suggested 8 pages. For example in Figures 1 and 2 the authors show the behavior of some specific units as examples, but it would be nice to see a graph showing quantitatively the behavior of all the units at each layer.<BRK>I want to love this paper. However, a few parts were poorly explained, which led to this reviewer being unable to follow some of the jumps from experimental results to their conclusions. If it is for a single neuron, then that neuron will have, for a single input example, a single scalar activation value and a single scalar gradient value. The sign of the gradient will either be +1 or  1. Or is the layer shown (e.g.“stage3layer2”) the penultimate layer?
Reject. rating score: 4. rating score: 4. rating score: 4. <BRK>I believe an interesting paper lies within this, and were this a journal, would recommend edits and resubmission. However, in its current state, the paper is too disorganized and unclear to merit publication.<BRK>Overall, a substandard paper. 2.2 The noise evaluation in Section 5.3 is nice, but not related with the Section 4. If the extra space is necessary, perhaps this paper is better suited for another publication? The results are interesting, but are lacking implementation details.<BRK>SummaryThe paper is well written but does not make deep technical contributions and does not present a comprehensive evaluation or highly insightful empirical results. This could be an interesting direction for future exploration of the ideas in this work, where there might be additional technical novelty and more space for empirical contributions and observations.
Reject. rating score: 4. rating score: 6. rating score: 7. <BRK>The paper proposes to improve the kernel approximation of random features by using quadratures, in particular, stochastic spherical radial rules. It is an interesting paper, but it seems the authors are not aware of some existing works [1, 2] on quadrature for random features. Given these previous works, the contribution and novelty of the paper is limited.<BRK>This paper shows that techniques due to Genz & Monahan (1998) can be used to achieve low kernel approximation error under the framework of random fourier feature. Pros1.It is new to apply quadrature rules to improve kernel approximation. Unless the paper can provide a better characterization of the constants (like the ORF paper), it does not provide much insight in the proposed method. 2.Approximating an integral is a well studied topic. Why is Genz & Monahan 1998 better than other alternatives such as Monte Carlo, QMC etc? But this trick can be used for Monte Carlo as well.<BRK>The exposition of the quadrature method is difficult to follow, and the connection between the quadrature rules and the random feature map is never explicitly stated: e.g.equation 6 says how the kernel function is approximated as an integral, but does not give the feature map that an ML practitioner should use to get that approximate integral.
Reject. rating score: 5. rating score: 6. rating score: 6. <BRK>Furthermore, simulation and real data examples to explore the properties and utility of the method are required. Overall, the paper presents an interesting approach but the work lacks maturity. This is not adequately explained in the section on estimation.<BRK>The paper is well written and the method reasonably well explained (I would add an explanation of the spectral estimation in the Appendix, rather than just citing Rodu et al.2013).Additional experimental results would make it a stronger paper. It would be great if the authors could include the code that implements the model.<BRK>The paper presents an interesting spectral algorithm for multiscale hmm. The derivation and analysis seems correct. However, it is well known that spectral algorithm is not robust to model mis specification. It is not clear whether the proposed algorithm will be useful in practice.
Reject. rating score: 3. rating score: 4. rating score: 5. <BRK>The basic idea is to learn a so call meter learner to measure similarities of the same words between the target domain and the source domains for help learning word embedding for the target domain with a small corpus. Another major concern is that the technical contributions of the proposed model is quite limited.<BRK>This paper presents a lifelong learning method for learning word embeddings. The general problem space here   how to leverage embeddings across several domains in order to improve performance in a given domain   is important and relevant to ICLR. What are the inputs and outputs of the meta learner, and how will it be used to obtain embeddings for the new domain?<BRK>There is reasonable novelty in the proposed method compared to the existing literature. The baselines chosen are 1). Comments:The paper puts itself nicely in context of the previous work and the addressed problem of learning word embeddings for new domain in the absence of enough data is an important one that needs to be addressed.
Accept (Poster). rating score: 6. rating score: 6. rating score: 5. <BRK>This paper improves adversarial training by adding to its traditional objective a regularization term forcing a clean example and its adversarial version to be close in the embedding space. My main concerns are:	  The adversarial objective and the stability objective are potentially conflicting. Have the authors considered this issue? Can they elaborate more on how they with this? It may be significantly more difficult to make this work in such setting due to the dimensionality of the data.<BRK>The authors proposed to supplement adversarial training with an additional regularization that forces the embeddings of clean and adversarial inputs to be similar. The analysis on the gradient correlation coefficient and label leaking phenomenon provide some interesting insights. The writing of the paper could be improved. For example, "Transferability analysis" in section 1 is barely understandable;2. The authors might want to consider shrink it down the recommended length.<BRK>The paper presents a novel adversarial training setup, based on distance based loss of the feature embedding. + novel loss+ good experimental evaluation+ better performance  way too long  structure could be improved  pivot loss seems hackyThe distance based loss is novel, and significantly different from prior work. It seems to perform well in practice as shown in the experimental section. Judging the content of the paper alone, it should be accepted. However, the exposition needs significant improvements to warrant acceptance. This paper is 12+1 pages long, plus a 5 page supplement. The structure of the paper could also be improved.
Accept (Poster). rating score: 9. rating score: 7. rating score: 6. <BRK>The authors prove a generalization guarantee for deepneural networks with ReLU activations, in terms of margins of theclassifications and norms of the weight matrices. They compare thisbound with a similar recent bound proved by Bartlett, et al.While,strictly speaking, the bounds are incomparable in strength, theauthors of the submission make a convincing case that their new boundmakes stronger guarantees under some interesting conditions. The analysis is elegant. Very nice paper.<BRK>This paper combines a simple PAC Bayes argument with a simple perturbation analysis (Lemma 2) to get a margin based generalization error bound for ReLU neural networks (Theorem 1) which depends on the product of the spectral norms of the layer parameters as well as their Frobenius norm. The main contribution of the paper is the simple proof technique to derive Theorem 1, much simpler than the one use in the very interesting work [Bartlett et al.2017] (appearing at NIPS 2017) which got an analogous bound but with a dependence on the l1 norm of the layers instead of the Frobenius norm. I enjoyed reading this paper. I verified most of the math. Detailed suggestions  1) The authors should specify in the abstract and in the introduction that they are analyzing feedforward neural networks *with ReLU activation functions* so that the current context of the result is more transparent. 2) A footnote (or citation) could be useful to give a hint on how the inequality 1/e beta^(d 1) <  tilde{beta}^(d 1) <  e beta^(d 1) is proven from the property |beta tilde{beta}|<  1/d beta (middle of p.4).<BRK>This paper provides a new generalization bound for feed forward networks based on a PAC Bayesian analysis. The resulting generalization bound is similar (though not comparable) to a recent result of Bartlett et al (2017), however the technique is different since this submission uses PAC Bayesian analysis. The resulting proof is more simple and streamlined compared to that of Bartlett et al (2017). Typos: Several citations are unparenthesized when they should be. Also, after equation (6) there is a reference command that is not compiled properly.
Accept (Poster). rating score: 8. rating score: 8. rating score: 6. <BRK>The quest of visualizing neural network decision is now a very active field with many contributions. I am missing at least one figure with comparison with more state of the art methods (e.g.I would love to see results from the method by Zintgraf et al.2017 which unlike all included prior methods seems to produce much crisper visualizations and also is very related because it learns from the data, too). And the S_w even better?<BRK>The authors propose a novel quality criterion for signal estimators, inspired by the analysis of linear models. The authors s quality criterion for signal estimators allows them to do a quantitative analysis for a problem that is often hard to quantify. They also propose two new explanatory methods, PatternNet (for signal estimation) and PatternAttribution (for relevance attribution), based on optimizing their new quality criterion. * Quality: The claims of the paper are well supported by quantitative results and qualitative visualizations.<BRK>Yet, the results are rather convincing. However, the contribution could have been much stronger based on a detailed derivation with testable assumptions/approximations, and if based on a clear declaration of the aim. At present the "state of the art" discussion appears quite narrow, being confined to recent methods for visualization of discriminative deep models. The authors convincingly demonstrate for the linear case, that their "PatternNet" mechanism can produce the generative process (i.e.discard spatially correlated "distractors").
Invite to Workshop Track. rating score: 7. rating score: 5. rating score: 4. <BRK>This work to my knowledge is the first to use a DSL closer to a full language. One way it could be improved is if it were compared with another system.<BRK>It would be stronger to compare against a range of different approaches to the problem, particularly given that the paper is working with a new dataset. The baselines are just minor variants of the proposed method.<BRK>Since the problem descriptions are generated syntactically using a template based approach, the improvements in accuracy might come directly from learning the training templatesinstead of learning the desired semantics. What all function names are allowed in the DSL (Figure 1)? Questions for the authors:Why was MAX_VISITED only limited to 100? What wouldthe performance be for a simple brute force algorithm with a timeout of say 10 mins?
Invite to Workshop Track. rating score: 7. rating score: 5. rating score: 4. <BRK>Summary: This paper proposes a dynamic memory augmented neural network for question answering. So it is not clear if there is a huge reduction in the inference time when compared to other models that the authors compare. However, the proposed model looks like a nice piece of interpretable reasoning module. What is the input to the softmax function? I would like to see the mean and variance in the performance.<BRK>To achieve that, the starting point seems to be a strength GRU that has the ability to dynamically add memory banks to the original dialogue and question sentence representations, thanks to the use of imperative DNN programming. Unfortunately, this is the best understanding I got from this paper, as it seems to be in such a preliminary stage that the exact operations of the SGRU are not parsable. After a nice introduction, everything seems to fall apart in section 4, as if the authors did not have time to finish their write up. With PyTorch being so readable, I wish some source code had been made available.<BRK>The authors propose a model for QA that given a question and a story adaptively determines the number of  entity groups (banks). The paper is rather hard to follow as many task specific terms are not explained. This was the core motivation behind this work and the authors fail to discuss this completely.
Accept (Oral). rating score: 7. rating score: 7. rating score: 7. <BRK>A nice contribution of the paper is to show that this can highly improve classification performance on the task of sentiment analysis.<BRK>Overall, the paper contribution is of great benefit.<BRK>In order to obtain an analytic formulation of the decomposition, the authors propose to linearize activation functions in the network.
Reject. rating score: 3. rating score: 3. rating score: 4. <BRK>The paper explores neural architecture search for translation and reading comprehension tasks. It is fairly clearly written and required a lot of large scale experimentation. However, the paper introduces few new ideas and seems very much like applying an existing framework to new problems. A new idea in the paper is the stack based search. Also, how stable are the results you obtain, did you rerun the selected architectures with multiple seeds?<BRK>This paper experiments the application of NAS to some natural language processing tasks : machine translation and question answering. My main concern about this paper is its contribution. The difference with the paper of Zoph 2017 is really slight in terms of methodology. The results on WMT are not state of the art. At the end , the reader can be sure these experiments required a significant computational power.<BRK>This paper proposes a method to find an effective structure of RNNs and attention mechanisms by searching programs over the stack oriented execution engine. Although the new point in this paper looks only the representation paradigm of each program: (possibly variable length) list of the function applications, that could be a flexible framework to find a function without any prior structures like Fig.1 left. However, the design of the execution engine looks not well designed. Comparison in experiments looks meaningless. The conventional method (Zoph&Le,17) in row 3 of Table 1 looks not comparable with proposed methods because it is trained by an out of domain task (LM) using conventional (tree based) search space. According to the footnote, there are largely different settings about the value of \beta, which suggest a sensitivity by changing this parameter.
Invite to Workshop Track. rating score: 6. rating score: 5. rating score: 4. <BRK>While the paper points out that they introduce multiple inductive biases that are useful to produce human like sentences, it is not entirely correct that the objective is being learnt as claimed in portions of the paper. I would like this point to be clarified better in the paper. I think showing results on grounded generation tasks like machine translation or image captioning would make a stronger case for evaluating relevance. I would like to see comparisons on these tasks. While I really like the motivation and the evaluation proposed by this work, I believe that fixing the mismatch between the goals and the actual approach will make for a stronger work. As pointed out by other reviewers, while the goals and evaluation seem to be more aligned with Gricean maxims, some components of the objective are confusing. Grounded generation like MT / captioning would have been a more convincing evaluation.<BRK>This paper argues that the objective of RNN is not expressive enough to capture the good generation quality. In order to address the problems of RNN in generating languages, this paper combines the RNN language model with several other discriminatively trained models, and the weight for each sub model is learned through beam search. I like the idea of using Grice’s Maxims of communication to improve the language generation. I have some detailed comments as follows:  The repetition model uses the samples from the base RNNs as negative examples. More analysis is needed to show it is a good negative sampling method. Do we still need a separate repetition model? Equation 6 and the related text are not very clearly represented. It would be better to add more intuition and better explained. It would be interesting to see deeper analysis about how each model in the objectives influence the actual language generation.<BRK>This paper proposes to improve RNN language model generation using augmented objectives inspired by Grice s maxims of communication. Pros:  Well motivated and ambitious goals  Human evaluation conducted on the outputs. Cons:  My main concern is that it is unclear whether the models introduced are indeed implementing the Gricean maxims. Similary, for the entailment model, what is an "obvious" entailment"? Not sure we have training data for this in particular. The results seem to be inconsistent. As far as I can tell these ideas could have been more simply implemented by training a re ranker to score the n best outputs of the decoder. Why not try it?
Accept (Poster). rating score: 7. rating score: 7. rating score: 6. <BRK>This paper proposes a simple post processing technique for word representations designed to improve representational quality and performance on downstream tasks. 4) I think it s likely that there is a strong connection between the optimal value of D and the frequency distribution of words in the evaluation dataset.<BRK>This is a really nice experiment, and I think it could easily be part of the main paper (perhaps swapping with the stuff in section 2.2). The experiments conducted in the paper are comprehensive. The correlations, and the distribution of high frequency words) seems to be quite different for each of the three models?!<BRK>This paper provides theoretical and empirical motivations for removing the top few principle components of commonly used word embeddings. Perhaps there is a connection to the findings in this paper? This is known to significantly affect performance.
Reject. rating score: 4. rating score: 5. rating score: 5. <BRK>This paper postulates that an adversarial perturbation consists of a model specific and data specific component, and that amplification of the latter is best suited for adversarial attacks. I therefore recommend it be rejected. I m not sure this is true. "The best metric should be human eyes, which is unfortunately difficult to quantify".<BRK>This paper focuses on enhancing the transferability of adversarial examples from one model to another model. The main contribution of this paper is to factorize the adversarial perturbation direction into model specific and data dependent. Motivated by finding the data dependent direction, the paper proposes the noise reduced gradient method. The paper is not mature. The authors need to justify their arguments in a more rigorous way, like why data dependent direction can be obtained by averaging; is it true factorization of the perturbation direction?<BRK>The problem of exploring the cross model (and cross dataset) generalization of adversarial examples is relatively neglected topic. On the plus side the paper presents very strong practical evidence that the transferability of the examples can be enhanced by such a simple methodology significantly. However this hypothesis seems as a tautology as the splitting is engineered in a way to formally describe the informal statement.
Reject. rating score: 4. rating score: 4. rating score: 5. <BRK>+ Quality:The paper discusses an interesting direction of incorporating humans in the training of a generative adversarial networks in the hope of improving generated samples. Results are preliminary. However, the current manuscript is not ready for publication.<BRK>The technical idea of this paper is to introduce a separate score in the GAN training process. Summary:The goal of the technique to involve human interaction in generative processes is interesting.<BRK>The statement lacks scientific support. However, instead of user interaction, it models it by a simulated measure of the quality of user interaction and then feeds it to a Gan architecture. + The idea is very interesting. The expectation which is set in the abstract and the introduction of the paper is higher than the experiments shown in the Experimental setup.
Reject. rating score: 3. rating score: 3. rating score: 7. <BRK>It is also necessary to inspect claims that are made by researchers to avoid the community straying in the wrong direction. The main problem however is that it seems that this submission completely misrepresents the intent of (Mirowski et al, 2016) by using a straw man argument, and makes a rather unacademic and unsubstantiated accusation of lack of repeatability of the results.<BRK>This paper proposes to re evaluate some of the methods presented in a previous paper with a somewhat more general evaluation method. This proposed paper rejects some of the claims that were made in Mirowski et al.2016, mainly the capacity of the deep RL agent to learn to navigate in such environments. All of the reported results are what you would expect. By the way to me results presented in figure 5 are not enough to claim that the agent trained on random map is implementing a purely reactive wall following strategy. I therefore recommend not to accept this paper in its current form.<BRK>This evaluation includes training the agent on a set of training mazes and testing it s performance on a set of held out test mazes. Although there are some (minor) differences between the implementation with Mirowski et al.2016, I believe the conclusions made by the authors are mostly valid. I would firstly like to point out that measuring generalization is not standard practice in RL.
Accept (Poster). rating score: 8. rating score: 7. rating score: 5. <BRK>My comments:The paper is well written and I really enjoyed reading this paper. However I would encourage authors to rephrase their claim of emergent translation (the title is misleading) as the authors pose this as a supervised problem and the setting has enough constraints to learn a common representation for both languages (bridged by the image) and hence there is no autonomous emergence of translation out of need. I see this work as adding communication to improve the translation learning. In current form of equation 1, I think you are not including the distractor images into account while computing the loss? What is the size of the vocabulary used in all the experiments? Because Gumbel Softmax doesn’t scale well to larger vocabulary sizes and it would be worth mentioning the size of your vocabulary in all the experiments. Are you willing to release the code for reproducing the results?<BRK>The proposed approach closely mirrors that of Nakayama and Nishida, 2017 in that image aligned text is encouraged to map to similarly to the grounded image. Unlike in this previous work, the approach proposed here induces this behavior though a multi agent reference game. The key distinction being that in this gamified setting, the agents sample many more descriptions from their stochastic policies than would otherwise be covered by the human ground truth. Strengths:   The paper is fairly clearly written and the figures appropriately support the text. I would have liked to see some context as how these results compare to an approach trained with aligned corpora. Obviously, outperforming such a model is not necessary for this approach to be interesting, but it would provide useful context on how well this is doing.<BRK>Please find my detailed comments/questions/suggestions below:1) IMO, the paper could have been written much better. At the core, this is simply a model which uses images as a pivot for learning to translate between L1 and L2 by learning a common representation space for {L1, image} or {L2, image}. There are several works on such multimodal representation learning but the authors present their work in a way which makes it look very different from these works. IMO, this leads to unnecessary confusion and does more harm than good. For example, the abstract gives an impression that the authors have designed a game to collect data (and it took me a while to set this confusion aside). 5) Some of the choices made in the Experimental setup seem questionable to me:     Why  use a NMT model without attention? I don t think this is a fair comparison. The authors can mention their results without normalization if that works well for them but it is not fair to drop normalization from the model of N&N if that gives better performance. 6) It would be good to mention the results of the NMT model in Table 1 itself instead of mentioning them separately in a paragraph.
Accept (Poster). rating score: 8. rating score: 8. rating score: 5. <BRK>First of all, the paper is very well written and structured. The certificate is derived with rigorous and sound math. An innovative approximation based on insight into a relation to the MAXCUT algorithm is shown. Both the performance of the new training objective and the tightness of the cerificate are analyzed empirically showing that good agreement with the theory and good results in terms of robustness against several attacks. In summary, this is an innovative paper that treats the subject with rigorous mathematical formalism and is successful in the empirical evaluation.<BRK>This paper develops a new differentiable upper bound on the performance of classifier when the adversarial input in l_infinity is assumed to be applied. The paper already mentions about this direction and it would be interesting to see the experimental results. Therefore minimizing this upper bound together with a classification loss makes perfect sense and provides a theoretically sound approach to train a robust classifier. In conclusion, I recommend this paper to be accepted, since it presents a new and feasible direction of a principled approach to train a robust classifier, and the paper is clearly written and easy to follow.<BRK>This paper derived an upper bound on adversarial perturbation for neural networks with one hidden layer. However, I find there are some limitations/weakness of the proposed method:1. The method is likely not extendable to more complicated and more practical networks, beyond the ones discussed in the paper (ie with one hidden layer)2.
Reject. rating score: 4. rating score: 7. rating score: 8. <BRK>The paper presents an alternative way to implement weight decay in Adam. The idea presented in the paper is interesting, but I have some concerns about it. First, the authors argue that the weight decay should be implemented in a way different from the minimization of a L2 regularization. This seems a very weird statement to me. I am not even sure how I should interpret what they propose. The fact is that SGD and Adam are optimization algorithms, so we cannot just change the update rule in the same way in both algorithms and expect them to behave in the same way just because the added terms have the same shape! I am not sure this is enough to be considered as a scientific proof. Also, the empirical experiments seem to use the cosine annealing of the learning rate. This means that the only thing the authors proved is that their proposed change yields better results when used with a particular setting of the cosine annealing.<BRK>At the heart of the paper, there is a single idea: to decouple the weight decay from the number of steps taken by the optimization process (the paragraph at the end of page 2 is the key to the paper). I think that the proposed implementation should be taken seriously, especially in conjunction with the discussion that has been carried out with the work of Wilson et al., 2017 (https://arxiv.org/abs/1705.08292). However, I would like to add a couple more points to the discussion:   "Optimal weight decay is a function (among other things) of the total number of epochs / batch passes." in principle, it is a function of weight updates. Another ICLR 2018 submission has an interesting take on the norm of the weights and the algorithm (https://openreview.net/forum?id HkmaTz 0W&noteId HkmaTz 0W). This third part also seems to correspond to the difference between Adam and AdamW through the way they branch out after following similar curves. One wonders what causes this branching and whether the key the desired effects are observed at the bottom of the landscape. Therefore, it is not completely reasonable to expect all such possible discussions to take place at once. The paper as it stands is reasonably self contained and to the point. Edit: Thanks very much for the updates and refinements.<BRK>This paper investigates weight decay issues lied in the SGD variants, especially Adam. Current implementations of adaptive gradient algorithms implicitly contain a crucial flaw, by which weight decay in these methods does not correspond to L2 regularization. To fix this issue, this paper proposes the decoupling method between weight decay and the gradient based update. This is an interesting finding. Their experimental results to validate the effectiveness of their proposed method are well organized.
Invite to Workshop Track. rating score: 6. rating score: 6. rating score: 5. <BRK>This paper proposes to re formulate the GAN saddle point objective (for a logistic regression discriminator) as a minimization problem by dualizing the maximum likelihood objective for regularized logistic regression (where the dual function can be obtained in closed form when the discriminator is linear). The observations are interesting, despite being on the toyish side. I am not an expert on GANs for domain adaptation, and thus I can not judge of the quality of the experimental comparison for this application, but it seemed reasonable, apart for the restriction to the linear discriminators (which is required by the framework of this paper). They motivate their approach by repeating the previously made claim that the naive gradient approach is non convergent for generic saddle point problems (Figure 1); while a gradient approach often works well for a minimization formulation.<BRK>This paper studies a dual formulation of an adversarial loss based on an upper bound of the logistic loss. The method is demonstrated on a toy example and on the task of unsupervised domain adaptation. Did the authors tried their approach to non DA tasks, such as generating images, as often done with GANs? However, I wonder if the fact that the method has to rely on a simple classifier does not limit its ability to tackle other tasks. The DA results are shown with a linear classifier, for the comparison to the baselines to be fair, which I appreciate. What is done here is quite different, but I think it would be worth discussing these relationships in the paper.<BRK>It relies on the logistic regression model as the discriminator, and the dual formulation of logistic regression by Jaakkola and Haussler. It is mentioned that results are expected to be lower than those produced by methods with a multi layer classifier as the discriminator (e.g.Shen et al., Wasserstein distance guided representation learning for domain adaptation, Ganin et al., Domain adversarial training of neural networks?). The “reverse validation” method described in Ganin et al., Domain adversarial training of neural networks, JMLR, 2016 might be helpful.
Reject. rating score: 4. rating score: 5. rating score: 5. <BRK>The paper presents results across a range of cooperative multi agent tasks, including a simple traffic simulation and StarCraft micro management. The architecture used is a fully centralized actor (Master) which observes the central state in combination with agents that receive local observation, MS MARL. Pros:  The results on StarCraft are encouraging and present state of the art performance if reproducible. Cons: The experimental evaluation is not very thorough:No uncertainty of the mean is stated for any of the results. I wonder if this is due to the very small batch size used ("a small batch size of 4 ").<BRK>This paper investigates multiagent reinforcement learning  making used of a "master slave" architecture (MSA). On the positive side, the paper is mostly well written, seems technically correct, and there are some results that indicate that the MSA is working quite well on relatively complex tasks. Please explain what would be the differences with CommNet with 1 extra agent that takes in the same information as your  master . So it is not quite clear to me what explains the difference. It this the case? What are the standard errors? * I cannot understand figure 7 without more explanation.<BRK>The paper proposes a neural network architecture for centralized and decentralized settings in multi agent reinforcement learning (MARL) which is trainable with policy gradients. Authors experiment with the proposed architecture on a set of synthetic toy tasks and a few Starcraft combat levels, where they find their approach to perform better than baselines. First, authors do not formulate what exactly is the problem statement for MARL. Is it an MDP or poMDP? Currently, it looks Based on that, I can’t recommend acceptance of the paper.
Reject. rating score: 6. rating score: 6. rating score: 6. <BRK>Paper Strengths:* The proposed coupled ensembles method truly show impressive results in classification benchmark (DenseNet BC L   118 k   35 e   3). * Detailed analysis on different ensemble fusion methods on both training time and testing time. * Simple but effective design to achieve a better result in testing time with same total parameter budget. * More analysis can be conducted on the training process of the model. Will it converge faster?<BRK>Strengths:* Very simple approach, amounting to coupled training of "e" identical copies  of a chosen net architecture, whose predictions are fused during training. * The practical advantages of the proposed approach are twofold:1. Weaknesses:* Although results are very strong, the proposed models do not outperform the state of the art, except for the models reported in Table 4, which however were obtained by *traditional* ensembling of coupled ensembles. It d be valuable to understand whether this kind of result holds for other network architectures or whether it is specific to this choice of net. Overall, the paper provides limited technical novelty.<BRK>Existing literature with branching architecture either employ a 2 stage training approach, training branches independently and then training the fusion network, or the branching is restricted to local regions (set of contiguous layers). In effect, this paper extends the existing literature suggesting end to end branching. Although joint end to end training of branches certainly brings value compared to independent training, but the increased resource requirements may limits the applicability to large benchmarks such as ImageNet.
Reject. rating score: 4. rating score: 5. rating score: 5. <BRK>If your goal was to improve reconstructions in ALI, one could simply add an reconstruction (or cycle) penalty to the ALI objective as advocated in the (not cited) ALICE paper (Li et al., 2017   "ALICE: Towards Understanding Adversarial Learning for Joint Distribution Matching"). The failure to control the experimental conditions makes this comparison inappropriate. There is no quantitative evaluations at all. While many GAN papers do not place an emphasis on quantitative evaluations, at this point, I consider the complete lack of such an evaluation as a weakness of the paper. I assume that the last term in Eqns 3 should have G(z) as opposed to G(z ,E(x)).<BRK>The paper proposes a modified GAN objective, summarized in Eq.(3). Overall Comments:Originality: the proposed IVE GAN algorithm is quite novel. Clarity: Overall clear, while important details are missing. Please see some points in Detailed Comments. This is a strong point to claim. In Eq.(3), the expectation \E_{z ~ P_Z} in the 3rd term is NOT clear, as z is not involved in the evaluation. From the supplement tables, It seems that the novel sample G(z , E(x)) is implemented as G evaluated on the concatenation of noise sample z  ~ P_{Z } and encoded feature z E(x).<BRK>Variations on this theme have been presented previously (see, e.g."Learning to Generate Chairs with Convolutional Neural Networks" by Dosvitskiy et al., CVPR 2015). I m curious why no regularization (e.g., some sort of GAN cost) was placed on the marginal distribution of the "image type" encodings E(x). The use of GANs is a somewhat novel extension of existing conditional GAN methods. The paper is reasonably written, though many parentheses are missing.
Accept (Poster). rating score: 7. rating score: 7. rating score: 5. <BRK>For instance, it seems that the separable convolution presented in Section 2.1 were introduced by (Chollet, 2016) and are not part of the contribution of this paper. The authors should thus clarify the contributions of the paper. In terms of significance, the SliceNet architecture is interesting and is a solid contribution for reducing computation cost of sequence to sequence models. I wonder if the proposed approach could be applied to other sequence to sequence tasks in NLP or even in speech recognition ? The three equations just before Section 2.2 should also be adapted as they seem redundant with Table 1.<BRK>I think the paper constitutes a good contribution, and adjustments to the experimental section could make it a great contribution. The relationship between spatial convolutions, pointwise convolutions, depthwise convolutions, depthwise separable convolutions, grouped convolutions, and super separable convolutions is explained very clearly, and the authors properly introduce each model component. Perhaps as a consequence of this, the experimental section feels squeezed in comparison.<BRK>However, I believe that more experiments should be performed and the explanations could be more concise. I would have enjoyed a parameter comparison in Table 3 as it is claimed this architecture has less parameters and additional experiments would be welcome. As it does not reach the state of the art, "super separable convolutions" could be compared on other tasks?
Accept (Poster). rating score: 7. rating score: 6. rating score: 6. <BRK>The paper considers the problem of online selection of RL algorithms. For example in their conclusion they mention “Fairness of algorithm evaluation is granted by the fact that the RL algorithms learn off policy”. This is not correct. This weak notion of regret is captured by the short sighted pseudo regret.<BRK>The performance measure used by the authors is rather misleading ("short sighted regret"): they compare what they obtain to what the policy discovered by the best reainforcement learning algorithm \underline{based on the trajectories they have seen}, and the trajectories themselves are generated by the choices made by the algorthms at previous time. Overall the paper is well written, and presents some interesting novel ideas on aggregating reinforcement learning algorithms. The authors use UCB1, but they did not try KL UCB, which is stricly better (in fact it is optimal for bounded rewards).<BRK>Wouldn t it make more sense to optimize regret accumulated on this global time? This also suggests the proposed algorithm may be quite sub optimal in terms of total number of decision steps. Indeed, if we left apart the choice for this performance measure, the paper is relatively well written and provides both theoretical and practical results that are of interest. The proposed strategy thus seems a bit naive since different algorithms from the set \cal P may generate trajectories of different length.
Reject. rating score: 4. rating score: 4. rating score: 5. <BRK>The experiments have been performed on a 2D grid, where the agent has partial observation. Paper Strengths:  The proposed approach outperforms the baselines. (2) There is no quantitative result for the zero shot experiments, which is one of the main claims of the paper. (3) The ideas of using instructions for navigation or using attention for combining visual and textual information have been around for a while. So there is not much novelty in the proposed method either. (4) References to attention papers that combine visual and textual modalities are missing. The authors mention other environments are unstable, but that is not a good excuse. There are various environments that are used by many users.<BRK>* The paper claims that the fusion method realizes a *minimalistic* representation, but this statement is only justified by an experiment that involves the inclusion of the visual representation, but it isn t clear what we can conclude from this comparison (e.g., was there enough data to train this new representation?). * It isn t clear that much can be concluded from the attention visualizations in Figs. The paper proposes a neural architecture that employs an RNN to encode the language input and a CNN to encode the visual input. Further, the attention figure in Fig.7(b) seems to foveate on both bags. This test set is very small, with only 19 instructions. Related, there is no mention of a validation set, and the discussion seems to suggest that hyperparameters were tuned on the test set.<BRK>This paper solves the problem of navigating to the target object specified by language instruction in a 2D grid environment. It requires understanding of language, language grounding for visual features, and navigating to the target object while avoiding non target objects. Although Figure 6 and Figure 7 try to show insights about the proposed attention model, they don’t tell which kernel is in charge of which visual feature. Blurred attention maps in Figure 6 and 7 make it hard to interpret the behavior of the model. The interpretation of n in the paper is vague, the authors should also show qualitatively why n 5 is better than that of n 1,10. It would be better to test the proposed method in a similar scale with the existing 3D navigation environments (Chaplot et al., 2017 and Hermann et al., 2017).
Accept (Poster). rating score: 7. rating score: 4. rating score: 4. <BRK>The model allows making relatively long term predictions with uncertainties. The models are used to perform model predictive control to achieve informative actions. Sec 7.1: how is the optimization for the MPC performed? The paper reads well.<BRK>As far as I can understand, there are no quantitative results in simulation at all, and the real world results are not good, indicating something like 15 degrees of error in predicting the pose of a single object. Overall, this seems like a good workshop paper, but probably substantial additional experimental work is needed in order to evaluate the practical usefulness of this method. The biggest issue with the paper though is with the results.<BRK>Negative aspects:Although the premise of the paper is interesting, its execution is not ideal. The formulation of the problem is unclear and difficult to follow, with a number of important terms left undefined. I would assume this is a serious typo, but cannot confirm given that the relation between the minimize cost C and the Renyi entropy H is not explicitely stated. why are both f and z parameters of C?
Reject. rating score: 2. rating score: 4. rating score: 7. <BRK>Here are my main critics of the papers:1. Why the same intuition of UIE can be applied to RNNs? 3.The paper proposed the new architecture RIN, but it is not much different than a simple RNN with identity initialization.<BRK>The paper investigates the iterative estimation view on gated recurrent networks (GNN). Questions:  Section 2 suggests that use of the gate  in GNNs encourages to learn an identity mapping.<BRK>The authors observe that the proposed recurrent identity network (RIN) is relatively robust to hyperparameter choices. Review: I very much like the paper. I might have missed something. Also some curves in the appendix stop abruptly without visible explosions. Would RINs readily learn to reset parts of the hidden state?
Invite to Workshop Track. rating score: 6. rating score: 5. rating score: 5. <BRK>Pros and cons:pros : see abovecons:My problem with the paper is lack of experiments on public datasets. Besides presenting results on publicly available data, the paper would also be improved by adding a baseline in which the logits of the language model are added to the logits of the seq2seq decoder at training time. Further experiments indicate that through the integration of a language model at training time the seq2seq s decoder can be smaller as it is relieved of language modeling.<BRK>The experimental results show improvement for both baseline and adaptation scenarios. Pros:The approach is adapted from deep fusion but the results are promising, especially for the off domain setup. Why the decoder is single layer but for LM it is 2 layer? It would be more interesting to test it on more area, e.g.Machine Translation. In the paper, it still only compared with deep fusion with one decoder layer. I suspect deep decoder + shallow fusion already could provide good results.<BRK>It is hard to say with results on private data only, as it cannot be compared with strong baselines available in the literature. This paper argues this is not good as the ASR decoder and LM are trying to solve the same problem. I applaud the use of a public dataset todemonstrate some of the results of the new algorithm, and for this I am raisingmy score. Experiments on private data show that the ColdFusion approach works better than the DeepFusion approach.
Reject. rating score: 3. rating score: 4. rating score: 4. <BRK>This paper presents a nearest neighbor based continuous control policy. The overall algorithm is very simple to implement and can do reasonably well on some simple control tasks, but quickly gets overwhelmed by higher dimensional and stochastic environments.<BRK>SUMMARYThe paper deal with the problem of RL. When a policy is to be executed, it does nearest neighbor search to find then closest trajectory and executes it.<BRK>This paper is clearly written and it is important to compare simple approaches on benchmark problems. While these simple tasks are useful for diagnostics, it is well known that these tasks are simple and, as the author s suggest "more challenging tasks  .... are necessary to properly assess advances made by sophisticated, optimization based policy algorithms." However, the originality and significance of this work is a significant drawback.
Reject. rating score: 4. rating score: 4. rating score: 5. <BRK>This is indeed an interesting point, however the proof is almost trivial and I am not sure if this provides any significant contribution for the future research.<BRK>Now, the conceptual innovation of GANs is that this minimax formulation can be turned into a zero sum game played by two algorithmic architectures, the generator and the discriminator.<BRK>So, having slow worst case convergence may not necessarily be an issue with higher capacity GANs, and this paper does not address this issue with the results. The paper shows that when the discriminator is unconstrained, even though the generator is constrained to be linear, the convergence rates are exponentially slow in the dimension.
Reject. rating score: 4. rating score: 5. rating score: 7. <BRK>3.There is a lack comparison to other methods such as Shaham et al.(2017).Why is using earth mover distance better than MMD based distance? Doesn t this also prevent optimal results ? This section could be improved by demonstrating the approach on more datasets. How does this compare to the near identity constraints in resnets in Shaham et al.?<BRK>3.Label the y axis in Fig 2. 4.The fact that you have early stopping as opposed to a principled regularizer also requires further substantiation. The paper is interesting but could certainly do with more explanations.<BRK>This contribution deal with nuisance factors afflicting biological cell images with a domain adaptation approach: the embedding vectors generated from cell images show spurious correlation. A possible improvement is to try other means for the embedding instead of the Euclidean one.
Reject. rating score: 3. rating score: 5. rating score: 6. <BRK>The description of the proposed method is very unclear. From the paper it is very difficult to make out exactly what architecture is proposed. No, z_L is a vector of latent variables. What is meant by "number of layers connecting the stochastic latent variables"? There are multiple possible reasons for this, but after skimming the code I believe it might be due to a faulty implementation of the variational lower bound.<BRK>Furthermore, how could FAME advance the previous state of the art? The results on the natural images are not complete. Could the layers encode the spatial information better than a deterministic convolutional layer with the same architecture? Please strengthen the motivation and originality of the paper.<BRK>SummaryThis paper proposes VAE modifications that allow for the use multiple layers of latent variables. Is there a way to support this experimentally?
Reject. rating score: 4. rating score: 5. rating score: 5. <BRK>The paper aims to improve the accuracy of reading model on question answering dataset by playing against an adversarial agent (which is called narrator by the authors) that "obfuscates" the document, i.e.changing words in the document. The authors mention that word dropout can be considered as its special case which randomly drops words without any prior. I think the idea is interesting and novel. While there have been numerous GAN like approaches for language understanding, very few, if any, have shown worthy results. First, CBT: NE and CN numbers are too low. Did you also make changes to the dataset? In short, the only comparable dataset is CBT, which has too low accuracy compared to a very simple baseline.<BRK>Summary:This paper proposes an adversarial learning framework for machine comprehension task. Authors report results in 3 different reading comprehension datasets and the proposed learning framework results in improving the performance of GMemN2N. Please check the minor comments. This is just using the idea of adversarial learning (like GAN) and it is not related to self play. 4.In section 2, first paragraph, authors mention that the narrator prevents catastrophic forgetting. It would be more clear if you can provide a complete pseudo code of the learning procedure. This will be clear if you provide the pseudo code for learning.<BRK>In that work a "distractor sentence" is manually added to a passage to superficially, but not logically, support an incorrect answer. It was shown that these distractor sentences largely fool existing reading comprehension systems although they do not fool human readers. This idea seems interesting but very difficult to evaluate. An adversarial word replacement my in fact destroy the factual information needed to answer the question and there is no control for this. The performance of the question answering system in the presence of this adversarial narrator is of unclear significance and the empirical results in the paper are very difficult to interpret.
Reject. rating score: 4. rating score: 4. rating score: 4. <BRK>The paper proposes to address the quadratic memory/time requirement of Relation Network (RN) by sequentially attending (via multiple layers) on objects and gating the object vectors with the attention weights of each layer. The proposed model obtains state of the art in bAbI story based QA and bAbI dialog task. I think this is a significant achievement given the simplicity of the model. Cons:  I am not sure what is novel in the proposed model. Rather, this exactly resembles End to end memory network (MemN2N) and GMemN2N. Please tell me if I am missing something, but I am not sure of the contribution of the paper.<BRK>RN constructs pair wise interactions between objects in RN to solve complex tasks such as transitive reasoning. Likewise, other graph based networks (which although may require strong supervision) are able to decode quite cheaply. The relationship network considers all pair wise interactions that are replaced by a two hop attention mechanism (and an MLP). It would not be fair to claim superiority over RN since you only evaluate on bABi while RN also demonstrated results on other tasks. More specifically, RN uses an MLP over pair wise interactions, does that allow it to model more complex interactions than just selecting two hops to generate attention weights. How did you generate these stories with so many sentences? Without experiments over other datasets and wall clock time results, it is hard to appreciate the significance of this improvement. One direction to strengthen this paper is to examine if RMN can do better than pair wise interactions (and other baselines) for more complex reasoning tasks.<BRK>other questions / comments:  "we use MLP to produce the attention weight without any extrinsic computation between the input sentence and the question." The model achieves good results on bAbI compared to memory networks and the relation network model. I found it difficult to understand how the model is related to relation networks, since it no longer scores every combination of objects (or, in the case of bAbI, sentences), which is the fundamental idea behind relation networks. I d encourage the authors to do a more detailed experimental study with more tasks, but I can t recommend this paper s acceptance in its current form.
Reject. rating score: 5. rating score: 5. rating score: 5. <BRK>The paper proposes an architecture for efficient deep lifelong learning. It is an interesting idea to remember previous examples using the compact representation from autoencoder and use it for transfer learning. 1.It seems that reconstructed data from autoencoder does not contain target values.<BRK>This paper addresses lifelong learning setting under resource constraints, i.e.how to efficiently manage the storage and how to generalise well with a relatively small diversity of prior experiences. My main concern with this paper is that it is not easy to grasp the gist of it.<BRK>While the core idea of this paper is reasonable, it provides little insight into how episodic experience storage compares to related methods as an approach to lifelong learning. Finally, the manuscript lacks clarity.
Reject. rating score: 4. rating score: 5. rating score: 6. <BRK>This paper presents methods for query completion that includes prefix correction, and some engineering details to meet particular latency requirements on a CPU. The notation that you use is a bit sloppy and not everything is introduced in a clear way.<BRK>Much of the paper provides background on LSTMs and edit distance, which granted, are helpful for explaining the ideas. Secondly, the experiments could have more thorough/stronger baselines. Overall, it’s nice a nice study of the query completion application.<BRK>This paper focuses on solving query completion problem with error correction which is a very practical and important problem. I am not familiar with the state of the art methods in this field.
Reject. rating score: 3. rating score: 5. rating score: 6. <BRK>This paper extends the PixelCNN/RNN based (conditional) image generation approaches with self attention mechanism. Pros:  qualitatively the proposed method has good results in several tasksCons:  writing needs to be improved  lack of motivation  not easy to follow technique detailsThe motivation part is missing. Why self attention is so important for image generation?<BRK>While the paper is well written, the motivation for combining self attention and autoregressive models remains unclear unfortunately, even more though as the reported quantitative improvement in terms of log likelihood are only marginal.<BRK>I think the paper would benefit from a little bit more work, but I am open to adjusting my score based on feedback. The presentation/text could use some work. I also felt too much of the architecture is described in prose and could be more efficiently and precisely conveyed in equations. This is a meaningless result, not only because the images were cherry picked. Analyzing samples generated by a generative model (outside the context of an application) should therefore only be used for diagnostic purposes or to build intuitions but not to judge the quality of a model.
Reject. rating score: 5. rating score: 5. rating score: 6. <BRK>This paper introduces a new design of kernels in convolutional neural networks. The original one or the modified one? It would be interesting to report the accuracy of a standard ResNet with 1.325*width as a comparison, as well as the runtime of such a model. I understand that such improvements are not easy to achieve, but one could wonder if they are not due to the randomness of initialization/mini batches.<BRK>Summary:This paper proposed a sparse complementary convolution as an alternative to the convolution operation in deep networks. 8.Figure 5 is hard to understand. The paper is easy to follow and the idea is interesting. The novelty of this paper is limited. Without such experiment, it is unclear whether the improved performance comes from the sparse complementary kernels or the increased number of kernels. The two methods are used separately. However, there is no experimental comparison with these methods. It yields smaller receptive field than the proposed method when the model depth is very small.<BRK>This paper presented interesting ideas to reduce the redundancy in convolution kernels. They are very close to existing algorithms. The CW SC kernel can be regarded as a redundant version of interleaved group convolutions [1]. I would like to see more discussions on the relation to these methods and more strong arguments for convincing reviewers to accept this paper.
Accept (Poster). rating score: 7. rating score: 6. rating score: 5. <BRK>The authors demonstrate this using a Taylor expansion of a standard residual block first, then follow up with several experiments that corroborate this interpretation of iterative inference. * I recommend that the authors also check out Figurnov et al CVPR 2017 ("Spatially Adaptive Computation Time for Residual Networks") which proposes an “adaptive” version of ResNet based on the intuition of adaptive inference. Additionally, parts of the paper are not as clearly written as they could be and lack rigor. This includes the mathematical derivation of the main insight — some of the steps should be spelled out more explicitly.<BRK>This paper investigates residual networks (ResNets) in an empirical way. Overall, the experiments and discussions in the first part of Section 4.2 and 4.3 appears to be interesting, while other observations are not quite surprising. Could you elaborate more on this?<BRK>I think the author should place more focus to study "real" iterative inference with shared parameters rather than analyzing original resnets. The the above said, I think the more important thing is how we can benefit from iterative inference interpretation, which is relatively weak in this paper. Thus the final classification layer should be retrained for every addition or removal of residual blocks.
Accept (Poster). rating score: 7. rating score: 7. rating score: 3. <BRK>[After author feedback]I think the approach is interesting and warrants publication. In the paper the authors also need to describe the differences between their work and the concurrent work of Maddison et al.and Naesseth et al.[Original review]The authors propose auto encoding sequential Monte Carlo (SMC), extending the VAE framework to a new Monte Carlo objective based on SMC. The approach is interesting and the paper is well written, however, I have some comments and questions:  It seems clear that the AESMC bound does not in general optimize for q(x|y) to be close to p(x|y), except in the IWAE special case. Is there a typo in the bound given by eq.(17)?Seems like there are two identical terms. Might this be a typo in 4.1? You still propose to learn the proposal parameters using SMC but with lower number of particles?<BRK>I think there are still some issues, but this work is both valuable and interesting, and it deserves to be published (alongside the Naesseth et al.and Maddison et al.work).This paper proposes a version of IWAE style training that uses SMC instead of classical importance sampling. Going beyond the several papers that proposed this simultaneously, the authors observe a key issue: the variance of the gradient of these IWAE style bounds (w.r.t.the inference parameters) grows with their accuracy. * Equation 3: Should there be a (1/K) in Z? There are a few things I think could be cleared up, but this seems like good work (although I m not totally up to date on the very recent literature in this area). (Although [particle] MCMC is probably a better choice if one wants extremely low bias.) I would recommend at least summarizing the main findings of Appendix A in the main text.<BRK>We have a set of observed variables Y, we posit a latent set of variables X, the mapping from the latent to the observed variables has a parametric form and we have a prior over the parameters. The authors propose an algorithm which uses sequential Monte Carlo + autoencoders. The authors show the AESMC works better than importance weighted autoencoders and the double ELBO method works even better in some experiments. This was very unclear to me. The introduction/experiments section of the paper is not well motivated. What is the problem the authors are trying to solve with AESMC (over existing methods)? There is only one experiment comparing the gains from AESMC, ALT to a simpler (?) We see that they do better but the magnitude of the improvement is not obvious (should I be looking at the ELBO scores as the sole judge?Does AESMC give a better generative model?).
Accept (Poster). rating score: 9. rating score: 7. rating score: 6. <BRK>  This paper shows an equivalence between proto value functions and successor representations. Automatic option discovery from raw sensors is perhaps one of the biggest open problems in RL research.<BRK>This is a well written paper with interesting (and potentially useful) insights.<BRK>The paper extends the idea of eigenoptions, recently proposed by Machado et al.to domains with stochastic transitions and where state features are learned. The approach is evaluated in a tabular domain (i.e., rooms) and Atari games.
Reject. rating score: 3. rating score: 4. rating score: 4. <BRK>Paper Summary:This paper looks at empirically measuring neural network architecture expressivity by examining performance on a variety of complex datasets, measuring dataset complexity with algebraic topology. The paper first introduces the notion of topological equivalence for datasets   a desirable measure to use as it is invariant to superficial differences such as rotation, translation and curvature. The definition of homology from algebraic topology can then be used as a robust measure of the "complexity" of a dataset. (The authors look at using CIFAR 10, but project this down to 3 dimensions   as current methods for persistent homology cannot scale   which somewhat invalidates the goal of testing this out on real data.)<BRK>The authors propose to use the homology of the data as a measurement of the expressibility of a deep neural network. The paper is mostly experimental. On synthetic data, it is shown that the number of neurons of the network is correlated with the homology it can express. To be convinced, I would like to see much stronger experimental evidence: Reporting results on a single layer network is unsettling. So what about networks with more layers?<BRK>It explores empirically the relations between Betti numbers of input data and hidden unit complexity in a single hidden layer neural network, in a purpose of finding closer connections on topological complexity or expressibility of neural networks. So this part is just preliminary but incomplete to the main topic of the paper. The following are some examples.
Accept (Poster). rating score: 7. rating score: 6. rating score: 6. <BRK>The results presented in the paper are convincing.<BRK>In the context here this didn’t seem a particularly relevant addition to the paper. The work is put in context and related to some previous relaxation approaches to sparsity.<BRK>This paper presents a continuous surrogate for the ell_0 norm and focuses on its applications in regularized empirical regularized minimization. Pros:   The paper is clearly written, self contained and a pleasure to read.
Reject. rating score: 4. rating score: 4. rating score: 6. <BRK>This paper proposes integrating information from a semantic resource that quantifies the affect of different words into a text based word embedding algorithm. It also not surprising that adding information about affect improves the ability to predict sentiment and the tone of emails.<BRK>This paper introduces modifications the word2vec and GloVe loss functions to incorporate affect lexica to facilitate the learning of affect sensitive word embeddings. The methodologies of Kiela et al (2015) and Bollegala et al (2016) are very similar to those proposed in this work. Unlike prior work, this is a real valued instead of a binary quantity.<BRK>This paper proposed to use affect lexica to improve word embeddings. They extended the training objective functions of Word2vec and Glove with the affect information. Their experimental results showed that their proposed embeddings outperformed standard Word2vec and Glove. It is not convincing enough for me.
Reject. rating score: 4. rating score: 5. rating score: 5. <BRK>Previous work used tensor trains which decompose the tensor as a chain. Here the authors explore a tree like decomposition. The authors only describe their model using pictures and do not provide any rigorous description of how their decomposition works.<BRK>MERA itseld in a known framework in QM but not in ML. Although the idea seems to be fruitful and interesting I find the paper quite unclear. It is almost impossible to reproduce the results based on such iformal description of tensorization method. This is my main point for critisism. UPDATE: The revised version seems to be a bit more clear.<BRK>The paper contains interesting new ideas and is generally well written. It would be nice to highlight this potential benefit in the introduction. A limited number of experiments on CIFAR10 suggests that the method may work a bit better than related factorizations. Unfortunately this ConvNet is far from state of the art, so it is not clear if the method would also work for better architectures.
Invite to Workshop Track. rating score: 7. rating score: 5. rating score: 4. <BRK>This paper introduces the concepts of counterfactual regret minimization in the field of Deep RL. Specifically, the authors introduce an algorithm called ARM which can deal with partial observability better. The results is interesting and novel. There seems to be a bit of a disconnect before and after section 3.3. There are some papers that could be connected. An experiment illustrating this effect could be illuminating.<BRK>Quality and clarity:The paper provides a game theoretic inspired variant of policy gradient algorithm based on the idea of counter factual regret minimization. The paper claims that the approach can deal with the partial observable domain better than the standard methods. I believe the algorithm introduced  in this paper, if it is presented well, can be  an interesting addition to the literature of Deep RL, e.g.,  in terms of improving the rate of convergence. However, the current version of paper  does not provide conclusive evidence for that as in most of the domains the algorithm only converge marginally faster than the standard ones.<BRK>The claim that ARM is more robust to partially observable domains is supported by experiments where it outperforms DQN. There are several things to like about this paper:  The authors do a good job of reviewing/referencing several papers in the field of "regret minimization" that would probably be of interest to the ICLR community + provide non obvious connections / summaries of these perspectives. A lot of the cited literature was also new to me, so it could be that I m missing something about why this is so interesting. The ultimate algorithm *seems* like it is really quite similar to other policy gradient methods such as A3C, TRPO etc.
Accept (Oral). rating score: 9. rating score: 8. rating score: 7. <BRK>The authors  experiments support their theory showing that their mixed strategy leads to gains over a vanilla dilated convolutional net. I found the paper very well written despite its level of mathematical depth (the authors provide many helpful pictures) and strongly recommend accepting this paper.<BRK>The baseline WaveNet is only somewhat well described as "convolutional;" the underlying network unit is not always a "size 2 convolution" (except for certain values of g) and the "size 1 convolutions" that make it up are simply linear transformations. * As this last condition is relaxed in the present paper, making the space of networks under analysis more similar to the traditional space of recursive NNs, it might be worth mentioning this "alternative history" of the WaveNet.<BRK>This paper theoretically validates that interconnecting networks with different dilations can lead to expressive efficiency, which indicates an interesting phenomenon that connectivity is able to enhance the expressiveness of deep networks. In the experiments, see line 4 of page 9, the authors instead used ReLU activation $g(a, b)  max{a+b, 0}$. The conclusion is useful for developing new tools for deep network design. Cons:In order to show that the mixed dilated convolutional network is expressively efficient w.r.t.the corresponding individual dilated convolutional network, the authors prove it in two steps: Proposition 1 and Proposition 2.
Accept (Poster). rating score: 7. rating score: 7. rating score: 7. <BRK>The authors of this manuscript transformed the k mer representation of DNA fragments to a 2D image representation using the space filling Hilbert curves for the classification of chromatin occupancy. In generally, this paper is easy to read. The components of the proposed model mainly include Hilbert curve theory and CNN which are existing technologies. But the authors make their combination useful in applications. Some specific comments are:1.<BRK>The result, in my opinion, is not sufficient to support the assumption that we could predict the DNA structures solely base on the sequence. This ms makes a novel way to transform the DNA sequence into a 3 dimensional tensor which could be easily utilised by CNN for images. It is therefore biologically interesting to predict if a new DNA sequence could be a binding site. I am not familiar with neural networks and do not comment on the methods but rather from the application point of view.<BRK>To address these points, the authors should compare Hilbert CNN to models of the same capacity (number of parameters) and optimize hyper parameters (k mer size, convolutional filter size, learning rate, …) in the same way as they did for Hilbert CNN. I therefore increased my rating from ‘6: Marginally above acceptance threshold’ to ‘7: Good paper, accept’. Best, The authors present Hilbert CNN, a convolutional neural network for DNA sequence classification. Major comments 1. They further claim that their method can ‘better take the spatial features of DNA sequences into account’ and  can better model ‘long term interactions’ between distant regions. This requires major changes of introduction and discussion.
Invite to Workshop Track. rating score: 8. rating score: 6. rating score: 5. <BRK>Current approaches to 2D image processing based on Deep Neural Networks provide very accurate results and a wide variety of different architectures for image modelling, generation, classification, retrieval. The lack of DL architectures for 3D data is due to complexity of representation of 3D data, especially when using 3D point clouds. Considered paper is one of the first approaches to learn GAN type generative models. The paper is well written, results of experiments are convincing, the authors provided the code on the github, realizing their architectures.<BRK>This paper introduces a generative approach for 3D point clouds. More specifically, two Generative Adversarial approaches are introduced: Raw point cloud GAN, and Latent space GAN (r GAN and l GAN as referred to in the paper). The quantitative results also support the visuals. I would like to see comparison experiments with voxel based approaches in the next update for the paper. I expect these updates to be reflected in the final version of the paper itself as well.<BRK>Summary:This paper proposes generative models for point clouds. First, they train an auto encoder for 3D point clouds,  somewhat similar to PointNet (by Qi et al.). First, they show that the autoencoder s latent space is a good representation for classification problems, using the ModelNet dataset. Methods such as the r GAN score well on the latter by over representing parts of an object that are likely to be filled. The comparison of the evaluation metrics could be useful for future work on evaluating point cloud GANs. The paper simultaneously proposes methods for generating point clouds, and for evaluating them. Overall, I think that this paper could serve as a useful baseline for generating point clouds, but I am not sure that the contribution is significant enough for acceptance.
Invite to Workshop Track. rating score: 6. rating score: 5. rating score: 5. <BRK>QualityThis paper demonstrates that human category representations can be inferred by sampling deep feature spaces. The idea is an extension of the earlier developed MCMC with people approach where samples are drawn in the latent space of a DCGAN and a BiGAN. ClarityThe rationale is clear and the results are straightforward to interpret. ConsDoes not provide new theory but combines existing ideas in a new manner.<BRK>The idea of using MCMCP with GANs is well motivated and well presentedin the paper, and the approach is new as far as I know. However, as discussed in the introduction, the reason an efficientsampling method might be interesting would be to provide insighton the components of perception. On these insights, the paper feltincomplete. Finally, the effect of choosing GAN features vs a more "naive" featurespace is not explored in detail.<BRK>Figure 2 is also not clear. Just the FLD projections of the MCMCP chains are difficult to interpret. This seems interesting but I didn t find any result on that in the paper. It would be interesting to see a discussion on why MCMCP Density is better for group 1 and MCMCP Mean is better for group 2. I like this paper. The addressed problem is challenging and the proposed idea seems interesting.
Accept (Poster). rating score: 9. rating score: 5. rating score: 4. <BRK>Lemma 2.1 can be put into appendix since it is not proposed by this work while the new theoretical analysis of Appendix 5.2 (or at least a summary) can be moved to the main sections2. Figure 9 can be moved earlier to the main section since it well supports one of the contributions of the proposed method (using the DCRNN to capture the spatial correlation)D2: Some discussions regarding the comparison of this work to some state of the arts graph embedding techniques using different deep neural network architectures would be a plus<BRK>The paper proposes to build a graph where the edge weight is defined using the road network distance which is shown to be more realistic than the Euclidean distance. To avoid the expensive matrix operation for the random walk, it empirically shows that K   3 hops of the random walk can give a good performance. Experiments show that the proposed architecture can achieve good performance compared to classic time series baselines and several simplified variants of the proposed model. Although the paper argues that several existing deep learning based approaches may not be directly applied in the current setting either due to using Euclidean distance or undirected graph structure, the comparisons are not persuasive.<BRK>The paper motivates the goal to obtain smooth traffic predictions, but traffic is not a smooth process, e.g.traffic lights and intersections cause non smooth effects. therefore it is difficult to follow this argumentation.
Accept (Poster). rating score: 7. rating score: 6. rating score: 5. <BRK>This is a nice paper. It makes novel contributions to neural program synthesis by (a) using RL to tune neural program synthesizers such that they can generate a wider variety of correct programs and (b) using a syntax checker (or a learned approximation thereof) to prevent the synthesizer from outputting any syntactically invalid programs, thus pruning the search space. In experiments, the proposed method synthesizes correct Karel programs (non trivial programs involving loops and conditionals) more frequently than synthesizers trained using only maximum likelihood supervised training. I have a few minor questions and requests for clarification, but overall the paper presents strong results and, I believe, should be accepted. Specific comments/questions follow:Figure 2 is too small. It would be much more helpful (and easier to read) if it were enlarged to take the full page width. Page 7: "In the supervised setting..." This suggests that the syntaxLSTM can be trained without supervision in the form of known valid programs, a possibility which might not have occurred to me without this little aside. For the versions of the model that use beam search, what beam width was used? Do the results reported in e.g.Table 1 change as a function of beam width, and if so, how?<BRK>The proposed approach claims two advantages over a baseline maximum likelihood estimation based approach. MLE based methods penalize syntactically different but semantically equivalent programs. Further, typical program synthesis approaches don t explicitly learn to produce correct syntax. The approach, and its constituent contributions, i.e.of using RL for program synthesis, and limiting to syntactically valid programs, are novel. Although both the contributions are fairly obvious, there is of course merit in empirically validating these ideas. The improvements over the baseline methods is small but substantial, and enough experimental details are provided to reproduce the results. However, there is no comparison with other approaches in the literature. The authors claim to improve the state of the art, but fail to mention and compare with the state of the art, such as [1]. I do find it hard to trust papers which do not compare with results from other papers. 2.Good empirical evaluation with ablations.<BRK>The authors consider the task of program synthesis in the Karel DSL. The best method very clearly depends on the taks and the amount of available data, but I found it difficult to extract an intuition for which method works best in which setting and why. On the whole this seems like a promising paper. That said, I think the authors would need to convincingly address issues of clarity in order for this to appear. It is not clear the how beam search is carried out. In equation (10) there appear to be two problems. Is syntax checking used at each step of token generation, or something along these lines? Presumaly we need a large corpus of syntax checked training examples to learn this model, which means that, in practice, we still need to have a syntax checker available, do we not?
Accept (Oral). rating score: 8. rating score: 7. rating score: 7. <BRK>Language models are important components to many NLP tasks. This paper argues the softmax is not expressive enough and proposes to use a more flexible mixture of softmaxes. The use of a mixture of softmaxes is motivated from a theoretical point of view by translating language modeling into matrix factorization. The ideas build up on each other in an intuitive way. The idea behind the paper is novel: translating language modeling into a matrix factorization problem is new as far as I know. Reporting that would help interpret the numbers.<BRK>The authors argue in this paper that due to the limited rank of the  context to vocabulary logit matrix in the currently used version of the softmax output layer, it is not able to capture the full complexity of language. This paper offers a promising direction for language modeling research, but would require more justification, or at least a more developed experimental section. Further, the first two "observations" in Section 2.2 would be more accurately described as "intuitions" of the authors.<BRK>The paper is grounded on a solid theoretical motivation and the analysis is sound and quite interesting. There are no results on large corpora such as 1 billion tokens benchmark corpus, or at least medium level corpus with 50 million tokens. The corpora the authors choose are quite small, the variance of the estimates are high, and similar conclusions might not be valid on a large corpus. Results on this data might be more convincing. "On the state of the art of evaluation in neural language models."
Accept (Poster). rating score: 8. rating score: 7. rating score: 6. <BRK>My only major complaint with the paper is that it does not extend the method to large scale problems on real data, for instance work from the last decade on sequence generation, speech recognition or any of the other RNN success stories that have led to their wide adoption (eg Graves 2013, Sutskever, Martens and Hinton 2011 or Graves, Mohamed and Hinton 2013). However, if the paper does achieve what it claims to achieve, I am sure that many people will soon try out UORO to see if the results are in any way comparable.<BRK>Post rebuttal update:I am happy with the rebuttal and therefore I will keep the score of 7. I d be worried about the variance in the estimate of rank one approximation. My intuition is that because of high variance it would be difficult to train this network, but I could be wrong. This is a very interesting paper.<BRK>The approach is interesting and could potentially be very useful. I have increased the score to 6 based on the comments and revisions from the authors.
Reject. rating score: 3. rating score: 4. rating score: 6. <BRK>I would still like to have a better justification on why should we care about RWA and fixing that model. The writing of this paper seriously needs more work. According to the results the model doesn t really do better than a simple LSTM or GRU.<BRK>The most interesting result is language modeling on Hutter Prize Wikipedia, where RDA very significantly improves upon RWA   but again, only matches a standard GRU or LSTM. To make the evaluation convincing enough for acceptance, RDA should be combined with those models and evaluated more competitively on multiple widely studied tasks.<BRK>The motivation of the paper and the approach taken by the authors are sensible, such as adding discounting was applied to introduce forget mechanism to the RWA and manipulating the attention and squash functions. The proposed method is using Elman nets as the base RNN. Overall, the proposed method seems to be very useful for the RWA.
Reject. rating score: 5. rating score: 5. rating score: 6. <BRK>Summary:The paper proposes a new dialog model combining both retrieval based and generation based modules. Answers are produced in three phases: a retrieval based model extracts candidate answers; a generator model, conditioned on retrieved answers, produces an additional candidate; a reranker outputs the best among all candidates. The approach is interesting: the proposed ensemble can improve on both the retrieval module and the generation module, since it does not restrict modeling power (e.g.the generator is not forced to be consistent with the candidates). The experimental section focuses on the task of building conversational systems. Given that this is a novel dataset, I don t know what the state of the art should be.<BRK>3.The human evaluation performance of a simple reranking ensemble between the authors  generation based model and their retrieval based model is significantly higher than multi seq2seq, suggesting that multi seq2seq may not be an especially powerful way to combine information from the two models. This novel multi seq2seq approach, which includes attention and a pointer network, increases reply diversity compared to purely retrieval or generation based models. When the original examples are non English, papers should also include the original in addition to a translation. The authors present a generation based neural dialog response model that takes a list of retrieved responses from a search engine as input.<BRK>The approach involves multiple steps. Then a concatenation of the query and the candidates are fed into a generative model to generate an additional artificial candidate. Each of these steps involves careful engineering and for each there are some minor novel components. Yet, not all of the steps are presented in complete technical detail. Consequently, it would be hard to exactly reproduce the results of the paper. Experimental validation also is relatively thin. While the paper report both BLEU metrics and Fleiss kappa from a small scale human test, the results are based on a single split of a single corpus into training, validation and test data.
Invite to Workshop Track. rating score: 5. rating score: 5. rating score: 5. <BRK>The contribution is marginal but interesting as a summaryThis paper analyzes the effectiveness of model pruning for deployment in resource constrained environments. Given this setup, the paper present a number of comparisons and experimental validations. There are several steps that are not clear to me. That is why I think would be interesting to do at group level as proposed in some related methods. I do not understand why measuring the non zero parameters if, in the baseline, there is no analysis on how many of these parameters can be actually set to 0 by pruning as a postprocessing step. Nevertheless, results are consistent with other approaches listed in the state of the art (pruning while training is a good thing).<BRK>Summary:This paper presents a thorough examination of the effects of pruning on model performance. Importantly, they compare the performance of "large sparse" models (large models that underwent pruning in order to reduce memory footprint of model) and "small dense" models, showing that "large sparse" models typically perform better than the "small dense" models of comparable size (in terms of number of non zero parameters, and/or memory footprint). Significance: The paper makes a nice contribution, though it is not particularly significant or surprising. The primary observations are:(1) large sparse is typically better than small dense, for a fixed number of non zero parameters and/or memory footprint. Results are not surprising, and are in line with previous papers.<BRK>This paper presents a comparison of model sizes and accuracy variation for pruned version of over parameterized deep networks and smaller but dense models of the same size. The paper demonstrates that pruning of large over parameterized models leads to better classification compared to smaller dense models of relatively same size. This pruning technique is demonstrated as a modification to TensorFlow on MobileNet, LSTM for PTB dataset and NMT for seq2seq modeling. The fact that most deep networks are inherently over parametrized seems to be known for quite sometime. The experiments are missing comparison with the threshold based pruning proposed by Han etal. Overall, the paper seems to perform experimental validation of some of the known beliefs in deep learning. The novelty in terms of ideas and insights seems quite limited.
Reject. rating score: 2. rating score: 3. rating score: 4. <BRK>The proposed techniques are a combination of previously published steps, and the new algorithm fails to reach state of the art on the tiny data sets. Is the point that GloVe is a bad algorithm? If the latter, then the experimental results are far weaker than what I would find convincing.<BRK>I hate to say that the current version of this paper is not ready, as it is poorly written. The authors present some observations of the weaknesses of the existing vector space models and list a 6 step approach for refining existing word vectors (GloVe in this work), and test the refined vectors on 80 TOEFL questions and 50 ESL questions. In addition to the incoherent presentation, the proposed method lacks proper justification. The paper is poorly written and the proposed methods are not well justified.<BRK>This paper proposes a ranking based similarity metric for distributional semantic models. None of this work is cited, which I find inexcusable. 2.The evaluation is limited, in that the standard evaluations (e.g.SimLex would be a good one to add, as well as many others, please refer to the literature) are not used and there is no comparison to previous work. The results are also presented in a confusing way, with the current state of the art results separate from the main results of the paper. In other words, what is supposed to be the take away, and why should we care? As such, I do not recommend it for acceptance   it needs significant work before it can be accepted at a conference.
Reject. rating score: 2. rating score: 3. rating score: 4. <BRK>While the idea would be interesting in general, unfortunately the experiment section is very much toy example so that it is hard to know the applicability of the proposed approach to any more reasonable scenario. The experimental setup is 4x4 grid world with different basic shape or grey level rendering. Anyway, it would be good to have a quantified metric on this, which is not just eyeballing PCA scatter plots.<BRK>My opinion is that this paper is not ready for publication. The proposed approach combines convolutional layers for processing input images, Asynchronous Advantage Actor Critic for deep reinforcement learning task and adversarial approach to force the embedding representation to be independent of the visual representation of games. However, the authors do not explain how this architecture can be used to do the domain adaptation.<BRK>The key idea is that the agent learns a shared representations for tasks with different visual statistics  A lot of important references  touching on very similar ideas are missing. In the last section authors mention the intent to do future work on atari and other env.
Accept (Poster). rating score: 7. rating score: 6. rating score: 4. <BRK>The paper proposed a novel regularizer that is to be applied to the (rectifier) discriminators in GAN in order to encourage a better allocation of the "model capacity" of the discriminators over the (potentially multi modal) generated / real data points, which might in turn helps with learning a more faithful generator. It seems the concept of "binarized activation patterns", which the proposed regularizer is designed upon, is closely coupled with rectifier nets.<BRK>The paper proposes a regularizer that encourages a GAN discriminator to focus its capacity in the region around the manifolds of real and generated data points, even when it would be easy to discriminate between these manifolds using only a fraction of its capacity, so that the discriminator provides a more informative signal to the generator. The regularizer rewards high entropy in the signs of discriminator activations. Figure `1 is a fantastic illustration that presents the core idea very clearly. I d also appreciate further discussion of the relationship of this approach to Wasserstein GANs, which also attempt to provide a clearer training gradient when the data and generator manifolds do not overlap.<BRK>Numerical experiments that demonstrate the diversity increment on the generated samples are shown. Concerns.The paper is hard do tear and it is deficit to identify the precise contribution of the authors. The point is, a) The second term will introduce  low correlation in saturated vectors, then the will be informative.
Reject. rating score: 4. rating score: 4. rating score: 4. <BRK>However, this is not accurate, BinaryNet actually achieves 41.8% top 1 accuracy for Imagenet with Alexnet (e.g., see BNN on table 2 in Hubara et al.). This paper proposes a method to quantize weights and activations in neural network during propagations. However, the experimental results are not sufficiently convincing that this method is meaningfully improving over previous methods. The residual binarization idea is interesting.<BRK>6.Could the authors also validate their proposed method on ImageNet? The author should also have a discussion on these works. This argument is not well supported by the experiments. I prefer to see two plots: one for Binarynet and one for the proposed method.<BRK>On the other hand, this comparison is not fair for ResBinNet as well. This paper proposes ResBinNet, with residual binarization, and temperature adjustment. I appreciate a lot that the authors were able to validate their idea by building a prototype of an actual hardware accelerator. The model size was much larger in BinaryNet than in ResBinNet.
Reject. rating score: 4. rating score: 4. rating score: 5. <BRK>Motivated by this interpretation, the authors propose a new similarity metric between data points, which leads to a new manifold embedding method. Main comments:This direction is interesting. But unfortunately the paper is confusingly written and several points are never made clear. The conveyed impression is that the proposed methods are mainly incremental additions to the framework of Culpepper and Olshausen. Still not convinced, unfortunately.<BRK>The paper provide nice illustrative experiments arguing why transport operators may be a useful modeling tool, but does not go beyond illustrative experiments. A citation to this seminal work would be appropriate. This is also exactly the type of data considered in the paper. Since the provided experiments are mostly illustrations, I would argue that the significance of the paper is limited.<BRK>Experiments on the swiss roll and synthetic rotated images on USPS digits show that the proposed method could learn useful transformations on the data manifold. However, the experiments in the paper is weak. The swiss roll is a very simple synthetic dataset.
Reject. rating score: 4. rating score: 5. rating score: 7. <BRK>The paper starts with a discussion about desirable properties of a loss function and points out the fact that (plug in) empirical versions of the gradient of this quantity are biased, which limits its interest, insofar as many learning techniques are based on (stochastic) gradient descent. In its current state, this argument looks artificial. Indeed, zero bias can be a desirable properties for an estimate but being biased does not prevent it from being accurate. The authors propose to use instead the Cramer distance, which is a very popular distance in Statistics and on which many statistical hypothesis testing procedures rely, and review its appealing properties.<BRK>The motivation for using the Cramer distance is that it has unbiased sample gradients while still enjoying some other properties such as scale sensitivity and sum invariant. or is it due to the fact the KL 	divergence is not symmetric? or ?2.The main argument for the paper is that the simple sample based estimate for the gradient using the Wasserstein 	metric is a biased estimate for the true gradient of the Wasserstein distance, and hence it is not favored with	SGD type algorithms. Was this issue overlooked in the literature? Similarly, the experiments for the multivariate case using GANs and	Neural Networks do not really deliver tangible, concrete and conclusive results.<BRK>The authors investigate how the properties of different discrepancies for distributions affect the training of parametric model with SGD. The KL divergence is not is scale sensitive, and the Wasserstein metric does not provide unbiased gradients. The authors thus posit the Cramer distance as a foundation for the discriminator in the GAN, and then generalize this to an energy based discriminator. However, overall I thought the paper did a nice job presenting some context for GAN training.
Reject. rating score: 5. rating score: 6. rating score: 6. <BRK>The central idea is to frame learning in terms of a new kind of Q value that attempts to smooth out Q values by framing them in terms of expectations over Gaussian policies. To be honest, I didn t really "get" this paper. While I may or may not have understood the core technical contribution, I think the experiments can be critiqued: they didn t really seem to work out. Figures 2&3 are unconvincing   the differences do not appear to be statistically significant. Perhaps you should cite that?<BRK>I think I should understand the gist of the paper, which is very interesting, where the action of \tilde Q(s,a) is drawn from a distribution. The major weakness is that throughout the paper, I do not see an algorithm formulation of the Smoothie algorithm, which is the major algorithmic contribution of the paper (I think the major contribution of the paper is on the algorithmic side instead of theoretical). So I suggest moving them to the Appendix and make the major focus more narrowed down.<BRK>The paper is very clearly written and easy to read, and its contributions are easy to extract. The inclusion of proof summaries in the main text would strengthen this aspect of the paper. On the negative side, the paper fails to make a strong case for significant impact of this work; the solution to this, of course, is not overselling benefits, but instead having more to say about the approach or finding how to produce much better experimental results than the comparative techniques.
Accept (Poster). rating score: 7. rating score: 6. rating score: 5. <BRK>This paper describes a first working approach for fully unsupervised neural machine translation. Preliminary results in a semi supervised setting are also provided. This is solid work, presenting a reasonable first working system for unsupervised NMT, which had never been done before now. The work shares some similarities with He et al.’s NIPS 2016 paper on “Dual learning for MT,” but has more than enough new content to address the issues that arise with the fully unsupervised scenario. I feel that the paper’s abstract over claims to some extent. Also, the experimental section shows clearly that in getting the model to work at all, they have created a model with a very real ceiling on performance. Also, I found the paper’s notation and prose to be admirably clear; the paper was very easy to follow. Regarding over claiming, this is mostly an issue of stylistic preference, but this paper’s use of the term “breakthrough” in both the abstract and the conclusion grates a little. This is a solid first attempt at a new task, and it lays a strong foundation for others to build upon, but there is lots of room for improvement. Regarding the ceiling, the authors are very up front about this in Table 1, but it bears repeating here: a fully supervised model constrained in the same way as this unsupervised model does not perform very well at all.<BRK>The authors present a model for unsupervised NMT which requires no parallel corpora between the two languages of interest. While the results are interesting I find very few original ideas in this paper. Even sharing the encoder using cross lingual embeddings has been explored in the context of multilingual NER (please see https://arxiv.org/abs/1607.00198). Because of this I find the paper to be a bit lacking on the novelty quotient. Unsupervised MT in itself is not a new idea (again clearly acknowledged by the authors). 2) I am not very convinced about the idea of denoising. Specifically, I am not sure if it will work for arbitrary language pairs. In fact, I think there is a contradiction even in the way the authors write this. On one hand, they want to "learn the internal structure of the languages involved" and on the other hand they deliberately corrupt this structure by adding noise. I am not very sure that the analogy with autoencoders holds in this case. 4) This point is more of a clarification and perhaps due to my lack of understanding.<BRK>unsupervised neural machine translationThis is an interesting paper on unsupervised MT. It trains a standard architecture using:1) word embeddings in a shared embedding space, learned using a recent approach that works with only tens of bilingual word papers. The paper reads as preliminary and rushed, and I had difficulty answering some basic questions:* In Table (1), I’m slightly puzzled by why 5 is better than 6, and this may be because I’m confused about what 6 represents. But the text suggests that 6 is trained on much more than 100K parallel sentences; that is, it differs in at least two conditions (amount of parallel text and use of monolingual text). * I’m very confused by the comment on p. 8 that “the modifications introduced by our proposal are also limiting” to the “comparable supervised NMT system”. According to the paper, the architecture of the system is unchanged, so why would this be the case? This comment makes it seem like something else has been changed in the baseline, which in turn makes it somewhat hard to accept the results here. Comment:* The qualitative analysis is not really an analysis: it’s just a few cherry picked examples and some vague observations.
Accept (Poster). rating score: 8. rating score: 7. rating score: 7. <BRK>This paper extends the previous results on differentially private SGD to user level differentially private recurrent language models. The idea of training differentially private neural network is interesting and very important to the machine learning + differential privacy community. It adapts techniques from some previous work to address the difficulties in training language model and providing user level privacy.<BRK>Summary of the paper The authors propose to add 4 elements to the  FederatedAveraging  algorithm to provide a user level differential privacy guarantee. The impact of those 4 elements on the model a accuracy and privacy is then carefully analysed. 3.Is your method compatible with other optimizers, such as RMSprop or ADAM (which are commonly used to train RNNs)? 2.Strong experimental setup that analyses in details the proposed extensions. 3.Experiments performed on public datasets.<BRK>I found the paper to be lacking in that respect. Positive aspects of the paper: The paper is a very strong empirical paper, with experiments comparable to industrial scale. Both these contributions are important in the effectiveness of the overall algorithm. Concern: The paper seems to be focused on demonstrating the effectiveness of previous approaches to the setting of language models.
Accept (Poster). rating score: 7. rating score: 7. rating score: 6. <BRK>MAML (Finn+ 2017) is recast as a hierarchical Bayesian learning procedure. However, I recommend including the references to Mukhdalai 2017 and Sung 2017 in the footnote on TCML to strengthen the point more generically, and show that not just TCML but other non shallow architectures are not considered for comparison here. The role of the task prior and how it emerges from early stopping (ie a finite number of gradient descent steps) (sec 3.2) is original and technically non trivial, and is a contribution of this paper.<BRK>The paper reformulates the model agnostic meta learning algorithm (MAML) in terms of inference for parameters of a prior distribution in a hierarchical Bayesian model. This provides an interesting and, as far as I can tell, novel view on MAML. The paper uses this view to improve the MAML algorithm. The writing of the paper is excellent.<BRK>SummaryThe paper presents an interesting view on the recently proposed MAML formulation of meta learning (Finn et al). The toy example is quite difficult to interpret the first time around and does not provide any empirical insight into the converge of the proposed method (compared to e.g.MAML)             I do not think the empirical results provide enough evidence that it is a useful/robust method. Sec 6+7: The paper clearly states that it is not the aim to (generally) formulate the MAML as a HB.
Accept (Poster). rating score: 8. rating score: 6. rating score: 5. <BRK>Summary:This paper proposes a non recurrent model for reading comprehension which used only convolutions and attention. Proposed method performs better than existing models in SQuAD dataset while being much faster in training and inference. My Comments:The proposed model is convincing and the paper is well written. Is it because it does not achieve SOTA? The proposed data augmentation is a general one and it can be used to improve the performance of other models as well. I think it is ok to have some deterioration in the performance as you have a good speedup when compared to other models. Given the sufficient time for rebuttal, I am willing to increase my score if authors report results in an additional dataset in the revision.<BRK>This model does not use any recurrent operation but it is not per se simpler than a recurrent model. Furthermore, the authors proposed an interesting idea to augment additional training data by paraphrasing based on off the shelf neural machine translation. On SQuAD dataset, their results show some small improvements using the proposed augmentation technique. Overall, this is an interesting study on SQuAD dataset. I would like to see results on more datasets and more discussion on the data augmentation technique. Interesting information could be:  how is the performance of the NMT system?<BRK>Firstly, I suggest the authors rewrite the end of the introduction. Secondly, none of the above three ideas are well evaluated in terms of both speedup and RC performance, and I will comment in details as follows:(1) The CNN+self attention was mainly borrowing the idea from (Vaswani et al., 2017a) from NMT to RC. The novelty is limited but it is a good idea to speed up the RC models. Moreover, the whole model architecture is only evaluated on the SQuAD dataset. As a result, it is not convincing that the system design has good generalization. (3) I like the idea of data augmentation with paraphrasing. Currently, the improvement is only marginal, but there seems many other things to play with. I am looking forward to the test performance of this work on SQuAD.
Accept (Poster). rating score: 8. rating score: 7. rating score: 7. rating score: 6. <BRK>The paper is well written and proposes a simple, elegant, and well motivated solution for the memory bottleneck issue in graph neural networks. Nonetheless it would be good to include these results in the paper to avoid confusion. My initial concerns have been addressed and I can fully recommend acceptance of this paper.<BRK>The latter implies that GCNs can have a large memory footprint, making them impractical in certain cases. This is a very good paper. The ideas are solid, the writing is excellent and the results convincing. 2.The timing of GraphSAGE on Cora is bizarre. It is by far the smallest dataset. I understand that semi supervised learning is not the purpose of this paper, however matching previous results would dispel any concerns about setup/hyperparameter mismatch. Please add label axes to Figure 2; currently it is very hard to read.<BRK>The paper presents a novel view of GCN that interprets graph convolutions as integral transforms of embedding functions. This addresses the issue of lack of sample independence in training and allows for the use of Monte Carlo methods. 3) what loss g is used in experiments?<BRK>I think that batch training of GCN is an important problem and authors have proposed an interesting solution to this problem. However, I am not satisfied with how the probabilistic problem formulation was presented in the paper. I would appreciate if authors were more upfront about the challenges of the problem they formulated and limitations of their results. Hence, the distribution changes and subsequent nodes are dependent on previous ones. There is clearly a dependency and change of distribution. Hence it is not clear how to deal with large datasets (e.g., Reddit). Proposed approach is interesting and the direction of the work is important given recent popularity of the GCN.
Accept (Poster). rating score: 8. rating score: 6. rating score: 5. <BRK>The main contribution of this paper are:(a) a proposed extension to continuous stack model to allow multiple pop operation,(b) on a language model task, they demonstrate that their model gives better perplexity than comparable LSTM and attention model, and (c) on a syntactic task (non local subject verb agreement), again, they demonstrate better performance than comparable LSTM and attention model. Additionally, the paper provides a nice introduction to the topic and casts the current models into three categories   the sequential memory access, the random memory access and the stack memory access models. Their analysis in section (3.4) using the Venn diagram and illustrative figures in (3), (4) and (5) provide useful insight into the performance of the model.<BRK>The different memory models are evaluated on two standard language modeling tasks: PTB and WikiText 2, as well as on the verb number prediction dataset from Linzen et al (2016). Overall, I enjoy reading this paper: it is clearly written, and contains interesting analysis of different memory architecture for recurrent neural networks. One small negative aspect of the paper is that the substance might be a bit limited. In the experimental section, which I believe is the main contribution of the paper, I would have liked to see more "in depth" analysis of the different models. I found the experiments performed on the Linzen et al.(2016) dataset (Table 2) to be quite interesting, and would have liked more analysis like that. pros/cons:+ clear and easy to read+ interesting analysis  not very originalOverall, while not groundbreaking, this is a serious paper with interesting analysis.<BRK>The authors propose a new stack augmented recurrent neural network, which supports continuous push, stay and a variable number of pop operations at each time step. They thoroughly compare several typical neural language models (LSTM, LSTM+attention mechanism, etc.), and demonstrate the power of the stack baed recurrent neural network language model in the similar parameter scale with other models, and especially show the superiority when the long range dependencies are more complex in NLP area. However the corpora they choose to test the ideas, are PTB and Wikitext 2, they re quite small, so the variance of the estimate is high, similar conclusions might not be valid on large corpora such as 1B token benchmark corpus. Finally the authors should do some experiments on machine translation or speech recognition and see whether the model could get performance improvement.
Reject. rating score: 3. rating score: 4. rating score: 4. <BRK>This paper proposes using long term memory to solve combinatorial optimization problems with binary variables. The authors do not exhibit much knowledge of combinatorial optimization literature (as has been pointed out by other readers) and ignore a lot of previous work by the combinatorial optimization community.<BRK>Learning to solve combinatorial optimization problems using recurrent networks is a very interesting research topic. The architecture was described but not really motivated.<BRK>The entire problem is a supervised learning problem, and the memory controller is just a non differentiable decision within the neural network.
Reject. rating score: 4. rating score: 5. rating score: 5. <BRK>The proposed approach is interesting, but I feel that the experimental sectiondoes not serve to show its merits for several reasons. First, it does notdemonstrate increased scalability. CEGIS both selects fewer examples and has a shorter mediantime for complete synthesis. I feel that this issomething the authors should at least compare to in the empirical evaluation. There are typos andgrammatical mistakes throughout the paper.<BRK>Typos:  The paper needs a cleanup pass for grammar, typos, and remnants like "Figure blah shows our neural network architecture" on page 5. Overall: There s the start of an interesting idea here, but I don t think the quality is high enoughto warrant publication at this time. Experiments are only presented in one domain, and it has some peculiarities relative to more standard program synthesis tasks (e.g., it s tractable to enumerate all possible inputs). It seems that thisapproach only works due to the peculiarities of the formulation of the only task that is considered,in which the program maps a pixel location in 32x32 images to a binary value.<BRK>General purpose program synthesizers are powerful but often slow, so work that investigates means to speed them up is very much welcome—this paper included. For the most paper, the paper is clearly written, with each design decision justified and rigorously specified. The probability of what? The paper is not an application paper about inferring drawing programs from images; rather, it proposes a general purpose method for program synthesis example selection.
Accept (Poster). rating score: 7. rating score: 6. rating score: 6. <BRK>This paper considers the problem of one/few shot density estimation, using metalearning techniques that have been applied to one/few shot supervised learning. The paper could be improved significantly, e.g., by showing how to scale the attention based architecture to problems with more data or by designing an architecture specifically for use with MAML based inference. I have some concerns about the architectures and experiments presented in the paper.<BRK>This paper focuses on few shot learning with autoregressive density estimation. The model is interesting, however, several details are not clear, which  makes it harder to repeat the model and the experimental results. In the experiments, the author did not explain the meaning of "nats/dim" and how to compute it.<BRK>This paper focuses on the density estimation when the amount of data available for training is low. All in all, the paper has some interesting ideas. The paper presents two independent method. The first method is effectively a PixelCNN combined with an attention module. First, the novelty is rather limited. Why not use the same feature map for computing the attention and computing eq (7)? Last, it is unclear what is the connection between the first and the second model.
Reject. rating score: 4. rating score: 6. rating score: 6. <BRK>in the examples they give, it seems to be the sharpness of the images. While the authors are clear about the fact that this is a mismatch, I did not find it well motivated why it was  "the right thing to do" to match the training prior, given that the training prior is potentially not at all representative or relevant. For example, authors cover the usual story that random Gaussian examples  lie on a thin sphere shell in high d space, and thus interpolation of those examples will like on a thin shell of slightly less radius. Authors note that models may be trained for a certain distribution (e.g.uniform or Gaussian) but then "used" by interpolating or jittering known examples, which has a different distribution.<BRK>This paper is concerned with the mismatch between the input distribution used for training and interpolated input. There are a number of errors or inconsistencies in the paper, and the experiments results, compared to those given by SLERP, see rather weak. Below please see more detailed comments. In Introduction, the authors claim that "This is problematic, since the generator G was trained on a fixed prior and expects to see inputs with statistics consistent with that distribution."<BRK>The authors demonstrate experimentally a problem with the way common latent space operations such as linear interpolation are performed for GANs and VAEs. Secondly, the authors spend a lot of space on the precise derivation of the optimal transport map for the uniform distribution. All the optimal transport theory is nice, but it s helpful to know that this is simple to apply in practice. Train with a spherical uniform prior. This would also work for vicinity sampling. It would be interesting to plot more points around the midpoint to see the transition in more detail.
Reject. rating score: 5. rating score: 5. rating score: 7. <BRK>The authors propose a block sparsity pruning approach to compress RNNs. The other is to prune, but with a very specialized schedule as to the pruning and pruning weight, motivated by the work of Narang et al 2017 for non group sparsity. Also, the paper contains a good overview of related work in compression, and is not hiding anything. This is not well presented. What should an implementer be aware of in order to avoid that?<BRK>Thanks to the authors for their response. Though the paper presents an interesting approach, but it relies heavily on heuristics (such as those mentioned in the initial review) without a thorough investigation of scenarios in which this might not work. Scenarios like these need to be evaluated to understand the impact of the method proposed in this paper. Though the paper presents some interesting evaluations on the impact of block based sparsity in RNNs, some of the shortcomings of the paper seem to be :  The approach taken consists of several heuristics rather than following a more principled approach such as taking the maximum of the weights in a block to represent that block and stop pruning till 40% training has been achieved.<BRK>While previous works have proposed to 0 out weights, especially for the case of RNNs, in an unstructured way, the current paper proposes to 0 out weights blocks at a time via thresholding. The resulting networks are sparse, memory efficient and can be run more efficiently while resulting in minimal loss in accuracy when compared to networks learned with full density.
Accept (Poster). rating score: 7. rating score: 7. rating score: 7. <BRK>This paper presents a new reinforcement learning architecture called Reactor by combining various improvements indeep reinforcement learning algorithms and architectures into a single model. 2.Empirical comparisons are restricted to the Atari domain. The different modules are integratedwell and the empirical results are very promising.<BRK>The resulting algorithm is evaluated on the ATARI domain and shown to outperform other similar algorithms, both in terms of score and training time. Ablation studies are also performed to study the interest of the 4 contributions. I find the paper interesting.<BRK>This is confusing. The recent Rainbow paper finally established a long overdue clear benchmark on Atari. It is not clear to which extent this has an impact on the results. 3.3 mentions sequences of length 32 but 3.4 says length 33.
Accept (Poster). rating score: 7. rating score: 7. rating score: 6. <BRK>They leverage the non parametric nature of graph neural networks to show that their approach is capable of transferring policies to different robots more quickly than other approaches. The significance of this work is in its application of GNNs to a potentially practical problem in the robotics domain. Abstract: I take issue with the phrase "are significantly better than policies learned by other models", since this is not universally true. Pros:  The paper presents an interesting application of GNNs to the space of reinforcement learning and clearly show the benefits of their approach for the specific task of transfer learning. Cons:  The contributions of the paper should be more clearly stated (see comment above). The problem the authors have chosen to tackle is perhaps a bit "specific", since the performance of their approach is only really shown to exceed the performance on agents, like centipedes or snakes, which have this "modular" quality.<BRK>This paper proposes NerveNet to represent and learn structured policy for continuous control tasks. It shows that this structured controller can be easily transferred to different tasks or dramatically speed up the fine tuning of transfer. The idea to build structured policy is novel for continuous control tasks. It is an exciting direction since there are inherent structures that should be exploited in many control tasks, especially for locomotion. I believe that this paper could inspire many follow up work.<BRK>The policy is represented as a graph neural network over the agent s morphology graph and message passing is used to update individual actions per joint. The exposition is fairly clear and the method is well motivated. I see no issues with the mathematical correctness of the claims made in the paper. However, I do think using agent morphology is an under explored idea and one that is general, since we tend to have access to this structure in continuous control tasks for the time being. As a result, I believe this submission would be of interest to ICLR community.
Reject. rating score: 5. rating score: 6. rating score: 7. <BRK>The additional material does strengthen the paper. This paper would be much stronger if it surveyed a wider variety of gradient free optimization methods. However, the additional evaluations are not thorough. That s interesting and useful to know, but is still a relatively small contribution, making this paper borderline.<BRK>Quality: The paper studies an important problem given that public ML APIs are now becoming available. Clarity: The paper is clear and well written. Also, it was not clear what how \delta can be chosen in practice to increase the performance of the attack. Could the authors comment on that? Significance: The results in the paper are encouraging, but it is not clear whether the setting is realistic.<BRK>While there are some similarities with Chen et al.with respect to utilizing finite differences to estimate gradients, I believe the work is still valuable for its very thorough experimental verification, as well as the practicality of their methods. The authors may want to be more explicit about their claim in the Related Work section that the running time of their attack is “40x” less than that of Chen et al.While this is believable, there is no running time comparison in the body of the paper. They compare themselves to transfer black box attacks, where the adversary trains a proxy model and generates the adversarial sample by running a white box attack on that model.
Reject. rating score: 4. rating score: 7. rating score: 7. <BRK>This paper describes an approach to generating time sequences by learning state action values, where the state is the sequence generated so far, and the action is the choice of the next value. Local and global reward functions are learned from existing data sequences and then the Q function learned from a policy gradient. Unfortunately, this description is a little vague, because the paper s details are quite difficult to understand. Though the approach is interesting, and the experiments are promising, important explanation is missing or muddled. Perhaps most confusing is the loss function in equation 7, which is quite inadequately explained. This paper could be interesting, but substantial editing is needed before it is sufficient for publication.<BRK>This article is a follow up from recent publications (especially the one on "seqGAN" by Yu et al.@ AAAI 2017) which tends to assimilate Generative Adversarial Networks as an Inverse Reinforcement Learning task in order to obtain a better stability. The adversarial learning is replaced here by a combination of policy gradient and a learned reward function. If we except the introduction which is tainted with a few typos and English mistakes, the paper is clear and well written. Being not expert in GANs I found it pleasant to read and instructive.<BRK>This paper considers the problem of improving sequence generation by learning better metrics. Specifically, it focuses on addressing the exposure bias problem, where traditional methods such as SeqGAN uses GAN framework and reinforcement learning. Different from these work, this paper does not use GAN framework. Furthermore, it also introduces partial reward function that measures the quality of the subsequences of different lengths in the generated data. This is similar to the idea of hierarchical RL, which divide the problem into potential subtasks, which could alleviate the difficulty of reinforcement learning from sparse rewards. The idea of the paper is novel. However, there are a few points to be clarified. But the authors do not explain how the rewards (or action value functions) of different lengths are aggregated together to update the model using policy gradient. It is not clear why the future subsequences that do not contain y_{t+1} are ignored for estimating the action value function Q in (4) and (5). But it is not clear why specifically dropping the sequences that do not contain y_{t+1}.
Accept (Oral). rating score: 8. rating score: 7. rating score: 6. <BRK>This article clearly describes how they designed and actively trained 2 models for question reformulation and answer selection during question answering episodes. The reformulation component is trained using a policy gradient over a sequence to sequence model (original vs. reformulated questions). I was first skeptical by the use of this technique but as the authors mention in their paper, it seems that the sequence to sequence translation model generate sequence of words that enables the black box environment to find meaningful answers, even though the questions are not semantically correct.<BRK>This paper proposes active question answering via a reinforcement learning approach that can learn to rephrase the original questions in a way that can provide the best possible answers. Especially, step by step examples (for all alternative models) from input (original question) to question reformulations to output (answer/candidate answers) would be useful to understand how each module/variation is having an impact towards the best possible answer/ground truth.<BRK>For instance, each query can be viewed as different formulation of the same question and can be issued concurrently. The RL formulation essentially tries to mimic this process. For a human user, having a series of queries to search for the right answer is a natural process, but it s not natural for a computer program.
Invite to Workshop Track. rating score: 5. rating score: 5. rating score: 4. <BRK>The proposal is to replace the non linearity in half of the units in each layer with its "bipolar" version   one that is obtained by flipping the function on both axes. Nevertheless, it is interesting to note the result that the proposed approach converts a deep network that does not train into one which does in many cases.<BRK>For every neuron out of two, authors propose to preserve the negative inputs. Did you try bipolar activation function for gated recurrent neural networks for LSTM or GRU?<BRK>6.The theorems and proofs are rather preliminary, they may not necessarily have to be presented as theorems. It would be interesting to see the results of the new activation function on LSTM.