Reject. rating score: 5. rating score: 5. rating score: 5. rating score: 6. <BRK>The most simple and naive algorithm seems to provide similar speed ups to the much more complicated proposed T2PE; especially when considering the much larger improvement of the naive method (see Fig.11), none of the other proposed methods seem justified to me. This last point suggests that at least one of the following must be true:  * non trivial transfer is not as important as intuition would lead us to think;  * this benchmark suite does not provide a good testbed for assessing an HPO method s ability to transfer; or  * none of the non trivial proposed algorithms do a good job transferring and can therefore not argue against the previous point. Given this important contradiction, I must recommend a rejection. My recommendations would be to:  focus on creating a good benchmark suite (perhaps focus on a single or two domains as introduce many variants, instead of four domains with only two variants);  focus on vetting the surrogates in terms of their fidelity to the task they are meant to simulate; and  focus on demonstrating that accounting for the slight modifications in the subsequent HPO does indeed provide a benefit over the naive thing to do. Same for the violin plots. Figure 4, and indeed any mention of the two methods therein, can be entirely removed from the paper; other than to perhaps mention that they were tried and failed results in the appendix. A much more interesting replacement for that figure would be Figure 11.<BRK>This paper is addressing a problem which is quite relevant in practice, namely how to warmstart HP optimization after small changes have been done to the ML model. The paper does not elaborate on the motivations of these "developer changes". One could suspect many are attempts to modify/improve the HPO process itself, over the *same* model. And this is not really new, there is lots of prior work to help shaping search spaces, both by quantifying HP relevance or by learning search ranges. Say, a developer modifies the value range of an HP. What other motivation would there be than mistrust in the previous range, but no change of algorithm. In fact, the 8 benchmarks are all of that sort. These would call for more difficult transfer strategies. Given that the paper is mainly empirical, one would expect a more thorough and wider evaluation. It should be noted that best first is standard in HPO practice, this is the first thing one does for transfer. Their T2PE essentially works just as well, and a combination of the two works slightly better. While the paper categorizes types of modification, the empirical evaluation does not differentiate among them anymore. Why not also use GP BO?<BRK>The paper is motivated by the situation where a machine learning algorithm has development adjustment and we would like to reuse the tuning results of previous hyperparameter optimization. The paper calls it HT AA problem, which certainly is an interesting problem given software can get updated often. The paper proposes four simple baseline algorithms for the HT AA problem. For the empirical study, a set of eight benchmarks for basic HT AA problem are presented. To reach given objective values, T2PE can be 1.0–1.7x faster than TPE, and best first 1.2–2.6x faster comparing with old HPO. The pros of the paper include:1. Although the topic of the paper is not one of the most popular, some readers might find it interesting and can be benefited from it. The cons include:1. Need explain when the only optimize new and drop unimportant methods can be useful. If not useful as the experiments demonstrate, why propose them? 2.Look like TPE is the method that show speedup for the benchmarks.<BRK>The authors then propose a few strategies to transfer knowledge from previous HPO runs and evaluate them on a series of simulated benchmarks. The paper is very well written. RF and/or GP based HPO methods are extremely popular and would have been easy to integrate with the best first baseline. I ll give the paper a weak accept for now. Parameters used for the benchmark surrogate model should also be given (if defaults of Eggensperger are used, simply mention this). This threshold is likely to be met during the convergence phase of algorithms, and this phase appears noisier  (i.e.looking at how the phases of transition differ between the true benchmark and the RF surrogate benchmark differ in Eggensperger et al.2015).Have you given this any thought? The method you end up recommending only has its detailed performance shown in the appendix. This is perhaps due to the used of those split violin plots, which force you to display only two methods per plot. A more strictly meaningful metric here is accuracy (assuming there is only one dataset per benchmark).
Reject. rating score: 5. rating score: 6. rating score: 6. rating score: 6. <BRK>The paper extends (Al Shedivat et al., 2018) by directly conditioning the meta policy on a distribution of other agents  policies. In my opinion, the major contribution of this paper is a new multiagent meta learning theoretic framework that explicitly accounts  for the dynamics of all agents. The novelty is somewhat limited. However, this is subject to the two agent setting.<BRK>This paper studies meta learning in multi agent reinforcement learning. In experiments, most questions are answered by the two matrix games. Why not choose RoboSumo in Al Shedivat et al.(2018) as an experiment? The experimental results need to be more elaborated. **After rebuttal**The responses address my main concerns. But, I also agree with other reviewers that the novelty of this paper is somewhat limited.<BRK>This paper makes a full derivation of the meta learning gradient estimation for multi agent adaptation. While the theoretical part of the paper is clear and well explained, the experimental setup is missing a lot of details to be interpreted:  In each experiment, it seems (but never explicitly formulated) that "agent i" (agent 1, since all experiments are involving 2 players) is doing the meta learning algorithm (meta MAPG, meta PG or LOLA) while the other (agent 2) is a naive agent initialised with defective/cooperative policies.<BRK>This paper considers the problem of meta learning in a multi agent environment under the assumptions that:* the learning agent s policy evolves over time as a function of the other agent s actions* the other agents  policies evolve potentially using the learning agent s actions. Another question is whether the approach can be used successfully to tune the inner learning process, e.g.by incorporating the policy gradient step size and other hyper parameters into phi_0. The main contribution of this paper is to extend the ideas of Al Shedivat et al.in a way that exposes the other agent s learning dynamics to the policy optimization (as opposed to treating them as a non stationarity). Overall I think this is a solid paper, which would benefit significantly from more ambitious problems.
Reject. rating score: 2. rating score: 3. rating score: 5. rating score: 5. <BRK>The scheme of HD CNN (Yan et al., 2015) is similar to the latter, but is not represented in experimental comparisons.<BRK>The experimental results claim gains over some baselines, e.g., combined acc. This work doesn t answer the question of _why_ one would even want to predict both simultaneously well. As an example, from Figure 2, it s unclear what exactly changed from the LHS and RHS, how, and why it s meaningful; and in Figure 3, it s not obvious without work how the input, output and z term relate.<BRK>Minor:* The aspect ratio of Figures 2 and 3 need to be adjusted. Experiments of naturally adversarial examples are not motivated earlier in the introduction.<BRK>The baselines seem too weak to demonstrate the superiority of the proposed model. To sum up, the pros of this paper include:  a valuable research topic  a fancy model  clear experimental detailsThe cons include:  the model is not very general  baselines are too weak  the aspect ratio and resolution of figures seem improper Maybe the authors could consider representing restrictions among concepts in the vector space.
Reject. rating score: 5. rating score: 6. rating score: 7. rating score: 9. <BRK>** Summary **This paper proposes a new benchmark suite for black box optimization algorithms, which contains a mixture of existing tasks such as nevergrad functions and RL tasks from MuJoCo. The proposed meta algorithm is built on a set of “hand crafted selection rules” (in the authors’ own words). This does not seem like a meaningful contribution to the ICLR community, but may be interesting in other, more applied venues. 2) The rules based system works surprisingly well, which begs the question of whether a learned version could perform better, which may be of interest to the ICLR community. ** Weaknesses **1) I don’t think ICLR is the right venue for this work, which seems more engineering focused and may be more suited to an applied venue. 2) Most of the benchmarks in this suite are all included elsewhere already. This might be more useful for industrial applications, rather than ICLR. 4) Given that this paper is about benchmarking optimization algorithms, the presentation of the results is poor. It is almost impossible to read the plots, and there is no central table/comparison of the different methods. In fact, ARS didn’t say they were SOTA, it simply said that was what TRPO got as a baseline. *NeurIPS 2019*.<BRK>The paper proposes a benchmark suite for black box optimization that covers moredifferent types of problems than existing benchmarks. They derive an algorithmselection system for black box optimization from it and evaluate its performanceempirically, comparing to other black box optimization solvers. While the general motivation for a new benchmark suite is clear, the specificproperties the authors list are not. and one line reproducibility, but within the general context of thepaper, which seems to focus more on generalizability of results, this seemsunimportant. While the majority ofproperties listed in Table 1 are obviously important for black box optimization,it seems that the authors started from the properties that their benchmark suitehas. ABBO is listed asone of the main contributions of the paper and it should be explained in moredetail how it was created and how it works.<BRK>The paper proposes a benchmarking suite to overcome the problem low generalizability with black box optimization algorithm. This is a relevant contribution to the machine learning however there are several drawbacks which pushes back it s acceptance into ICLR. (a) The state of the art discussion was good but why benchmarking is crucial and how it is implemented in Optimsuite was very short in description. That was supposed to be the main highlight of the paper whereas discussion in that part was not clear at all. Specifically, more descriptions were needed in terms of what features to include/exclude in the benchmarking suite and how that helps in generalizability. (b) I know author(s) mentioned about interpretability as future work however I felt really challenging to understand the benchmarking suite especially when you have a combination of academic benchmarks and real world optimization problems together. I think that s a very significant challenge in implementing ABBO.<BRK>SUMMARYThe contribution of this work is two fold: it collects an extremely wide range of benchmark problems for black box optimization, and it proposes a new algorithm called ABBO. This statement is very general. I understand that there are space constraints, but in this form the presentation of the results is of limited value. The term "fitness" in evolutionary computation is no better. However, black box optimization is an extremely wide area, ranging (at least) from mathematical optimization over evolutionary computation all the way to machine learning. I really do not understand why the machine learning community keeps talking about losses (and sometimes regrets) when it comes to optimization. There is one more problematic statement in the abstract:"A single algorithm therefore performed best on these three important benchmarks, without any task specific parametrization." Well, this "single algorithm" is really an algorithm selection machine. RECOMMENDATIONOverall this is a very nice and valuable paper with two significant contributions. Technically it is "a single algorithm", but it is much more useful to think of ABBO as a selection and configuration method. [Side note: I very much like the use of Powell s algorithms for fine tuning of approximate solutions found with more robust methods.]
Reject. rating score: 4. rating score: 4. rating score: 5. rating score: 6. <BRK>Then, many gradient boosting modules learn to capture task specific information to fit diverse distributions more effectively. The experiments show that the proposed meta gradient boosting framework (with 5 gradient boosting modules) achieves better or competitive results compared to the baselines. Therefore, I am mainly interested in the model agnostic meta learning  line of work because of its potential in reinforcement learning where the ability to adapt to unseen scenarios is crucial. This paper presents a reasonable way to implement this idea. This includes the architecture of the weak learner, the number of gradient boosting modules, the updating strategy for the gradient boosting modules, the boosting rate, etc. The suggested baselines were merely briefly discussed but not added to the comparison.<BRK>The authors propose a meta gradient boosting framework that uses a base learner to learn shared information across tasks and gradient boosted modules to capture task specific information. This is supported numerically only in the case of regression experiments. However, why this is important to the meta learning problem remains unclear. In Section 3, the model is not explained clearly: 	What is a weak learner?<BRK>The paper proposed a method to incorporate neural networks into gradient boosting for meta learning with a limited number of samples in each task. 3.In terms of the application of the proposed algorithm, I think it is rather limited. 1.The paper claimed that the learning and updating strategy proposed in the method ensured a weak base learner. I have to say that I am not satisfied by the claim.<BRK>This study is presented clearly, and the core idea is interesting. However, the presented novelty is limited to a globally (for all tasks) and locally (task specific)  learning paradigm using a framework inspired by (Badirli et al., 2020). The authors have presented experimental results for both regression and classification setups, which are interesting. In my opinion, the paper has relatively high quality and can be interesting for the ICLR community. One question regarding Figure 4 (a), you have mentioned that adding more weak learners causes difficulties in capturing multi mode patterns, but from this figure, one can see that it is not completely true for the early epochs (before 400), comparing the two weak learners versus one weak learner case and it seems more like fluctuations, how would you explain it.
Reject. rating score: 5. rating score: 5. rating score: 6. rating score: 6. rating score: 7. <BRK>The paper identifies the gradient vanishing issue in the robustness of binary quantized networks. The novelty is limited, since it brings the temperature scaling approach, an existing method, to the problem of attacking binary quantized models. The paper writing is not constructed for easy understanding. Comments and questions:1. What’s the difference between these two?<BRK>This paper studies the robustness of quantized neural networks against adversarial attacks. In general, I think the idea is interesting. More discussions on this are welcomed. The authors claim that poor signal propagation such as gradient vanishing or gradient exploding should be a problem for adversarial attacks.<BRK>For linear loss functions, temperature scaling only linear rescales the gradients. The improvements are most significant for FGSM but FGSM is not recommended for the robustness evaluation. Given the limited technical novelty and small improvements for linear loss functions, my score remains unchanged. ###### SummaryThe paper studies the robustness of binary neural networks (BNNS), which at first look have higher robustness than full precious neural networks. ###### Pros:1) The paper studies the robustness of BNNs. To address this issue, they devise a novel low computational complexity technique. Can the authors explain these results?<BRK>**Update** : Since most of my issues have been addressed, I have changed my rating from 4 to 6Summary:This paper studies the robustness of quantized networks against gradient based adversarial attacks (for L2 and Linf norms), showing how quantized models suffer from gradient vanishing, giving a false sense of security via gradient masking. To circumvent this issue, the authors propose temperature scaling approaches that can overcome this masking, achieving near perfect perfect success in crafting adversarial inputs for these models.<BRK>This work starts by questioning the apparent robustness of quantized networks and demonstrates that such robustness is more so a failure of the attack algorithm in picking up the gradient signal. The authors address this by tuning a scalar multiplier applied to the network logits, which doesn’t modify the model’s decision boundary. Through analyzing the Jacobian, two approaches are proposed to determine the scalar $\beta$ without tuning it by performing the attack. 2) how about sweeping $\beta$ and plotting against adversarial accuracy? The proposed approaches for determining $\beta$ largely depend on the Jacobian; therefore, there should be more investigation on if scaling the Jacobian correctly is more important than getting good error signals from softmax.
Reject. rating score: 4. rating score: 5. rating score: 5. rating score: 6. <BRK><Summary>This paper addresses the problem of abstractive dialogue summarization. The proposed approach is evaluated on the SAMSum as one of the largest abstractive dialogue summarization benchmarks, on which it shows competitive performance over recent models. It proposes a two step coarse to fine approach for abstractive dialogue summarization; it first extracts category labels and key phrases from each dialogue turn, and then generates final summaries by controlling granularity. 3.It tests the proposed approach with four recent pre trained language models including DialoGPT, UniLM, PEGASUS and BART. (3) In summary, it is hard to find methodological novelty in the proposed method. 2.Experimental results are rather weak. This should be clarified in the draft.<BRK>The paper proposes CorDial for abstractive dialogue summarization. CorDial extends BART by generating an intermediate "summary draft" which provides weakly supervised signals and controling the length of the final summary. Although the use of the proposed summary draft would help solve the first challenge, it is hard to see any correlation between the other problems mentioned and the proposed solutions in the paper. This is especially the case for the controlling of the summary length. Why is this useful specifically for dialogue summarization?<BRK>This paper proposes CorDial, a new method for dialog summarization. CorDial employs BART xsum as its backbone model, which is a pre trained language model finetuned on XSUM summary dataset. Overall, the paper presents an interesting, practical recipe for dialog summarization. However, the method proposed here somewhat lacks in novelty, and some part of the paper is not clearly written. In 2.2, how are the intents annotated? It would be better to have some descriptions on model architectures in the experiment section.<BRK>This paper is well written and investigates dialogue summarization that has not received much attention. It proposes a new model called CORDIAL which can generate a summary draft followed by a final summary. The paper slightly overclaims its usefulness in the draft summary generation. (3) The method looks applicable to the generable summarization task. More results of this are also interesting.
Reject. rating score: 3. rating score: 5. rating score: 6. rating score: 6. <BRK>This work considers the graph deconvolution networks. It seems most parts are based on the previous works. However, it is not clear to me how to combine these two methods since they are very different. 2.In both Section 4.1 inverse of GCN and Section 4.2 wavelet de noising, the authors talked about two methods but didn’t explain why they are related to the proposed graph deconvolution networks. Firstly, why the authors believe this is a contribution to the graph deconvolutional networks. To my understanding, there is no technical contribution for this paper. It would be better to show this in the experimental parts. I didn’t understand how the image are converted into graph and how the inverse GCN and the proposed models are used to them. 4.The experimental results are very limited. The baseline methods are not well established. There are far better results on these datasets such as GIN. The authors claim that the unsupervised settings are used.<BRK>Graph Autoencoders with Deconvolutional NetworksThe paper proposes a graph deconvolutional network to reconstruct the original graph signal from smoothed node representations obtained by graph convolutional networks. The Visualization section of the paper shows an advantage of the proposed approach compared to other methods for reconstruction. Cons: The potential of a graph deconvolution operator are not fully exploited in the paper, mainly because of the considered tasks that do not require deconvolution because they are not intrinsically reconstruction tasks. Graph Autoencoders are usually applied to tasks of graph generation such as molecule design, where they are one of the most suited approaches. Considering the Ablation results, the improvement with respect to the ablation approaches seems marginal. Again, my opinion is that the considered tasks are not well suited for the proposed model. Minor:ALATION GCN  > ABLATION Rebuttal I acknowledge having checked the authors  response and the revised version of the manuscript, that has been improved since the first revision. Authors did not answer to my request for more details about the hyper parameter selection procedure that has been adopted.<BRK>The main contribution of this paper is that the authors design a graph deconvolutional network that combines inverse filters in the spectral domain and de noising layers in the wavelet domain. Many experiments are done and many previous methods are compared to show the effectiveness of the proposed networks. My reason for the rating:Before the experiments part, I think authors clearly present the methods and related works. However, I have some questions for the experimental setups that they present at the end of the paper, which somehow confuses me. Pros:1.In general, this paper is well written and the presentation is clear and easy to understand for the method parts. 2.The results well demonstrate the proposed network as reported. $10$ and $15$  seem approximation numbers to the reviewer. Although other reviewers may still question the novelty and contributions, I think I would stay with my score according to the comparison and good results reported in the paper.<BRK>However, there are some issuses should be solved. Pro:The proposed method is very novel. The writing and organization of this paper is good. The authors provide a clear and detailed describtion for the proposed method. This should be added as a baseline to show that encoder decoder framework is better. In addition, the authors should also mention how many pooling layers are used in encoder decoder framework. Overall: I vote to accept this paper considering the importance of the task and the novely method proposed.
Reject. rating score: 2. rating score: 2. rating score: 4. <BRK>E.g.It isn t clear why the works that specify reward a certain way in Doroudi et al.(2019) show a disadvantage to the current approach, the works in table 4 don t specify why the current approach is an advance over their contributions. This relates to my question about RoboTutor and this work. Per the weaknesses in the review above, I recommend the paper for rejection.<BRK>Questions for the authors:* Are there structural reasons why Q learning (e.g.DQN) cannot be applied in this setting? This section indeed state how the current work is different from past work, but it does not motivate the differences. If others were successful with different methods, why change them for this paper? There’s a learned policy in here, but what is the learned representation you want to highlight for this specific venue on representation learning (ICLR)?<BRK>But how is that helpful? Finally on the empirical side, the authors reference the work of Shen et al.who applied DQN to this ITS problem, but the authors do not provide an empirical comparison to this DQN based approach. In summary, this is a good idea and the results are showing promise, but the paper is not ready for publication in its current form.
Reject. rating score: 6. rating score: 6. rating score: 6. rating score: 7. <BRK>The authors have conducted a range of experiments to validate the performance of the proposed method. However, there are some concerns as follows. It also requires careful hyperparameter setting as shown in the implementation details. Besides, the title of the paper is also misleading since it is about learning with noisy labels instead of contrastive learning.<BRK>The authors of the paper propose to use the contrastive loss, the mixup prototypical loss, and a reconstruction loss to regularize the learned representation in order to achieve robustness under various kinds of noise like label noise, out of distribution input, and input corruption. A major complaint I have regarding this submission is the lack of novelty. While the empirical results are encouraging, the proposed method is merely a combination of various previously proposed methods. However, no ablation study was done with respect to the effect of the mixup on the prototypical loss. How would regular prototypical loss perform without mixup in the context of these experiments? While I still do think that the novelty of the paper is a little bit lacking, I think the experiments are carefully conducted and the empirical results seem to be encouraging.<BRK>#######################################################################Summary: The paper proposes noise robust contrastive learning to combat label noise, out of distribution input and input corruption simultaneously. Training samples with confident pseudo labels are selected for supervised learning to clean both label noise and out of distribution noise. The effectiveness of the proposed method has been evaluated on multiple simulated and real world noisy datasets. The design is reasonable and interesting. 2.As the key contribution of this paper, mixup prototypical contrastive loss combines mixup technique and prototypical contrastive loss. I am concerning about the fairness in the experimental comparison.<BRK>This paper proposes a new methodology for noisy robust learning by augmenting the simlr methodology with a Mixup style augmentation and noise cleaning. The paper proposes a new methodology for training models in the presence of label noise. In addition, to the standard augmentation, the method also uses a Mixup style augmentation for the strongly augmented images: that is the projection of a randomly selected image is combined (using a convex combination) with the original image and then another contrastive loss is enforced for the class prototyle. The quality of the writing is high and the paper presents a plausible combination of several strong methods. The weakness of the paper is that it is only measured on cifar 10/100 and these datasest are often not very representative of real noisy datasets with label noise of complicated structure. Another weakness is that this paper presents a relatively complex approach composed of 4 different methods, each of them are well studied and with very limited ablation analyses. It is plausible that the combined approach should do well, still it has limited novelty as it is the straightforward combination of four known approaches also the paper does not give a disciplined overview and study of the contributions of the different components.
Reject. rating score: 3. rating score: 3. rating score: 7. rating score: 8. <BRK>The study is based on segmentation and classification where the input image is placed on a background canvas. Update after Rebuttal:I do appreciate the argument given in appendix A.1 about the relative position that changes with cropping and that boundary information may get lost. Reason for score:This submission discusses an important and relevant topic. However, the issue that the feature maps between the shortcut and residual connection do not align with the bilinear interpolation seems to be much harder take into account for the network. To me it seems an actual example on the ImageNet classification (i.e.Table 2) that shows that the degradation in performance is not due to the resampling/misalignment of the feature maps in the ResNet would be very important. This is the same major concern I share with AR4 even after the rebuttal. Strengths:Padding is omi present in current convolutional and "fully" convolutional architectures but its effect is not studied well. This is not clearly motivated in the current form of the manuscript as far as I can tell. Unclear how no padding is conducted in ResNet. Section 3 indicates that a standard ResNet 18 is modified to a no padding version by removing the padding from the convolutions and using bilinear resize to match the sizes. In Table 3 the white padding seems to contain stronger location information than the black padding which contradicts the main thesis of the submission that says black padding contains more location information. However, Table 3 uses different grid sizes for black and white padding. Are the different grid sizes the reason for this discrepancy?<BRK>The paper seeks to understand how different padding modes and canvas colors affect the performance of a convolutional neural network in classification and semantic segmentation tasks. Cons: dubious choices for network design and experiments, counterintuitive results that are not properlyexplained or analyzed. If there was a strong effect it would be an interesting finding indeed. The added experiment in A.1 does not address the question of misaligning the features between skip branch and convolution branch. In other circumstances, we would see that each corner in Fig.8(c, d) is different. This is because Fig.8(b) will contain four identical blobs after the convolution, and Fig.8(c) is by definition a central crop of Fig.8(b).Thus, the bits we see in Fig.8(c) are different quadrants of this blob, and they are in fact all different. The figure makes it look like there are only small differences by oversaturating the colors to the point that only the footprint of the convolution can be seen. The experiment also misses the larger point: The different architectures may cause differences in the results that exceed the effects of the padding itself. The paper claims to measure the latter, but I believe both AR2 and I are concerned that the paper may be measuring the architectural effects instead. I m extremely concerned about the paper s decision to use bilinear interpolation to match the resolution with skip connection in a ResNet architecture, when padding is not used. The composite will thus not have the features from the two branches line up with each other  anymore. What Horizontal and Gaussian mean in Table 1 should be explained. The effect seems inverse to Figure 1.<BRK>This paper studies the effect of padding on the Convolutional Neural Network. The authors try to answer the following questions: 1) what type of padding provides the most position information, 2) does the background value affects model accuracy when processing a patch on a canvas, 3) which part of the image suffers the most from the boundary effect, and 4) whether the position information provided by padding improves or degrades model performance. In order to answer these questions, the authors design multiple tasks and perform extensive experiments. First of all, an implicit hypothesis in the experiments is that the boundary effect is generic to the architecture. Second, as an empirical study with newly defined tasks, it is important to demonstrate whether the results are significant. Third, some of the arguments are based on qualitative results. Finally, given that a learned representation can be distributed, I don t think the dimensionality estimation (Table 3) alone provides useful information.<BRK>The topic of the paper is interesting and important. The padding strategy seems to be a small but largely overlooked aspect in CNN learning. While many papers attempt to reduce/improve the effect of position in images. This paper gives a different perspective on the padding patterns that essentially causes these position sensitivity. The Hypothesis raised in this paper are important factors for training networks for various tasks. And the paper gives useful conclusions based on experiments. The current analysis are based on CIFAR, it would be more convincing if these hypothesis are further validated on larger datasets such as imagenet or coco (for segmentation). A minor suggestion, it would be more informative if the author in related work/analysis compare with the positioning methods used in transformers.
Reject. rating score: 4. rating score: 5. rating score: 6. rating score: 7. <BRK>To achieve this, the author provide a lower bound for the worst case performance, though the lower bound does not take the uncertainty of the finite samples into account. In addition, the algorithm 1 proposed by authors requires to calculate a model discrepancy between $p_{w}$ and other environments $p_{i} \sim P$, which is impractical to estimate by samples if the discrepancy is total variation distance. To achieve this, the authors assumes that the transition is lipschitz, with the requirement of tunning lipschitz constant. The authors only evaluate on three environments, which I think is not enough, can the authors add more mujoco benchmarks? Overall I think there is a gap between the methodology presented in the paper and the final practical algorithm, and the  lower bound presented in the paper does not take the uncertainty caused by the finite samples into account, which will not give guidance to design empirical algorithms since the variance of the mc return of the policy is large.<BRK>They derive a lower bound for the worst case performance, which comprises the average performance, policy change, and the statistical distance between the worst and average case environments. The algorithm is backed by the theoretical guarantee of monotonic worst case performance improvement. cons:  The assumption that the transition dynamics model is L Lipschitz with respect to the environment parameter seems to be strong. Some of the experimental results are not convincing. In Figure 1, what does the shaded area stand for? It would be good if notation presents the dependence on the policy of $p_w$, e.g.$p_w^\pi$.<BRK>Motivated by the domain transfer problem in RL where policies are trained on simulators that may not reflect perfectly the reality, this paper propose a new policy optimization algorithm named MRPO that is expected to be robust to changes in the environment s dynamic. This domain randomization model formally equivalent to a single (continuous) MDP where the the environment s dynamic is parametrized by the initial state distribution (for instance by enriching the MDP states by the p parameter). The experiments study both the 10% word case returns and the avarge returns. A few experiments on free to use environments would improve the reproducibility of the paper.<BRK>This paper theoretically derives a lower bound for the worst case performance of a given policy over all environment, and in practical, the proposed method, monotonic robust policy optimization(MRPO) carries out a two step optimization to imporve the lower bound such as to maximize the averaged and worst case policy perfomance. The Theorem.1 makes the connections between the averaged and the worst case performance, such that maximizing the worst case performance can be solved by maximizing the averaged performance problem with some trajectories from environments with both poor and good enough performance. 1.For Lemma 1: The conclusion is based on the assumption that the the worst case $\rho(\pi|p_w)   \max_p \rho(\pi|\rho)$ is bounded (Proof A.1). However, such equation does not strictly holds without bounded reward function. The author should stated the condition. Similarly, if the updated policy changes very little, can we make $\pi_{new} \pi_{old}$ ?
Reject. rating score: 2. rating score: 3. rating score: 4. rating score: 4. <BRK>Thus, despite studying an interesting topic, I think this work needs to be significantly revised and extended before publication. First of all, I couldn t find any such claims in that paper. 4) Several of the submission s other theoretical claims are quite sloppy as well. In conclusion, as the original review mentioned, I believe that the presented "loop penalty" idea may well have conceptual merit, but I encourage you to think more carefully how you "sell" it, because so far neither the original submission nor the rebuttal present convincing arguments that it is better than the alternatives either theoretically or empirically.<BRK>With regards the analysis, I believe that the following is problematic:* A large part of the analysis focuses on uniformity. The claim immediately follows and the proof in the paper is unnecessary. I would encourage the authors to spend more time on the practical questions of how to estimate the loop penalty in non tabular domains.<BRK>### Review SummaryI like the idea and think it could develop into a good paper. The first contribution (introduction of success rate and association of success rate with undiscounted return) is obvious, and could be presented in a much more compact form. Actually, as presented, I believe the statement and proof of Theorem 1 are incorrect given that in the definition of “success rate” the authors require the agent to be successful with at most T steps. That said, I have a couple technical issues (they should be easy enough to fix), take minor issue with the current presentation (again, this should be easy to fix), and importantly, would want to see this method tested against the the right baselines (easy to fix, but require some work) before I can recommend acceptance.<BRK>The presented proofs have some nice arguments why the value issue happens in this setting. However, there are some issues with correctness and coverage of related work that I discuss below. If so, how do you choose the threshold, it would seem to be highly dependent on the environment dynamics. The “AT” or “ANY” queries seem to consider exactly the issue discussed in this paper and do not require discounting.
Reject. rating score: 5. rating score: 5. rating score: 5. rating score: 6. <BRK>Given an image captured by a stereo camera, SBEVNet performs an inverse perspective mapping (IPM) to obtain an initial feature volume, which is further processed to generate the BEV layout. The system is trained end to end in a supervised learning setup. ## Minor remarksThe following remarks have had no impact on my assessment of the paper, and as such I don t expect the authors to respond to these. Concurrent approaches such as [F] can be cited and discussed. ## WeaknessesI see a few major and a number of other minor concerns that impact my perception of this paper. I m hoping the discussion period helps address some of these, and I m open to revising my score in light of evidence contrary to the following claims. **Problem setup** It is unclear from reading the paper and supplementary material if the problem setup is infact "amodal" layout estimation (i.e., if scene points outside of the camera view are predicted in the BEV layout). Approaches like (Schulter et al., 2016) and (Mani et al., 2020) operate in this "amodal" setup, while others such as PseudoLidar [C] and (Lu et al., 2019) only predict points that are visible in the input image. I also find it a tad weird (and unexplained) that the performance of various baselines do not seem to follow a set pattern/trend across the CARLA and KITTI datasets. In Appendix A.2, the authors seem to indicate that they used a very different process to train MonoLayout (i.e., using random images from the train set as opposed to using OpenStreetMap and/or adversarial training).<BRK>The paper proposes an end to end network for layout estimation from stereo images. The stereo estimate is used to project image features into a birds eye view representation which is processed using a U net which predicts a semantic scene layout. The approach is evaluated on the KITTI and Carla generated datasets. However, the paper provides reasonable baselines by modifying existing networks for this task* Avoids the need for an intermediate representation (i.e.point cloud) by directly mapping features from the disparity volume into birds eye view coordinates* Plots in the appendix are interestingWeaknesses:* While the task itself is new, closely related forms of the problem have been studied. In the synthetic CARLA dataset, where a ground plane can be accurately computed, there seems to be a large advantage of using the IPM module.<BRK>The paper proposed to estimate the semantic layout in the bird eye s view from a pair of stereo images. The main novelty/contribution lies in how to organize and exploit the information from the stereo images. The performance was evaluated on the KITTI and CARLA datasets. Given a pair of stereo images, there are various options to exploit the image information, where this paper provides a framework by exploiting the stereo information in the bird eye s view. A principled question is what is the real superiority of estimating the layout in the bird eye s view. In section 3.4.4, the paper claimed that "We pass the concatenated stereo BEV feature map and IPM BEV feature map to a U Net (Ronneberger et al., 2015) network to generate the semantic map C". However, the loss evaluation applies to the IPM features and Stereo features separately, namely, $\mathcal{C}_i^{IPM}$ and $\mathcal{C}_i^{Stereo}$. If two estimations are made as the network output, which one will be used for performance evaluation? The paper conducted experiments on the KITTI and CARLA dataset. The current title did reflect these properties.<BRK>Interesting problem but the paper can be improvedThis work aims to directly estimate the world layout in front of the vehicle from a pair of stereo cameras. Instead, it warps the cost volume features to the bird eye view (BEV) and do semantic segmentation from BEV using U Net. After rebuttal:I still think this work has a interesting task setup, though it indeed has many faults (after reading the responses and other reviewers):1. It seems that IPM is not really useful in practice. 2.It is also not sufficient to large occlusion, and thus there is no explanation for its advatange over `estimating accurate depth`3. 5.It is still not clear how to emsemble several models (with different trained weights) in this work. I only see it is effective on the synthetic dataset CARLA but not on KITTI. It seems quite slow if the resolution is 512x288 (for CARLA) or 640x256 (for KITTI). The current version is not clear and there are typos.
Reject. rating score: 3. rating score: 4. rating score: 4. rating score: 6. <BRK>The authors propose an interpretation of feed forward neural networks and recursive neural networks as chain graphs (CGs). Since you assume no undirected connection between variables in the same layer, then all chain component in your chain graph actually contain a single variable. Furthermore, I would appreciate if the authors could relate their approach to the SFNN model, and what differs from that interpretation. If you follow the CG interpretation from Koller and Friedman then you assume NNs are LWF CGs. I believe the CG interpretation is not justified, nor required to derive the results you present in this paper.<BRK>Furthermore, this chain graph interpretation has been used to propose a new approach (architecture), which is a partially collapsed feed forward. This further establishes to interpret feed forward as an approximate probabilistic inference with using linear approximations. Some concrete examples are shown to be analyzed  based on the chain graph formulation. The overall context (analysis) seems straightforward to interpret the neural networks with chain graphs, but it is hard to achieve some meaningful information from this new interpretation to improve the current neural network models in terms of learning procedure or optimization. I fully agree the future works (open questions) in the conclusion and discussion section that this work still needs more investigations although this paper is a good initiative work.<BRK>The authors look for ways to frame neural networks in terms of probabilistic models. The theme is relevant, however the novel ideas that are in the paper seem to have relatively small significance. The translation from neural networks to chain graphs is not surprising; actually if I understand the translation we basically get a Bayesian network out of the directed acyclic graph encoding the connections in a neural network (I must say that the translation could be presented in a more didactic fashion.In any case, given this translation, the authors are able to frame some techniques from neural networks as techniques from chain graphs. On top of this, I find it troublesome that the authors "prove" results by saying things like "approximately linear" and "approximately" this and that. Also the last paragraph of Section 3.2 is very hard to parse (the main point there is not clear at all).<BRK>Original comments:In this paper, the authors provide new interpretation of neural networks via chain graphs, which can be used as a new theoretical framework to understand the behavior of neural networks. Pros.1.Although both PMG and Neural networks are based on graphs, very little research has been proposed to bring the two fields together. I would be particular interested in the third question. Overall, I think this is an interesting paper.
Reject. rating score: 5. rating score: 5. rating score: 6. rating score: 6. <BRK>This paper presents a theoretical scenario where point wise measure of adversarial robustness falls short in comparing model robustness, then conduct experiments to show that robustness curve is a more meaningful evaluation metric from a global perspective. I mainly agree with the authors on the argument that point wise measurement of robustness may be insufficient in explaining model robustness. Cons:  The robustness results presented in Table 1 seems far below the state of the art robustness. For instance, in the last row (\epsilon 8/255), the robust test error of AT is 0.92, which is much higher than the reported statistics in (Madry et al., 2018). Similar architecture is used for the robustness curves in Figures 3 and 4. Is there a way to define the perturbation strength for different input locations based on your computed inter class statistics? Check if you are using the correct file.<BRK>The problem that they identify is that if you only evaluate adversarial accuracy at some numbers of threshold, you might conclude that some models (and the method that was used to train them) are more robust than others while it would be incorrect at other thresholds. While I agree that you are going to get more information if you compute a full robustness curve than if you sample it at a bunch of points, I m not convinced that it is worth the effort. One thing that I would recommend the authors is to make clearer the distinction between robustness curves as they described them (based on finding the closest adversarial example) vs. plotting on a robustness curve the pointwise measures and interpolating through them. I don t understand how the robustness curves are generated for the L infinity case? If PGD is used to find adversarial examples, it s not likely to be the "closest" adversarial examples that is going to be found, in all likelihood it s going to be one that matches the epsilon given as input to the PGD attack (due to the projection)? There is nothing that is even encouraging the sample to be close to the input beyond the constraints used for projection. As a result, it s hard to decouple the robustness curve from the attack that it used internally. Minor Notes & Typos:  To improve the look of the paper, it should be possible to include manual linebreaks in the title so that it s not broken in every line.<BRK>The author argues that robustness for a specific epsilon may not be enough and suggests robustness curves as an alternative. I think in general this paper provides some interesting empirical studies. My detailed comments are as follows. 1.The overfitting of specific epsilon value is expected but interesting to see and I think this is one of the main reasons why a robustness curve is necessary. 2.The authors suggest a robustness curve as an evaluation metric. However, I haven t seen any works on improving the robustness for all epsilon values globally. One possibility is that there is a trade off between small epsilon performance and large epsilon performance, similar to the trade off between robustness and accuracy (epsilon 0 performance). 3.(Minor) Please refrain from only using color to distinguish curves in figures as it may not be friendly to readers with color blindness.<BRK>The only concern the reviewer has is the contribution compared to the previous work who proposed the robustness curve (C. Gopfert et al.2020).it seems this paper s contribution is highly based on  the proposal of robustness curve, and providing more explanations and discussions. The experiments and its discussion are strong proofs to support that the robustness curve should be the better criterion. The authors introduced the recently proposed robustness curves to provide a global perspective including the scale as a way to distinguish small and large perturbations. (3) The authors released code to reproduce all the experiments for the current popular frameworks.
Accept (Poster). rating score: 8. rating score: 5. rating score: 4. rating score: 3. <BRK>This paper provides an method for computing an upper bound for the spectral norm of the linear transformation induced by a convolutional layer. An upper bound was first introduced as a heuristic by Miyato et al, but they did not prove any bounds. The authors use the exact computation of singular values of a convolutional layer in Sedghi, Gupta and Long to prove that the Miyato heuristic is indeed an upper bound. They further generalize Miyato s method to find 3 additional heuristics, all of which are proved to be upper bounds, and then show empirically that the minimum of these bounds gives a much tighter bound, often very close to the exact value. The authors show that their bounds can be used for regularization, and produce results similar to the spectral projection method in Sedghi et al on CIFAR 10. I would like Section 4.3 to be explained a bit better, so that I can understand the significance of the results more clearly.<BRK>Summary:In this paper, an upper bound of the spectral norm of Jacobian of a convolutional layer is presented. The proposed bound enables us to compute the gradient much faster than the exact methods. The problem is well motivated with a sufficient literature survey, which will stimulate the ICLR communityCons:C1. The terminology of "difficult/easy" for gradient computation is unclearC3. The technical contributions are relatively marginalDetailed comments:Overall, the paper is clearly written, and easy to follow the content. My concerns:  [C1] One step power method proposed by Miyato+ (2017; 2018) is not compared in the experiments. Since the proposed method is a generalization of Miyato s method, it looks like a natural way to make comparisons, e.g.in Section 4.2. Without comparison, the significance of this study would be not fully validated   if Miyato s method achieves almost the same performance as the proposed method, the practical value would be degraded. It seems there is no technical definition of them. However, 1 does not contribute to the practical value, because when we use the spectral norm as a regularizer, the constant factor is absorbed in hyperparameter beta in (1).<BRK>## SummaryThis paper propose to study the Lipschitz constant of convolutional layers and to give an easy to compute and differentiable upper bound. All these bounds would be much easier to derive/read if using tensor notation. Basically, what is done here seems to simply be computing the spectral norms of tensor slices unfolded along different dimensions. I would update it to `Fantastic four: differentiable upper bound on the Lipschitz constant of convolutional layers`. The derivations for the 4 bounds are very straightforward from the results from `Sedghi et al.(2019)` as it is simply different unfolding of the hollow tensor that is obtained with the Jacobian of the convolution layer in the Fourier domain. The Jacobian is given in the preceding paper so the technical contribution is not very large. My main concern with this paper is that the proposed evaluation is not sufficient to show the benefit of the proposed method IMO. Indeed, computing all the singular value will be much more expensive than computing a bound on the largest one. As the complexity of the bound is at least linear in the total shape of the filter, the running time should be mutliply by 64, which would give similar runtime as the one in Ryu et al (2019). This should be trimmmed down to the minimum. Section D: The complex part here is not to compute the gradient of the convolution (which is simply computed with the correlation) but the shape of the gradient of the spectral norm of the Jacobian.<BRK>## Summary of the paperThis paper proposes an improved method to calculate an upper bound of the spectral norm of the 2d convolutional layer. Experiments on MNIST and CIFAR10 show that regularizing spectral norms using the proposed method improves generalization. Additionally, they showed that their method could extend and improve an existing method for computing certified robustness accuracy. ## Review### SummaryThe paper is clearly written and easy to understand. However, the paper misses important literature and also seems to misunderstand existing methods. And thus, you can directly take the gradient of the spectral norm, and major deep learning frameworks efficiently compute its gradients. 2.Missing literature> this is the first work that derives a provable bound on the spectral norm of convolution filter as a constant factor (dependant on filter sizes, but not filter weights) times the heuristic of Miyato et al.(2018)It s at least mentioned in Cisse et al.(2017).Additionally, efficient and differentiable computation of linear operators appeared in the literature repeatedly (Tsuzuku et al.2018, Scaman & Virmaux 2018). They are not limited to the 2d convolutional layers, but any linear layers implemented using deep learning frameworks. Tsuzuku et al.2018 also applied the method to improve certified robustness accuracy. I was not convinced that the proposed method has more advantages over these methods. 3.Weak experimentsThe experimental results seem noisy, and also the gain by the proposed regularizer looks marginal. Additionally, I am not convinced that the tighter bound results in better generalization. The proposed method is very similar to Yoshida & Miyato 2017, and it is not intuitive that the proposed method works better than it. Lipschitz regularity of deep neural networks: analysis and efficient estimation.
Reject. rating score: 4. rating score: 5. rating score: 5. <BRK>This paper proposes a genetic programming approach for programming by example systems, where the genetic search is seeded with fragments that are predicted by a neural network to be relevant to the input output examples. The fragments above a threshold are used for seeding the genetic search and candidates that do not include them are penalized during the search. In contrast, many neural models for synthesis work entirely off of randomly sampled programs. The term "find rates" is used without defining. In Section 4.2, it is said that "all programs with a 90% find rate or better ..." are used. What is the find rate of a program and how does it relate to the find rate of the search procedures? What is the relation between the problems whose fragments are used for training the classifier and the problems used in this experiment? From the point of view of baselines, there are two types of baselines missing. One is a neural method for straightforward program generation from examples; any suitable one from the cited papers can be used. Another is a non neural selection method for fragments; a simple mining method can be applied to the data from the correct programs from the initial genetic search. Can you spell out how the distance D for identifying repulsors is computed, for instance, if some edit distance is used then which one?<BRK>At train time, the system learns in a self supervised manner by bootstrapping off previously discovered programs, augmenting them to produce a large corpus of similar problems, and then training the network to predict lines of code present in this augmented data set, similar to DeepCoder. Overall the method is sound, but I have two objections which caused me to lean toward reject:+ First, the experiments seem small scale, and even on these small domains the absolute performance gain is modest (for one dimensional array manipulation, their method solves 55% vs 38% with vanilla genetic programming, as measured on 40 synthesis problems). This is at odds with the claim that the "approache is scalable to the search space of Turing complete languages"   why not try on a standard benchmark, such as SyGuS? The promise of the proposed method hinges on its raw performance as well as its general applicability, and at present the experiments do not highlight these two factors. How does your method compare? To be clear, not every paper has to beat standard benchmarks   new good ideas that are carefully investigated are also good, and Experiments 1 2 do a great job of showing experimentally the different elements of the approach in action. But there is enough existing work (see "missing citations") in the conceptual neighborhood of this submission that I really think they should compare on some standard ish benchmarks, because the approach on its own seems too similar to prior works.<BRK>The main idea of the paper is using neural networks to provide "starting hints" to a program synthesizer that is based on genetic programming. As written, Section 3 presents a lot of technical detail and high level motivations are hard to notice and appreciate until one reads the entire paper. In contrast, So & Oh, whose tasks the authors re use in this work, start with task/program examples and give a much more formal presentation of the entire synthesis system. However, there s no comparison with the DeepCoder approach, either on this paper s DSL/dataset, or on DeepCoder s. I appreciate the authors  argument that the two DSLs significantly differ in expressiveness. However, most programs in this dataset have <10 lines, and comparison with enumerative or neural guided synthesis approaches would be both feasible and informative for them. As such, the paper is definitely valuable for the GP program synthesis community, but less valuable to the ICLR neural program synthesis community at large. I also some questions regarding the validity of experimental setup and suggestions on the presentation clarity (see below). ### Evaluation questionsThe experimental setup is a bit unorthodox. The authors present individual components of their approach simultaneously as both (a) ablation experiments and (b) stages of their end to end pipeline. Without it, different experiments now clearly map to different research questions:  Experiment 3 is the "main" end to end performance/generalization evaluation. Experiment 2 is the evaluation of classification accuracy of that individual component. First, the inclusion of Experiment 1 tasks into the final measurement is odd   they are indirectly part of the "training" data in this setup because they influence the selection of useful fragments. It s unclear which of the newly solved problems are "training" and which are "testing" without inspecting Tables 26 27 line by line. If Experiment 1 evaluated every possible valid fragment from the ground truth program, how could the approach find a fragment that s better than the best one?
Reject. rating score: 4. rating score: 5. rating score: 6. rating score: 7. <BRK>This paper addresses the well known phenomenon of model collapse in generative adversarial networks (GANs). In particular, this paper identifies catastrophic forgetting of the discriminator as a potential source of mode collapse and proposes a multi discriminator framework called DMAT as a solution. 3.Discriminators are dynamically added as needed, allowing the network to scale indefinitely to new modes. This also saves some computation at the beginning, when a large number of discriminators may not be necessary. 4.A fairly wide range of experiments were run, and adding DMAT appears to lead to stronger results (higher Inception Score, lower Frechet Inception Distance) across a both an assortment of models and datasets. As a result, I recommend rejection. Cons:1.I have concerns over the novelty of this work. The primary motivation of the paper (GAN mode collapse being due to catastrophic forgetting, leading to generator oscillations) is a known phenomenon and has been thoroughly explored before in [1,2]. In particular, [2] also takes a continual learning inspired approach (regularization, as opposed to expansion) to propose an easy to add method, and visuals very similar to this submission’s Figure 4 can be found in both [1] and [2]. There are only limited comparisons with these methods in the Experiments, but Table 2 for example does demonstrate that D2GAN (which has 2 discriminators) also manages to cover all modes. As the authors do acknowledge that the hypothesis of "catastrophic forgetting leads to GAN oscillations" was introduced in other work, then the primary contribution here is replacing one continual learning method for preventing catastrophic forgetting in GAN discriminators with another, and despite the rebuttal, I still find the proposed solution to be rather ad hoc.<BRK>The paper proposes and approach to mitigate mode collapse in arbitrary GANs. This is relies on the assumption that mode collapse relates to catastrophic forgetting in the GAN discriminator. They propose a heuristic to avoid mode collapse of the discriminators by adding additional ones during training. The results show improvements with respect to baseline regarding mode covering and FID. Strengths:  the synthetic experiment to quantify mode collapse looks elegant,  the authors report systemic improvement in FID over baselines. Overall, this seems and interesting research direction, but evidence for the main claim of the paper is lacking. In the current state I tend to recommend rejection.<BRK>  Summary  This paper studies specifically the mode collapse problem of GAN. It hypothesizes that mode collapse in training GAN is closely related to catastrophic forgetting, and proposes a method to improve the catastrophic forgetting issue to mitigate the mode collapse. *Forgetting collapse interplay experiments*D6 – In the proposed method, there are multiple discriminators. Although the idea of multiple adversarial has been discussed in many existing works (some related works are missing in the paper, see below), this paper is different at posing a new scheme and specifically use it to address catastrophic forgetting. Are all features extracted from all discriminators chosen for the quality evaluation in forgetting collapse interplay experiments? However, the hypothesis of a relation between mode collapse and catastrophic forgetting, which was discussed in [c] (missing also), is not new. S2 – The experiments show significant improvements over baseline models. Weakness  W1 – Some parts of the algorithms are not clear. Are the authors training GAN models with hinge losses but not discussed in the paper?<BRK>The contributions of this paper revolve around the simple but interesting idea that mode collapse and missing modes in GANs are due (at least in part) by catastrophic forgetting in the discriminator, as the discriminator (and the generator) see a non stationary training distributions due to their interaction. The authors provide toy experiments clearly illustrating this phenomenon and then propose an algorithm (DMAT) based on multiple discriminators taking charge for different parts of the input space (different modes). They also have nice experiments suggesting a strong link between mode collapse and catastrophic forgetting, thus also reinforcing the main claim. * several clarity issues:    alpha_t in Alg. with a maximum of one real sample per mode : how would you know, on real data, since the modes are not known a priori? hence this thinking only works for toy data where the modes are known.
Reject. rating score: 4. rating score: 4. rating score: 6. <BRK>+++Pros.The idea learning embeddings via angular similarity is interesting and important for exact hashing retrievals; and the design of experiments on different domains to validate the proposed method is worth encouraging. There are some technical minors or typos, such as the binary cross entropy loss in Eq.~(5) (should be “+\beta(1 y_{ij})log(……)”). The authors concluded that “The learned representations show superior performance in the exact hashing retrieval setting”. But the results in Table 2 does not support it. Besides, the authors mentioned that … a “word2hashes” model which is novel to the best of the authors’ knowledge …, and I do not agree with such statements. Based on the above pros and cons, I think this paper is interesting, but the contribution is limited; thus, I would make a REJECT recommendation. Need careful writing and presentation.<BRK>They modify the approach Deep Hashing Networks (Zhu et al., 2016) with a new loss function. Rather than use a sigmoid based loss function, the authors argue that a loss function based on angular similarity and SimHash would be better. They show improvements over related methods. Overall, I found this paper to be incremental compared to previous work, such as (Zhu et al., 2016). The authors do improve over related methods in the experiments. However it is not clear if this is due to the choice of loss function or the use of negative mining.<BRK>########Pros########  The paper is well written and easy to read  The authors have explained the background, motivation and prior work quite comprehensively  The idea of learning embeddings such that nearest neighbors have the same hash is interesting. In practice, for IR, can this method support a ranking of the items based on query similarity? For example, if for query q, documents A, B, C are relevant in that order (most relevant to least). Experiments don t appear to be necessarily a fair comparison that can show the merits of this approach conclusively. Q4.Section 4.3, Table 3: LSE has a substantially lesser number of clusters, making it difficult to isolate the merits of the method vs. outcome that is a result of just lesser clusters. E.g.Recall can be better just by returning a larger cluster, which is probably the case if the number of clusters are lesser.
Reject. rating score: 4. rating score: 5. rating score: 5. rating score: 6. <BRK>This paper proposed to learn a regression model using "skewed data", which is defined as the subset of training samples with true target above certain threshold. The predictive distribution was forced to match the true target distribution p(y) through an adversarial network. Experimental results on synthetic benchmark data showed the proposed approach performed better than naively applying regression model on the skewed data. In practice, it s more likely that values within certain ranges were over (or under) sampled, leading to mismatch between the empirical target distribution and the true target distribution. This strong assumption makes the proposed approach not applicable to most real world problems. It s not clear how p(y) was estimated from the labeled dataset, where only samples with true target above certain threshold are available.<BRK>##########################################################################Pros: (1) In practice, the distribution of reported data could be different from the true distribution. (2) This paper provides comprehensive experiments using four datasets. ##########################################################################Cons:(1) The proposed approach is based on the assumption that labeled data are skewed and unlabeled data follow the assumed true distribution. Also, in the experiments given in Section 4, all the datasets are not originally skewed, but they are artificially changed to skewed datasets. It would be convincing to provide a real example in which the assumption of the proposed approach holds. (3) It would be ideal if some theoretical results are presented to support the proposed approach. ### Updates:I thank the authors for their response. Some of my concerns are addressed. However, unfortunately, I still think the assumption of the proposed approach is too strong to have broad applications.<BRK>This paper proposed a semi supervised learning approach to improve the regression model trained on output skewed data. The proposed model that combines an AAE that generates the output distribution, and an adversarial model that enforces the distribution of the predicted output to resemble the true distribution of the output. The paper is in general well written. (1)	 The paper assumes that the training data are often highly skewed (intentionally) but the true distribution of the output can be easily estimated or obtained. Then why should we use the proposed approach? Can you provide a concrete application where getting more labeled examples is very costly yet estimating the label distribution is easy and cheap? (2)	The paper did not explain where the target distribution in the adversarial part of AAE comes from. How does it connect to using the information of the unlabeled data or the true output distribution? Can you incorporate a real world application, e.g., drug design (mentioned by the paper), to showcase the performance and usefulness of the proposed approach?<BRK>Overall, I would say that future versions of the paper could look into a task and dataset that are close to their domain of applicability and where they can contribute an increase in performance. The training data does not represent the true real world distribution. In order to overcome this obstacle, the work presents the following contributions:  A semi supervised method that uses unlabeled data (that follows the real distribution) in order to improve predictions. Although I have a concern with the AAE regularization. Although, I believe the scenario is not explained in a way that is simple enough to understand instantly. Maybe there could be some improvements in the abstract and the introduction. Also I noticed that the introduction repeats the abstract, there could be some more precise explanations instead of this. For example (Kim et al.2020)?Issues:  Section 3.2: "To be a useful feature for Rpost, latent vectors should be arranged in a similar way to p(y), which possesses information about how the labeled dataset is skewed." p(y) is a distribution of labels right? What happens when it is very different from the true label distribution? My biggest concern, and a question to other reviewers and the authors: Are there any other comparable methods to the proposed method, or other methods that solve this task in comparable scenarios?
Reject. rating score: 5. rating score: 6. rating score: 6. rating score: 7. <BRK>Evaluation:Overall, the paper provides a thorough theoretical analysis to prove that the approach allows to recover topic posterior information. While the theoretical analysis is interesting, I believe the presentation, clarity and writing of the paper should be improved. I suggest to provide a stronger motivation for the theoretical analysis with an outlook for future work where this analysis could be useful. This is a strong assumption. However, the approach in this paper does not recover any topics specifically. The experimental baselines are weak.<BRK>The author proved the contrastive estimation reveals topic posterior information given the topic modeling assumptions. And in experiments, linear models can get relatively good performance. I like the idea of  using topic models as a way to represent the document level information, but it is disappointed to see that the proposed method doesn t provide as good performance as the simply averaging word embeddings. Strong points:1) Detailed proof and detailed experiments, compared with many baseline models. 2) Using topic modeling as a tool to understand representation learning is interesting.<BRK>SummaryThis paper presents a new contrastive learning algorithm for document representation. The authors also show the learned function can be represented by combining the topic posterior distribution and topic likelihood distribution. Experiments show that the suggested learning algorithm can identify hidden topics from a synthetic dataset. I think it is hard to say that this is a fair comparison. The authors explain why they do not show the results of them in the Related work section. But I think we can run the same experiments in Arora et al., 2019 with the suggested algorithm representation. Can we recover topics that are word distributions from the representation?<BRK>This submission considers contrastive learning approach to representation learning under topic modeling assumptions. It proves that the proposed procedure can recover a representation of documents that reveals their underlying topic posterior information in case of linear models. Overall, the submission is a solide pice of work and well written. This line of research could be interesting to the theoretical topic modelling community.
Reject. rating score: 4. rating score: 5. rating score: 5. rating score: 5. <BRK>The motivation of this paper that controls the graph generation is interesting. 2.Most recent papers on graph generation are listed. I am worried that the generator might learn the data distribution directly from the shadow information rather than through the adversarial training. In other word, I am curious whether it is due to the additional information or the shadow information that the generator could generate the synthesized random walks as genuine as the real random walks. 2.This paper is similar to NetGAN in terms of both generator and discriminator, and thus, the novelty of this paper is limited. 3.There is a trade off between the controllability of the graph generation and the quality of the synthesized graph. When we try to control the graph generation to achieve the desired structure, then the quality of the synthesized graph diminishes, because the desired graph might be very different from the original graph. However, it seems that in the experiment, the synthesized graph generated by proposed method is the closest to the statistics of the original graph, which is not well justified.<BRK>The paper is clearly written, ideas are well presented, and it is easy to follow the pipeline of the method. One highlight of the paper is the shadow network, which can carry multi modal auxiliary information. Not only semantic features but also graph statistics can be considered during the generating process, and explainable graphs are constructed accordingly. As a minor comment, I would suggest the authors giving more details on the proposed Shadow Cast part and fewer details on the generator and discriminator part, as they are quite similar to the NetGAN setting. In the proposed model, synthetic shadow walks are directly translated from real sampled shadow walks. Since both input and output sequences carry the homogeneous information, it could be easy and less challenging for the seq2seq model to mimic the original shadow walk. These baselines may indeed not be comparable in the conditional generation task. In addition, there are other conditional graph generative methods [1, 2] proposed in recent years. 3.Following the second concern, the survey of the related work is not comprehensive. There are other lines of works trying to generate the node and edge attributes of the target graph conditioning on input graphs. To conclude, the contribution of the paper seems to be limited if the shadow network is constructed only on node class. It would be more comprehensive to leverage other node properties to build shadow network and generate graphs.<BRK>This paper differs from NetGAN in that the sampling of the walks is conditioned on certain "shadow" information outside the graph structure (e.g., community labels). One weakness of this paper is that the presentation is rather verbose but still not to the point. For example, the authors heavily use several concepts that are vague and whose meanings are not clear until deep into the paper. These concepts include "explaining graph generation", "controlling the generative process", and "shadow". I also doubt if it is worthwhile to state two future works in the problem formulation section. For example, following Figure 3(b), to generate the effect of "legal (red) internal surge", how does the proposed method compare with naively adding random edges to the "legal" cluster? Standard errors are important. What the proposed method can control appears to be exogenous of the graph structure (e.g., node attributes). Hence, it appears unlikely that one can use the graph structure as the controllable condition. For example, I am skeptical that the proposed method can generate a graph that leans toward a desired degree distribution.<BRK>The paper proposes a novel problem   explainable graph generation,  and proposed a controllable generative model to mimic network structures as well as network properties. In particular, the proposed model, SHADOWCAST, is developed based on the recent proposed graph generative model NetGAN, and exploits the auxiliary information to guide the generation process. However, it seems the proposed algorithm is more about the generation of attributed networks (Please correct me here if I am wrong). If yes, I believe there are a series of attributed graph generators in the literature, ranging from random graph models to deep graph generative models. [Literature Review] I am not sure whether it is a standard way for ICLR that the paper can be presented without a section of related work. It would largely help reviewers with less background knowledge to appreciate your work and shed light on the connection between your proposed model to the existing methods. Moreover, on Page 5, it seems the box showing the generative process is directly copied from NetGAN, which is not very professional. It is interesting to me why the proposed method can outperform NetGAN with a large margin. Overall, I still lean positive to this paper for its novel problem and strong results. But, without clear my concerns above.
Reject. rating score: 3. rating score: 6. rating score: 6. rating score: 6. <BRK>The idea is to learn the adjacency graph of the model together with the weights of the networks. The theoretical foundation of this work is unclear. In general, I really welcome this type of work: non conventional, experimental and quite radical in terms of approach. However, unfortunately, the authors do not provide a convincing description of their approach. Could you please provide the confidence intervals for all the results you presented?<BRK>The main difference seems to be that in the proposed method the graph is dynamic (i.e., it depends on the input instances), instead  in (Yuan et al., 2018) the graph is learned but fixed for all the input samples. In the experimental results, I would have expected a deeper ablation study on the importance of the dynamic graph, since this is the main contribution of the paper. Overall, the paper is well written and easy to follow. My only serious concern is the degree of novelty with respect to (Yuan et al., 2020), which was published at ECCV 2020.<BRK>The experiments show consistent improvement in image classification (ImageNet) and object detection (COCO). Pro:  I enjoy the interpretation of the “weighted edges” as a dynamic architecture, able not only to address a more general class of models but also to adapt to each input accordingly (using a second order approach). (maybe a regularization term that encourages that would be useful for computational efficiency). I agree with the other reviewers that the novelty of this paper is quite limited. However, the idea of using a dynamic, learned graph from a general large search space of models is interesting, provides good empirical results on both image classification and object detection and the authors provide ablation for the new components that motivate the paper.<BRK>Note that this is also a problem for [1] but, in that case, the sparse connectivity mitigates the issue. While this idea could be seen as a relatively small modification of [1], it is still very interesting and the results prove its effectiveness in commonly used tasks. I also have a concern about memory requirements that I would like the authors to address.
Reject. rating score: 5. rating score: 6. rating score: 7. <BRK>The paper proposes a graph network (called HGN), aiming to better leverage commonsense knowledge graphs (KGs) to solve commonsense question answering and reasoning tasks, by jointly generating representations for new triples from KGs, determining relevance of the triples, and learning graph model parameters. Pros:   Overall, the paper is easy to follow, although there are a number of typos or grammatical errors that need to be fixed. The overall idea is clear. Compared to existing methods (e.g., those PG models proposed (Wang et al.2020)), the novelty of the current submission is rather limited the proposed model of jointly generating new triples and learning (pruning) the graph structure with the network parameters is an interesting, but a pretty incremental idea. The empirical comparison to previous work (e.g., Wang et al.2020) needs to be clearer to help understand the empirical advantages of the proposed models. More comments:   The paper uses much space to discuss neural symbolic approaches. Given the vague benefit of doing so, it may be better to use the limited space to focus more on establishing the contributions w.r.t.existing models; e.g., more details about (Wang et al., 2020) can be provided and compared to in both methodological and experimental analyses.<BRK>The paper proposes a question answering model that is augmented with a common sense knowledge graph (KG). How many (what proportions) were scored 0, 1 or 2 for both the graphs? (b) Current methods over retrieves facts (edges) from the KG leading to a lot of unrelated facts that potentially makes reasoning noisier and harder. The paper first retrieves all possible facts from the KG connecting entities in the question and answer. It would be nice to cite those work as well. Recommendation:  In light of the current weaknesses of the paper, I am giving it a score of 5 and I look forward to the discussion. 11/22 I am deciding to keep the same scores as before. Lastly, an entropy term is added to the objective function to encourage more peakiness (and sparsity). The model is tested on three common sense QA benchmarks and on three of them they beat the baselines albeit only around 1 2%. I think the paper still lacks motivation wrt the GPT2 model generating missing edges. Strengths:* Developing models that can use symbolic external knowledge present in common sense KGs and also overcome the sparsity in KG is important and this model is a step in that direction* The paper achieves a little improvement in performance in all three datasets and ablation experiments are helpful in understanding the results* The paper is clearly written and it was easy to follow for the most partWeaknesses & clarifying questions for the authors:* My biggest complain of the paper in its current form is that several modeling choices were not motivated at all. For example, generating edges between nodes using GPT 2 language model is fairly non standard. However, the paper lacks any motivation on why this is the right approach to generate facts which are not captured in a KG. * Following up on the previous point, there could have been several other modeling choices. * The GPT 2 modeling choice was also moved to the appendix and I think it should definitely be moved to the main section of the paper as it is one of the core technical contribution of the paper. I would be curious to know how this simple model worked and if it didn’t why was the case.<BRK>  Summary  In this paper, the authors propose a new approach towards incorporating knowledge graphs (KG) into commonsense QA frameworks. KGs are helpful for adding structured "world" information, which neural symbolic architectures can leverage to do commonsense reasoning, e.g., "what is the expensive resource in printing on paper?" (paper).In such architectures, however, the authors argue that KG quality is a large impediment (e.g., missing or incorrect edges, distracting nodes, etc). Experiments are conducted on a number of commonsense reasoning tasks with multiple KG sources, and compared to relevant baselines. Justification for Score  This paper is well written and well evaluated. The proposed method is also relatively simple and intuitively motivated. Perhaps not a game changer for commonsense QA, but still a reasonable contribution that I would recommend for acceptance. Strengths  + The paper is clear and well written. + The experimental section is strong. For a number of the results the variance is large compared to the relative difference it would helpful to also include tests of significance for these improvements. On OpenbookQA the model significantly underperforms T5 based models. I still think the work is good, and can warrant acceptance. However, I still find the empirical results to be only moderate at best (though I appreciated the authors  rebuttal and significance testing). I am keeping my score the same.
Reject. rating score: 3. rating score: 4. rating score: 5. rating score: 5. <BRK>This paper proposes Expectigrad, which is a new optimizer for nonconvex optimization. Therefore, I keep my score. In terms of the strengths of the paper, I found the idea of using normalization $n_t$ depending on the sparsity interesting. For example, AdamNC method in Reddi et al., 2019 already shows that with such an update for second moment estimate, one can obtain convergence. This algorithm is also analyzed by Chen et al., 2018 in the nonconvex setting, which seems to be missed by the authors. In terms of the analysis, the authors make the simplifying assumptions of $\beta 0$ in page 5 and ignoring sparsity in page 16, which basically converts the algorithm to Adagrad. The authors use the expected gradient norm as the optimality measure, which is the standard one for nonconvex optimization. Regret is used for online optimization, which is more general than stochastic optimization which is considered in this paper. Moreover, it seems that the improvement of Expectigrad compared to SOTA is not significant. Empirical merit of the method is also marginal and some important comparisons are missing.<BRK>The paper proposes Expectigrad, which replaces the exponential moving average in Adam and RMSProp with averaging over all historical gradients. Could you please elaborate more on why average over all historical data can be better than EMA? Imagine the case when I first have very small gradients (but non zero so n_t also adds up) for a long time, and suddenly I ve got a gradient that is much larger. With this example, I can easily construct some scenarios that lead to divergence. Does that mean you are comparing different algorithms with the same hyperparameters instead of doing grid search for each algorithm separately? 3.In the algorithm description and equation (1), it does not make sense to have a square of a vector: what does it mean? Is that element wise square or two norm of the vector? It is also weird to have the scalar n_t the same boldface as the other vectors.<BRK>Summary: The paper proposes a new adaptive method for stochastic optimization with convergence guarantees. Strength:+ The paper proposes an adaptive stochastic optimization scheme called Expectigrad for training deep neural networks. Expectigrad is shown to converge (to a stationary point) at the same rate as other recently introduced adaptive methods such as Adam, and Yogi. To motivate their algorithm, the paper studies the Reddi problem introduced in Reddi et al., and show that there exists an averaging scheme to prevent divergence in that "class" of problems. Perhaps some discussion on this in the paper would make it clear to the reader. Third, in section 4.1 (above Theorem 1), I m not sure what "topological" changes are? Indeed, this is an important distinction that needs to be clarified. Do the training curves provided have confidence? The proposed method is a modification of existing averaging schemes to schedule learning rate, but more experimental evaluations are required to determine the benefits of algorithm.<BRK>This avoids normalizing historical gradients by future gradients. Pros:(1) The idea of normalizing on the fly is interesting because it is more sensitive when the gradient dynamic is non stationary. (2) The algorithm performs well on the examples considered in the paper. Concerns:(1) Theorem 1 assumes $\beta   0$ "for simplicity". On the Reddi problem, the gradient is not sparse, and thus Expectigrad with $\beta   0$ is equivalent to AdaGrad, if I am not mistaken. In order to justify the algorithm, it is necessary to prove a similar result for the case $\beta > 0$, since it is set to be $0.9$ in the experiments. Reddi et al.(2018) have results of this kind (which only requires $\beta < \sqrt{\beta_2}$ where $\beta_2$ is the EMA constant of the second moment). (2) The paper claims (in the bottom of page 6) that Expectigrad has strictly better complexity than Yogi.
Reject. rating score: 4. rating score: 6. rating score: 6. rating score: 7. <BRK>I still like the work and believe it is promising. Also, it seems there is still experimental bias in the evaluation (on malware), which hinders a bit the opportunity to assess the actual effectiveness of the authors  approach. I would really encourage the authors to reason about the points raised in the review in a more principled way. ## CommentsI found the paper well written and motivated. [1] https://s2lab.kcl.ac.uk/projects/intriguing/ (IEEE S&P 2020)[2] https://www.usenix.org/system/files/sec19fall_pendlebury_prepub.pdf (USENIX Sec 2019)## Edit after discussionI thank the authors for the clarification.<BRK>Experiments on malware data and image data show that it leads to the best trade off between classification accuracy and robustness. Shouldn t it be the case that different techniques be used to manipulate the inputs for testing? Assuming such perfect knowledge of the adversaries techniques and using it to train a robust classifier seems unrealistic. How do the proposed techniques apply in more realistic situations where we would not know the adversary s techniques? Writing wise, the paper is an easy read.<BRK>I have some concerns about the current version, which are detailed below. 1.For image classification task against combined Linfinity pixel perturbation and hue shift attack, it appears that the authors use the same attack hyperparameters in training and testing. Will the advantage in the proposed method over adversarial training still exist for different train/test hyperparameter settings? The numerical results (especially on image classification) should be expanded to better support the robustness claim2.<BRK>The readers might think that $y$ depends on $x$. In fact, here $y$ should be fixed for all $x\in C_i$, isn t it? My main concern is about the clarity of the paper, especially its proofs. I think this problem is practical and important to the adversarial learning community. Why the authors use the extra step? This statement is a little confusing.
Accept (Poster). rating score: 7. rating score: 7. rating score: 7. rating score: 6. <BRK>Summary: This paper proposes a new algorithm for solving neural ODEs. Strengths:+ Identifies a nice connection between invertibility and memory efficiency. Beyond neural ODEs, this could enable use of larger models where invertible networks are useful (e.g.normalizing flows)+ The theoretical analysis of stability is useful to build intuitionConcerns / weaknesses:  Most experiments in the paper use damping_factor $  \eta   1$. It is obvious that $m 1$ of these are unnecessary for gradient computation since they don t affect $z(T)$. In the same vein, it seems that the adjoint method needs to do a separate solve of the reverse ODE because of loss of information.<BRK>In this paper, the authors propose a method that is both reverse accurate and has low memory cost. The ALF solver already exists, so the main contribution of the paper is simply to apply the ALF solver to neural ODEs. There are several typos in the script, so I think it would be a good idea for the authors to read through the script again to fix those. The experimental results are quite impressive. In several places, the authors write O(N_f + 1) which instead should be O(N_f)  The authors often write “constant memory cost with respect to integration time”. This allows the authors to run their model on large scale datasets like ImageNet which was not previously possible with most neural ODE methods.<BRK>Summary: This paper presents a memory efficient asynchronous leapfrog integrator for numerically solving neural ODEs, referred to as MALI. The authors also give a rigorous theoretical analysis of MALI, and also discuss a "damped" version with an increased stability region. The method is proven as accurate as the standard numerical ODE solvers. I would be happy to see an experimental evaluation of the "A stability". Typo in the title of section B.3.2.<BRK>1) SummaryThe manuscript proposes a reversible integration scheme for approximately estimating the gradient of neural ordinary differential equations. + The reversibility property of the solver leads to a memory footprint that does not depend on integration time. 3) Concerns  The concept of Neural ODE models, their scope and their expected usefulness should be better motivated. It does not seem that the paper makes code and data available to the public. What is the intuition behind the "continuous depth" idea in this scenario?
Accept (Poster). rating score: 7. rating score: 5. rating score: 5. <BRK>As a result of being fully offline, the agent can no longer explore in the new task at test time, but instead receives randomly sampled transitions from the new task, from which it must infer the task. Results indicate that in this outperforms PEARL as well as multi task offline RL with BCQ. Pros:The problem this paper tackles is important, and is formulated in a practical way. Additionally, the proposed negative power variant of the metric loss does seem to better encourage separation of tasks with different task labels, and seems like a good tool for metric learning in general. Lastly, the experiments do compare against the relevant baselines to the best of my knowledge, and on tasks similar to those used in prior works.<BRK>They demonstrate experiments where FOCAL outperforms baselines like PEARL. My main concern is the limited novelty of the proposed solution and some missing ablations (c.f weaknesses). I encourage the authors to incorporate feedback for the reviewers and work towards a stronger submission. This significantly reduces the scope of the algorithm. The novelty of the proposed algorithm is also quite limited it builds on the framework of PEARL by using (i) distance based metrics for task inference, which have been studied in the multi task literature (ii) offline policy learning, where it uses off the shelf baselines from the offlineRL literature. Miscellaneous:  The paper has several minor grammatical errors which could be corrected in the final draft.<BRK>Thanks for the feedback. The main contribution of the paper comes from combining the context based approach with metric learning. The method of using metric learning has been well motivated by toy experiments on the context embedding. Ablations: The authors compare their method (FOCAL) to Batch PEARL while describing the Batch PEARL as “a vanilla version of FOCAL without behavior regularization or distance metric learning.” It would be interesting to compare FOCAL and Batch PEARL both with the behavior regularization to observe the effect of metric learning in isolation. The only difference is that the setting is offline and using metric learning to encode the context instead of KL divergence. I wonder if there is an experiment with a different transition depending on the task.
Reject. rating score: 3. rating score: 5. rating score: 5. rating score: 6. <BRK>Proof of concept experiments are included to allow discussion of how providing this classification of MDPs to an agent that needs to learn how to perform some externally designed task allows it to learn to complete the task faster and discussion of how the agent s observations affect its learning and behaviour. This is a very important aspect of the project, so it is critical to explain in plain language what you did. **UPDATE after author rebuttal**The authors have made substantial improvements to the paper over the rebuttal period, but there is still work to be done to make this paper communicate as clearly as it should for publication. For the sake of reproducibility and understanding, it is necessary to know what algorithm was used for clustering and what algorithm was used to adapt the agent s behaviour to increase the amount of intrinsic reward received. Helping the reader to think about the elements of $H$ as random variables and to think of causal factors as determining which environment you end up in would aid understanding of Definition 1. **Recommendation**I am recommending that this paper be rejected because it is not written with sufficient clarity to understand the research project being reported.<BRK>#### **Summary**This paper develops an intrinsic reward to help identify factors of variation within a family of MDPs. I do however have strong reservations about how the paper is presented and as such cannot recommend it for publication as currently written. The reviewers all agree that this is a very valuable research direction and the authors have begun an extremely interesting line of inquiry. However, there is still a good deal left to be completed prior to this paper being fully suitable for publication. #### **Strengths**  The paper appropriately criticizes prior approaches for not learning a disentangled embedding over the contextual parameters of variation within an environment. This follows from the discussion I had with the authors about the general scoping and applicability of the proposed causal curiosity mechanism. There are no specific details provided for the tasks used in the experiments.<BRK>The hidden state variables are interpreted as causal factors that control important aspects of the environment  dynamics. Under this interpretation, the paper advocates for the use of a reward that encourages learned skills that exercise individual components of the hidden state. The paper claims the learned skills are qualitatively meaningful, and that they enable agents to solve downstream problems without any additional training. The paper presented several interesting ideas with the potential for high impact in robotics and reinforcement learning. I have concerns about the empirical evaluation, which I detail in several followup questions: 1. 11.Over how many trials was data collected for the results in figures 3 and 5?<BRK>## SummaryThe authors introduce *causal curiosity*, an intrinsic reward that allows an agent to discover causal factors in an environment. .., h_k\\}$ is called a set of $\epsilon$−causal factors if for every $h_i \\in H$, there exists a sequence of actions that clusters the state trajectories into two sets $S$ and $S_0$ such that..." I can t figure out what the $h_i$ are and how "a sequence of actions" could exist for each of them. In Section 4.2, the two proposed downstream task (`lifting` and `travel`) seem very close to the behaviors learned during the self supervised phase. However, I do not find this very convincing. Overall, this strikes me as an interesting direction to pursue, but the experiments have not fully convinced me that the proposed approach of "causal curiosity" (ie.finding a sequence of actions that maximizes the inter cluster distance between trajectories and minimizes the intra cluster distance) is the right approach. That being said, I believe the paper is interesting enough to be accepted, especially if reviewers with more background on the topic judge it valuable.
Accept (Poster). rating score: 7. rating score: 7. rating score: 6. rating score: 6. <BRK>Be positive and generous. The patterns are either generated using simple “compositional rules” (lines, loops or trees) or sampled from a distribution that has almost identical 0th, 1st and 2nd order statistics as the ones generated through the “compositional rules”. The authors go on to show supporting evidence that most likely, humans are using a compositional inductive bias when trying to uncover the hidden pattern, whereas the meta learning algorithms do not. This conclusion is arrived at using the relative performance of both humans and the algorithm on the cases where the pattern was generated using the two different schemes. Please include some examples. I think it’s a possible (and perhaps simpler) alternative explanation of why humans would be better at them. If so, have you checked and removed these? One somewhat deeper weakness of this work is that one can claim that there is no inherent notion of compositionality in the patterns of the data per se, but rather in the manner through which they were generated.<BRK>Post revision update The authors have been very helpful and addressed many of my concerns, and I think the revised paper is a substantial improvement. Original Review This paper proposes to explore whether meta learning approaches can exploit a compositional structure in their tasks to generalize, and compares this ability to humans. To do so, the paper introduces a grid dataset consisting of generative grammars for generating compositional grids, as well as a null task distribution which is non compositional but matches on certain low order statistics. These tasks are interesting and new. Human subjects perform better at the compositional distribution, whereas models perform better at the null distribution. * The paper could use some more discussion of the distinction between statistical patterns and compositional rules. For example, perhaps humans would be very good at inferring chains, but not the more complex structures (like trees). Thus, it is difficult to conclude that compositionality is the factor underlying the results.<BRK>This work is an exploration of model behaviour upon meta learning tasks with compositional structure. The authors discover that, unlike humans, machine learning models do not readily pick up on the underlying compositional generative structure of a set of tasks, and hence cannot match the performance of humans. Conversely, when the task is structured to leverage other statistical patterns, models do well. Taken as a whole, this is a nice piece of work. The presentation is well crafted, and I believe the experiments are well planned. The authors design a set of structure learning tasks using a generative grammar. I encourage the authors to spell out some more details of the methods. The authors argue that the non compositional boards could not have been generated by the defined grammar, which seems fair, but they also argue that these boards are necessarily non compositional. It might not be a "simple", "interesting", or human interpretable one, but it would nonetheless be a grammar, and would be compositional. This fact makes it all the more important to define compositionality. I look forward to a discussion in the rebuttal.<BRK>The authors point out, rightly, that meta learning algorithms have meta biases and it is important to understand these, from both the scientific and engineering perspectives. It is well written and raises good questions. Unfortunately, I can t end up agreeing or disagreeing with the claims made in the paper, or really understands how well they are supported by evidence, because I find that they use terms that don t seem to be sufficiently technically well defined. what is statistical structure? Is that an externally measurable property of the agent s behavior and the way it generalizes to new environments? It seems like if a fixed size representation can generalize to very large instances, then that is more clear evidence of compositionality (but then I m thinking of compositionality as a property of a representation, not of externally measurable behavior.) If so, then the meta learning problem is to learn the task distribution, in some sense. It would help me understand the task set better if there were a slightly more in depth description of the chains, trees, and loops and described how the grammar generates the compositional tasks in figure 2.
Accept (Poster). rating score: 9. rating score: 7. rating score: 7. rating score: 3. <BRK>The experiment section is well organized, supports the paper s major claims, and is empirically compelling. However this reviewer is not clear on the quality of these local optima (i.e., when do we get "trapped"?). Of course, the experiment section indicates many benchmark tasks are amenable to this decomposition; but perhaps reasoning about this would help in (re)defining multi agent problems to encourage success, e.g., it would be interesting if adding actions that communicate information directly between agents mitigates the local optima problem.<BRK>##########################################################################Reasons for score:  The paper is very well written. However, its theoretical justification is weakened by assuming a tabular case of pi and Q functions which is rarely the case in practice. Last but not least, it would be nice to see comparisons with more recent value decomposition algorithms such as NDQ and QTRAN/QPLEX. Overall I gave it a score 7. The paper is organized in very clear fashion. 3.Its method of linear decomposition of the central critic into individual critics is simple yet powerful. The paper gave adequate theoretical support to this method. It s unclear that the baselines are the SOTA results on the chosen domains.<BRK>Those weights are themselves learned and depend on the observed history. The authors introduce the idea of value decomposition, which was investigated first in the value based methods, to the actor critic scheme. The experimental results suggest that the proposed combination of value decomposition and off policy critic training has a good performance. The writing of the paper is clear. Is it the value function wrt \pi or \pi^0? Some theoretical analyses (e.g., A3 or C) are only loosely related to the actual proposed method, although I agree that a direct analysis would be difficult to conduct.<BRK>The paper is somewhat difficult to read and can be made better by deferring the details about previous methods to appendix. However my main concern is with the problem of centralized decentralized mismatch (CDM) motivated in the paper and its proposed solution itself. As such a critic is supposed to be "true" to the policy, the requirement of decentralization has little bearing on the variance of policy gradients. In general this not true and requires many strong assumptions, see for ex. While the authors acknowledge this, bypassing the actual complexity for modelling a joint critic with a linear one will in general render it insufficient to model inter agent interactions. 4.There are some unsupported claims which need better explanation like "This becomes problematic because a negative feedback loop is created, in which the joint critic is affected by the suboptimality of agent i, which disturbs policy updates of other agents" How is that so? claims like this need to be well supported. 1 holds beyond tabular settings.
Reject. rating score: 3. rating score: 4. rating score: 5. rating score: 5. <BRK>This work studies the adaptive proximal gradient descent method, and specifically studies the group sparsity. To encourage the group sparsity, a regularizer which is a combination of $\ell_1$ norm, block $\ell_1$ norm and $\ell_2$ norm square is used. After proposing the update rule, the paper analyzes the convergence and regret guarantee of the algorithm. However, I m not sure if the contribution is enough for the conference, as it is known that the block $\ell_1$ norm can encourage the block sparsity, and the computation of proximal gradient is fairly standard and straightforward. The convergence of proximal gradient method is not too different from gradient method as well. I think it can be more interesting if the work can focus on the statistical property of the regularizer $\Psi$. I d leave it to other reviewers.<BRK>The contribution is to introduce regularizers to adaptive optimizer for neural network optimization. Will this modification affect the algorithm and its convergence analysis? It extends several existed adaptive optimizers. The regularizers are not practical since there have many parameters, e.g., $\lambda_1$, $\lambda_2$, $\lambda_{21}$, the size of groups, the way of the group partition. 3.The experimental results are not very convincing. (1) In the experiments, the authors evaluate the performance of optimizers mainly based on the learning performance and sparsity. The roles of the regularization terms and the values of $\ell_{21}$ are quite different in two compared models.<BRK>They demonstrate the effectiveness by inducing sparsity on several models used in the benchmarks. Reason to Score: Weaker experimentation, lack of standard baselines   including them can improve the paper. I have listed my concerns below and hopefully authors can address them during the rebuttal period. Could authors contrast their work with algorithm presented in: https://static.googleusercontent.com/media/research.google.com/en//pubs/archive/41159.pdf which includes an implementation in: https://www.tensorflow.org/api_docs/python/tf/keras/optimizers/FtrlIs the contribution an extension by using group sparsity? Thank you for running those experiments. (a) Magnitude pruning typically invovles a fine tuning phase after removing the weights, was this carried out?<BRK>Summary:The paper proposes a novel framework to add sparsity regularizers to adaptive optimizers in deep learning such as Adam, Adagrad... A generic algorithm is proposed and the theoretical convergence and regret guarantees are provided. It provides a unified way to derive practical optimization algorithm of deep networks parameters under sparsity constraints. Some concerns are adressed: for instance some empirical results are moved to the main paper,  the tuning of the hyper parameters is discussed and details about the groups of variables are provided. After rebuttal:  I  have read the response of the authors. Convergence guarantees, in terms of bound on the regret, are established using results from stochastic convex optimization and based on primal dual analysis. The baseline model is now moved to Adam with weight pruning. Concerns:   A concern about the paper is the lack of justification of the proposed regularization scheme. If so, which other application domains may benefit from the group sparse optimizers? To improve the readability of the paper, the most prominent empirical results should be moved in the main paper. All the experimental results can not be deferred to the appendix as this leads to unpleasant to read Section 4.
Accept (Poster). rating score: 7. rating score: 6. rating score: 6. rating score: 6. <BRK>Here they focus on  deep residual neural networks where the activation units satisfy certain conditions defined via a differential equation (most common units in practice abide by this property) and present L_\infinity approximation results which is the strongest possible sense of approximating a function. Using the weights, one can try to obtain specific values as the network s output and hence the universal approximation property follows. Currently, several parts of the paper read off as a bit mysterious for non experts.<BRK>The authors consider residual networks. Using the results of the control theoretic problem of controllability they proved the universal approximation property of the deep neural networks. The proof technology itself is quite interesting and is based on the ideas of Lie algebras. After reading authors comments.<BRK>This paper studies the universal approximability of residual networks. I think that this result can be used to prove the expressive power of compressed/pruned networks. If this approximation is valid, the main contribution given by Theorem 4.4 is significantly weakened.<BRK>Main concerns:   The paper contains some interesting ideas and new results, but it seems it is more an incremental work to existing literature and the contribution might be a bit limited and not significant enough. From theoretic sense, the existing relevant work by Li et al., 2019 has already established very similar results (also through dynamic system and control theory) for universal approximation capability of such deep residual neural networks, but just in the sense of L p norm rather than L infinity norm (where p can be any number between 1 and infinity).
Reject. rating score: 3. rating score: 4. rating score: 5. rating score: 5. <BRK>I appreciate that the authors have taken the time to address two of my comments but I believe major concerns still remain unaddressed, so my evaluation remains unchanged. For one model some data points may turn out to be not "difficult" but for a different model they may, which casts further doubt on the general applicability of this method in this setting. Note Zhao et al use it to identify adversarial examples, which by definition are specific to a model. I believe this could be made better with a concrete discussion of the same. If the idea is to identify model s vulnerabilities and not use these as common evaluation sets, then this is more or less the same as generating adversarial examples (but called "difficult" examples, in which case this would be better presented as an application/extension of Zhao et al to identify adversarial examples in NLP). Pros:  This paper describes a simple method for understanding difficulty in terms of ease of perturbation. Cons:  While the idea of using FIM to identify examples closer to a classifier’s decision boundary is interesting, it is a new application to NLP even though the idea has been discussed extensively in Zhao et al.(2019).However, the paper doesn t show why these examples may be useful. It is not clear as to how generalizable this approach is. The paper further suggests that evaluation sets should increase FIM but provides no theoretical/empirical justification for the same.<BRK>The authors propose to find examples near the decision boundary using the largest eigenvalue of the Fisher information matrix, arguing that this value gives us a sense of how stable the model prediction is near the input. I’d argue against calling examples selected by FIM “difficult” and “easy”, maybe “unstable” and “stable” is better. Why do we need FIM? This paper did not introduce a new perturbation, it introduced a way to identify examples that have high potential to be perturbed into adversarial examples. Under this framing, I’m not sure that the proposed method is evaluated against proper baselines. That is probably a fair assessment, but those work have a different goal in mind. Figure 1(a): no details were provided as to how the figure is generated for IMDb examples. Are these real examples & real models or is this figure illustrative?<BRK>This paper proposes an analysis technique for studying the  difficulty  of a pair of test dataset examples in NLP. Strengths: * the idea of using the Fisher Information Matrix to categorize the difficulty of (contrast) examples in NLP seems new to this reviewer and well motivated. * The paper argues that many of the existed (hand crafted) perturbations are not difficult enough for models because they lie far away from the decision boundary, which could possibly lead to insights about how to build better contrast sets later on. It seems like those authors were able to mine  difficult  examples where  difficult  could be defined as resulting in a flipped prediction   at least in their experimental setup. * Perhaps this is more for future work, but to this reviewer, it is not clear what we learn about the insights of fragility of NLP models from this direction.<BRK>While I think there is great potential in the idea of using eigenvalues of the FIM to identify difficult examples, I think the experimental results are not sufficient to support the case as described above. The authors illustrate this with a toy example. This in turn can help in developing more robust models. One way to show this might be to perturb all or some subset of examples in the test set, and plot the number of perturbations needed to flip the classifier output against largest eigenvalue of FIM. b.The reasoning that high eigenvalues correspond to points near the decision boundary is made based on an illustration on a linearly separable toy example. c. The only case against contrastive sets and counterfactual examples is that they dont have a high eigenvalue for the FIM. A case is made comparing the proposed metric against contrast set and counterfactual examples (recent works). These should be described in more detail so paper is self contained.
Reject. rating score: 3. rating score: 3. rating score: 5. <BRK>Summary and contributions  The paper takes advantage of NAGO and proposes to search for a family of student network architectures instead of a single architecture, aiming to be more sample efficient. Weaknesses  There have already exist KD NAS approaches and the main difference of this work is to search for a family. This objective mainly takes advantage of the generator in NAGO, so the contribution and novelty should be reduced accordingly. But this fact doesn’t logically lead to idea of ‘searching for a family’. Also, I think it isn’t clearly articulated that how this family of student architectures can benefit knowledge distillation. Besides, the logic and results shown in the visualization of Figure 6 are not clear enough to me.<BRK>This paper applied knowledge distillation (KD) on network architecture generator optimization (NAGO) which is one of NAS. On CIFAR100 dataset, CRD in the original paper used WRN 40 2 as a teacher and trained the student of WRN 16 2, which has only 0.7M parameters with an accuracy of 75.64. On CIFAR10 dataset, the compared models (WRN 16 1 and two WRN 40 2s) have fewer parameters than that of NAGO for AutoKD. The authors should clarify this. Why the accuracies of NAGO in Figure 4 look low compared to the other results in the paper? Using KD on NAS leverages additional computational cost, but it is not clearly compared quantitativelyComments)  The method is incremental, and the novelty is limited. The experimental results comparing with other methods are biased to the proposed method, where the competitors  performances are not fairly compared, so it is hardly convincing the results and the effectiveness of the proposed method.<BRK>Summary: This paper proposes searching for an architecture generator that outputs good student architectures for a given teacher. The authors claim that by learning the parameters of the generator instead of relying directly on the search space, it is possible to explore the search space of architectures more effectively, increasing the diversity of the architectures explored. Pros:+ The paper is clear overall. No results on ImageNet. The claim that a standard neural architecture search would produce architectures sampled from the same distributions and therefore not be efficient in exploring the space of students is insufficiently explored. Additionally, the paper does not have information about comparisons with other architecture search algorithms for knowledge distillation or surrogate functions, therefore the introduction of a new framework may not be warranted under the claims of the authors.
Accept (Spotlight). rating score: 7. rating score: 7. rating score: 6. <BRK>Theoretically:The paper is both conceptually and formally descriptive. Combined with appendix details, the authors present a very full picture of their work, which is very much appreciated. To the best of my knowledge, this appears to be a novel approach to the challenge of adversarial model reconstruction. Directly relevant related work:Philosophically speaking, there are some direct competitors that are not addressed, in the sense that they are methods to safeguard against leakage / maintain model privacy. I only argue that these should be addressed/discussed by the authors, not that these papers invalidate the authors  novelty claims. For that reason, I believe the paper is strong enough for acceptance as is.<BRK>In particular, the authors proposed information laundered model where the input and output of the true model are perturbed. The objective is to minimize the KL divergence between the true model and laundered model with mutual information between input and output as constraints. This paper is well written. The authors claim the proposed framework and algorithm can be also applied to continuous space. However, it is not clear to me how algorithm 1 would work in continuous space.<BRK>The proposed method is information theoretic. The paper does not qualitatively nor quantitatively compare their approach to any other prior protection approaches, such as [1], [2] or [3]. Mutual Information and Expected Value (used in the loss function) are both average notion. I wonder what this means for the provided protection. would there be some sample inputs that could extract a big part of the model? I think a study of this, or at least a discussion would be very helpful. I have updated my score based on the changes and the clarifications made on the related work, and also the results of the mounted attacks.
Reject. rating score: 5. rating score: 5. rating score: 6. rating score: 6. <BRK>Strengths  The paper is clearly presented, and the effect of batch normalization in the dynamic range of a quantize layer is interesting. Weaknesses  There is not qualitative analysis of the problem of the dynamic range and how the proposed method alleviates it. The number of iterations in the experiments is a fixed number. Overall, I think the novelty of the paper is limited, with concerns about the experiments. I also read the other reviews and responses. I think the paper has improved in the revised version. However, I m still concerned about the novelty, which still remains relatively incremental, as also pointed by other reviewers. I update my rating to 5.<BRK>Pros:  The paper is well motivated and the method seems valid. Cons:  The writing quality is poor. This paper is over claimed. To improve the quality of this paper, more rigorous mathematical notations and some visual experiments should be provided. Markus Nagel et al.**********After rebuttalThe revised version has a better shape. In particular, I like the analytical experiment (Fig.2), which demonstrates that the proposed scheme can improve the wide dynamic range. Some issues still prevent it from being accepted. However, there is even no definition of the dynamic range in the paper which may make readers hard to understand the mechanism of PfQ. But the analysis of Eqn. I would like to increase my rating to 5.<BRK>How would the variance distribution be after the "workflow"? Therefore, authors provide a new method call Pruning for Quantization (PfQ) and a workflow to solve the model compression problem practically. 4) More proof reading will make the paper look better. ### ProsClear idea about PfQ and the mathematical results provide the reason why the authors want to do so. In the same sentence, "for the learning in cifar100"  > "for the learning in CIFAR 100". IEEE transactions on pattern analysis and machine intelligence (2018). 2) It is lack of explanation why the workflow is needed for the model compression with PfQ. The reason why multiple PfQs are executed is missing. A more clear algorithm would be better for the presentation of the "workflow".<BRK>Pros:This paper makes an important observation that channels with small variance lead to a problem that the filter weights will have a wide range after fusing the BN layer into convolution. The proposed method is also compared with several other quantization methods. Although the current experiments show the benefits of PfQ, it is better to have more experiments to make the validation part more solid. There are some questions which are not clear in the paper:1. What about the results of using / not using PfQ for per channel quantization? Is it still suffer from the channels having small variance? 2.DFQ is used as the quantization method. In general I think this paper is a good work on quantization and model compression.
Reject. rating score: 4. rating score: 4. rating score: 4. rating score: 6. <BRK>The authors first an approximation to the gradient norm (GN) that is the norm of the gradients for only fully connected layers (AGN). Then they empirically evaluate the correlation between AGN and GN as well as GN and the generalization error. In Section 3, the authors conclude that the correlation between AGN and generalization error is not consistent in a wider family of models. I share concerns with others reviewers and I highly encourage authors to consider answering questions suggested by Reviewer 3 at the end of their discussion. As such, I m reducing my rating to 4. But this doesn’t necessarily mean gradient norm is a poor measure of generalization. The link in Section 2 is only empirically shown for a MNIST, Fashion MNIST, and CIFAR 100 datasets and there is no theoretical reason to justify this. Gradient norm can be computed efficiently in much less time than the claimed time in the paper. It is not clear if the conclusions from MNIST, Fashion MNIST, and CIFAR 100 datasets would hold for other datasets. Fig 1, a d: the norm of the gradient on MNIST should tend to zero by the end of the training as the loss tends to 0. AGN seems not to be a robust measure based on Figure 3. The authors could study better approximations to gradient norm and their robustness. Or only try it out? BackPACK: Packing more into backprop.<BRK>The paper empirically investigates the sum of gradient norms as a measure to determine the generalization abilities of a neural network. Lastly, the paper shows that the measure is not effective in predicting the generalization ability of DNNs with different architectures. The paper hints at that, saying that for well trained networks the measure correlates more strongly with the generalization gap. Thus I have lowered my score by one. It seems, the gradient path norm does not necessarily outperform those measures but might be related. To illustrate my point, assume your data is drawn with x uniform in R and y   cos(v*x) + eps, where eps is some Gaussian noise. That is, the data is a noisy cosine function. If we now initialize our model randomly, it can start at a steep part of a local minimum and jump into the next and from there on to the next with fairly large gradients until it ends up (hopefully) in the global minium, where it will not escape (all also depending on the learning rate, of course). In this hypothetical (and arguably quite artificial) example, the path from most initializations to the global minimum would have large gradient norms. If we instead initialize close to a local minimum, the gradients will be small and if we start close enough to the minimum, the model will not escape. Thus, the gradient norm of the path is very small, yet the generalization error is large (the gap then depends on the actual sample and can be either small or quite large).<BRK>Authors of the paper conducted many numerical experiments on the relationship between generalization and their proposal on an approximated form of gradient norms. It might be a good submission to workshop, but not qualify for ICLR main conference. Many experiments in the gap should be carried on. The conclusion is that their  approximated gradient norm  is not well behaved and has many constrains in application, so I would mark these as merely observations rather than  contributions .<BRK>In this paper, they provide the empirical studies  to understand the effectiveness and efficiency of the use of the gradient norm (induced by [the Li et al., 2020]) as the model selection criterion. In conclusion, they do not recommend using (approxiamte) gradient norm for model selection in practice. Pros: 1:  In this paper, they propose an approximate gradient norm (AGN) based on an accelerated approximation (Goodfellow, 2015) of gradient norm that only computes the loss gradient in the Fully Connected Layers, which can significantly reduce the computation cost (200∼20,000 times faster) than the original one. They also find in empirical evaluations that AGN and GN behave identically with respect to empirical generalization gap. 2: They carry out extensive experiments to validate the correlations between generalization performance and AGN, and find that, when the models are well fitted, AGN well corresponds with the empirical generalization gap, but for the  under fitted models, the correlations  between empirical generalization gap and AGN are not consistent. These results confuse me, can we draw a new conclusion that, when the model is simple,  the  correlations  between AGN and generalization gap are consistent, but for complex models, the correlations are not consistent.
Reject. rating score: 4. rating score: 5. rating score: 5. rating score: 6. <BRK>This paper proposed neighbor class consistency regularization together with an entropy based weighting factor to tackle the problem of unsupervised domain adaptation. The paper is generally well written and easy to follow. Update:Thanks for the authors  response. I thought the contributions of this work are incremental, and the authors  rebuttal did not convince me well. What s more, the authors even did not clearly point out which kind of mutual information maximization they used in either the manuscript or the rebuttal. 2.I think the work lacks novelty. The author should at least acknowledge the relations between the paper and these works. The paper claimed its state of the art performances on various benchmarks. 6.More hyper parameter analysis would help improve paper quality.<BRK>This paper tackles Unsupervised Domain Adaptation. However, it should be avoided to evaluate unsupervised domain adaptation methods since there are no labeled data in the target domain in a real setting. The authors focus on the intrinsic discriminative feature for target samples. The experimental results show that the proposed method achieves state of the art performance using the same backbone network. **Pros**  The consistent approach making use of neighborhood structure in the target domain is well motivated and easy to understand. The proposed method achieves SoTA performance on several benchmark datasets. How do they tune them? In the last paragraph of Section 1, an abbreviation "NC" is used without an explanation. **Overall rating**Although there should be more descriptions about the existing method and experimental results, the reviewer is leaning toward acceptance.<BRK>This paper addresses the unsupervised domain adaption (UDA) problem. Particularly, the paper proposes to impose neighbor class consistency on target features to preserve intrinsic discriminative nature of target data and presents an entropy based weighting scheme to improve robustness against the potential noisy neighbor supervision. Extensive experiments show the effectiveness of the proposed method. However, the distribution discrepancy between source domain and target domain may be very large, such as unsupervised cross dataset person re identification. 2.From Table 3 5, NC SP outperforms the state of the art methods with only a small margin. 3.It is suggested to evaluate the influence of hyper parameter \lambda in Eq.(10) and (11)4.<BRK>Based on the observation that the target features from source pre trained model are highly intrinsic discriminative and have a high probability of sharing the same label with their neighbors, a simple but effective method to impose Neighbor Class Consistency on target features is proposed. Whether the authors have repeated the experiments in several times. It is better to show the variance of the results in Table 2, which may be more suitable to demonstrate the effectiveness of ENC. [2] In Eq.(9), it forces that the anchor samples should be closed to the neighbor samples by comparing with the distance between the anchor sample and the augmented sample. It is also better to show the comparison in the experiments. [3] For the neighbors, when k is equal to 1, the proposed method achieved the best performance. Hence, the neighbors with the minimum distance could not guarantee that they share the same labels.
Reject. rating score: 4. rating score: 6. rating score: 6. rating score: 7. <BRK>This paper proposes a new value based method using risk measures in cooperative multi agent reinforcement learning. Applying risk sensitive reinforcement learning in multi agent reinforcement learning is interesting, but several points can be improved.<BRK>However, I think they are not usually observed in the environment StarCraft II, since the policies of the enemy are fixed. The paper is very clear and well structured. Expanding value decomposition methods to the risk sensitive field is a novel idea, and it shows competitive performance in empirical studies.<BRK>The proposed method relies on CTDE and features a dynamic risk level predictor. Although CVaR optimization is an important problem for MARL and the experimental results seem to be convincing, the formulation of the problem and the description of the proposed method are not written with enough clarity.<BRK>Strengths:1) The paper is well written, and versed in the pulse of related works on the topic. Importantly, their work has some conceptual justification. This is not easy to discern. 5) In general, a discussion of the technical innovations required to establish the theorems is absent. What is new? Minor Comments:1) References missing regarding risk sensitive RL:Zhang, Junyu, Amrit Singh Bedi, Mengdi Wang, and Alec Koppel.
Reject. rating score: 5. rating score: 6. rating score: 6. rating score: 6. <BRK>The method seems like a merge of Split merge model and the Poly Encoder model. Showing speed up ratio in large amount of data is important for judging the effectiveness of the proposed approach. If the numerator is for $\hat C_{TV}$, it should not include $C_T$. 10.As in the method part, the author shows the theoretical speed up ratio.<BRK>2.The operation cost on CPU is not shown in the paper. The motivation is insightful, and the method seems reasonable, while the experiment is weak. Following is my detailed concerns. 5.This approach tries to improve efficiency of image text retrieval while has some sacrifice on effectiveness.<BRK>Cons:   Limited contribution of the proposed method. Systematic experiments demonstrate the state of the art effectiveness and efficiency of the proposed method. The hyper parameters \alpha and \beta in the equation of complexity of transformer layer are not clearly stated. In addition, the corresponding analysis is not provided.<BRK>After that, the attended vision and text probes are concatenated and fed into cross modal self attention layers to interact in the high level one tower architecture. Overall, the definition of the task and the overall architecture of this paper are both clear and straightforward, making this paper easy to understand. Besides, the paper provides comprehensive experiments on performance and computation cost to validate the effectiveness and efficiency of their proposed method. Here are some of my questions and concerns for the paper:1. 3.According to Section 3.5, the \overline{w} is the sum pooling of \hat{w}_1, …, \hat{w}_N while it is the output of l transformer layers in Figure 3.
Reject. rating score: 5. rating score: 5. rating score: 5. rating score: 5. <BRK>The paper proposes the use of mixed image sizes during training. If one knows in advance that the average image size would be smaller than 224, one can train with a fixed image size that is smaller. The last issue is the robustness to different image sizes. However, the authors use gradient smoothing only in their proposed method and do not use it in the baseline method (why not?).<BRK>This paper also does multi scale training but only introduces some minor improvements that are neither breakthroughs nor that they provide any interesting insight. Several training improvement methods are introduced to improve the multi scale training. Paper s strengths  The paper is quite well written. Code and models are provided for reproducibility.<BRK>This is inaccurate, since Touvron et al.(2019) actually show that slightly increasing the test resolution improves accuracy, due to the discrepancy in object sizes introduced by data augmentation (cropping). We show that reducing the average image size at train  ing leads to a trade off between the time required to train the model and its final accuracy.". The authors do not report any statistical significance metric. However, I would not consider this a novel contribution, since the trade off between speed and accuracy is well known.<BRK>This paper presents a mixed size CNN training scheme, using several different input image sizes for one single model training. Critical related works and comparison are missing. Besides, as described by the authors, NeurIPS 2019 paper “Fixing the train test resolution discrepancy” also considers how to enhance the performance of CNN models when applying them with different input image sizes. Why BN calibration does not work on the other image sizes when the model is trained with a fixed image size? Furthermore, in NeurIPS 2019 paper “Fixing the train test resolution discrepancy”, this line of methods work pretty well.
Reject. rating score: 4. rating score: 5. rating score: 5. rating score: 7. <BRK>Summary: The paper proposes an implicit tensor factorization approach for learning time varying node representations over dynamic networks. Strengths:   The paper focuses on an important problem of representation learning for time varying graphs as several real world networks consist of interactions between nodes that occur and change over time. I will be happy to revisit my score based on the author’s response to the concerns raised by me above. The authors show that the method provides significant empirical performance on both tasks compared to non tensor based methods and comparable performance to recently proposed approach in [4]. While the authors claim that their key contribution is to use these techniques for higher order structures than a matrix. However, skip gram type of methods with negative sampling for 3 order tensor have been well studied in relational learning literature [1] for static case. This has also been extended to 4 order tensor to consider temporal dimension and presented in previous ICLR [2]. Hence, the novelty of the technical contribution is not very clear. I am not able to see how this method would scale to such large graphs.<BRK>In this paper, the authors propose learning node embeddings of time varying graphs. The paper is well written, and easy to follow. My main concern is with the degree of novelty. I think the work is relevant, but am not convinced that the novelty is sufficient to warrant acceptance. It seems a rather straightforward extension of SGNS as well as the idea that the shifter PMI matrix can be factorized, to a tensor. The datasets the authors use are also quite small, so I m not convinced that the method is scalable. Indeed, scalable tensor factorization is a challenging problem. Is there a note on scalability? How large can you go? From table 3: The results from HOSGNS are significantly better (sometimes 99%) compared to the baselines. for the task in Table 3, it seems the (stat) version of the model works a lot better, and adding (dyn) actually makes it worse.<BRK>Main IdeaIn this paper, the authors studied the problem of time varying graph embedding problems. The authors generalized skip gram based graph embedding method to time varying graphs. The authors show that the method can be used to factorize time varying graphs as high order tensors via negative sampling. StrengthThe paper is well written and technically sound. The authors provided theoretical analysis for the approximation of negative sampling to tensor factorization. The proposed method is more like an acceleration to traditional tensor factorization method for time varying graphs. In this case, the authors should (1) include tensor factorization baselines to compare the accuracy and (2) carry out experiments on efficiency comparison with classic tensor factorization methods. The scalability of the proposed method is another concern.<BRK>Clarity: The motivations of learning representations for  time dependent proximity graphs generated from contact tracing are well explained. Novelty: The paper extends a previous proof by Levy and Goldberg, about how Mikolov’s SGNS is implicitly factorizing a word context matrix, to higher order tensors . This higher order generalisation applied to time varying graphs is used to learn embeddings, which are shown to effectively encode graph structure and dynamics. Impact: The paper proposes an extension of a previous well known proof for higher order tensors, which leads to an embedding technique which is shown to perform well on real world datasetsThe code and datasets have been made freely available. Correctness: Several experiments have been conducted to demonstrate effectiveness of learned embeddingsTheoretical groundwork for the proposed model has been well laid.
Accept (Poster). rating score: 7. rating score: 6. rating score: 6. rating score: 4. <BRK>The proposed approach is tested on multiple datasets. The main contribution of the paper is that it proposes a way to sample batches of data that would satisfy a given fairness criterion and incorporates that into model training. The paper is well written and points to a promising direction in handling fairness using (non causal) notions of fairness.<BRK>The paper is well organized and written. However, the criterion used for selecting the final setting is not clear in this paper. [Con 1] It would be helpful for the authors to summarize their contributions more from the fairness aspect. I am confused about the ability of FairBatch to mitigate disparate accuracy.<BRK>The authors outline the optimizations corresponding to several fairness objectives, and provide an empirical demonstration which compares the proposed approach to the current state of the art where the proposed method performs quite well. What I don’t quite understand is the advantage of modifying the batches versus incorporating the inclusion weights in the loss function itself.<BRK>I believe the paper is worth publishing after re working the theoretical part. Despite my tendency to have this novel idea and work published, I believe the authors need to be more careful in writing and dealing with the theoretical section of the paper.
Reject. rating score: 3. rating score: 4. rating score: 6. rating score: 6. <BRK>The paper proposes a method for improving the generalisation of model based  RL algorithms. The paper proposes that one should learn a distribution of transition models, which they do by training with Dropout to improve generalization in model based RL. While the paper is addressing an important problem, sadly it lacks in novelty, it is unclear if the results are significant and there is a lack of discussion and comparison to related work in this direction. For example:   * [Kurutach et al., 2017](https://arxiv.org/abs/1802.10592)  which trains a model free algorithm on purely imaginary data and uses an ensemble of transition models to avoid overfitting to errors made by the individual models. al., 2017](https://arxiv.org/abs/1702.01182) seems to be also using dropout as an approximation to an ensemble of transition models to reduce overfitting in real world navigation tasks. It would be helpful to discuss how your work differs from this work? 3.It is quite unclear why the latest results of [Ha & Schmidhuber, 2018](https://arxiv.org/abs/1809.01999) with adjusted temperature weren’t used as baseline. They are better across the board, and the arguments provided in Section 4.1 do not seem highly convincing. Did you try to combine your method with a temperature variant of World Model? 4.While the effects of the dropout probability during inference has been studied in the paper, there is no discussion of why the dropout probability during training has been fixed to 0.05 in all the experiments. Was this as a result of a hyperparameter tuning? 5.In Ablation 4.4, it appears that step based dropout is performing better than fixed episode dropout. Overall, the problem being addressed by the paper is interesting, however, the contribution isn’t particularly novel. At the same time, the experiments are slightly lacking and it is hard to assess the significance of the results, for example seed variability seems high when/if present (it is actually unclear what the reported variability corresponds to in Table 1 & 2) and most figures only show a single curve without any error bars. Hence, I believe this work is not ready for publication at this time. However, I encourage the authors to improve their work by a) discuss the relevant related work e.g.ensemble methods b) discuss differences to [Kahn et.<BRK>The paper introduces the use of dropout for World Models. The argument put forward is that different dropout masks essentially lead to different “dream” environments. The authors compare their approach to the original World Models paper and the recent Game GAN work in the two original World Model environments (Doom and Car Racing). The authors appear to show results that outperform these baselines, and they then perform ablation studies to fully dig into the implications of this application of dropout. This is a relatively simple approach, but that is not necessarily a bad thing and has the potential to be quite valuable. The evaluation shows positive results (though I have concerns I’ll get to in a moment), and the ablation studies offer valuable insights. I am concerned with the authors evaluation and the baseline performance. From the original World Models paper Ha and Schmidhuber’s approach achieved an average return of 1092 +/  556, compared to the value reported in this paper as 849 +/  499. Both of the scores reported by the World Models paper notably outperform the dropout approach from this paper. It is unclear why this is the case. Potentially the authors of this paper may have used a different optimization method besides CMA ES. I am willing to be convinced otherwise but based on the evaluation potentially having major flaws and this being the crux of the paper, I lean towards rejecting it. Questions:  There’s a claim of the size of possible different environments in section 3.2.2, but how different are each of these environments from one another? A visualization or some summarizing statistics would be helpful here. It may be obvious from up above, but can the authors account for the discrepancy in the performance of the original World Models approach? The final sentence of the abstract is a bit awkward and the beginning of the related works section rapidly changes tenses when describing prior work. Overall though the paper is very well written.<BRK>##########################################################################**Summary**:This paper presents a novel idea of using Dropout in the learned dynamics model for domain randomization. While domain randomization has been extremely popular in the field of sim to real transfer, it has been relatively less explored when the dynamics model itself is learned. The paper applies Dropout on the recurrent dynamics model (LSTM) and shows that it helps narrow the reality gap between the imagined (dream) environments and the reality. Overall, I find the idea to be intuitive and simple. The paper also provides good results in a few environments. But the environments used in the paper are not strong enough to support the claim that using Dropout on the learned dynamics model can bridge the reality gap as all the environments are in simulation. ##########################################################################**Strengths**:The idea of applying Dropout on a learned dynamics model is new. And it is simple to implement the idea in the Recurrent World Model. The paper is well motivated, and it is clear that domain randomization requires extra efforts to be applied on a learned dynamics model, and Dropout is one method to incorporate domain randomization into the recurrent dynamics model. The paper does not really show the capability to narrow the sim to **real** gap but rather the sim to **sim** gap. It remains unclear whether such an approach will improve the performance of transferring a policy trained in simulation to the real world (for example, transfer a navigation skill from a simulated agent to the real robot). Suggestions on more baselines:* A baseline where other regularization techniques (such as weight decay, entropy maximization) are used to prevent the dynamics model M from being overfitted. If we use other regularization techniques such as adding weight decay and maximizing the entropy of the distribution to make sure the dynamics model `M` is underfitted or less overfitted, does the final performance also get improved? Even though the paper uses a Gaussian Mixture, it could be that after supervised training, the variance of each component becomes small without Dropout. So do you sample a component from the Gaussian mixture for each dimension separately? In Figure 2: should there be an arrow that goes from `z` into `M` as the dynamics model `M` takes as input the latent state `z` and action `a`. It would provide more insights if the paper can provide some visualization on diverse simulated trajectories generated from the recurrent model. For example, start from a state `o_0`, apply the same sequence of actions `a_0, a_1, …, a_n`, then use the recurrent model with dropout to generate many sequences of future observations (reconstructed by the VAE decoder). "Recurrent world models facilitate policy evolution."<BRK># Summary:The paper builds on the "World Models" (Ha & Shmidhuber 2018) framework with ideas inspired by Domain Randomization (Tobin et.al.2017 and others). # Strengths:    I find the problem well motivated and clear. The solution is intuitive and simple # Weaknesses:   Overall I find the contribution over the original World Models work relatively minor. This strikes me as little more than World Models + MC Dropout (Gal and Ghahramani 2015) in the VAE. Perhaps I m missing something about why this is challenging or difficult. Related to the above, there are few statements that I find I bit puzzling. There is a high emphasis put on the fact that we should also apply dropout at test time. It is argued for example that "DDL’s use of dropout is different from traditional applications of dropout. Generally dropout is only applied at training time but not inference time" and similar things are repeated other places. Application of dropout at test time is exactly MC Dropout (Gal and Ghahramani) and is very commonly done. I agree that randomizing over the model changes the underlying MDP that the RL agent using for learning. This same problem is known in the domain randomization literature.
Reject. rating score: 4. rating score: 5. rating score: 7. rating score: 7. <BRK>Summary:This paper provides an approach for causal inference in observational survival dataset in which the outcome is of time to event type with right censored samples. That is, the representation learning component is directly taken from (Shalit et al., 2017) and the objective function for the survival analysis component is directly taken from (Chapfuwa et al., 2018).<BRK>**Summary and key claims**This paper repurposes the balanced representation learning framework for estimating treatment effects, originally proposed in (Shalit et al., 2017), for the survival prediction setup. The "generative modeling" part of the model is somehow alien to the original problem of estimating treatment effects on survival outcomes. Developing a generative model for event times based on planar normalizing flows. **Originality and significance**Overall, I think that the paper is a straightforward application of the balanced representation method in (Shalit et al., 2017) to survival outcomes.<BRK>This paper is very well written. The extension of individualized treatment effects to survival data is in some sense straightforward as both areas are quite mature and can be unified with aggregated loss functions dealing with biases of different type. The proposed solution, metrics and datasets proposed for this problem are compelling though and I believe will serve as a benchmark for further studies on treatment effects and survival data. I don t see a meaningful difference between this statement and that given by (Shalit et al, 2017), nor a proof in the Appendix. Similarly, Survival based deep learning architectures have been developed which could have been considered as well [2].<BRK>Disclosure: I found this paper online during review process https://arxiv.org/abs/2006.07756This is a comprehensive paper with interesting application of counterfactual inference under survival analysis setting. •	It nice that the proposed nonparametric approach in this paper can adjusts for bias from confounding due to covariate dependent selection bias and censoring (informative or non informative).
Reject. rating score: 3. rating score: 4. rating score: 4. rating score: 4. <BRK>This paper studies a method to give upper bounds on the total number of linear regions in a deep ReLU network. Is there a bound on the number of neurons in the hidden layer?<BRK>Summary: This paper extends on the framework of matrix computation in Hinz & Van d Geer (2019) to give a tight upper bound for linear regions. Having earlier layers with smaller dimensions can lead to decrease in the number of linear regions.<BRK>  Overview of the paper  The number of regions can indicate the expressive power of neural networks. Does it say something about the structure of a set or the size of a set? The main contribution is Theorem 1 and 2, which gives a better choice of \gamma. The experimental results then verifies the chosen \gamma parameters yields a tighter upper bound of the linear regions.<BRK>This paper studies the counting of linear regions of a multi layer ReLU network and gives an upper bound on the number of linear regions that is tighter than existing results.
Reject. rating score: 4. rating score: 6. rating score: 6. <BRK>This paper develops new stability bounds for SGD. This paper considers both convex and nonconvex cases. The motivation should be further clarified. In particular, it is not quite clear to me the advantage of considering normalized loss as compared to the standard loss. It seems that the authors do not introduce new techniques. The stability bounds grows as a linear function of the batch size. However, RELU networks are not smooth.<BRK>This paper discusses the uniformly statbility and on average stability of SGD on convex and nonconvex optimization algorithms. 3.If the length is not limited for appendix, it s more clear to put Hardt et al proof in it. The convex case extends from Hardt et al.For the nonconvex case, the result is concrete and can be applied to neural nets. So $w$ and $A(S)$ are also data?<BRK>Summary:This paper considers the generalization bound for stochastic gradient descent. The authors leverage normalized loss function to analyze the stability of SGD algorithms which further yields the generalization bound. It is not very clear what is the motivation behind it and how this assists the theoretical analysis. Here are some comments:1) The idea of the normalized loss function should be discussed in detail.
Reject. rating score: 3. rating score: 3. rating score: 5. rating score: 6. <BRK>Coefficients of h can be seen as samples of an underlying continuous domain function g.The filter corresponding to the discrete sequence h is H (Fourier transform of the impulse response h) and the filter corresponding to g is G. H is periodic even if G is not periodic. So, we can forget the other periods and focus only on a single period. For a pure “bandlimited property”, one needs to consider an infinite length sequence h. I hope the authors can prove the contrary because otherwise, there is no need for more explanation. The conclusion is that if you use convolution kernels with short size (2x2 or 3x3 for instance), so only very few filters can have nearly zero magnitude in their Fourier transform and the bandlimited assumption is not really a sharp forcing to zero, but a weighted frequency penalization.<BRK>They suggest that it is due to the architecture of neural network and not the structure of natural images. They build on wavelet transforms (oriented bandpass filters) because it is a translation invariant representation known to be sparse. How would you get if you were fitting random 3x3 filters with oriented bandpass filters ? ### Minor comments* The text in the figures is too small...<BRK>The reviewer is not convinced by the analysis provided in the paper. The reviewer argues that the types of filters learned by CNNs should be task dependent. As for ImageNet (or generic) classification, the CNNs tend to learn oriented bandpass filters. 2.The reviewer agrees that $w(x)$ should be localized, but it is not necessarily lowpass (as  Gaussian in figure 2). 4.The reviewer suggests the authors empirically analyze some more CNNs optimized for different tasks to see whether the results hold across a wide range of tasks. Minor comments:It is a very well written paper, and the reviewer enjoys it during the review.<BRK>The main argument is that oriented bandpass filters are the eigen functions of localized convolution operators and in order to span the input signal space (regardless of its a structure) the network will always learn these functions as filters. There are, however, many open questions here, many of them I think the paper should have addressed:* rotation of the basis functions   the main argument of the paper is that these filters are learned because the are the eigen functions of the convolutional operator and any task that requires spanning the input space ( all tasks) will learn these. For example, any rotation of these learned filters would be such a basis (but the filters would not be oriented band pass filters in that case). I don t think there is a good explanation here as to why this specific basis is learned out of all possible bases. the filters used in some of these networks are only 3x3   this may result in a very coarse estimate for the bandpass oriented filter parameters. when trained on natural images. These results, as the authors suggest, are related to the structure of natural images   is this related in any way to the proposed explanation? * why do later layers get even better? does it have to do with filter size?
Reject. rating score: 3. rating score: 6. rating score: 7. <BRK>This paper present an efficient network, named BasisNet, which combines recent advancements in efficient neural network architectures, conditional computation, and early termination. BasisNet can be applied to any network architectures. My main concern is about the novelty of the paper. The authors also claim that they simply combine three kind of works together and achieve a good performance with bag of training tricks (including AA and KD). The main claimed contribution is the basis model synthesis. However, it is just a simple extension of CondConv [1] and DynamicConv [2,3] to the whole model, where the dynamic inference of whole model is also not new [4,5]. The early exit technique is also widely exploited in previous works and the early exit method here is similar to BrachyNet [6]. In addition to the novelty, another worry is about the actual inference latency and the practicability. The synthesized weights are assigned to the BasisNet layer by layer. This process will cost large latency and memory since it must be done online and one by one. "Branchynet: Fast inference via early exiting from deep neural networks."<BRK>If the mb 1, like in a common inference use case, does the basis model require the model synthesis for each image? (2) MAdd is a good indicator of hardware efficiency for a model but definitely not the only one from the point of view of a hardware designer. Could the authors show some advantages of using BasisNet on TPU or other hardware optimized for it? (3) One minor issue on claiming the early termination as an advantage of the BasisNet over the Mobilenets. Other than these three points, the paper is well written and enough experiments are done to support the authors  point that the BasisNet can provide better accuracy and MAdds than the MobileNets.<BRK>Results outperform recent SOTA like the noisy student. The authors repeatedly highlight that their method has the advantage of capturing  global knowledge  when compared to related works. However, there is only an intuitive understanding of global knowledge: the knowledge that is semantic aware and ready for prediction in the lightweight model is global knowledge. The concept is vague, confusing, and lacks a clear definition. What knowledge can be called global or local? The authors need to carefully address this issue as the concept is a key motivation but not properly described. Besides, the authors need to add experiments to support their claim that global knowledge is indeed better than previous local knowledge in some aspects (sec3.2 Key difference). 2.Lacking simple baseline results or ablation results. This baseline is to show the improvements brought by the joint training framework.
Accept (Poster). rating score: 7. rating score: 6. rating score: 6. <BRK>This paper proposes a backdoor attack method using warp based triggers, which distorts the global structure of the image (input) making the attack undetectable/un noticeable by humans. **Post Rebuttal**The authors have addressed my comments and revised the manuscript accordingly. However, I have some remarks about its clarity and some of the experiments that I hope the authors address. Demonstrating the attack on a variety of datasets (MNIST, CIFAR10, GTSRB)2. *Significance* (8/10):This is a significant work and opens the door to new questions in the field of backdoor attacks. Thorough experiments that validate the effectiveness of the method. The experiments do not discuss that.<BRK>1.Summary  This paper presents the imperceptible warping based back door attack technique. 2.Strong points  The technique produces the imperceptible poisoned image. 3.Weak points  I am not sure that the experiments are enough to show the technique s superiority. Why did you perform experiments only on small datasets such as CIFAR 10, MNIST, and GTSRB? I am not confident in the review because I am not familiar with the attack algorithms.<BRK>This paper proposes a new backdoor attack method on vision classifiers, where a warping based trigger is adopted instead of patched triggers. Can the authors show some such examples? 2.I understand that the backdoor in this paper is the unique warping function. What is the extension and what is the difference from the original TPS? Currently it is not discussed in the paper, which may not be good for interested readers. I would like the authors to address the issues I raise.
Reject. rating score: 4. rating score: 4. rating score: 5. rating score: 6. <BRK>Pros   1.The paper is well written and organized. The authors did not rebut many of my negative concerns. Most critically, I feel that the method is lacking in a theoretic proof of a strict bound, which is the primary contribution of the paper. If published metrics comparable to the experiments shown in the paper are available, these should be included for side by side comparison.<BRK>In this paper, the authors propose a certifiable watermarking method for neural networks. As the authors admit, the proposed certification method is somewhat artificial and does not hold in real life scenarios. for instance if the model has 1000 parameters, changing 1 of them to be x1000 times larger and leave the rests of the parameters uncharge? 2) Did the authors experimented with other NN verification/certification methods such as the one proposed in [4]? If that is the case, how do you suggest to do such thing? 4) The authors reported results using lr of 0.1, 0.001, and 0.0001.<BRK>[1] to the watermark embedding and extraction process, it is possible to ensure that the watermark is robust to watermark removal when the network parameters are modified by less than a certain calculated value. Specifically, the proposed method adds Gaussian noises to parameters instead of images. The article is well written and gives a detailed description of the related work, as well as a clear flowchart of the algorithm. In this case, the perturbation happens on the pretrained released neural networks. The technical contribution of this paper is a bit weak in that they mostly followed [1] and the only interesting point is the expansion into the new scene of model watermarking. The authors should provide results on larger benchmark dataset, e.g.ImageNet for verifying the method, as well as providing comparison results between a large dataset and a small dataset.<BRK>The proposed method exploits the randomized smoothing techniques for a certified watermark of neural networks. The idea itself is novel and interesting. Different from the defense against adversarial example, in the case of watermark detection, not only the detection accuracy but false detection of non watermarked models should be considered. Since the proposed method is quite close to adversarial training, one concern is that models trained with adversarial training might be falsely detected as the watermarked model. If my understanding is correct, Col. 1 simply certifies that the lower bound on the trigger set accuracy. Can we say that models without watermark cannot attain this trigger set accuracy?
Accept (Spotlight). rating score: 8. rating score: 7. rating score: 6. rating score: 6. <BRK>The paper provides generalization bounds for seemingly complex neural networkson the basis of much simpler ones. That is a good idea and something that is currently very relevant I think and the approach seems to be the natural one to take. In Lemma 1.1. The in sample error. The authors state that their work can be applied to  generalization bound of Arora et a. 18 that only worked for a compressed network but not the original one.<BRK>The paper overall is of good quality. The topic of this work is of great significance given that understanding the generalization error of neural nets is relevant to the community. Authors also show some experiments on the behavior of their bounds, and in particular, on margin distributions. Which seem to be the case of Figure 3(b)? To give an example, consider a task where an overparametrized complex network generalizes well in practice, it seems unlikely to me that a (very) sparse or pruned network would "perform similarly". I would appreciate if authors can comment on this concern.<BRK>This paper provides an upper boundary of the generalization error of networks: the sum of its training error, the distillation error, and the complexity of the distilled network.<BRK>Different from the traditional error analysis, this paper focuses on bounding the divergence bettween the test error and the training error by the the corresponding distillation error and distillation complexity, e.g., test error  is bounded by training error + distillation error + distillation complexity. The current learning theory analysis may be important to understand the theoretical foundations of distillation strategy in deep networks. 1)What is the relation between the original network complexity and the corresponding distillation error +distillation complexity? 2)Is the derived upper bound (e.g., Theorem 1.2) tighter than the traditional one? 3)What are necessary/sufficient theoretical conditions for the effectiveness of distillation strategy (from the generalization error bounds)?
Reject. rating score: 4. rating score: 4. rating score: 4. rating score: 5. <BRK>This work analyzes the effect of co distillation for distributed training under moderate batch sizes. Using distillation like techniques to improve synchronous SGD training is an interesting direction. In addition, the writing is good and easy to follow. W2: The contribution of this work seems to be incremental. Considering distillation / co distillation s regularization effect is not exactly a novel direction. W3: Experiments could be done on top of stronger baselines. W5: if the paper s goal is to demonstrate co distillation as a viable distributed training alternative, it would be also valuable to compare it with other related training methods like gossip SGD etc.<BRK>Comparable papers that focus on empirical evidence usually test their hypotheses on five or more models, in order to demonstrate that the technique is truly generic. This information should be clearly presented. Even if no theory is presented, I imagine the paper could have presented at least a rate function with controllable hyperparameter (similar to learning rate), and systematically conducted experiments for different values of the hyperparameter.<BRK>This paper aims to have a closer look at the role of codistillation for distributed training. Authors provided an answer with their empirical observations. Then, the authors claim that the codistillation may over regularize and study how to modify the training configurations to avoid it. Deep understanding on how it takes effect will help us better trade off the bias and variance in distributed training. 1) It is common sense that, regularization may “over regularize” and can “reduce overfitting”. Did I miss anything? The technical contribution need be highlighted.<BRK>> The results are promising and the experimental results seem quite comprehensive. This paper studies an alternative, called "codistillation". I do think that the paper is on track towards an interesting discovery, but I would like to see a deeper/more detailed analysis to be convinced. Pros: > The paper s results are certainly intriguing and likely to lead to further investigations of codistillation as an alternative or complementary approach to local SGD for reducing communications in distributed training.
Accept (Poster). rating score: 9. rating score: 7. rating score: 6. rating score: 6. <BRK>I maintain that this is high quality novel work that contradicts a widely held belief within the field and as such is a clear accept. The authors propose a systemic study of this relationship and conclude that both shape bias and increase corruption robustness are both byproducts of style variation during training, that is they share a common causal mechanism, but that shape bias does not itself cause corruption robustness. In addition to the elegance of the result, the authors use of “texture randomization via texture feature randomization” is a an elegant (and much faster) implementation than regenerating an entire dataset with many different textures.<BRK>The paper disproves the hypothesis that addressing shape bias improves robustness to corruptions of neural networks, which has been stated by the previous studies [1, 2]. The paper demonstrates that the degree of shape bias of a model is not correlated with classification accuracy on corrupted images via experiments. In addition, this paper shows that through fine tuning the affine parameters of the normalization layers, a CNN trained on original images can achieve comparable, if not better, performance than a CNN trained with data augmentation. Also, the authors visualize the results that explicitly compare the models’ accuracy on corrupted images. The authors present an interesting analysis towards Stylized ImageNet (SIN) dataset by separating the dataset into different factors that are used to generate SIN. This provides insightful perspectives on shape bias and corruption robustness.<BRK>Summary of the paper:This paper tries to study whether increasing shape bias of a neural network trained with imagined will make it more robust to common corruptions such as gaussian noises. The paper falsified this point by producing a data augmentation method which leads to more shape biased network yet less susceptive to common corruptios. Strength:The paper does provide a counter example to the common hypothesis that increase shape bias can lead to more robust network. This provides insight to future researches on understanding how shape bias and texture bias affects on neural network robustness. Since I’m not an expert in this field, I will recommend borderline scores to hear about the authors’ response.<BRK>The studied direction is important for understanding the learned representation of CNNs. The proposed points, using edge maps and style randomization, are simple and effective for making CNNs focus on shape information. Also, the discussion with the common corruptions is intriguing and helps us better understand the relationships between the shape based representation and the robustness. This is a little bit counter intuitive due to Stylized edge only contain more uncorrelated texture information. The reviewer wonders if the quality of the Edge data is not very high and result in the phenomenon. after rebuttal I ve read all reviews and the rebuttal, and thank the authors for their efforts and extra experiments.
Reject. rating score: 4. rating score: 6. rating score: 6. rating score: 6. <BRK>Experiments on the NAS bench 101 and NAS bench 201, as well as on the NASNet search space to validate the effectiveness of the method. The proposed method is conceptually simple and efficient. It gets a small performance improvement by learning in a maybe better subspace. In Fig.1, why is there a sudden increase for the strong predictor (at near 15625 samples.) Will it still work?<BRK>Summary of contribution: The authors propose an interesting approach to address the sample efficiency issue in Neural Architecture Search (NAS). Compared to other existing predictor based methods, the approach distinguishes itself by progressive shrinking the search space. The author provided an explanation of how their algorithm works, evaluate the algorithm on both NASBench 101 and NASBench 201, and show their methods work in practice on bothSome critiques:1. If the authors believe the development in BO community is wrong, please justify that using acquisition is indeed not important, i.e.exploration is not necessary, with extensive experiments. Getting a good results on CIFAR10 and ImageNet can be tricky.<BRK>Overall this work moves into the right direction of trying to improve the performance of predictors. Pros:  The paper provides experiments on different datasets, and evaluates different predictors to validate the approach. The approach seems to have a significant speedup on the search time, but I would also like to see the results in terms of GPU days which is a common metric. A broader comparison should be made with DARTs approaches that have been shown to be generally faster.<BRK>The paper proposes an idea of jointly optimizing the sampling policy and predictor in NAS. With this method, we avoid to train a predictor which performs well on the whole search space and search a model with less queries. ### advantagesThe authors propose that training a good accuracy predictor for models in the whole search space is difficult.
Reject. rating score: 4. rating score: 5. rating score: 6. rating score: 7. <BRK>The paper trains hard attention for image classification. To propose these locations, the method needs an already trained image classifier conditioned on glimpses and their locations. It is visible that the paper required a lot of work. Unfortunately, the method is not very practical and the method is not better than simpler alternatives. Main cons:1) The greedy minimization of the expected posterior entropy is not optimal, if additional glimpses can be done in the next steps. 2) The searching in the pre generated dataset of images is not practical. It would work worse on more diverse images. The method was tested only on faces and birds. 3) If the aim is to speed up training of new networks, a simpler alternative is to distill the attention locations and classification outputs from a pretrained Recurrent Attention Model (RAM+).<BRK>This paper introduces a way to annotate a glimpse sequence for an image. Using the obtained annotations, hard attention can be trained with a partially supervised way. My concerns mainly focus on the motivation and evaluation. Specifically, the authors claim using hard attention to do image classification may enable the use of modern approaches to computer vision in low power settings such as mobile devices. But from the paper (besides the computation of glimpse sequence is computationally expensive.) at the end, the proposed method needs to run the glimpse network several times, and the structure of the glimpse network is similar to standard classification network, which needs only a single forward pass on the whole image. Will this lead to an unfair comparison or data leakage? It s better to include standard datasets for image classification such as ImageNet, CIFAR, stc.<BRK>This paper presents a learning framework for a hard attention mechanism. The glimpses captured by the attention mechanism are guided by the goal of minimizing output uncertainty for a downstream task such as classification. The authors pose this problem in a probabilistic framework which is based on Bayesian optimal experimental design (BOED). They devise a tractable approximation to the entropy over images and glimpse sequences and search for the glimpse sequences which minimize the output entropy of a recurrent classification model. With the ability to incorporate supervised glimpse training into the attention mechanism, the authors are able to alleviate the cold start issue which many hard attention models over images suffer from. While the probabilistic formulation described in the paper is fairly detailed, a figure showing how the attentional variational posterior ($g_{AVP}$) is integrated into the rest of the attention model in Figure 1. Is $g_{AVP}$ trained through some form of model distillation with the attention model? CUBs annotated features were not intended to be a glimpse sequence, so how was the order of the annotations chosen?<BRK>This paper frames hard visual attention as a Bayesian optimal experimental design (BOED) problem from which the optimal locations to attend to are those with greatest expected reduction in the entropy of classification. With the methodology from BOED literature, this paper approximate the optimal behavior to generate  near optimal  sequences to partially supervise the learning of hard attention. Overall, I think the paper is easy to read, the proposed idea is solid and well motivated, although the actual implementation seems to be quite complicated which involves additional two different models (attentional variational posterior and image completion model). It is mentioned that the mask is concatenated as an additional channel. How can we make sure we can predict the label correctly from a random glimpse? Do we need to train a different $r_{img}$ first every time before testing on a new dataset? For instance, as the supervisions are generated to approximately maximize the entropy reduction, it is still possible that such attention is not optimal for neural network to predict the final label.
Reject. rating score: 5. rating score: 5. rating score: 6. rating score: 6. <BRK>It proposes to use Hamiltonian Monte Carlo (HMC) to sample the next states (instead of IID samples) and matrix completion to learn a low rank Q matrix. Do Fig.2(a) to (c) correspond to the same (optimal) actions? 1.It can be applied to only tabular problems with known dynamics, which is restricted as many control problems are continuous, and their underlying dynamics are unknown. Q learning can be applied in the RL setting, but not the method proposed in this paper.<BRK>Strengths:1) To my knowledge, this is the first credible effort to apply Hamiltonian Monte Carlo to Q learning in order to avoid some degree of the random sampling required for its almost sure convergence. At least the algorithm novelty is apparent. Weaknesses:1) The preliminaries section is disjoint/fragmented. This overall disjointedness then makes the conceptual innovation of this work more mysterious to understand, which is a concern. I strongly suggest the authors consider better explaining the links between step (4) of Algorithm 1 and proximal methods in any revision of this work.<BRK>This paper studies appling Hamiltonian sampling to computing Bellman equation (thus Q learning iterations) approximately. How is this possible in our setting? This is the most confusing point from my point of view. Why does the Q table have low rank property? I think at least this assumption should be justified empirically, otherwise, the theoretical result doesn t make any sense. As a result, this may be very hard to verify rigorously. But at least some preliminary evidence should be presented.<BRK>In this paper, a Hamiltonian Q learning is proposed by combining Hamiltonian Monte Carlo with matrix completion. The so called Hamiltonian Q learning takes minimization optimization and essentially claims an equivalence of energy in physics model, which might not always make sense. More environments are also necessary. 3.It is not clear that how accurate it is to assume the Q table to be a low rank matrix. Therefore, I am not convinced by the contributions.
Reject. rating score: 3. rating score: 4. rating score: 4. rating score: 4. <BRK>## Summary of the WorkThe work propose a method which allows us to synthesize meta RL and meta IL, by pre training and conditioning a context based off policy meta RL algorithm on imitation data. It is not immediately clear what the authors mean by "task families." The authors note that they augment the demonstration data with "imperfect demonstrations", but are silent about how this is achieved (and it must be done with great care). Related Work* Meta learning and meta RL far predates Wang and Duan.<BRK>Summary: This work seeks to efficiently learn new tasks by combining meta RL and imitation learning (IL). Strengths:  This paper studies an important problem: how can we quickly learn new tasks? This paper claims that PERIL is capable of exploring beyond demonstrations, but the tasks that this paper evaluates on don’t seem to require much sophisticated exploration. The experimental results are generally quite encouraging. This seems to be a fairly restrictive assumption, since $z$ is learned by PERIL, and therefore, it seems unreasonable for an expert policy to also be able to condition on $z$.<BRK>## Summary of workThis work proposes PERIL, a method for combined Meta Imitation Learning and Meta Reinforcement Learning using context based meta learning. They are using rewards, so they should be able to do better. The data from the expert demonstrations and trajectories are used for meta learning updates. This is not a standard term.<BRK>Summary:This paper introduces PERIL, a meta RL method that combines demonstration trajectories and trajectories collected by the policy, in order to adapt to a new task. Therefore, even though the idea seems promising, I think the paper is not quite ready for publication. I think this is a central question that should be very clearly answered in the paper. That s an important difference!
Accept (Poster). rating score: 8. rating score: 7. rating score: 7. rating score: 6. <BRK>This paper leverages the basic building blocks and conducts a block wise quantization. Nice results are obtained with the proposed method. The proposed method is cheap to implement and pushes the post training quantization to 2 bit. This problem may inspire the community to find a better measurement in future work. (it may be impossible to the full Hessian matrix for the whole network).<BRK>I couldn t follow the method described in the paper. The authors are basically trying to address post training quantization by perturbing the the weights of a trained DNN. The authors draw a link between this optimization problem and optimizing for the "reconstruction" of the output activations of a block (see Equation 7). The method only seems to work on ReLU networks, so it s restricted to CNNs, but this is still extremely useful. The experimental results are very strong.<BRK>This paper explores the post training inference quantization. 5).It would have been nice to verify this approach on tasks other than vision, such as speech and NLP tasks. The authors did comprehensive comparison with SOTA approaches, not just PTQ, but also quantization aware training and mix precision frameworks. I have some questions wish to be clarified.<BRK>This paper proposes BRECQ which is a new Post Training Quantization (PTQ) method. This does not seem to be correct/necessary at least for uniform quantization. Did the authors consider other bit precision settings? While the paper is trying to address an interesting problem and there are a lot of empirical results, but I had a very hard time to follow the paper s main idea and the "final" proposed algorithm.
Accept (Poster). rating score: 7. rating score: 6. rating score: 6. rating score: 6. <BRK>The paper shows that it is possible to fool exact verifiers using numerical instabilities. It proposes network architectures that can exploit numerical issues in order to get certificates from verifiers based on architecture such as MIPverifiy and that can at the same time accepts adversarial examples within the certificated epsilon ball using a simple trigger. This raises important security issues and as the author suggest, I do believe that such problem car arise in many situations. The problem is first put into light on a very simple architecture and then on more complex ones and then with a added backdoor on existing network. * about the defence, I wish there were more experimental results with different epsilon values, to have a better intuition on the global behavior of the defence.<BRK>The authors show that certain complete neural network verifiers can be mislead by carefully crafted neural networks that exploit round off errors, which when large magnitude values overwhelm low magnitude values. Also, it obfuscates the experimental results, of which there are quite a few in the paper. For that alone, I think this paper merits attention, even if  numerical errors can mess up neural networks  is a well known fact. I believe that the method is this work leads to arbitrarily large differences, but I think this is something that should be explicitly explored.<BRK>This paper argues that, although existing complete neural network verifiers can provide some guarantees on the robustness, these verifiers have overlooked potential numerical roundoff errors in the verification, and in such cases the provided guarantees may be invalid. They also showed it is possible to insert a backdoor to the network such that the backdoor is missed by the verifier while it can trigger some behavior desired by the attacker. This can be important to ensure the robustness of complete verifiers against some potential adversarial networks or backdoors. * The authors demonstrated the existence of the numerical error problem via constructing adversarial networks to fool complete verifiers. Additional comments:* I find Sec.6 is probably not very consistently and clearly written.<BRK>The authors show how to make their networks look a bit less suspicious and they discuss a way to detect neural networks that have been manipulated in the way they suggest. However, I think works like the current one are important to publish such as to practically demonstrate the limitations of the "guarantees" given by certain robustness certification systems and to motivate further research. The related work is incomplete. The technical sections are written well enough to be understandable, and the main technical contribution is a pattern of neurons we can insert into a neural network in order to make it behave in an arbitrary way that is invisible to the considered verifier. The paper might benefit from a discussion of this possibility and an explanation why it was not attempted. The new section 2.4 is appreciated, though it seems the paper still does not say that incomplete methods can deal with round off error by sound overapproximation.
Accept (Poster). rating score: 7. rating score: 7. rating score: 6. rating score: 6. <BRK>Paper is about the problem of minimizing a convex function R(A) subject to a rank constraint rank(A)< r Paper builds on an algorithm (GECO, for Greedy Efficient Component Optimization) proposed by Shalev Shwartz et al.(2011), which was already proven to find a solution to the problem in:O(r* kappa_{r+r*} R(0) / epsilon) where kappa is related to the function s condition number.<BRK>This paper studies the problem of rank constrained convex optimization. One clarification: is the Greedy algorithm (Algorithm 1, with Optimize rather than Optimize_Fast) exactly the same as in Shalev Shwartz et al.(2011)?If so, this could be made more clear, especially since the implication is that the benefit for this case appears to be just in terms of the improved analysis for a logarithmic dependence.<BRK>This is a fairly general problem that contains several special cases such as matrix completion and robust PCA. The arguments in the analysis look sound to me. For matrix completion, the authors compared their approach to SoftImpute (Mazumder et al 2010), which is a well known approach in this literature.<BRK>For example, it is claimed that the analysis of Theorem 1 improves over [Shalev Shwartz 2011], but it is not clear where the improvement comes from.
Reject. rating score: 3. rating score: 4. rating score: 4. rating score: 4. <BRK>The authors report the correlation between the different metrics. **Recommendation**I am recommending that this paper be rejected on the basis of lack of appropriate evidence for their claims and inappropriate use of language to describe curiosity, a word with a diverse history in the literature. However, the experiments are not appropriately designed to provide reliable results and the paper includes substantial errors in understanding the existing literature, and as the paper is essentially an empirical survey, appropriately representing the other literature is critical.<BRK>The authors propose that such task agnostic can be used as intrinsic signals for training RL agents when task reward and human data are not available in an environment. The analysis of these metrics  correlation with human data is still an interesting piece of result but is not significant enough to become the sole contribution of an ICLR paper.<BRK>* *Downsampling:*These task agnostic metrics rely on the downsampling of the frame. However, the methodology of the current version of this paper is not good enough. In order to update my score, I would need a more rigorous correlation study that asserts the significance of the correlations (using corrections). All details required for reproducibility are present and the code will be released.<BRK>There is a lot of repetition in the text so it should be possible to be brief and concise but add important results to the main text. The curiosity and exploration is an important topic for RL research and we need more in depth analysis of existing methods. The difficulty I ve with the paper is that it s not clear what exactly you re after here. Section 3.1, discretisation: What is the effect of the choice of 8x8 on the overall results?
Accept (Spotlight). rating score: 9. rating score: 7. rating score: 7. rating score: 6. <BRK>The paper presents a new approach to multivariate probabilistic time series forecasting. The authors propose an interesting workaround by trying to learn the shape of the output distribution directly with conditioned normalizing flows. *Originality*To my best knowledge, the proposed approach is new. The multivariate forecasting models suffer from the problems with scaling and the paper’s proposed combination of transformers with conditioned normalizing flows has the potential to overcome this problem. Pros* Conditioned normalizing flows can ease the design choice of the output distribution* The method scales well in terms of number of time series to forecast* Great empirical resultsCons* Since the scaling is a big issue for multivariate models, it would be useful to compare training/inference times between the evaluated methods.<BRK>The paper proposes a method to provide probabilistic forecasts of multivariate time series taking dependencies between series into account even for large dimensions. The complexity of the models are O(TD^2) or O(T^2D) but the GP copula baseline is O(TD). The time complexity given for RNN (O(TD^2)) is misleading given that this is a naive approach, for instance the GP copula methods have O(TD) runtime with a RNN. The transformer method may still be faster due to parallelism but to make an efficiency claim, the runtime should be measured given that the theoretical complexity is higherDespite the accuracy improvement and the fact that the paper is well written and motivated, I am currently not inclined to accept it given some ambiguities raised by the experiments but I believe those points could be clarified in the author rebuttal.<BRK>There is not much novelty from a theoretical perspective either, as the model is mostly a combination of two sub models; a normalising flow architecture and an autoregressive component. The experiments are run several times and results are reported with standard deviations   excellent! Indeed, the baseline methods chosen for comparison in the paper are both reasonable and strong. In conclusion, the authors have satisfactorily addressed my concerns with the submission. # SummaryThe paper proposes to use normalising flows for probabilistic modelling of multivariate time series. # QuestionsComments that will affect my score:1. Combined, the two parts of the model allow for probabilistic trajectories to be sampled for predictions into the future. # Evaluation## Strong points* Using normalising flows for multivariate time series modelling is a good idea. Minor questions not likely to affect my score:1. The proposed model is certainly interesting, and the idea of using flows to correlate time series is very good.<BRK>This work explores combining an RNN and a neural density estimator for forecasting in multivariate time series. In addition, variations of the architecture with attention and other density estimators are examined. The architecture, RNN+MAF and variants, is evaluated by CRPS score on several datasets. I have a feeling that the motivation and the  related work are incomplete. * there was work on forecasting time series with uncertainty using end to end deep learning architectures (rather than involving GP GCP etc) which the paper does not cite. For example, https://deepai.org/publication/sequential neural models with stochastic layers and other variants of stochastic RNNs, as well as neural processes, https://arxiv.org/abs/1807.01622. I would expect them to be visually indistinguishable on such a simple model.
Reject. rating score: 3. rating score: 3. rating score: 4. rating score: 5. <BRK>The author compared the proposed method with SiamFC++ and SiamCAR in experimental results, but the review of these methods and explanation of difference in methodology are missing in this paper. 3.The comparison with SOTA methods are not comprehensive. 2.The contribution of this work is rather limit. The resolution of Figure 1 is rather low, and the author did not even explain which one corresponds to the challenge situation of three aspects in figure caption.<BRK>Overall, this is "yet another siamese tracker", which is not sufficient for ICLR acceptance. There is no significant insight, training, updating novelty or theoretical. The performance of the tracker is evaluated on UAV, VOT 2018 and VOT 2019.<BRK>#### 2.strengthsThe experimental results show that this method has excellent performance on several public benchmark datasets. #### 3.weaknesses  The novelty of the paper is deficient , the proposed method such as cross attention mechanism [1], anchor free regression [2, 3]  have been previously exploited in existing models. In experiments part, there are missing some important algorithms to compare, such as SiamBAN which is the baseline method for this paper. SiamFC++: Towards robust and accurate visual tracking with target estimation guidelines. Besides, in figure 4, the arrangement of the pictures does not seem to match the description text. #### 6.Relation to prior workIt is an incremental work based on the prior work.<BRK>+.The final results are good, and ablation study shows the effectiveness of these modules. ".However, Wang et al.(2020b) did not use spatial attention module. The technical contribution is weak. The main components, including channel attention, spacial attention and the anchor free network, are not new.
Reject. rating score: 3. rating score: 4. rating score: 4. <BRK>The authors propose a neural architecture search (NAS) algorithm inspired by brain physiology. My question is: What is the problem with using biologically implausible NAS methods or with the fact that some approaches are incompatible with current understandings in neuro biology? For instance, as pointed out by LeCun, in the case of airplanes, we developed aerodynamics and compressible fluid dynamics, and we figured out that feathers and wing flapping were not crucial. I would encourage the authors to rewrite the paper in a clearer way, identifying the core contribution and challenges, as well as the motivation and the rationale behind the approach they use. This seems a severe limitation in the neural architectures it can generate.<BRK>The goal of the proposed Bractivate is to search an architecture from the predefined search space which performs well and efficient, as the proposed efficiency loss simultaneously constrains the parameter and training cost of candidate. However, the proposed method misses many crucial details, which may make the empirical results not convincing to me. My major concerns are as follows:1) The paper did not provide the micro architecture of model genotypes, so the initialized search domain is ambiguous. 2) The article did not give experimental results or explanations to prove the rationality of the formulation in Eq.1.Why the block with max (Ab) should be chosen? And what the difference if the algorithm chooses another block? 3) The GPU hours it takes to finally search an optimal architecture. Besides, there’s a strict upper limit of **8** pages for the main text of the initial submission this year.<BRK>This idea is of great interest. You claim that the healthy have more dendritic branches in their brain, but the differences between their somas are not discussed. Thus, it may enough to only add skip connections on a network to improve its performance. You should explain the meaning of the color. In caption, you should explain the effectiveness with more detail. However, this paper has too many problems to be solved, as mentioned above.
Accept (Poster). rating score: 7. rating score: 7. rating score: 7. rating score: 6. <BRK>This paper proposes a reinforcement learning algorithm FMDP BF for episodic Factored MDPs. Compared to previous results for FMDP, the proposed algorithm utilizes Bernstein type confidence bounds and new bounds on the factored transitions to reduce the regret order. The paper also connects a class of constrained RL to FMDP and  provide a sample efficient for the constrained RL problem. A regret lower bound is also provided which matches the upper bound except for a $\sqrt{n}$ and logarithmic factors. In the proof of Lemma F.2, the constant in the log term is not consistent with Lemma D.2. The result seems to be correct by first applying the inequality.<BRK>This paper focuses on episodic, factored Markov decision processes and proposes the FMDP BF algorithm, which is computation efficient and improves the previous result of the FMDP algorithm. The author also provides a theoretical lower bound for the FMDP BF algorithm and shows that the FMDP BF algorithm s regret is near optimal. However, I still have some suggestions about this paper. It is better to add some examples for the factor MDP setting.<BRK>This paper studies RL in episodic factored MDPs (FMDPs) in the regret setting. The paper introduces an algorithm called FMDP BF, which is a model based algorithm implementing the optimistic principle by maintaining upper and lower confidence bounds derived using empirical Bernstein type confidence sets. The paper presents a high probability regret bound for FMDP BF appearing to be superior than existing results for episodic FMDPs. It also reports a regret lower bound for FMDPs. Applying FMDP BF to this (originally non factored) RL problem, the paper derives tighter regret bounds than what would be obtained otherwise. The review of the relevant literature seems adequate. Currently, there is only comparison against (Osband and Van Roy, 2014). A precise and specific comparison is necessary here. Could you please explain whether this can be improved or not?<BRK>They provide two main contributions on this question. Overall, based on my evaluation on the theoretical and practical impact of the paper, I find that the contributions fall marginally below the acceptance threshold. The second contribution is on the generalization to an episodic FMDP with knapsacks problem, where the authors generalize the approach in the first contribution to provide a regret bound. While I understand that there are quite a lot of technical calculations involved, the improved regret bound compared to Osband & Van Roy (2014) appears to be a direct outcome of replacing the confidence bounds used in Osband & Van Roy (2014)  by the confidence bounds proposed in Azar et al.2017 on each of the factors in the factored model.
Reject. rating score: 4. rating score: 4. rating score: 6. <BRK>The *identification* phase is based on clustering classification failure regions in a GAN latent space and the *correction* phase is based on fine tuning the classifier with additional synthetic samples from the GAN. The most problematic point, in my opinion, is the only marginal improvement on the test data, indicating that the suggested training method only improves the specific "failure scenarios", making it is similar to adversarial training methods used to gain adversarial robustness.<BRK>The authors present a system DEFUSE which is geared towards identifying and correcting classifier performance when labels are assigned incorrectly. There are three phases that are used to design DEFUSE: (1) Identify unrestricted adversarial examples using Variational Auto Encoders (2) Use a clustering approach to distill the above examples into failure scenarios and (3) Correct the classifier predictions. How does this choice compare with other clustering approaches?<BRK>In the examples provided in the paper, the authors state that they used 5 workers (annotators) and the majority vote was used to decide the final label. The experimental results show that the proposed technique finds significant failures in all datasets, including critical failure scenarios. After correction, the performance of the method improves. An interesting aspect of the method, which distinguishes it from similar techniques, is involvement of users/experts in the training process to indicate the classification errors in order to improve the performance of the method in the future.
Accept (Poster). rating score: 7. rating score: 7. rating score: 6. rating score: 5. <BRK>This paper proposes a novel deep approach to the learning of spatio temporal point processes via normalizing flows. The NHP and the RMTPP are also able to model the spatial temporal point process if the losses are change to the metrics on Euclidean space. It seems that the proposed model contains a jump CNF for the mark probability $p^*(\boldsymbol{x}_t|t)$. It is not very clear how the authors incorporate the attention mechanism to CNF.<BRK>The paper proposes a neural ODE based point process for spatio temporal data. Under the general framework, three particular variants are proposed: they handle data with different characteristics and have different computational efficiency. The presentation is clear; some technical parts are enjoyable to read. The empirical results on log likelihood comparison look compelling. For temporal comparison, there isn’t any neural baseline model. Can authors appropriately acknowledge these connections?<BRK>The paper is generally well written. Or what are the flow parameters that needed to be estimated. How  will the neural network structure look like? The GRU model is not well elaborated and is a little unclear. It s good to elaborate on them.<BRK>This work investigates a new class of parameterizations for spatio temporal point processeswhich uses Neural ODE to enable flexible, high fidelity models of discrete events that are localized in continuous time and space. Also, the jump CNF can model the abrupt change of the spatial pdf but the Time Varying CNF and Attentive CNF cannot.
Accept (Spotlight). rating score: 9. rating score: 7. rating score: 7. <BRK>This paper is to propose a distributional sliced Wasserstein distance to address the limitations of standard SW and Max SW. This reviewer personally enjoys reading this paper. The authors need to discuss it in details.<BRK>Extremes of the family are the sliced Wasserstein and max sliced Wasserstein distance. How many parameters are used in the neural network defining $f_\phi$? Strengths:Overall the paper is a clear and original contribution to the field of sliced Wasserstein distances. beginning new line.<BRK>Observing that not all the directions are meaningful, the authors propose a variant of SW where the expectation over all the directions is replaced by an expectation over a distribution of directions. Nevertheless, I believe this question is out of the scope of the paper, and I changed my final rating accordingly to the new version of the manuscript. I really believe this paper could also lead to interesting comparisons. All in all, the presented method is a variant of SW that builds on several previous works exploring a similar idea (how to better sample the directions), such as max SW or subspace robust Wasserstein distances. As such, it can be considered relatively incremental, but this should not totally prevent publications if the computational benefits/performances are very good. Vol.108.2020.), plus it does not have the limitation/artefacts of the sliced Wasserstein.
Reject. rating score: 5. rating score: 5. rating score: 6. rating score: 6. <BRK>But how we do know? What is more, Tank et al do very well on Lorenz 96 too. # 5.1It is not okay for you to bold ACD s performance value in table 2. You may want to add that key word as that says a lot to people from outside this field, and again, emphasises the limitations of this causal discovery (and most in fact) method. ## 5.2.2This experiment is very interesting. The figures are good too. # Experiments## 5.1It would be helpful if you could show the inferred adjacency matrix of the particle dataset, alongside the GT adjacency matrix. These experiments feel very selective. Your goal is to demonstrate ACD s superiority over the other methods, granted, that s the process and we all know it.<BRK>Although lacking in detail, it is very clearly written. Possibly, the greatest weakness of the NRI approach in this formulation is the seeming reliance on relatively smooth noiseless dynamics. That would not be a problem if the paper would manage to indeed demonstrate that the model is useful for causal discovery in multivariate time series. The experiments are limited to very specific kinds of signals and thus not convincing of the approach generality. It would be simpler to say that indeed an unrolled representation of a graph of limited Markov order is used.<BRK>I am not sure whether that approach is promising or not but it is expected from an experimental work to handle a more general setting. What if there are some small changes in the dynamics across different training samples. 4  About handling latent confounders: As noted by the authors, recovering causal relationships is challenging in the presence of latent variables. 5  The presentation of the paper could be improved:*Please provide the exact definition of $\hat{\mathcal{G}}_x$, $\mathbb{G}$, $\mathbb{X}$, $r$,....*Please explain the two methods (the amortized encoder and TTA) in more detail on page 4. "Invariant causal prediction for sequential data."<BRK>This paper studies the problem of learning causal graphs from time series data. Overall, I think it is a very interesting idea to consider shared dynamics and, in some sense, to  learn a score function  from data, compared with existing score based methods that usually need to put assumptions on data generations to make the score function sound. I would be happy to increase my score if authors can address my concerns. Identifiability and Consistency: (a) identifiability is an important problem to causal discovery from observational data. (c) Another part, related to this learned score function, is whether the overall approach is consistent. Writing: the overall writing is very good, but could be improved for some places. I do not see why ACD does not apply to this case.
Reject. rating score: 3. rating score: 3. rating score: 4. rating score: 5. <BRK>The paper show results on the Open Graph Benchmark of several existing methods, and concludes that GAE cannot learn structural link representations while SEAL can. However, I felt the lack of a contribution on the paper. Pros:  Clear explanations of existing work. In your experiments, due to the way the paper presents the label trick as the novelty, I was expecting to see the boost of performance of such trick on the existing architectures. Minor comments:  "Following (Srinivasan & Ribeiro, 2020; Li et al., 2020)" should be a textual citationOverall rating:Due to the main problems stated, I cannot recommend the paper for publication at ICLR.<BRK>The authors tried to explore the key differences between two link prediction methods. It can be seen from the experimental results in two SEAL papers[1][2] that the node embedding does not help improve performance. [1]Inductive Matrix Completion Based on Graph Neural Networks[2]Link Prediction Based on Graph Neural Networks However, some statements are not precise.<BRK>For example, the work of Xu et al (ICLR 2019) draws on the Weisfeiler Lehman graph isomorphism test to develop a theoretical framework in which to analyse the expressive power of Graph Neural Networks. The analysis in this paper extends the theory with respect to the link prediction problem and provides theoretical justification for the strong performance of SEAL on link prediction benchmarks. 1.The main methodological contribution in this paper is the concept of the labelling trick, which can be used to improve the representational power of GNNs and explain the strong performance of SEAL.<BRK>The paper focuses on the link prediction task for graph neural networks. More specifically, it compares GAE and SEAL by providing theoretical evidence why GAE is not able to learn structural link representations, which as a result leads to suboptimal performance in the link prediction task. 3.Xu, Keyulu, et al."How powerful are graph neural networks?." More dataset can be found here: https://paperswithcode.com/task/link prediction5. There are also existing work that combine the idea of knowledge graph embedding with GNN models in link prediction [1].
Reject. rating score: 4. rating score: 4. rating score: 5. rating score: 5. <BRK>It was awesome to see the authors willing to respond to the various suggestions and questions that the reviewers provided. The paper has improved over time but some significant areas of improvement remain. It would be nice to have that analyzed. Overall, the discussion about path probabilities is confusing. I do however urge the authors to continue in this line of work. It would ve been nice to see a more dedicated comparison between CDT and the specific approaches in the literature.<BRK>Edit:I have read the authors  response and the other reviews. The authors use "heuristic agents" as experts in their experiments. This suggests that discretization of CDTs must be performed in a more nuanced way than otherwise stated in this work. In the RL experiments, the authors compare to a MLP. Without this comparison, conclusions cannot be drawn about the "parameter vs performance" benefits of CDT.<BRK>Although the authors provide some preliminary experiments on the tree depth in Figure 5, it is unclear why only $1+2$, $2+2$, $3+2$ are considered in CartPole v1 and $2+2$, $2+3$, $3+2$, $3+3$ are considered in LunarLander v2. The key novelty of this paper is to add a low rank representation learning, i.e., feature learning tree, before the decision making tree. Therefore, it is expected that the number of parameters of CDT is less than that of SDT.<BRK>This is a very detailed suggestion but the gist is that the presentation of this table could be much improved. Do the authors have any explanation for why this is the case? However, I found the figures hard to read, and the heatmap in the tree nodes also needed more explanation, which made me further concerned about the interpretability of the feature representation part of the trees. While the trees being proposed may be more compact, are they more interpretable?
Reject. rating score: 2. rating score: 4. rating score: 5. <BRK>Title:  The title is misleading. The title claims that the proposed model is for "Automatic Music Production". Did they use a pre trained model? Music Production involves many other tasks like mixing, mastering and so on, none of which are a part  of this study. Do the artefacts not interfere with the  downstream task? There are no details about the CycleGAN used in the paper. The authors mention that they use the Demucs algorithm for source separation. I think the description needs to be significantly more rigorous. "Nevertheless, only raw audio representation can produce, at least in the long run, appealing results in view of music production for artistic and commercial purposes."<BRK>They report promising results on the first task; however, the model is not as successful on the second (more challenging) task. The problem is challenging, and meaningful solutions may bring innovative and creative solutions to music production. The literature is well covered, with a few missing citations (see below). While adapting past music generation work for arrangement generation is not trivial, the authors could have still used variants of CycleGAN and other unpaired image to image translation models for comparison. Therefore, I would strongly encourage the authors to build upon their existing work and re submit the revised paper to ICLR or another conference such as ISMIR. Specific comments   As mentioned above, the authors should have added more "experimental settings." "While showing nice properties,"  The authors only mention that Demucs solve audio source separation (for the data the authors use) and the algorithm is time equivariant. However, the text reads like the authors would like to state other properties as well.<BRK>The authors propose a CycleGAN model to learn transformations between source and accompaniment domains. I ask because having listened to the provided examples, it sounds like there could be some modulation artifacts in the reconstruction that could be traced to the choice of window function. That said, I do think there are areas in which this paper could be significantly improved, both in terms of experimental design and exposition. The experiments presented here make use of both pre separated stems (MusDB) and automatically separated signals produced by DEMUCS on the FMA dataset. 3.It is not demonstrated that including the FMA data is necessary or beneficial for this task (though it s not unreasonable to expect that this is indeed true). However, many technical details are omitted that make it both impossible to reproduce and difficult to interpret. Figure 1 suggests a log scaling, but does not provide details.
Accept (Poster). rating score: 6. rating score: 6. rating score: 6. rating score: 5. <BRK>The paper introduced an unsupervised approach for 3D physical object segmentation from the video. The proposed algorithm decomposes the scene into several 3D primitive shapes, and learn to generate latent descriptors for a generated model to reason the 3D properties. The system also enforces assumptions that object move according to physics, and eventually produce a scene representation in a self supervised manner. Experimental results suggest that the proposed approach is effective and the ablation study is helpful too. Moreover, the writings of this paper are clear and the visualizations and diagrams in the paper are informative and easy to follow. I have some additional comments regarding the paper:  In table 1,  the normalized cuts method is a very classical algorithm, but the performance is very good. I understand it s beyond the scope of the paper but the world also consists of many elastic objects and infants are likely to discover those objects well. I think the claims in those section makes sense, but I was wondering if authors can provide some comments on the performance of POD Net on more challenging scenes (maybe for more complicated backgrounds as a starting point) or if there is a way to compare with existing self supervised segmentation methods.<BRK>Summary:This paper proposes a deep network architecture to attack the important problem of discovering physical objects from observing videos only. The architecture is mainly an extension of the MONet architecture with a physics loss and a multi scale inference scheme. The physics loss encourages the inference module (that masks out the objects) to be more consistent with the observed motion in the scene. $\sigma$ is being overloaded (it was used in the dynamics model section). The method outperforms reasonable baselines on both a synthetic dataset and a real world dataset. Object discovery in videos as foreground motion clustering. However, I believe it might be worth mentioning other methods that discover objects by using motion, such as motion segmentation. Overall, I think the paper poses an interesting contribution with interesting results. The authors have clarified the equations, addressed the concerns of claims of no supervision, and provided more experiments and metrics to back the current set of claims. I do believe this paper to be interesting enough for an ICLR paper, and have updated my score accordingly. The method claims that the method is unsupervised. The text claims that the unprojection can be done assuming camera parameters and the plane height, but I don t see how this is the case. Questions:  Questions about the equations:      Please define $alpha_{\psi}$. Additionally, how many iterations is this? Can multiple objects be present at the same pixel location? If the math/text and claims of no supervision are fixed and/or addressed, I would be willing to increase my score. Eq (4):          I believe the LHS is missing notation: $p(\mathbf{x}_i | \mathbf{z})$, as this equation should be for a single pixel.<BRK>This paper proposes an unsupervised approach for discovering 3D objects from video. Given training videos with objects moving around in a scene, the paper learns to decompose the given images into segments. Each segment is associated with a 3D location (translation, rotation and scale) as well as 3D dynamics (linear and angular velocity), and latent vector capturing the appearance. The paper conducts experiments on videos rendered using the ShapeNet dataset, and videos of real block tower falling (from Lerer et al.). Lastly, the paper reports an experiment for using the proposed model to predict physical plausibility of video stimuli. This doesn t evaluate the other properties that the paper claims to infer (3D geometry and position of each object). There are a number of papers that tackle unsupervised edge detection (and consequently detection of object segments), see Isola et al.ECCV 2014, Unsupervised Learning of Edges, Li et al.CVPR 2016. A comparison to such unsupervised segmentation techniques should be made. Lastly, the paper is missing details and hard to read: a) at test time, is the segmentation done on a per frame basis or on the basis of the whole image sequence, b) what does the physics loss capture   is it trying to induce a first order motion (zero acceleration) on the object, c) why is the learned physics model not used to judge the physical plausibility, d) the paper assumes a parametric form for objects (3D cuboids), but baselines it compares to likely don t (eg: normalized cut doesn t), why are experiments in Figure 7 a fair comparison? In summary, I quite like the design of the unsupervised technique to learn about objects and their physical properties, I do not find the experiments convincing. Update: I thank the authors for providing additional 3D metrics, and comparisons to unsupervised 2D segmentation techniques. I have updated my rating.<BRK>The paper proposed an unsupervised learning model, POD Net, that learns to discover objects from video. The authors develop an inference model that performs image segmentation and object based scene decompositions on overlapping sub patches, and a generative model, which contains an unprojection step, a constant velocity dynamic model and a VAE, to reconstruct the original scene. With the novel dynamic model to predict motions for the 3D object primitives, the POD Net learns to segment objects with better physics. And in the approach that to unproject 2d masks into 3d primitives to compute the motion is novel to me, it allows the proposed approach to better discover objects with physical occupancy on the 2D videos. In particular, the intuition behind the method is described well. ++ Concerns:The main concern on the paper is that although it claimed itself as a 3D object discovery method, all its evaluations are done on 2D datasets with 2D metrics. Although there are some reasonable improvements shown on these metrics, we do not know what is the capability of this work in terms of recovering 3D segmentation mask and 3D pose. This I consider incomplete for a work that claims its main difference w.r.t.prior work to be getting to 3D object discovery. There have been a significant amount of prior work on unsupervised 3D object discovery (many of them on RGB D) that is missed by the authors:Herbst et al.Toward Object Discovery and Modeling via 3 D Scene Comparison. 3DV 2016exist and they provide ways to evaluate unsupervised 2D 3D object discovery (one can start from RGB and deduct 3D pose and velocity). So I don t think the authors have enough excuses to not show any 3D results. The author claims that the POD Net is an unsupervised method, meanwhile bashing other methods of using pre training (last paragraph of Section 1). However, their unprojection model and the project model is pre trained and it is the same kind of supervision as the pre trained segmentation models in other work. In Sec 3.1 and 3.2, it is not clear how the author counts object discovery performance w.r.t.the time dimension, e.g.how is it handled if the ground truth mask is completely occluded?
Accept (Poster). rating score: 8. rating score: 6. rating score: 5. <BRK>The motivation of this paper is clear. This allows others to follow their work and reproducing their experiments. Some mathematical introduction is nice, but they have to be directly related to the approach of this paper.<BRK>Finally, the writing of the paper can be improved. The paper proposes a simple fix to drawbacks of previous works of on the topic, which e.g.propose minimax approaches.<BRK>The revision addresses several of my concerns. Overall, the paper seems to carry several nice ideas, but the theoretical discussion needs to be significantly improved. *****Review update: I thank the authors for their response and for revising the paper based on the comments.
Accept (Poster). rating score: 9. rating score: 7. rating score: 6. rating score: 6. <BRK>This is a non synthetic dataset from the largest repository of proofs written by human experts in a theorem prover, which has a broad coverage of undergraduate and research level mathematical and computer science theorems. Based on this dataset, the model need to fill in a missing intermediate proposition given surrounding proofs, named as IsarStep. The paper mined a large corpus of formal proofs and defined a proposition generation task as a benchmark for testing machine learning models’ mathematical reasoning capabilities. The experiments and analysis also reveal that neural models can capture non trivial mathematical reasoning.<BRK>This paper presents a non synthetic dataset generated from the Isabelle AFP, the largest mechanised proof repository for the task of filling in a missing intermediate proposition given surrounding proofs. This is a serious problem to evaluate the paper. Is this aligned with the convention in the community?<BRK>Furthermore, in which sense is solving IsarStep "a first step towards the long term goal of sketching complete human readable proofs automatically"? I believe that the task proposed in the paper provides new insights on the weaknesses of deep learning models. I m not able to see why and how IsarStep can drive this advancement though.<BRK>The paper introduces a dataset for proposing intermediatelemmas/conjectures based on a repository of Isabelleformalizations. This is a useful task and such datasets are useful. To those cited, I wouldadd [1] based on the large E_conj dataset [2] created in 2017.
Reject. rating score: 2. rating score: 3. rating score: 4. <BRK>The paper is poorly written. This is especially the case for theproposed algorithm (the core contribution). This section is verydifficult to understand, and notations are awkward. However, the experimental results are quite well presented(to be compared with the beginning of the paper).<BRK>Summary:This paper proposes WordsWorth score (WW score), a score to represent the importance of the word obtained from the trained model. Then, the score is applied to the greedy attack proposed by (Yang et al., 2018). Despite the paper stating that this paper is based on the greedy attack (Yang et al., 2018), the contribution of this paper is limited to calculate the word score from the trained classifier and applied it to the greedy attack. If not, some studies or discussions about this representation should be included. The reviewer suggests the authors to review the paper several times before submission. This paper states that $F$ is the trained classifier.<BRK>This paper proposes a new and simple way to determine word importance for black box adversarial attacks on text classification models. While being an interesting paper, at a high level I am concerned about several points:  The paper is unpolished and at times hard to follow. Are the WW scores capturing anything that simpler statistical methods do not? Many implementation details are lacking, which could be an issue for reproducibility. In some cases, the difference seems around 0.1 / 0.15 absolute AUC.
Reject. rating score: 6. rating score: 6. rating score: 6. rating score: 6. <BRK>The direction is exciting, but even with the updates I believe this paper needs quite some work to improve the presentation and clarity, which is why I have not updated my score._ In terms of the result section, I think the paper could be improved. In a more recent paper Jumelet (2020) repeated this analysis also for BERT.<BRK>Such analysis using influence paths has been established in prior work, but the present paper expands on this work with definition of "multi partite" patterns, and by introducing a method for identifying strongly influential paths in the model. Overall, I think there are certainly interesting analyses that could come out of this work, but the current paper does not provide a clear enough contribution to be ready for publication. What output of the analysis will be used as an answer to the key questions?<BRK>Could you explain why is there a difference? The influence of the subject and the intervening noun is similar to that of LSTM (Lu et al., 2020). ### Recommendation:Overall, I incline toward rejecting. This paper provides an instrument to explain BERT, but I have a hard time understanding the result itself (influence paths or patterns). I also have a major concern with the final analysis and its findings. I think the author should provide a more rigorous analysis on the interpretability of the proposed method, as well as an explanation of BERT contextualization.<BRK>Also, I think the word "input" here is ambiguous: you have an "analysis input", which is $x$; you also have "model input", which are samples from $D(x)$. Although this method is in principle applicable to any DNNs, this paper is mostly focused on BERT. #### Weakness  It looks like the proposed method is largely based on (Lu et al, 2020); the major difference is the introduction of Multi partite patterns, which basically expands the objects of influence analysis from paths to patterns (partial paths). This doesn t look like a significant novelty.
Reject. rating score: 4. rating score: 6. rating score: 7. <BRK>##########################################################################Reasons for score:  Overall, I d vote for rejection for this paper. Hopefully, the authors can address my concern in the rebuttal period. Pros:	1.The paper is straightforward and clear to read. Cons:	1.The motivation of the proposed framework is not strong. Besides that, the improvement of constrained MO MPO is minor.<BRK>It is not clear to me if there is a novel contribution except for the idea to look at the Pareto front instead of a Lagrangian relaxation. I don t think that this is true.<BRK>The weakness of the paper is that the method is a bit incremental compared to MO MPO and that it may be hard to reproduce without code.
Reject. rating score: 3. rating score: 4. rating score: 4. rating score: 5. <BRK>In particular, the authors define a bidirectional RNN with attention by a fixed point equation and then transform it to a variant of the Transformer block. Pros: + This paper proposes a framework of transforming RNNs to Transformer style models by incorporating the attention mechanism. + Several ablation studies are conducted to support the design choices. The difference between the proposed StarSaber block and the Transformer block needs to be highlighted and discussed more clearly in the main content. The dataset and the pretraining task used for experiments are not commonly used and small scale. The paper is not very well written and is a bit tedious, for example:    1)	The related work section is not well organized and includes some unnecessary contents like the discussions about pretraining and MLM. 3.How do you tune the learning rate for all the models?<BRK>I still think there s no significant difference between vanilla Transformer. While it s hard to say whether it s better than multi head self attention in Transformer and the authors don t have an ablation study on this. Cons:1.It is hard to tell the benefits of the proposed method compared to vanilla Transformer. The motivation for integrating RNN cell is not clear enough for me. If Feed Forward Network is a big concern on speed, there should have some fair comparison on speed. 3.The paper needs some further proofread.<BRK>Major comments 1. In equation (3), there is a weight matrix V, while it becomes V^l in equation (4). Changing V to V^l is irrelevant to the fixed point scheme. I believe that based on the motivation, the weight matrix V should be shared. Where the masked attention and linear transformers come from based on your derivation in Equations (3) (5)? 5.I want the authors to report the computational and memory cost of the proposed model over the baselines. StarSaber or Starber? 5.There are many other typos, which raises my concerns about how serious the authors are in writing this paper and conducting experiments.<BRK>Summary:This paper proposes a way of "transforming" RNNs into attention based models. Overall the paper is promising and I would like to see more work on this, but right now there is a lot of space for improvement:* the authors should actually try to solve the system of equations that expresses their model, even if they use a different technique from Bai et al.(2019)   otherwise just reframe your paper as a modification to the transformer architecture and don t mention the fixed point approach. This approach is interesting but not novel. It is not surprising that this helps, since they don t do large scale pretraining, but it s also dangerously close to cheating to include the test set in pretraining, even if the test labels are not available.
Accept (Poster). rating score: 8. rating score: 7. rating score: 7. rating score: 6. <BRK>To this end the authors propose a novel approach based on Gaussian process classification with Polya gamma augmentation, one vs each Softmax posterior approximation and with a novel cosine *similarity* kernel (in composition with deep kernels). The approach is validated in comprehensive comparative empirical experiments involving multiple datasets, and is demonstrated to be top performing in both accuracy and uncertainty management.<BRK>******##################################################Summary:The paper proposes a novel Bayesian method for few shot classification. The proposed classifier makes use of the commonly used Polya gamma augmentation, but with likelihood replaced by a one vs each softmax approximation. The authors demonstrate better accuracy and uncertainty quantification in benchmark datasets. The paper is nicely implemented and the proposed method is clearly motivated from existing methods and show promising performance.<BRK>1.Summary and contributionsThis paper aims to improve the accuracy and uncertainty quantification in FSC using GP classifier. They use Polya Gamma augmentation for tractable inference and introduce one vs each (OVE) approximation instead of softmax to apply PG’s property to the multi class scenario. The method is clearly stated. I understand that OVE likelihood is introduced due to PG’s incompatibility to multiple classification. 6.RecommendationThis paper combines a novel idea of PG augmentation and OVE approximation into FSC, but still requires clear placement among the existing methods and more discussion on the reasoning of the results.<BRK>This work studies the GP based few shot classification problem with one vs each softmax approximation and polya gamma augmentation. I recommend acceptance of the paper for the reasons below. Although the GP classification with Polya gamma random variables is not new, the one vs each softmax approximation is a new idea as far as I know to reconcile the softmax link function with Polya gamma augmentation.
Reject. rating score: 4. rating score: 5. rating score: 5. <BRK>However, I don t think the results on ModelNet10 provide strong support for this method. It is not clear from to me the novelty compared to (Tencer & Potter, 2020)Minor Points:* The paper mentions trying voxel_grid and graclus but does not discuss what theses operations are for* The organization of the paper could be improved.<BRK>I have concerns on this application, because the super pixel based image recognition using GCN is not a dominant approach in 2D image recognition,  compared with other recognition networks based on regular pixel grids. My major concerns on the work are the limited novelties, insufficient comparisons, and limited significance on the application of super pixel image classification.<BRK>The paper defines simple differential operators at nodes in a graph (gradient, Laplacian) and uses them in the proposed graph convolutional layer. Perhaps not surprisingly, while beating the state of the art in graph convolutional neural nets (CNN) on MNIST/CIFAR, the accuracy is still far from regular CNNs for image classification. The introduction only mentions image data as fitting this requirement.
Reject. rating score: 4. rating score: 4. rating score: 5. rating score: 5. <BRK>The authors propose an RL framework, called ScheduleNet trained by clipped REINFORCE, for minmax multiple traveling salesman problem (minimax mTSP), which uses a clipping idea to stabilize learning process as PPO does. Unfortunately, the proposed method has poorer performance than existing works, in particular, OR Tool. This decreases the merit significantly. Table 2 is not completed. The behavior of ScheduleNet need to be studied further, e.g., when your algorithm works well, and not.<BRK>Summary of the paper:This paper proposes a deep reinforcement learning (DRL) approach for learning a solution strategy for the minimum makespan multiple Traveling Salesman Problem (mTSP). The authors make two main contributions towards establishing a DRL approach to min makespan mTSP:1  They propose a specialized graph neural network architecture which combines known ingredients in a way that is suitable to the structure of the mTSP;2  They modify the RL training algorithm to take into account the intricate discrete structure of the makespan objective, which stabilizes the training process. Compared to some other learned and non learned algorithms, ScheduleNet seems to be competitive. Weaknesses:1  Experimental evaluation leaves many questions unanswered;2  Motivation for tackling yet another variant of the TSP is not very strong, in that it is unclear that practitioners solving mTSP in practice would be interested in using the proposed method. This way you can compute the exact approximation ratio.<BRK>Summary The paper proposes a reinforcement learning approach to solve the min max multiple TSP, where there are multiple salesmen and the goal is to minimize the longest subtour while every city is visited by one salesman. You give an example “active worker” but it would be useful to list them all. 9.The authors claim that they propose a new approach for training “Clipped REINFORCE, a variant of clipped PPO without the learned value function”.<BRK>Then, the output of the MLP is used to get the final probability of choosing the next node. Major comment: The proposed algorithm looks interesting. Specifically, this problem can be modeled as multi agent cooperative RL problem, and this paper suggests a model to handles it as a single RL problem. (For example, see paper [1] which suggests a multi agent approach for a similar problem).
Accept (Poster). rating score: 8. rating score: 6. rating score: 6. rating score: 6. <BRK>### Summary The authors proposed an imitation learning algorithm that utilizes the primal form of Wasserstein distance to match agent’s and expert’s state action visitation distributions. They considered the upper bound of the primal form and devise the optimization method based on greedy coupling which makes learning suitable for sequential problems. I agree that this is a key idea of the submission, but I was wondering if it isn’t available to calculate the cost even when off policy RL is applied as was done in the experiments.<BRK>Summary: This paper proposes to use Wasserstein distance in the primal form for imitation learning. Compared with its dual form and f divergence minimization variants, it avoids the unstable minimax optimization. Their experiments demonstrate that this method has a better performance compared with baseline methods. + Why the complexity of the algorithm is O((|S| + |A|)D)?<BRK>The authors present a well written paper with many detailed experiments. While I do agree that the number of demonstrations required is lower. This is my biggest concern. It would be nice to understand where this model truly succeeds or fails. Might be tangential, but this paper seems relevant at least in introducing the idea of inverse reinforcement learning and W distancehttps://arxiv.org/pdf/1906.08113.pdf<BRK>The paper develops an imitation learning (IL) method starting from the primal form of the Wasserstein distance, creating an upper bound (by replacing optimal coupling with a greedy coupling), and converting that into a practical, scalable algorithm (PWIL). I don’t have issues about this, but it would certainly be useful to include comments about the potential pitfalls of the approach.
Reject. rating score: 5. rating score: 6. rating score: 6. rating score: 7. <BRK>There might be some degree to which it is possible to adapt but this should be made more explicit and better discussed. ### Weakness  The motivation for this work is not sufficiently furnished. The authors claim that it is useful when there is a discrepancy between train/test distribution but do not provide reference to realistic situation where such a problem arise. Moreover, these assumptions are stronger than the one in `Chen et al.(2018)` and `Liu et al.(2019)` for $x_s$ realizing the sup in the expression of $b^t$ (Eq.7).<BRK>In the paper, authors propose a new error based thresholding mechanism for LISTA which introduces a function of the evolving estimation error to provide each threshold in the shrinkage functions. The authors also evaluated the proposed method on multiple synthetic or real tasks. Experimental results show that the proposed method achieves a better estimation error and  higher adaptivity to different observations with a variety of sparsity.<BRK>The benefits brought by the proposed EBT method are faster convergence and better adaptivity to a wider range of samples. To bypass the requirement of ground truth sparse signals, EBT uses the reconstruction error following a learned linear transform, which in theory has good coherence property with the dictionary and therefore can approximate the recovery error well. The authors also do real world photometric stereo analysis to show the superiority of EBT. The empirical experiments are solid enough to show the superiority of EBT. Cons:  My main concern is about that this paper is a little bit incremental, as it seems to be a very direct extension based on previous theoretical results. How many samples are used for training? In summary, I think this is a good extension paper but I have a little concern about its incremental contribution.<BRK>The experiment variety is of good quality: experiments across condition numbers and sparsities, validation of theorems, etc. Originality: I am not familiar with other works like this, and I like the idea. I wish we were given insight to how it affects nonlearned (F)ISTA algorithms, which are still widely used in practice. Clarity:The clarity of your idea would be improved using a graphic: for example showing the LISTA architecture, and what sort of computations are performed in a "feed in" direction toward the shrinkage operator.
Reject. rating score: 3. rating score: 5. rating score: 5. rating score: 7. <BRK>I strongly encourage the authors to make a clear empirical study and compare their method with prior works. If the contribution is empirical, it would be useful to compare it with existing methods. This problem is an important and challenging problem in the field of reinforcement learning. While this is a challenging problem, my judgment is that the paper is not ready for publication yet.<BRK>Support:I acknowledge the paper and its results to be of interest for self play in RL. In the appendix, it is rightfully argued, that part of the training could be parallelized. This is shown empirically and amended with a convergence proof.<BRK>1.Summary:This paper addressed an interesting topic on competitive self play reinforcement learning on two player zero sum games. Typically, many self play methods are only average case optimal and self play with latest agent fails to converge. These methods also address the problem of average case optimal. It will be good if you can evaluate the method on one of poker game, such as leduc. $N^0$ is a typo?<BRK>In this paper, the authors present a rule for selecting opponents for self play training in zero sum games: Train each agent i against the agent j that is "hardest" for i (in the sense that i s payoff is least among all candidate opponents when playing against j). On the one hand, it s hard to believe that "train against the toughest opponent" as a selection rule has never been tried before. The principled grounding of the selection procedure in saddle point optimization was especially interesting, as I have not encountered that corner of optimization before. The exposition is clearly written and well organized.
Reject. rating score: 4. rating score: 4. rating score: 5. rating score: 6. rating score: 6. <BRK>This paper aims to deal with the learning of noisy/corrupted labels based on the small loss criterion. To conclude, a series of experiments show that the new methodology proposed in the paper leads to state of art results. I think that the idea is nice and was very impressed by the numerical results reported by the authors. I advise the authors to make a in depth revision of their paper, introducing more carefully their method so it can be understand by a broader audience. I think that a lot of them are unnecessary for the introduction of the proposed methodology.<BRK>To overcome the dilemma of selecting the ball radius, the authors propose a surrogate objective, namely Wasserstein Normalization. The paper both theoretically and empirically validate their method. iii) Extensive experiments on synthetic and real world datasets. And it seems irrelevant to the practical problem. But the motivation of Wasserstein Normalization for uncertain samples is weak. Imagining the network has high confidence for certain samples (which is generally true in practice), then the mean of gaussian $m$ is uniform categorical distribution. The paper s approach is just injecting noise onto the network output of uncertain samples.<BRK>__0.disclaimer.__It is possible that I failed to understand a significant portion of this manuscript; I had a very hard time trying to understand the notations and the writing overall. __5.Rationale__I am curious what the general intuition behind the proposed approach is. Are we making any implicit assumptions on the nature of label flipping operations, or perhaps the learning dynamics itself? review summary.__The proposed method seems to be advantageous over the considered baselines, but I am not sure if the method would outperform other baselines as well. Also, the clarity of the manuscript is not quite good (in my opinion). The paper also refers to several related approaches, which could/should be compared or discussed against the optimal transport based method proposed by the authors. I strongly recommend adding (at least) an empirical comparison to the Li et al., as it shares a larger goal (in the sense that utilizing the information from mislabeled data) yet implemented with a different philosophy.<BRK>This paper propose a computationally efficient Wasserstein distributional normalization algorithm for accurate classification of noisy labels. Empirical results on CIFAR 10/100 and Clothing1M suggest that the propose algorithm outperforms other SOTA approaches. Overall, this paper is very well written and easy to follow. I am positive with respect to acceptance of this paper. Second, I would like to emphasize the technical quality of the paper. The concentration results in this paper are by themselves quite  elegance.<BRK>The paper is a contribution that aims at solving the label noise problem. While the overall idea is interesting, and the results described in the paper seem impressive and state of the art on many of the tasks considered, I believe that the paper is badly written and very hard to follow. The paper proposes a novel type of distributional normalization based on Wasserstein distance. I believe the paper is interesting and show strong empirical evidences that the method is worth considering.
Accept (Poster). rating score: 7. rating score: 6. rating score: 6. rating score: 6. <BRK>This paper proposes a new memory mechanism based on the Kanerva Machine inspired by computer heap allocation. The machine stores the knowledge more efficiently through the sharable part based system and the authors simplified the writing mechanism by designing the memory deterministic. 2.They evaluated their method on several datasets (e.g., DMLab Mazes or CIFAR10) and showed better performance than previous works. I think that section 3.3 requires the additional description. in the paper, the authors mentioned the key model is described in Section 3.4, but I can t find the description in the section. The correctness of their claim and Clarity:This paper is well written and correct I think, but if the section 3.3 and 3.5 was written more kindly, then it would be better. However, as I mentioned, for me, some sections are hard to follow and some interesting analyses are not included.<BRK>Pros:  The authors combine the idea of differentiable indexing in Spatial Transformer (Jaderberg et al., 2015) into the memory of Kanerva Machine (Wu et al, 2018a;b) and prove by experiments that this allocation scheme on the memory helps improve the test negative likelihood. The authors show the efficiency of Temporal Shift Module (TSM) (Lin et al., 2019) in the encoder of memory models. The experiments are well reported for different tasks, such as reconstruction and generation, on various datasets. To be specific, the K++ model is Kanerva Machine + Spatial Transformer + a powerful encoder (namely, Temporal Shift Module). Where do the results in Table 1 come from? The Kanerva machine: A generative distributed memory.<BRK>The paper proposes a generative memory (K++) that takes inspiration from Kanerva Machine and heap memory allocation. An episode of images is encoded into keys and the memory using Temporal Shift Module and transposed convolution. 2018. The model is trained by maximizing the ELBO of the conditional lnP(X|M). However, in the main text, no detail is given to elaborate on these points. What are the values of T and K used in the experiments?<BRK>This paper proposes a generative memory modeling method. The authors showed the proposed model achieved state of the art results on the binarized MNIST data set and the binarized Omniglot data set. Table 1 shows the result of one of the key experiments, however, it is not well formatted, and some measurements are not well aligned or not completed. The baseline model in the ablation study is too weak. Is it possible to compare with KM with multiple addressing keys?
Accept (Poster). rating score: 7. rating score: 6. rating score: 6. rating score: 6. <BRK>2.Same paragraph: "Apparently, the nonparametric nonlinear multivariate Hawkes processes are a suitable choice for representing the temporal dynamics of mutually excitatory or inhibitory interactions and synapses connectivity of neuron networks." From a conceptual standpoint, I am confused by the way in which the neuroscience term "functional connectivity" is used. This sentence needs some editing. I have three main points to raise to the attention of the authors (one of which is more of a question/suggestion), and several minor issues to highlight from the point of view of clarity.<BRK>In addition, the authors use a basis function set to express the influence function. It might help to explain how to read log likelihood in table 1 and, especially, table 2. Pros:They seem to achieve great runtime performance compared to some existing methods, one of which involves MCMC. Cons:This is a special application, useful for analyzing neural data.<BRK>This paper proposes a novel multivariate nonlinear Hawkes model by modeling the intensity as the product of a upper bounding intensity, and a sigmoidal function that maps the real valued latent intensities into a nonnegative real value in [0,1]. A standard EM algorithm is developed to perform inference. The authors clearly introduces the main techniques they used from [Adams2009,Donner2017], and their novel Hawkes model. Nevertheless, I think there also exist some other related multivariate nonlinear point processes can also capture self  and mutual inhibiting behaviors, e.g., [1]. The authors should consider more strongly related baselines for the comparisons.<BRK>The first comparison that comes to mind is the GLM. This allows for inference of excitatory and inhibitory interactions among point processes using the Hawkes process framework. ####EDIT####I am satisfied with the reviewers response and will raise my score to a 6. I think the authors should take care though in speaking practically about their work.
Accept (Poster). rating score: 8. rating score: 7. rating score: 6. rating score: 4. <BRK>**Thanks to the authors for the response. The paper should now be considered a clear accept. **The paper proposes a method for controlled text generation with pretrained (unconditional) language models. CoCon is a function not only of the prefix but also of some desired  content  sequence, which allows to control the content of the output text at inference time. The model is evaluated on its ability to generate output with desired content, to generate text of a desired topic, and to generate text of specific sentiment. The experimental section could be more thorough. Moreover, some of the results are difficult to interprete or non conclusive. The paper could benefit from a human evaluation. Why is this sufficient? * Table 2 suggests that CoCon without the adversarial loss achieves the best performance, drastically improving on content similarity while retaining comparable text quality and diversity. This makes me wonder why the adversarial term was introduced in the first place, and why it is apparently used in the other two experiments. * Why is the perplexity of CoCon (and PPLM) consistently lower than the perplexity of the baseline LM GPT 2? The paper would be even stronger if you had quantitative experiments for these, as the examples that are given are not very convincing.<BRK>The paper proposed a way to control the content output of a DNN based language model (GPT 2 in the experiment, but not limited to it). Experiments showed that the control is effective at directing the generated text. Experiments are based on strong baseline (GPT 2, PPLM and CTRL), and show clear advantage of the model. Clarity:The writing is clear and easy to follow. I have some minor comments but believe they are easily fixable. Originality:CoCon has clear but incremental difference than PPLM and CTRL. Significance: Controlling the generation of LM is not a novel task. This is an improvement on an existing problem with several solutions. "our approach aims to control the generation at a content level, beyond high leveltext attributes." 3) Page 5. cycle reconstruction loss. 4) Page 6 , 2nd paragraph "self supervised and requires no manually labeled data fully" is duplicated, can be removed.<BRK>SUMMARY:The paper proposes a self supervised technique for controlling the productions of a Transformer based pretrained generator. The technique consists in augmenting the architecture of the pretrained model with a special "content conditioner" (CoCon) block which is able to exploit a contextual condition. This appears to be a very worthwhile direction to pursue. *Related work and Alternatives to the CoCon block*. *Complexity of the overall model, intuition about the different losses, hyperparameters* The self reconstruction loss, by itself, appears to be problematic. Therefore the need to interpolate this loss with other losses (section 3.1). While you provide some ablation experiments, you do not much discuss the importance of these different losses. It is difficult for the reader to really assess the quality of the results. Here, a human evaluation with a clear evaluation protocol would really be useful. Overall, an interesting and important objective: self supervision of controlled text generation, with some nice ideas. **Written after rebuttal:**Thank you for the substantial improvements to the paper in terms of clarity (in particular Figure 2 is helpful) and additional experiments/human evaluations. Despite some underlying questions (from me and other reviewers) that remain, I have updated my score and am now leaning towards acceptance.<BRK>This paper tackles the problem of controlled text generation by converting it into a conditional text generation similar to (Keskar et al.19). It proposes an architectural modification to the transformer LM used in GPT2,  Specifically, a CoCon layer is added as a separate transformer block in the middle allowing self attention to be performed between the textual context representations LM_α(c) and the generations LM_α(x_{:t 1}) this is performed through concatenating the key and value matrices with the keys and values of the encoded textual context. Authors provide 4 different losses to train this additional layer. Pros:    The proposed method has an advantage over (Keskar et al.19) by 1) avoiding rigid control tokens and replacing them by textual context. There might be an issue with the Adversarial loss, being non differentiable (see Q1 below)  Evaluation could have been more thorough, specifically, when the proposed method has a superior topic and sentiment relevance in table 3 and table 4 this comes on the cost of perplexity. While this has been an issue in previous work as well, it would have been better to fix this issue and provide as better evaluation, a good method to evaluate this could have been plotting perplexity vs control satisfaction rate under a temperature sweep. There are missing details on how the textual contexts are selected during inference time. This makes the proposed solution very similar to control tokens by Keskar et al.19.One advantage of using textual control tokens is handling unseen "content inputs" at test time. Questions: Q1: This is a critical one, If I got this part correctly, the adversarial loss eq 18, requires to sample y from the LM, this is non differentiable. if that is the case, did you follow any necessary steps (e.g.RL or continuous approx) to overcome this non differentiability?
Accept (Oral). rating score: 8. rating score: 7. rating score: 6. rating score: 10. <BRK>Summary:The paper provides an interesting (and to my knowledge, novel) approach for learning a mapping of actions and observations from one domain/character to another reasonably similar domain/character. This mainly allows the transfer of skills (i.e.policies) from one domain/character to another. In order to learn these mappings, the authors use the idea of cycle consistency in generative adversarial networks and adapt it to the imitation learning task. Importantly, this allows them to obviate the need for paired state samples across domains. The authors show that their method not only works across modalities (vision from real robot to states in simulated robot), but it also works to some extent on different character morphologies. The limitations of the method are not discussed well. The code is not provided which hampers reproducibility. It seems strange that the "L1 error" increased when more data was available.<BRK>The cycle consistency is used over the space of actions across two domains: e.g., given data from X [in the form (x_t, a_t, x_{t+1})] and data from Y [in the form (y_t, u_t, y_{t+1})], the system should be able to discover an "alignment" between states across the two domains by enforcing that the action spaces be mappable to one another (either identical, as in the "cross modality only" applications, or via cycle consistency, as in the "joint model" applications). The authors go on to show that their approach, among other things, allows for strong alignment between real world images and their corresponding underlying state. Overall, the paper presents some solid results across a handful of interesting domains/applications. Particularly in the figures, the authors should make clear that the `y` images are in fact renders corresponding to the predicted state.<BRK>** Paper Summary **This paper proposed a novel framework to establish the correspondences across observations and actions, by using dynamic cycle consistency. The proposed method was applied and attained the robustness on various tasks, such as cross physics alignment, cross modality alignment, cross modality andphysics alignment. ** Paper Strength **+ The paper is well organized and written. + Borrowing the concepts of GANs and cycle consistency to solve the unpaired problems in cross modal correspondence makes sense and good performance gains are expected.<BRK>The key idea is to leverage the (existing) concept of cycle consistency, incorporating dynamics to formulate a cycle loss that spans vastly different domains. ##### Strengths* This work addresses an important, fundamental problem that arises in many applications, including state estimation and sim2real transfer. * The paper is well organized, readable and clear. * For cross modality alignment, paired training data are typically available. ##### RecommendationUnless I am missing important similar work, the paper makes a strong contribution in an important area. * Please comment on the listed weaknesses.
Reject. rating score: 4. rating score: 4. rating score: 4. rating score: 5. <BRK>Overall the paper represents a substantial amount of work, and it is clearly written. Nonetheless, I believe it is not significant enough so I can recommend acceptance.<BRK>However the paper does not explain how to apply the truncation in cases like this.<BRK>What is the purpose of the topic modeling experiments? [1] R. Liu, J. Regier, N. Tripuraneni, M. I. Jordan, and J. McAuliffe.<BRK>It is not clear what is the form of the prior and approximating posterior used in VAE experiments. This makes it hard to evaluate the paper. **cons*** I found it hard to follow the experimental section.
Accept (Poster). rating score: 6. rating score: 6. rating score: 4. <BRK>This paper studies the statistical properties of distributed kernel ridge regression together with random features (DKRR RF), and obtain optimal generalization bounds under the basic setting in the attainable cases. Optimal learning rates with a less strict condition on the number of local machines were first established in [JMLR 2020, 21(147): 1 63] (or [arXiv:1801.07226]) if not earlier. Numerical results on different data sets could be given to further exemplify the performance of the algorithm. Will this enlarge the computational complexity?<BRK>The paper analyses generalization properties of distributed kernel ridge regression (DKRR) with random features and communications. It studies optimal learning rates of the generalization bounds both in expectation and in probability. Within the same setup of random features, the number of partitions is relaxed to $O(|D|^{0.25})$ guaranteeing optimal generalization performance in probability (Theorem 2). Please elaborate on this.<BRK>The paper investigates an algorithm for distributed learning with random Fourier features. The main contribution of the work is a consistency bound. ##### clarityThe paper is clear and easy to follow. ##### quality & significanceI have a fundamental disagreement when it comes to the considered distributed setting. The computational complexity is linear in the dataset size and cubic in the number of features. What would be interesting is to use different sets of random features on different machines and then aggregate on the master machine.
Reject. rating score: 3. rating score: 3. rating score: 4. rating score: 4. rating score: 5. <BRK>[Summary]In this paper, a graph view consistent learning framework (GVCLN) is proposed. It is not clear how to select high confidence pseudo labels. However, the novelty of this work is limited and more experiments are necessary. [Cons]  The novelty of this work is limited.<BRK>This paper tries to address the limited training data problem by exploiting the consistency between different views of the graph data. However, important details and justification are missing. Moreover, with limited training data, how do you obtain ˋˋwell trained’’ two view networks? To summarize, without justification, the proposed model is not convincing. Without details of the model and implementation, this work is difficult to reproduce.<BRK>In particular, this work first uses graph neural networks and graph attention networks to construct two different latent features of the same data. Is it because the feature is different already? Plus, I did not find experiments that uses the data augmentation method (instead of the view consistent method). 3, the inclusion of pseudo labels are not well explained in this section. I was expecting to see more systematic analysis on this procedure, however, the current version can not fully convince me on this procedure. However, they are missing.<BRK>The two graph “views” are based on existing graph embedding methods. So, I consider the originality of this work is limited. How will the proposed method be compared with some contrastive learning in terms of performance? Cons:  The view consistency idea is good but not particularly new.<BRK>The framework is well demonstrated and the paper is easy to follow. Novelty: My main concern of the paper is about the paper s novelty. `Besides, The learning loss of the proposed method is similar to the self training loss in SimCLR v2 [2]. 2.Experiment: The experiment results are not sufficient. Results on more larger datasets [3] are expected to support the effectiveness of the proposed method.
Reject. rating score: 3. rating score: 4. rating score: 4. rating score: 5. <BRK>This paper explores the problem of machine translation for African languages. I agree with the authors that African languages have been underrepresented in NLP research in general and machine translation in particular. Unfortunately, I think that the work itself does not meet the standards of the conference in its current form. The dataset in question is a parallel version of the Bible. 2) "Our second contribution is the proposal of a new metric to evaluate similarity between languages". There were already in depth studies showing that multilingual training is helpful for low resource machine translation, see for instance [3]. An obvious baseline that is missing in the paper is to train a multilingual model on the combination of all languages.<BRK>This paper considers translation between African languages. However, the field of multilingual machine translation is a very well researched field, and it seems that the authors have developed their methodology largely independent of the literature in this field. In fact, there are already existing well researched methods on many of the topics presented in this paper. To give just a few examples:* *Pre training for low resource translation:* Liu, Yinhan, et al."Multilingual denoising pre training for neural machine translation." There are lots of methods that people have developed, and I think that they could be effectively applied to the very important problem at hand here!<BRK>This paper describes three contributions: (1) a new multilingual parallel corpus that covers 1,477 languages, with text from the Bible   for the region of interest in this paper (Cameroon) this means 22 languages instead of 18 with JW300; (2) a method for determining similarity of languages based on LM scores; (3) a series of experiments that evaluate the utility of adding a third language to a pair of interest based on either the similarity metric proposed by (2) or based on historical data, for use in a multilingual MT system. Working on MT for African languages is important, and I’d definitely like to see more of it. Smaller points:When citing Eberhard, I would include “M.” with the first name, not the last name. Crucial details are missing for all three of these.<BRK>This paper motivates clearly the need for research in machine translation of underresourced (and thus underresearched) African languages, and proposes ways to aid the training of MT systems using data from related languages. The main contribution is that two ways (and a random baseline) to select a langauge to add to the training of an MT model are evaluated (one based on linguists  clustering, one based on probabilities assigned through a language model of the target langauge) on pairs drawn from a set of three langauges, choosing new langauges from a set of 13. #### Strengths/what I loved:  The paper performs a very clearly motivated and well set up experiment with appropriate analysis. 2 make it sound especially like you are proposing a way to infer linguistic relations between languages ("a method for aggregating languages based on [a number of features]"). Not only training, but unfortunately also validation is performed only on Bible data and only on a cluster of 3 languages.
Accept (Poster). rating score: 7. rating score: 7. rating score: 5. rating score: 5. <BRK>This is an interesting idea where the authors propose "SurfaceFusion", where they use the source embeddings learned by the encoder to modulate the output of the decoder at the final layer. The authors then show through a series of experiments that attending over the encoder embeddings is useful, and propose a way to integrate the information from the embeddings directly into the last layer of the decoder, showing that this improves experimental results. Few comments:1) there are some grammatical and spelling errors, e.g.: "analyses", "which is the EncoderFusion expected to solve"2) while the premise and analysis are interesting, i am curious about the reasons behind the design choice of "SurfaceFusion". 3) what are the costs in terms of wall clock time when introducing an additional softmax operation for every token in the decoder? I ve read the response and other reviews and have updated my rating.<BRK>#####################   Summary   ####################This paper introduces the fine grained layer attention to evaluate the contribution of individual encoder layers and investigate how encoder layer fusion works, where the decoder layers have access to information for various encoder layers as opposed to only the final encoder layer as in standard Transformer. Based on the observations that the encoder embedding layer is important to the success of encoder layer fusion and the uppermost decoder layer pays more attention to the encoder embedding layer, this paper proposes SurfaceFusion, which only connects the encoder embedding layer to the softmax layer of decoders, leading to quite substantial gains in metrics such as BLEU. The authors have addressed all my concerns in the rebuttal and I vote for acceptance of this submission. 2.The proposed SurfaceFusion is well motivated by a series of experiments on different Seq2Seq tasks. Therefore, it is reasonable to speculate that the summarization model will receive more improvements from the proposed SurfaceFusion. A much wider empirical investigation would be appreciated. Overall, this paper is really clearly written, the proposed SurfaceFusion is novel and well motivated, and experimental results are very solid.<BRK>The paper proposes a *fine grained layer attention* (FGLA) to analyze Encoder Fusion method. While the introduction of FGLA allows for the analysis of the contribution of individual encoder layers, it changed the model’s architecture. As a result, the contribution of each layer found in Seq2seq with FGLA might not be the same as in the standard Seq2seq.Therefore, I think the conclusion might not be appropriate for the standard Seq2Seq model. This generalization is straightforward and simple. The later analysis in the paper is carried out on FGLA. However, I find that the same analysis can be performed with Transparent Attention. For experimental results, FGLA is not compared with other fusion methods. I think it’s worth spending some text/examples on why this is needed for those tasks.<BRK>The authors perform a thorough analysis of encoder fusion for Transformers: which encoder layer should the N th decoder layer attend to? It turns out that the final decoder layers often attend to the encoder embeddings, leading the authors to provide them to the last decoder layer which leads to small improvements of performance on machine translation, summarization and grammar correction tasks. These are nice results, but the gains are small and the models are tested in the very basic configurations. These tasks and techniques, as well as some numbers used to claim state of the art, are from a few years ago (e.g., SOTA on en de translation is higher currently than the authors claim and higher than their number). It would be interesting to see if the presented conclusions hold for larger models   esp. for a T5 Transformer on masked language modeling, as this would be a more commonly used model in 2020. It may well though make it even more important   it would be really good to know! Lacking these experiments, we cannot recommend acceptance at this point.
Reject. rating score: 4. rating score: 5. rating score: 5. rating score: 6. rating score: 7. <BRK>#### Strengths:  The idea of incorporating the notion of including safe/unsafe states to build a new advantage estimation is an interesting one, and I believe the link to Cost Sensitive Advantage Estimation and reward shaping is novel. How tough the problem becomes? The paper assumes that the agent has knowledge about what states are safe and which states are unsafe ($\alpha_t$ variable in Eq 4).<BRK>However, later the CSAE algorithm leverages the concept of "safety" of states. Experiments in control simulation tasks are provided. However, I think the problem setting needs further clarification, and the new advantage estimation and reward shaping are not justified enough. 3.The paper proposed two contributions but I did not follow how they are connected to each other.<BRK>The proposed algorithm is detailed in the Appendix, its main difference with the CPO algorithm is the formulation of the primal dual problem of equation (14) which rely on a beta worst case discounted cost (and CSAE) instead of the expected discounted cost.<BRK>The paper is well written and the experiments show good results compared to prior methods. However, I do have a few questions about the paper:1. Second, a more conservative estimation of the safety cost is proposed to further improve the safety of the robot during the learning process.<BRK>In particular, they dampen the estimated advantage associated with unsafe states, which encourages the RL algorithm to explore safe states more often during the learning process. 2.The authors compare the cumulative safe reward of different RL algorithms. However, the theoretical results in Theorem 1 compare the cumulative reshaped rewards. The authors describe how their method replaces the reward for an unsafe state as the average reward, and claim that this is generally better than previous reward reshaping methods, e.g.setting the reward for unsafe states to zero.
Accept (Oral). rating score: 8. rating score: 7. rating score: 7. rating score: 7. <BRK>This paper proposes a new principled approach to growing deep network architectures based on continuous relaxation of discrete structure optimization combined with a sparse subnetwork sampling scheme. The appendix has some, but very little has the main manuscript.<BRK>This paper proposes a novel NAS method that searches the model architectures by grows the networks. 2.The paper is well written and easy to follow.<BRK>Attention to randomness for this kind of training process seems especially important given the variances in results in the Lottery Ticket hypothesis paper. A large portion of the method consists of a search method over the space of possible sparse networks, combining it with growing the network to get a NAS like method. It has been observed, though, that in a sufficiently general space of this kind one can randomly sample connections and see high accuracies.<BRK>Summary:  This paper proposes a NAS type work for growing a small network to a large network by adding channels and layers gradually. The authors apply the method to both CNN and LSTM networks.
Reject. rating score: 3. rating score: 3. rating score: 4. rating score: 4. rating score: 5. <BRK>The experiments do not show any practical gain from MemTransformer. Last but not least, the experimental setting is flawed: e.g., the MT experiment cuts training at a suspiciously early stage, presumably contributing to the bad performance. In sum I do not think the paper is ready for ICLR. This is probably fine if the paper studies a set of clear research problems with well designed experiments and answers some interesting questions.<BRK>This work proposed adding memory tokens to store non local representations and creating memory bottleneck for the global information. The evaluation shows positive results adding memory to a Transformer network. The paper is not clear about how to obtain the memory input token. The layout of the paper and the design of experiments seems very arbitrary. The design of variants of Memory Transformers seems very arbitrary.<BRK>viii) The paper says that "BLEU score of MemTransformer 10 with 5 `[mem]` tokens shows it is still able to translate with acceptable quality". **My general opinion**: I personally think that memory augmentation is a simple but potentially useful technique that explicitly adds memory to a deep network, which, as the authors mentioned in their paper, is not a new topic. This is false. My detailed question and comments are below. x) The fact that there s no consistent improvement in Table 5 of the memory augmented Transformer on the base Transformer is very concerning to me.<BRK>If not, then why not? This is concerning because the baseline model seems highly undertuned. Among the three variants, the authors only provide results for MemTransformer on language modeling and GLUE. The main problem with the paper is that the empirical results are quite weak and don t show a consistent trend with regards to utility of memory.<BRK>The use of global tokens (like [CLS] token in Longformer as mentioned in the paper) is a well known technique that undermines the paper’s novelty. * No results for MemBottleneck and MemCtrl in table 5. * Memory extension experiments (table 3). At the same time, the models are not trained to perform in the absence of memory.
Accept (Poster). rating score: 7. rating score: 7. rating score: 6. rating score: 5. rating score: 4. <BRK>The paper achieves a significant speed up for the PointGoal Navigation task by,  Batched simulation of  environments (N > #CPUs)  Batched rendering of multiple environments with a single rendering request, effectively drawing the environments as one big frame  Reusing scene assets between environments being rendered in a way that doesn t hurt sample diversity but allows more environments to be rendered at the same time  Using multiple accelerators  Shows that comparable results can be obtained fasterThe paper is well written and clear. Similar ideas have been tried out but the combination of the ideas is well done and with a large efficiency improvement. One downside is that the setup is not easily applicable in general. Large efficiency gain with the same amount of resources. Allowed faster research iteration. Analysis on runtime (4.5) is clear and insightful. Cons:  Not easily transferable to other environments. These two numbers are with different types of GPUs which makes it look like the speed up is only 4x going to 8 GPUs when it s really 6x (72000/12900) and possibly more if batch size per GPU is kept the same. What effects does the changes have? e.g.does 256^2  > 64^2 alone hurt in any way? I suspect the Resnet makes up for some of it?<BRK>This paper proposed a novel RL simulator design that executes large batches of simulation simultaneously. Pros:* The paper tackles a very critical speed bottleneck in practice for RL training in a simulation environment. * Very significant speed improvement* The paper is well written and presentedCons:* The paper misses some important ablations to showcase each technical component s contribution to the final speedup. * It seems the choices of DNN architecture, policy optimization, and batch rendering are highly coupled. In many RL tasks, it might be necessary to render high resolution images. Thus it s essential to know if the proposed method is suitable for such a job. Is this framework suitable for heavy non rendering simulation jobs, e.g., if actor simulation or physical dynamics simulation is involved? The author should elaborate a bit more on why large batch training harms sample efficiency. And if the proposed batch rendering could be used together with these techniques? I also like the updated paper. I do think the paper would potentially provide great value to the community not only due to its open source effort but also as a general approach to improve simulator efficiency for RL, despite it s not "novel" methodology wise.<BRK>The paper proposed a high performance reinforcement learning system for training agents in 3D simulated environments. The system uses batching to significantly increases both simulation time and training time. It also proposes tricks for large mini batch policy optimization. The experiments do evaluate several important aspects of the proposed method. However, I want to see how generalizable the system design is. The current paper only evaluates the system for one environment and one algorithm, which is a major weak point.<BRK>This work extends the Habitat simulator to perform large batch training. Although this is mainly an engineering contribution, the approach is useful for the community. However I have some questions:1. You do not appear to specify the CPU memory requirements of this approach, I believe each Habitat environment instance requires a significant amount of memory, how many independant instances can be loaded in paralel with this approach and what is the memory requirement? What is the value of K that is discussed on the last paragraph of page 4? In table 2 some entries have standard error and some do not, if these results are now available please can they be included. 5.Would it be possible to update figure 3 with comparisions of the baseline methods.<BRK>This paper shows that batch simulation can accelerate reinforcement learning in 3D environments. I believe the problem that this paper is addressing is quite important. Long training time of RL agents is a big issue which makes evaluation of the current methods harder and, as authors mentioned in the introduction, unaccessible to many people due to limited number of people who have access to large clusters of computation. However, I find the paper limited in both novelty and impact. The idea of "batch" simulation is not new and it has been shown before, not only for complex environments (check Sample Factory from ICML 2020 for an async version) but also for simple ones such as atari when ported to the GPU (check NVlabs CuLE from NeurIPS 2020).
Accept (Poster). rating score: 8. rating score: 6. rating score: 6. rating score: 6. <BRK>al 2019.The method focuses on the manipulation of the latent space, in contrast to the popular image space editing methods. The paper is easy to read and understand. In comparison with the Voynov & Babenko model, how many of the directions that their model was able to find did you look at?<BRK>This paper presented a latent space editing framework for semantic image manipulation. 2.The results of the proposed method heavily depends on the GAN inversion.<BRK>This paper presents a new approach for the semantic image editing task by allowing the controllable transformation on the latent space. ‘Joint distribution sampling and training’ part in Section 3.2 is hard to understand. 5) The results in Figure 8 (a) are based on the global transformation T with the proposed losses. * Cons1) The manuscript contains lots of vague parts.<BRK>Specifically, this paper uses a pre trained GAN to synthesize images, a pre trained regressor to get the image attributes, and trains a network T to find meaningful latent space directions. The experimental results show that the proposed method performs better than other selected methods to some degree. 3) Lack of comparisons of continuous image editing with other methods.
Accept (Spotlight). rating score: 8. rating score: 8. rating score: 7. rating score: 6. <BRK>## Summary ##The paper proposes a modified loss function for supervised learning, in which the original loss at w is replaced with a maximum of the loss in a small p norm ball around w. An approximate way to compute gradients for this loss is presented, and evaluated in high detail on a variety of supervised learning problems where it is shown to consistently improve the overall generalization error. This is expected to hold in practice at a minimum.<BRK>## SummaryThis paper proposes and empirically evaluates SAM, an optimization method that is designed to seek out regions of uniformly low training loss. The method is derived from a bound on the generalization performance of parameters $w$ in terms of the maximal training loss in a region around $w$. This is an important question, but I totally agree with the authors decision to leave it for future work.<BRK>They also introduce a new notion of sharpness named m sharpness. Strength: * The paper is overall well written with clear motivation. Weakness:* There is no clear definition of the “sharpness“ that the algorithm tries to optimize. This discrepancy would become larger when the number of epochs training gets larger. would be helpful to have for evaluating the complexity of the method. Figure 2 is not intuitive as the loss contour value is not clear.<BRK>  OverviewThis paper proposed a learning algorithm using the idea of flatness. So, if the loss function is very spiky, there is a concern that the approximation will not work because the local gradient alone will not find the appropriate direction. The nice networks used in their experiments is supposed to use smoothing techniques for the loss surface, such as the batch normalization and the skip connections, but the concern is whether the proposed method works properly in neural networks with a more less smooth loss surface.
Reject. rating score: 4. rating score: 4. rating score: 4. rating score: 5. <BRK>However, if there are a lot of confounding variables in the experimental setup, in my opinion, it becomes very hard to draw reasonable conclusions from the experiments. The authors have not prepared a rebuttal.<BRK>They also give supporting experiments for this observation. The paper gives algorithms to test the Coherent Gradients Hypothesis (CHG) for larger models and larger datasets. 3) I did not completely follow the comment on adversarial initialization in section 4.1 of the paper.<BRK>The authors provide the code and detailed descriptions of all the experiments. I would recommend the paper for rejection, because from my point of view it does not contain sufficient novel contributions for a conference publication.<BRK>What about the plots for generalization of h train to e test and e train to h test? the paper overall reads like a sequel to CGH where you need to have read CGH to understand key plot points.
Reject. rating score: 4. rating score: 5. rating score: 6. rating score: 7. rating score: 8. <BRK>Even REM would have been fine. While the work seems promising empirically, I have a number of concerns about the method:1. Can it be compared to a more standard square root inverse counts style weighting in a tabular setting? The method, as it stands, seems to resemble aleatoric uncertainty more, and if this is the case, why not just use distributional critics? In fact, the Q values plots perhaps signify my belief about 2, that their approach works because it performs "better" Q learning similar to Agarwal et al.2020.Overall, I do not buy the claim that the real benefits are coming from uncertainty based Q functions, but perhaps more so like a stronger critic function. The paper doesn t ablate over showing the benefits of uncertainty over a policy constraint or comparing to a method that actually learns Q value estimates aware of the epistemic uncertainty making it hard to compare to these. This would validate if dividing by the variance is indeed transferable across domains. This is the uncertainty that we trypically need in offline RL and is also used in Theorem A.2 (for the high probability bound which is given the data), and that s why ensembles with Q functions consistent with themelves are typically used. By merging the target values across different dropout masks, the uncertainty is not timewise consistent. I would recommend the authors read the discussion on Posterior sampling for RL vs Optimism and Thmpson sampling for a discussion on this. I am a bit disappointed with the rebuttal.<BRK>Overall, I agree with the main motivation for the paper that tries to estimate and use uncertainty, and it seems that the method empirically achieves the goal. Still, I have a concern about the theoretic validity of MC dropout in the Q learning setup and the clarity of the papers (especially typos and proofs in Appendix). Hopefully, the authors can address my concern in the rebuttal period. In the meantime, the same algorithm could leverage uncertainty measured for transition dynamics with proper scaling as it is done for $Q$. Why can measuring epistemic uncertainty for $Q$ via MC dropout be valid (or effective)? What is the main advantage (or motivation) over other uncertainty measuring methods (such as deep ensemble) or other domains (such as transition dynamics learning)? Minor Comments:* Section 4.1, Variance equation: what is $\sigma^2$, what is the theoretic impact of this term, and how is this value decided from the implementation perspective? * Section 5.1: what do you mean by the ‘final’ replay buffer? Does this mean that the last 100,000 transition tuples are saved for the offline RL dataset? Also, it would be nice if you can include the uncertainty figure of Q which is trained without skewing. $i$ is used in another context, so this is confusing. * In Tables 1 and 2, when your results are the best, the results that lie in some confidence interval of yours should also be bolded for a fair comparison.<BRK>Building on BEAR, they estimate the epistemic uncertainty as the Var(Q(s, a)), and modify the update to downweight samples with higher variance. Reasons for the score:I vote for accepting the paper, with some improvements. I would strongly encourage the authors to improve the writing and presentation of the algorithm in favour of clarity. Weaknesses:  The baseline for MOPO is missing from the plots. As we are considering the stability of the algorithm as compared to other baselines (like BEAR) it would be critical to compare the same for MOPO (trained for a similar number of epochs). Taking this into account, I would consider an implementation with some variant of clipping (gradient or reward) to be a strong missing baseline.<BRK>The paper baselined against the results from D4RL and showed to outperform those more than half of the times. ##########################################################################Pros: Uncertainty is an important yet underexplored domain in Offline RL. Having a simple method that can be applied to actor critic based methods is attractive. As far as I know, there hasn’t been literature that combines dropout and Offline RL uncertainty estimation in the loss functionThe performance of the algorithm is decent on D4RL, a standard offline RL baseline. Thus it would be informative to mention the following in the paper: Is your model sensitive to the choice of beta? Can it be absorbed into the learning rate? It assumes that Q is bounded for any (s,a). Can you expand more on this? 3.Could you address the Cons above? I want to see whether it still overestimates, and if so by how much.<BRK>The authors present a simple and efficient method for training offline RL agents. They estimate the epistemic uncertainty of their model through dropout  variational inference. While this method is not novel, it has never been applied to offline RL as far as I know. On the basis of this epistemic uncertainty estimate, they regularize the policy search to avoid sensitivity to overestimates in poorly known states. The way this regularization is performed is taken from [Kumar2019]. There is not much to say about the paper. It is well written and positioned. One could that the method is the straigthforward application of dropout variational inference to [Kumar2019] algorithm, but it remains that the empirical results are quite significantly improving the state of the art, and as such, it deserves to be known. Red minus blue is the dropout variational variance, and this is what measures the uncertainty of the model (not singlehandedly the red term).
Reject. rating score: 3. rating score: 3. rating score: 4. rating score: 4. <BRK>Moreover, the experiment is too weak to support the claims. The paper’s technique contribution and originality are also limited. 1) The authors need to make a clear definition of the assumed application scenario so that the below problems can be avoided or solved. Therefore, the paper should discuss the prevention of “attacking by fake data”. It is unfair for the clients to be selected at an early stage.<BRK>This paper designs an equation, i.e., equation (5) in the paper, to measure the impact or contribution of each participant/agent in federated learning. Thus, it was often impossible for me to determine exactly what the authors would like to say or describe. Moreover, please treat the equations as the parts of sentences and make sure that the caption formats of Figures obey the ICLR format. I also have a serious concern about the novelty of this paper. Finally, the experiments should be refined to support its main claims.<BRK>The paper is based on the FedAtt paper that calculates weights based on the Euclidean distance between the server model and each client and for each layer. The language and grammar could be improved, and some of the formulations make it hard to read. The comparison to Shapley values is also not motived in any detail, thus further reducing the paper contributions  value. They could also have taken additional steps to improve the claims  confidence, e.g., only one dataset was used, which is relatively weak compared to the original FedAtt paper.<BRK>Summary:The paper proposes a new contribution measurement approach for federated learning. IJCNN 2019 Strengths: (1) The motivation of the paper is clear. (2) The studied area is important. Weakness:(1) The proposed idea lacks novelty and may not be applicable in general federated learning algorithms. It is not clear how the proposed approach can handle such cases. (3) The experiments are weak and not clear. More datasets are needed. c) From Figure 2, it is hard to say that the proposed approach has a similar measurement with SV.
Reject. rating score: 2. rating score: 3. rating score: 3. rating score: 4. <BRK>The chosen tasks and datasets (MNIST, FashionMNIST) are rather limited as well. In particular, the training procedure does not seem to be optimizing any divergence of this model and it is not clear how the encoder relates to the posterior of this model. This seems like a non sequitur.<BRK>Why is this used? Similarly: What is the structure of the representations produced by the model? Pros:1) This is a new Gaussian mixture based model.<BRK>The outputs of the layer are the component probabilities at each location, followed by channel wise normalization. There is also a mismatch between the training objective and the sampling process.<BRK>Concerning the usage of DCGMM as an outlier detector, it is not clear why only the likelihoods on the top most latent space ($Z_D$) are employed. The authors use DCGMM as a simulator, i.e., to sample images and devise a sharpening scheme that tries to work around the non invertibility of pooling layers (but also of the GMM layer?)
Reject. rating score: 5. rating score: 6. rating score: 6. rating score: 6. <BRK>The paper proposes a novel nerual block based on the classifical spatio temporal self similarity (STSS), named SELFY, which can be easily inserted into nerual architectures and learned end to end without additional supervision. SELFY could capture long term interaction and fast motion in the video with a sufficient volume  of the neighborhood in time and space. The nice point of the method is that it is heavily investigtated through experiments. Moreover, the paper is clear and seems correct ,technically. comments: The first concern is about  the limited novelty of the method. Despite revisting the self similarity, from section 3.1, the learning of generalized and long term information is a property of the STSS rather than a contribution of this work. How are parameters (5, 9, 9) and (9, 9 ,9) chosen? The best result occurs as the  temporal offsets being chosen from { 3, ..., 3}.<BRK>### Summary: This submission proposed a motion representation method based on spatio temporal self similarity (STSS), which represents each local region as similarities to its neighbors in both spatial and temporal dimension. In contrast, we leverage the whole volume of STSS ......", as to view the proposed method as a generalized & rich optical flow. Furthermore, STSS should be helpful for view invariant action recognition, i.e., one of the fundamental challenges from video data recognition. Empirical results on Something Something V1 & V2, Diving 48, FineGym show that the proposed achieves the state of the art results, though seems marginally. Maybe this part is addressed in the experimental section somewhere. The proposed neural block, SELFY, includes 3 parts as self similarity transformation, feature extraction, and feature integration, it seems none of them is original but the combination of these put into the end of end NN structure is new as claimed by authors.<BRK>The paper introduces SELFY, a neural module that learns spatio temporal self similarity across longer timescales in both directions to obtain visual features that provide consistent empirical gains on three action recognition datasets. Strengths:+ The paper is well written and shows strong empirical gains over prior SOTA. + The experiments for testing the robustness of the learned representations is nice to see and helps highlight the strength of the proposed approach. My concern is that all of the chosen datasets are very motion centric where temporal relationships are more important and this makes it somewhat advantageous to the proposed model and does not really allow us to ascertain its applicability to datasets where appearance plays a bigger role such as Kinetics or HMDB 51. from Equation 1 is cosine similarity since it was not explicitly mentioned anywhere.<BRK>#### GeneralThis paper proposes spatio temporal self similarity (STSS), which captures structural patterns in space and time, for action recognition from videos. The novelty and the originality of the paper may be relatively low because the concept of the STSS itself was already proposed in prior studies as mentioned in the paper. It may be interesting to discuss the relationship with self attention based networks such as [1 3]1. I guess these option do not work well, but I think it is beneficial to list all the results.
Accept (Poster). rating score: 7. rating score: 7. rating score: 6. <BRK>The proposed paper introduces a novel approach for the segmentation of sequences. The proposed method is based on two sided power distributions (TSP) that are mathematically well define and enable differentiability. Replacing a hard segmentation function with a soft differentiable warping function (two sided power distributions) seems technically sound and is an interesting novel solution to a difficult problem.<BRK>[summary]The paper proposes a relaxed way to solve the segmentation of sequence that can directly leverage the deep learning architectures. The relaxed model allows each segmentation parameter to be a linear interpolations between two consecutive parameters depends on a continuous warping function. It seems that this is a very novel method and may have impact across the area.<BRK>The paper describes the use of two sided power functions for differentiable approximation of segmentation in discrete sequences with monotonic segmentation (each event in the sequence is defined by one continuous interval). There is a number of baseline techniques and datasets for segmentation that the authors could compare to.
Reject. rating score: 5. rating score: 5. rating score: 6. <BRK>**Sequential Labelling Bias**:The observation about sequential bias is interesting and probably valid. I hope these can be provided during the rebuttal phase. **Post Rebuttal**The authors have addressed some of my concerns during the rebuttal phase. I think this should be improved, to increase the understanding of this topic. Why is that? Where does this method differ from the PN network?<BRK>It would be better to have separate notations for the theoretical quantities and their estimates. In reality it is the output of the model and the probability is what it is intended to estimate. G_\theta is defined twice, as the posterior probability and also as the output of the model. 3) In general, it seems that q_i and posterior cannot be uniquely identified from the labeled data.<BRK>After reading the intro, it was very unclear to me what problem the paper was trying to solve and what the contributions of the paper were. In part, this is because the main problem addressed by the paper, "sequential bias", is not defined until the second page despite being reference earlier and in the abstract. I further think that the authors have adequately addressed the concerns raised by the other reviewers and recommend accepting the paper. I recommend acceptance, though I think the paper could be substantially improved.
Reject. rating score: 3. rating score: 4. rating score: 5. rating score: 5. <BRK>I do think this is an interesting and important line of inquiry, and could lead to valuable insights (eg.proving comprehensively that regularization based approaches need to do a better job of capturing the data distribution, or showing they can be improved by doing so).<BRK>Since this is what the paper sets out to do, I thus recommend that its current form not be published. Since there are very few papers discussing the theory of continual learning, this part should be an easy fix. I actually believe that this idea is very elegant, simple, and could be shown rigorously.<BRK>In particular, there is no clear argument for the claim on page 5: “However, by hypothesis, \omega_{t 1} does not model the data distribution from C_{t 1} and therefore it does not model data distribution from C_{t 1}  classes.”. The authors argue that these kinds of methods require task labels at test time to correctly distinguish classes from different tasks. The paper is in general well written and easy to follow.<BRK>The paper presents an interesting conclusion that kinds of regularizations cannot learn to discriminate classes from different learning tasks. Both theoretical and experimental analysis is provided. It’s not sufficient to show the importance of the proposed conclusion.
Accept (Poster). rating score: 7. rating score: 7. rating score: 7. rating score: 6. <BRK>The paper targets an important problem in open domain QA   the training of the retriever for the purpose of determining a segment that may contain the answer. This paper goes in the same direction. Overall, the paper describes an interesting contribution to the problem. I vote for accepting it. Strengths:The proposed idea is interesting and complements those used in the existing work. One could imagine that strong attention could be paid to wrong passages, as the reader does not know the ground truth. In contrast, the idea used in the previous work   the inclusion of the answer in a passage is a positive signal   sounds more intuitive. The paper contains Table 1, which partly show this correlation.<BRK>Minor comments:  In the third paragraph of the first section, it sounds like the authors introduce the retriever/reader architecture, which is standard (and otherwise clear from the rest of the paper). The novelty of this paper is the use of reader cross attention scores as a proxy to train the retriever. The models are otherwise standard. It seems to be improving the state of the art on three datasets and over several recent baselines.<BRK>Paper Summary:* This paper proposes a technique to learn retriever models for question answering that does not require annotated pairs of query and documents. The proposed technique uses  attention scores of a reader model to obtain synthetic labels for the retriever. Experimental results with NaturalQuestions, TriviaQA, and NarrativeQA show that the technique achieved state of the art results. * The proposed method is novel. * Experimental results are strong. This is a surprising result. I think it would be very useful to discuss the study: "Is Retriever Merely an Approximator of Reader?"<BRK>The authors propose a training technique for information retrieval models in the context of (open domain) question answering. Assuming the existence of some reader model, the idea is to use internal information of that model as a training signal for a retriever. I support the acceptance of the paper, because I believe the technique and the choice of model score (cross attention score) are both interesting contributions. As the authors say, training retrieval systems is tricky, since there is usually not sufficient labelled data available and it might depend heavily on the task. The iterative training that exploits internal state of the “downstream” model that they describe is an interesting idea that deserves attention from the community.
Reject. rating score: 4. rating score: 5. rating score: 7. <BRK>This paper proposes an orthogonal subspace decomposition (OSD) method for the face clustering task. The OSD module can learn more discriminative node features by combining subspace learning and feature selection. Experiments on datasets demonstrate the good performance of the proposed method. However, there are still several issues that should be addressed. 1.The novelty is my major concern. 1) What do the neural networks refer to? 2) In this paper, the authors didn’t introduce the baseline GCN M. Please give a brief description of the role of GCN M in this method. In particular, how about the performance of OSD LP(GCN M) without L_{SR}? 4.The description of the LP module is not intuitive enough. 5.Maybe authors will present an estimation of running time compared with state of the art methods.<BRK>This paper proposes an orthogonal subspace decomposition (OSD) module, which helps the deep network learn more discriminative node features in graph based face clustering. The proposed OSD module is grounded in mathematical. 3.The comparison with SOTA and the ablation study prove the effectiveness of the four components of the OSD module. Could this module be applied to other frameworks like contrastive learning? More experiments could be conducted to study the generality of the module. 2.Why are the best results in Table 2 inconsistent with those in Table 1? Does it due to the different experimental settings? 3.The first ablation study seems to have little relevancy with the proposed module, but is a study of hyper parameters of LP(GCN M). It could be replaced by other ablation studies which provide a deeper understanding of the OSD module. is it impossible to employ your method to handle other data?<BRK>For graph face clustering purpose, this paper proposed subspace learning as a new way to learn discriminative graph node features, which is implemented by a new orthogonal subspace decomposition (OSD) module. OSD aims at more discriminative node features, better reflecting the relationship between each pair of face. Extensive experiments show that OSD outperforms state of the art results on IJB B and VoxCeleb2 face datasets. It is good to see this paper made an attempt on leveraging subspace learning as a feature selection tool into GCN. I have the following comments/concerns. 1.It is unclear how much computational cost was added due to the introduction of OSD module in training stage, as the authors have claimed no computational cost for the inference because the two subspace matrices are no longer needed. If it was preset, it is good to clearly explain in the paper and discuss how the method can be extended in this direction.
Reject. rating score: 2. rating score: 3. rating score: 4. rating score: 4. <BRK>The authors propose to estimate individualized treatment effects in a setting they describe as continual learning, batches of data becoming available over time. As I understand it, adaptability refers to domain adaptation yet how domains differ, how to merge them depending on their differences, or even an investigation of these problems is not discussed. This does not strike me as a problem in causal inference where datasets are typically small. For instance, \mathcal X is defined as the set of all observed variables, but then a specific realization x is written to be an element of \mathcal X (as if \mathcal X was a measurable space of possible realizations of a random variable). The notation \mathcal X_d is similarly ambiguous, does this mean each domain has different sets of variables or that the spaces in which they are defined differ? The experiments are underwhelming and certainly do not back up the claims made in the introduction. Most benchmarks for individualized treatment effects are semi synthetic (it would be advisable to stick to these data generating processes while modifying them to highlight specific features of your problem).<BRK>This paper considers adopting continual learning on the problem of causal effect estimation. Consequently, the paper presents a system that makes use of existing methods as a loss function (the sum of losses and regularization terms). It is difficult to observe the novelty of the method there is no apparent challenge arose in combining these methods as they can just be simply added. In addition to the lack of novelty, the paper has many non negligible mistakes in describing causal inference. "Treatment and control groups" usually are used to describe two groups under the context of RCT. Further, the bias we are talking in the paper is a "confounding bias" not a "selection bias". There are many other places the authors mentioning "selection bias". Is it just observational data?<BRK>The method seems reasonably motivated but the discussion around it does not provide enough intuition about the influence of the different pieces on the final prediction, and no ablation experiment are provided. If the authors address these issues, I m happy to change my score. 1.Without any real data  (at least semi synthetic) experiments, I do not think there is a good way to evaluate the significance of the problem and the proposed method.<BRK>In five synthetic datasets, the proposed method achieves similar results to CFR C, a strong baseline which requires to store all data and needs to be trained from scratch. I have some concerns about the motivation of this work. It is not difficult to save a checkpoint of the model trained on original data and use it for the original data whenever you want, given the fact the neural network based causal inference models are small. Or you can let the model do the inference and save the results periodically. In section 2, there is an issue in the strong ignorability assumption. It would be great if the authors can clarify. The notation \mathcal{X}_d assumes that the feature space can change over domains, but the ignorability needs a unified feature space.
Reject. rating score: 3. rating score: 3. rating score: 4. rating score: 6. <BRK>In this centralised setting, the non iid problem does not exist. Assuming the authors understand FedAvg to be a conventional approach, the averaging with $|x_k|/|x|$ (Eq.1) is not uniform but weighted by the local dataset size. The explanation of the approach is confusing to me, however. The approach for computing pi_k is not clear to me and I would like the authors to revise it. Since the non i.i.d challenges in FL are not limited to label skew, I believe the authors should make this explicit. Many issues with this paper remain and I encourage the authors to overhaul their work. Maybe the other reviewers can comment further on this. The authors claim that  data points available locally could be biased from the overall distribution .<BRK>This seems like an arbitrary choice to me and there seems to be a flaw here: if a local device receives very little data but its data come from a mixture component with large weight, its gradient will likely be biased (due to the lack of data) but will still dominate others (due to its large mixture weight)   I would like to hear the authors  thoughts on this. A theoretical convergence guarantee for the proposed algorithm is also provided.<BRK>In Section 3.2.3, the authors state that they assume that data are drawn from a Gaussian distribution « without loss of generality ». However, it is well known that e.g.image distributions are not Gaussian. 2/ Acceptance decisionDespite its seemingly good experimental results, I am in favour of rejecting this paper as correcting its weak points would require major changes. 3/ Supporting argumentsA/ The theoretical results (Sec 4, one page) are irrelevant to the problem tackled by the paper. However, the proposed method is relevant for neural networks with batch normalisation layers, so more than 1 layer, and therefore these networks yield non convex loss functions, which do not satisfy the hypothesis of the results. C/ Writing is not very clear and should be improved.<BRK>Theoretically, the authors show that FedDEC enjoys the same convergence rate as normal SGD. If my concerns on the current manuscript (please see "Cons") are addressed, I will be happy to improve my evaluation score. The paper is well written. 3.Extensive experimental results are provided for the image classification tasks under the simulated non iid environment, which indicates that FedDEC enjoys high effectiveness in improving the model test accuracy under the data heterogeneity. Section 3 discusses the formulation of FedDEC, however, it’s not super clear on the exact steps that FedDEC will run e.g.what information will the clients upload to the data center? ; what are the exact operations that the data center will conduct? 2.From the description in Section 3, it seems FedDEC always assumes the local clients to train only ONE local epoch. 3.The convergence rate of FedAvg has been explicitly studied in [1]. It would be helpful to make a clearer comparison between the convergence rates of FedDEC and FedAvg.
Accept (Spotlight). rating score: 7. rating score: 7. rating score: 6. rating score: 6. <BRK>The authors propose an interesting direction toward generalization in reinforcement learning. The authors propose a policy similarity metric (PSM) for measuring behavioral similarity between states. The authors present a contrastive representation procedure to embed state similarity metrics, which are instantiated with PSM to yield policy similarity embeddings (PSEs). Furthermore, I like that the contrastive metric embedding approach is relatively simple and that PSEs can be learned alongside RL using an auxiliary objective. 2.I would like to see how this approach performs on environments specifically designed to test generalization, such as the ProcGen [1] and MineRL [2] environments.<BRK>Strengths:   The proposed policy similarity metric is well motivated for generalization in RL, which is an important problem. The paper is very clearly written and well structured. I think including such a study would greatly help understand the sensitivity of learning PSEs to quality of the policies. I think the work is well motivated, approaches an important problem, and conducts very thorough experiments. Specifically, I’m trying to understand if the gains are only from PSEs as opposed to the combination of PSEs and data augmentation.<BRK>This paper defines a new metric for calculating similarities between states. The policy similarity metric is basically based on the $\pi$ bisimulation metrics, but the l1 distance term of rewards is modified to the distance between states transitioned by the grounded policy. [Originality & Significance]The originality and significance are sufficient for acceptance, as the authors devised a novel metric. [Strengths]+ Even though I didn’t thoroughly check all of the derivations, the metric seems to be more precise than $\pi$ bisimulation, similarity between states originated from different environments. + This approach can be easily applied to various RL models. Does the proposed method’s performance highly depend on hyperparameter settings?<BRK>Motivated by the issue of RL policy generalization, this paper explores improving generalization via a learned contrastive representation to embed states in, rather than through data augmentation or regularization alone. Strengths:+ I think this approach is theoretically well motivated, and the authors also give good intuition, especially in light of the latest work on self supervised learning.
Accept (Poster). rating score: 7. rating score: 6. rating score: 6. rating score: 5. <BRK>My questions and concerns were appropriately addressed. However, other reviewers raised some concerns regarding the experimental set up that I have missed. Thus, I will keep my score as is   7: good paper, accept. Summary:The submission studies Gaussian process models with the infinite width neural network kernels. In particular, through a series of experiments, the paper investigates their uncertainty properties and answers the question “how calibrated are the predictive uncertainties for in distribution/out of distribution inputs?”: i) a comparison between GP classification with the infinite width neural network kernels and finite width neural network classification was provided to test the calibration, (ii) a study of these kernels on regression tasks where the GP posterior can be obtained exactly, (iii) a study of using GPs with these neural network kernels on features extracted from a pre trained network, aka, deep kernel learning of Wilson et al with neural network kernels. Overall, the performance of these GP variants is promising and competitive to the best existing methods. The experiments are well designed and the comparison to relevant state of the art methods was provided. Whilst the theoretical contribution is on the light side (no new models or algorithms proposed in this submission), the results from the experiments outweighs this weakness and thus make this paper potentially relevant to the large Bayesian deep learning/GP community. At the risk of asking too much for a conference paper, perhaps since this submission is about understanding the uncertainty of these GP models, an experiment or two on this aspect could be included. Though there are approximate sampling methods that can scale to the network size considered in this submission but these were not considered. Matthews et al had some comparisons like this using BNN with HMC and GP regression.<BRK>No analysis of results here. I have improved my rating. I not not share the objection of reviewers 5 and 4 about the small size of the CNN, resulting in an inferior baseline in section 3, as the approach is still impractical and mostly a proof of concept that should encourage further research. They should be either explained/exploited or removed (recommended to make space for better explanations elsewhere)Appendix: lots of good material there, some of which could be moved to the main paper (and some material in the main paper should be moved to the appendix, in particular some rows in table 1 and 3) What matters is that improvements in section 5 are built upon a SOTA baseline, inspired by section 3. The key innovation presented in section 5: the NNGP is just added as a calibration layer on top of a pretrained NN. Results look excellent, but in the way they are reported in figures and tables, there are too many questions, inconsistencies and readability issues to be sure. In summary, this seems to be an unfinished write up where the rich material needs to be better organized with a clear presentation flow, and tables and figures need serious improvements. This seems extreme as I see an O(N^2) complexity in other work (Lee et al.) Same issue with the OOD data: it just appears in Figure 1 and table 1 without proper analysis. One later learns those are the same as Ovadia shift levels. What is “accuracy”?<BRK>However reviewers #1 and #2 make a good case that what matters are the improvements presented in Section 5. The authors should clearly caveat in section 3 that the results may not extrapolate to bigger CNNs or, ideally, add a data point with bigger CNNs. 2: SNR  > show acronym. ## Table 1 s architectures are too small (2 layer CNN?) Transforming the prior into a posterior using Bayesian updating is one such decision. This architecture is simpler than even the 5 layer 1998 LeNet , and is very far from the 2015 ResNet (He et al.) I realise now that by this you mean the same as what I asked above, whatever it is. (In fact, in my experience, variational inference for GP classification with these kernels has worse accuracy than classification as regression). ## Ignores similar work doing Bayesian linear regression on top of a NN baseThe paper "Deep Bayesian bandits showdown" (https://arxiv.org/abs/1802.09127) compares various Bayesian deep learning methods on a bandit task, where calibrated uncertainty is crucial. The one that seems to come out on top is to change the last layer of a network to a Bayesian linear regression layer. Table 3 should also acknowledge Bradshaw et al.(2017, https://arxiv.org/abs/1707.02476), which adds an RBF kernel on top of a neural network. All of these should perform similarly to NNGP LL, especially given how close the RBF kernel is to the NNGP in Table 2.<BRK>This is an interesting paper which evaluates the calibration of NNGPs, including around OOD detection. 2.I am a bit confused about the notion of an "ensemble" of NNGPs. 3.However, my primary concern is around the strength of the NN baselines. They find that the performance of their CNNs and C NNGPs with CIFAR 10 is very similar, with around 60% accuracy. For C NNGPs,  60% accuracy is pretty standard, but for NNGPs, it is really, really bad: with very little effort it is possible to get 90% accuracy, and with a little further tuning, you can get to 95%. This poor performance brings into question the relevance of their results. 4.It isn t clear what the infinite width layers give you that s better than either using Bayesian linear regression over the output weights, or deep kernel learning. 3.Exactly what is in table 3 is very unclear: does it aggregate over corruptions? What are 1k etc. (all of this should be in the caption).
Reject. rating score: 3. rating score: 4. rating score: 4. rating score: 4. <BRK>However, the experiments of the paper show that the network under study is not as competitive as previous baselines cited in this paper. c) The analysis of the homogeneous network and their accuracy/cost trade off.<BRK>Studying the use of parameter sharing in this context would thus also increase the potential impact of this work. Strengths:  Studying the effect of parameter sharing vs the use of independent parameters in recurrent types of architecture is interesting and could lead to a better understanding of these architectures  The paper is clearly written, and the methodology would be reproducibleWeaknesses:Contribution:  While I do believe that such a study could provide the community with a better understanding of architectures with re used blocks, the scale of the study performed in this paper is too small to draw conclusions. Minor comments:  In the caption of Fig.1, the authors mention that the parameters of the classification block are never shared. There is therefore no evidence that the conclusions drawn here will generalize to other architectures/tasks, which significantly limits the potential impact of this paper on the community.<BRK>Empirical studies are conducted to show the superiority of such homogeneous networks over ENet and CGNet on CamVid and Cityscapes datasets. Concerns:   The paper is not well organized which makes it hard to follow. The authors should revise the writing and clearly introduce the challenges, the contributions, the proposed architecture structure, the design insights, the experiment settings, and so on.<BRK>Pros:*  The idea of using a homogeneous network is interesting and re usable blocks are reasonable for the anytime prediction setting. *  The paper is easy to follow. The paper claims in the abstract that the method was evaluated on cityscapes but in the discussion section, it says they couldn t provide cityscapes experiments due to memory issues. 5.Related work on image pyramid and feature pyramid. In this paper,  the data block preprocesses the images into multiple scales and the re usable building block concatenates the features from multiple scales. However, the current manuscript still needs much improvement.
Reject. rating score: 6. rating score: 6. rating score: 6. rating score: 6. <BRK>The algorithm is a two step process, first selecting a set of "candidates" based on individual features, then filtering to a final subset which may account for interactions between candidate items. Very few grammatical mistakes. A significant amount of active learning literature deals with subset selection. It would be helpful for the authors to compare/contrast their proposed method with existing active learning results. However, I bring this paper up because it discusses a similar topic of trying to select data points from a larger dataset that provide a reasonable estimate of a loss function on the "population" vs. a "sample." However, the paper provides little (if any) intuition or formal justification for why the proposed method performs better. 6.The writing and figures lack clarity and are often difficult to follow. Is (4) an objective function? And why is there a period in the middle of the expression?<BRK>On the other hand the paper is hard to read and the experiments, while good to show the strength of the approach, do not cover usefullness for the mobile use case. The paper show how this general method can be applied to several tasks. For example, $D$ can contain the pixels of an image and the task can be to reconstruct the original image from the subset. This corresponds to learning to compress. As another example, with the same $D$ but a label prediction task, SSS learns to dynamically select sparse features for a classification task. The other tasks considered are dataset distillation (compressing a data set) and selecting prototypes for few shot classification. $D_s$ is constructed in two steps. This convinced me of its applicability. I think that the image reconstruction experiment should include a comparison to compression algorithms. I am on the fence with this paper. The current results are good and I think the proposed approach has some potential and will be interesting for the community.<BRK>This paper proposes a stochastic subset selection method for reducing the storage / transmission cost of datasets. The data selection algorithm consists a candidate selection stage and an autoregressive selection stage, parameterized with neural networks, and are trainable by gradient methods. The proposed method also seems to be more general than competing methods, such as coreset. From the selected images (e.g., Fig.10 and 11), I also don t gain an intuition on what does SSS actually select, and how do the selected instances differ from random subsampling. 2.The paper claims to address the dataset redundancy. From to my understanding, that is, removing similar instances. However, this claim is not explicitly supported by the experiments.<BRK>This work introduces a method to select instances from any set (stochastic subset selection, or SSS). The experiments demonstrate a diverse set of use cases, including feature selection and core set selection. The proposed approach is a two stage method involving candidate selection (learning a function $\rho$ to determine a Bernoulli probability for each input) and AutoRegressive subset selection (learning a function $f$ to generate probabilities for sampling elements from a reduced set); both stages use the Concrete distribution to ensure differentiability.
Accept (Oral). rating score: 8. rating score: 8. rating score: 6. <BRK>The authors provided a framework for inverse graphics, i.e., infer 3D mesh, light, and texture from a 2D image. They first use StyleGAN to generate realistic multi view images, and then trained their model with a differentiable graphics renderer. Real images look real, but we usually do not know the viewpoint (most annotated datasets are either small or have limited annotation quality). The authors proposed a method to generate images of the same category of objects with the same viewpoint (Figure 2), which addresses this issue   the viewpoints are known, and the images look real. The experimental part is impressive. It is a wise and elegant way to control the viewpoints of an object in StyleGAN. In summary, compared to previous methods, generating multi view images from StyleGAN solves the unrealistic and unknown viewpoint issue, and the results look impressive.<BRK>This paper proposes to couple a GAN, an inverse graphics network, and a differentiable renderer. The differentiable renderer is used to synthesize 2D images from 3D, which can be compared to the input for consistency. Strengths  The presented results look great. Weaknesses  The paper is overall well written, but leaves out some technical descriptions which make it not self contained and hard to reproduce. While many results are shown in the supplement, I would have loved to see a video that shows more results. Even if the results are not perfect, a video would help to judge the overall quality of the results. Summary  This paper shows an interesting pipeline with good results.<BRK>This has been an important hope as an application of GAN, and the beginning of the results section (4.1) seems to emphasize this aspect: less annotation time and higher quality than training a differential renderer on Pascal3D. This is again very appealing, but done in an unsatisfying way and with limited results (only on cars). The results of this step look good, but are only shown for cars, since reconstruction was also done for horses and birds, I can only assume that this doesn t work for other categories. To summarize, I think there are several cool stories in this paper and some appealing results. This would also be more similar in spirit to the proposed approach.
Reject. rating score: 4. rating score: 4. rating score: 6. rating score: 7. <BRK>#################Post Rebuttal Reviews: Thank the authors for the detailed responses. The proposed approach is an interesting add up for the Laplacian approximations. To enhance the capacity of Laplacian approximation, this paper proposes the *learnable uncertainty units (LULA)*. The LULA units do not affect the network prediction, but the Hessians of the LULA units are nonzero. However, the current version of the paper leaves me several questions regarding the methods. **Experiments**This paper conducts experiments for out of distribution detection in order to test the performance. These experiments are only my suggestions. In this scenario, the proposed method is not that different with a parameterized variance neural network $f_{var}(x)$.<BRK>The paper proposes a post hoc uncertainty tuning pipeline for Bayesian neural networks. More specifically, it tunes the extra weights by optimizing another objective borrowed from the non Bayesian robust learning literature, which encourages low uncertainty over real (extra, validation) data, and high uncertainty over manually constructed, out of distribution data. Another confusion I have is more related to the math behind. How would these values affect the uncertainty if their Hessian values are zero? It would be beneficial if the authors could explain more about why that s not the case.<BRK>***END OF UPDATE***The paper proposes an uncertainty estimation method for deep learning. The idea is, building on prior work on Laplace approximations for neural networks, to augment a pre trained network with additional units such that predictions with point estimates remain unaffected, while the variance may change. The weights of those units are then trained on the validation set/out of distribution (OOD) data such as to minimise the variance on the in distribution data and maximise it on OOD data. I assume you are treating the Hessian as constant? * That being said, I find the evaluation a bit narrow in that it only looks at out of distribution detection. Could you add e.g.an evaluation of the robustness to data shift?<BRK>END OF UPDATE The paper proposes a new method to learn uncertainty under Laplace approximations. The method relies on uncertainty units that do not change the predictions but increase the dimensionality of the parameters and help learn better uncertainty by the proposed loss function. Questions:  What are the hyperparameters that the method is sensitive for? I recommend to accept this paper.
Accept (Spotlight). rating score: 9. rating score: 6. rating score: 6. rating score: 10. <BRK>The wide diversity of tasks for which the model works well is unprecedented. Moreover, methods that learn to simulate physical processes more efficiently, and applications of graph neural networks are two research directions that have garnered a lot of interest in recent work. As an intersection of these two areas this paper should be of interest to a wide audience in the conference. In order to work on meshes, some modifications are proposed.<BRK>Can problems with millions of nodes be simulated on a recent GPU? The experiments demonstrate better performance than grid based or other graph based approaches on the selected problems. I think the paper has some good results but only scratches some important questions with respect to the mesh. I hope my questions and concerns in can be addressed. Cons:* The claim that this approach can be used to speed up simulations is not well supported. (The appendix mentions a V100 GPU but I could not find information about the CPU which is used for the GT simulation.) As an alternative the graph network can be run on the CPU.<BRK>ContributionsThe authors present a graph convolution architecture suitable for learning physical simulation on meshes. This way is experimentally and quantitatively demonstrated to be robust. 6   How are mesh space coordinates u_i’s attributed? The paper is well written. Is this an oversight?<BRK>The paper proposes a generic strategy to accelerate time integration of physical systems, including elasticity, fluid simulation, and cloth simulation. Would the method also work without and is this an extra feature? My understanding is that this is optional, and only helps, in particular for cloth. Is this due to training with noise? If so, are all of these using a single core or multiple?
Accept (Spotlight). rating score: 7. rating score: 7. rating score: 7. rating score: 6. <BRK>This paper proposes a method to improve data efficiency of DQN (specially, the Rainbow DQN) by training the reinforcement learning agent to jointly minimize the DQN loss and a loss for multi step predictions on its own latent states. + The paper is well written and generally easy to follow. Why is the prediction loss chosen to be negative cosine similarity? Does using quadratic loss work? The paper states that "using a separate target encoder is vital in all cases", this is confusing as the paper states later that with data augmentation, this is not needed (\tau 0 works best).<BRK>This paper proposes Self Predictive Representations (SPR), a self supervised representation learning algorithm designed to improve the data efficiency of deep reinforcement learning agents. The implementation is based on Rainbow as mentioned in the paper. There are interesting  ablation studies and insights in Section 5 but the authors could provide more. Regardless, Dreamer seems to be a an interesting base model to compare to.<BRK>Despite the aforementioned issues, the paper is mostly well written and the presented idea is valid and important. The authors use predictions in the latent space by using a learned transition model. To further improve their method, they add data augmentation to enforce consistent the representation to be consistent over multiple views of an observation.<BRK>The authors propose learning self supervised representations that are consistent across future time steps. They experimented with applying augmentations, as well as dropout of the features of both online and target networks (without augmentation). Thus, BYOL is computed as a sum of these two cosine similarity losses.
Accept (Oral). rating score: 9. rating score: 9. rating score: 9. rating score: 8. <BRK>The proof is complicated though and requires the analogy to the kernel regression as basis. This paper only discuss results of neural network using ReLU and GD.<BRK>Some comments/questions to the authors:  In Section 3.2, “diversity” of a distribution is informally defined in terms of training support and direction. The paper is well written, ideas and definitions clearly explained and experiments laid out in detail.<BRK>## SummaryThe paper studies how neural networks extrapolate. The authors theoreticallyexamine two layer ReLU MLPs with mean squared loss in the NTK regime and,building on these results, GNNs. + The mean squared loss is used throughout the paper. Overall I found the paper very interesting and fun to read.<BRK>The paper is well written and the recent literature is properly reviewed.
Accept (Spotlight). rating score: 8. rating score: 7. rating score: 6. rating score: 3. <BRK>This paper defines a new problem named slowdown adversarial attack to decrease inference efficiency of early multi exit networks. This is validated by the results, where the slowdown adversarial images have an 5% or 15% lower accuracy compared with clean images. 4.Last but not least, the authors show that traditional adversarial training is not a cure for the newly proposed slowdown attack, showing the challenging character of this new threat model. The current method pushes model predictions to uniform distribution, so it may be ineffective to generate adversarial images to fool classifiers.<BRK>Adaptive multi exit network is, by itself, a pretty new and under studied topic, let alone the adversarial study on top of it. It also proposed an efficacy metric for better evaluation. The results on white box and black box attack demonstrate the effectiveness: DeepSloth not only hurts the accuracy (also achieved by baselines), but also hurts the efficacy (only achieved by DeepSloth). So if we already got wrong prediction, why we care efficacy (which may cause inference timeout)? This is novelty of this paper, so I think should be emphasized somewhat.<BRK>Summary of the paper This paper studies a new category of adversarial attacks, i.e., attackers that try to slow down multi exit DNNs using adversarial examples. Clarity This paper is well organized and clear. Strongness   The paper suggests a new threat model. Weakness Some of the motivation of the attack is unclear.<BRK>This paper aims to indicate a new direction to conduct an adversarial attack: to delay the inference time/raise the computational costs of a  multi exit  model. However, both the motivation and novelty are lacking, I will argue for the rejection if other reviewers wish to accept it. From the perspective of the industry:1. This paper only employs the PGD attack and the adversarial training on a specific task, the novelty is lacking.
Reject. rating score: 6. rating score: 6. rating score: 7. <BRK>This paper investigates the phenomenon of double descent, also referred to as "more data hurts", in high dimensional linear regression using the least square estimator. The authors of this paper try to provide a new type of results that fill the gap between the two regimes (asymptotic vs non asymptotic). To do so they have managed to derive second order (CLT type) asymptotic results for different risks based on more refined random matrix theory results. #######################################################################pros:Asymptotic confidence intervals for different prediction risks  are derived. These results seem new. Indeed, using just first order asymptotic results we had to take both n and p to infinity and compare the ratios p/n. I think at this stage the authors should take the time to explain more their results and how they are different from previous ones. In particular by saying "explicitly" what happens when the sample size grows. Although that was clear from your introduction, but it is always good to draw conclusions for the reader after you state your results and remind your contributions.<BRK>This paper provides the central limit theorem type of results for generalization error of high dimensional least squares estimator. Previous results have shown that the prediction error converges to the asymptotic limit, but it is unclear how fast the convergence speed. This CLT type of results provides the answer and this explains why empirical simulations have shown good accuracy for a sample size of just 300 500. I think it is a nice piece of result to be added to the current double descent literature. First, this paper only focused on the cases when features are isotropic or signals are isotropic. The story of this paper will be more complete if the authors can also show results on these two points.<BRK>**Summary**: In this article, the authors characterized the second order fluctuation of the prediction risk of the (min norm) least square estimator, by assuming an underlying noisy teacher model $y_i   \beta^T x_i + \epsilon_i$, in the regime where the data dimension $p$ and the number of training samples $n$ grow large at the same pace. To the best of my knowledge, there are very few results on the second order fluctuations of the prediction, one possibly relevant paper is "Asymptotic normality and confidence intervals for derivatives of 2 layers neural network in the random features model" at NeurIPS 2020. **Recommendation**: This is a good paper that made solid contributions to the theoretical understanding of high dimensional least squares estimators and consequently, shed interesting light on the future design of more elaborate machine learning systems. I thus recommend it for publication at ICLR. **Detailed comments**: * Sec 1 introduction: "However, the existing asymptotic results, which focus on the first order limit of the prediction risk, cannot exactly guarantee the more data hurt phenomenon": it would be helpful to provide a more concrete illustrating example for the insufficiency of the first order analysis, or perhaps simply refer to the discussion above Figure 1 below. * The x  and y axes are hardly visible in Figure 1, and the same applies to other figures in this article as well. * below (1): is the independence between the entries of $\mathbf{x}_i$ necessary, should this be stated here?
Reject. rating score: 3. rating score: 6. rating score: 6. <BRK>This is done with a generative framework that generates both bounding boxes and segmentation masks for each object. CONS* The paper can be hard to read. * Good results, but on two toy datasets only. COMMENTSThe writing should be improved, as the paper can be hard to follow. On the other, the authors cite many concepts without introducing them in the paper (stick breaking, spatial broadcast decoder, multi otsu thresholding, etc) which makes it non self contained. I am not an expert on the topic so I may have missed relevant datasets/baselines. Detail: "Region of Interest" introduced after ROI has been mentioned several times.<BRK>An additional loss is introduced during training to make the segmentations produced by the MONet segmenter consistent with the proposed bounding boxes. does it tell us some fallacy or failure of the original model and if so, does it fix it? These are lacking here. * In general   I feel the performance on these datasets is quite saturated and I hope to see results on more challenging data in the future   the proposed method included* There is very little discussion about the choice of hyper parameters in the paper   how were they chosen? Post rebuttal comments:Thank you authors for the detailed response   I think some of of my concerns have been answered   the paper may be a valid contribution to the community and I am raising my score.<BRK>Update:In general, I am happy with the authors  responses. Instead of purely generating foreground masks in an RNN, they simultaneously predict the bounding boxes and segmentation masks using a Faster RCNN based framework. They did show the advantage of the introduced self supervised loss. Despite that such self supervised works are hard to work on real scenes, this paper does have some merits. [Paper strength]  The paper is well motivated, and the proposed approach seems to be reasonable. The self supervision between segmentation masks and detection bounding boxes is the main contribution. The selected K value is unclear. The visual results cannot demonstrate the advantage of the proposed approach.
Reject. rating score: 5. rating score: 6. rating score: 6. rating score: 6. <BRK>#### SummaryThe paper proposes a constrained reinforcement learning (RL) formulation relying on constraints written in a formal language. "Verifiably safe off model reinforcement learning." The paper cites some of the related works; however, it is unable to distinguish its contributions compared to the existing methods. 2.The experimental results of the paper are limited in terms of comparison with the existing methods. In particular, the only comparison is with baselines that do not use this automaton based side information. However, I am still inclined toward rejecting the paper. Nonetheless, the soft version of the constraints still lead to constraint violations and provide no safety guarantees. While the reader can probably infer some of them, the paper should be self contained in this regard.<BRK>update after rebuttal: I think the paper, with the added discussion, improved. The constraints are given as finite automata which are basically used as a product with the original MDP state space. The usage of formal languages is relevant and has been demonstrated to be helpful in a number of resultsWeaknesses  The contribution of the paper is not clear. Several other works have done very similar approaches, and in much more depth. I don t see the novelty, but I m happy to be convinced otherwise.<BRK>The paper builds on the constrained MDP framework (Altman, 1999), by considering the special case where the cost functions are defined in terms of states from a parser of a formal language. Although as the paper notes, formal languages (especially those derived from LTL formulae) have been used elsewhere in RL. Some clarifying questions I d also appreciate if the authors could address during the discussion:  1. The method requires hand designing appropriate constraints. It is not defined anywhere that I can see – should this be $\Sigma$? The clarity of the submission could be improved in places.<BRK>#### SummaryThe authors propose to use formal languages, specifically DFAs, as a mechanism to specify constraints in a constrained MDP setting. Results are largely in favour of the proposed method, esp. #### Cons  The significance of this work is perhaps lower. This makes action shaping significantly easier. Does the DFA get reset in the next step? #### ConclusionOverall, even though the significance is perhaps limited, I vote to borderline accept this paper due to the clarity and thorough evaluation.
Reject. rating score: 5. rating score: 5. rating score: 7. rating score: 7. <BRK>Pros:1.A very thorough set of experiments are given to explore how transformers can be used to track states in a game such as chess and results show that with enough data, transformers can predict the locations of pieces across a decent amount of history as well as predict legal actions. The authors say that they are using transformers to see how transformers can learn grounded language when world states are available but I do not see this work positioned with respect to other work on grounded language nor work on state tracking generally found in model based RL. the main contribution is the experimental design and the results themselves. Overall, the paper has some interesting ideas, experiments, and results but is not connected to the motivation/is not positioned well with respect to closely related work. Post author response:See comment below for further score justification.<BRK>It is not clear what you do with games of length 100 150. ### Pros: 	Blindfolded chess is an interesting benchmark for grounded language learning and as a testbed for models to track world state. ## Author response updateIn light of the author response, I have decided to increase the score to 5. ## Cons: 	The paper feels rushed at times: for instance, there are no references in text to Appendix D, despite it being an interesting demonstration of the analysis that can be done in this environment. The comparison to related work on world state tracking and grounded language learning could be substantially improved. This is good experimental work. This makes the contribution of this paper very narrow.<BRK>Summary: This paper is an interesting exploratory study analyzing the ability of language models to track the state of a chessboard. The authors adopt a clever chess notation which allows them to probe the language model s state tracking ability by looking at its next word prediction (akin to probes in [1]). 2.Well designed experiments and very interesting results in a mostly unstudied area. 3.Wel written paper with several baselines and ablations. The authors have conducted well thought experiments and reported interesting results. I m leaning accept, but I encourage the authors to keep working on this setup (perhaps using some of the suggestions discussed above) and try to check if any of the insights here can be transferred to better understanding of language models on natural language.<BRK>The former metric ismore stringent, as it s also measuring the strategic awareness of the model. + The paper is very clearly written, well organized, and easy to follow. Areas for improvement/questions for the authors:    I m a little confused as to why performance peaks with p 0.05/0.15 in the    UCI+RAP models. The reason for this is given as greater    "mismatch between training and inference"   I m not sure what this means. But my understanding was that this oracle would serve    as an upper bound on model performance. So how are we outperforming the    upper bound? Couple of minor issues:    At the bottom of Page 6, the text references results in Table 3, which    should be Table 2.
Accept (Poster). rating score: 8. rating score: 8. rating score: 7. rating score: 5. <BRK>* First result on convergence rate and global optimality of single timescale actor critic, which is very significant. Based on the mathematical rigorousness of the main text and the proof sketch, I assume that the paper is technically correct.<BRK>This is a theoretical paper. The paper studies convergence and optimality of the actor critic with the function approximation in a single timescale setting. The proposed scheme could complement the previous study of the AC methods in the two timescale. (3) The proposed AC methods rely on the population quantity, e.g., expectation over state action visitation probability. For me, the development of this paper is new, and I am more inclined to agree with the acceptance.<BRK>This paper studies the finite time performance of actor critic algorithm with PPO type update. (3) It seems that the neural network parameterization studied in the appendix is still a nested loop algorithm, not a single timescale algorithm that highlighted in the title and the story of this paper. I agree that it is meaningful to study the DNN in AC for the first time, but can the author compare the overall sample complexity in this paper with the sample complexity in Liu et al.(2019)?Does the LSTD critic help to improve the overall sample complexity comparied with the TD(0) type critic in Liu et at. (2019)?(4) It seems that the global convergence guarantee is highly depend on the soft max parameterization, which has limited practical applications.<BRK>The authors claim the one advantage of the single timescale setting is the fact that it is amenable to the off policy setting. Why can t the same argument be made for the two timescale setting? Another confusion I have is that the paper nowhere assumes that the optimal policy is contained in the policy class. How does one get a global optimality result without this? but gives no intuition.
Reject. rating score: 3. rating score: 4. rating score: 5. rating score: 5. <BRK>This paper compresses neural networks via so called Sparse Binary Neural Network designs. Experiments on small MNIST and CIFAR 10 datasets with two shallow and old neural networks are provided. This paper has obvious weaknesses. Unfortunately, they are missed by the authors. Poor writingThe paper is poorly written, including introduction (messy), related works (poor), proposed method (tedious) and experiments presentation (weak).<BRK>this paper propose sparse binary neural network for model compression and efficient inference on IoT devices. The paper combines pruning techniques and binary neural network techniques, to train a model that is ternary: {1, 0,  1}. I m also concerned about the novelty combining pruning techniques with binary neural network techniques.<BRK>Strength:  The paper is easy to read  The compression rate is very high  Comprehensive simulations for different hyper parametersWeakness:  Misleading notations  Simulation results are obtained from small datasets  Significant overlaps with previous works (limited novelty)Detailed comments:  Section 2 under the label “Sparsity”: It is not clear what the authors mean by using {0, 1} for weights instead of { 1, +1}. However, they fall short in providing a good accuracy for state of the art networks on ImageNet. Section 4: It would be interesting to see a comparison with ternary neural networks (e.g., [2]) in terms of sparsity degree and accuracy.<BRK>The demonstrations are moreover limited to MNIST and CIFAR10, meaning that there aren t any indication of how the method would fare on larger datasets like ImageNet, where the advantages of reduced precision and sparsification are arguably going to be most relevant. The strategy that the authors adopt is to modify binary neural networks like those in the mentioned references of Courbariaux et al.by encoding weights as 0/1, instead of  1/1. While practically valuable as a piece as a demonstration of how tolerant Neural Networks are to reduced precision and sparsification, this paper is methodologically quite incremental, if compared to Binary Net and all the subsequent papers deriving from that one.
Accept (Poster). rating score: 7. rating score: 6. rating score: 6. rating score: 6. <BRK>My understanding is that this paper extends the approach from https://www cs.stanford.edu/~acoates/papers/coatesng_nntot2012.pdf to use *patches* from an image as filters I enjoyed this paper quite a bit and the numbers are quite impressive (88.5 for a 2 layer network on Cifar 10 is great), and its good to understand these methods fall short on ImageNet, this may make sense for a 2 layer as the spatial pooling destroys a lot of structure. It is interesting to see these methods outperform a scattering transform. I enjoyed the rigorous ablations and study of the spectrum of patches.<BRK>SummaryThe authors propose using a data driven dictionary of patches on CIFAR 10 and Imagenet, with a shallow classifier (linear / one hidden layer CNN) obtaining results comparable to end to end deeper architectures. The proposed method consists in random sampling a dictionary of patches (2 * 10^3 for ImageNet) and using it to encode a binary representation (feature map), with Q nearest patches in the dictionary. Another strong point of the paper is that it achieves an impressive accuracy (88.5) on CIFAR 10, almost on par with end to end AlexNet and other patch based methods, using only a small fraction of the patches obtained from ImageNet. evaluation only on CIFAR 10 and ImageNet   could improve this by comparing with the scatter transform of Sifre & Mallat, on a common benchmark, e.g.texture / material dataset). In Sec.4, please add a brief explanation of spectrum definition and intrinsic dimension.<BRK>Briefing:This paper mainly investigates the patch based pre processing for image classification. The patch based pre processing is done by a simple convolutional kernel constructed in a data driven manner. Figure 3 seems to analyze the dictionary by covariance spectrum and intrinsic dimension, but it wasn t easy to catch the main idea. A more precise explanation of the experiments and the analysis is required. Comments:(1) Table 1.a shows that the proposed method achieved good performance despite the hard assignment. Or, if not, the importance of the dictionary construction on the classification task would be required. (6) It wasn t easy to catch how the paper constructs the kernel Phi, from the method.<BRK>This paper proposes a powerful non learning Kernal based baseline for ImageNet classification. The proposed non learning Kernal based baseline (which can be interpretable to a vector quantization) shows comparable results (88.5) with AlexNet (89.1) in CIFAR 10 top 1 accuracy. I think this comparison fairly shows that the proposed method is efficient than deep models in terms of the number of parameters. Hence, I d like to change my score from 5 to 6. **Cons and questions****[Motivation to the shallow method]**I wonder about the motivation of the proposed kernel based non learning method. ICLR 2019It will be interesting to many readers if the authors can provide additional analysis on the "important" patches for the image classification tasks. During the rebuttal process, the authors address most of my concerns well in their responses.
Reject. rating score: 4. rating score: 4. rating score: 5. rating score: 6. <BRK>This paper proposed a new distributed training method for GNNs. Thus, this dependence incurs communication between different workers in the distributed training of GNNs. It also provides some theoretical analysis and conducts the experiments to verify the proposed method. 1.The idea is simple. However, this method only focuses on the former one.<BRK>The paper presents a simple method to reduce the communication for the distributed sampling based training of GCN. How does the proposed method scale with more workers and larger compression ratios? 2) The proposed sampling method and the original sampling method. How will the idea of sampling local nodes more frequently work for other sampling methods? In experiments, the authors compared the proposed skewed linear weighted sampling with the LADIES sampling method. 4) The importance of distributed sampling based training of GCN is not explained.<BRK>This work considers the challenge of distributed training for GNNs. The approach is a locality aware importance weighted sampling procedure. The writing is good; the reader can quickly understand the approach and the main points of the paper.<BRK>The paper presents a sampling based approach to speeding up training of GCNs in distributed systems. To reduce this communication, the paper introduces a variant of the node sampling approach of Chen et al.(2018b) and Zou et al.(2019), where the probabilities of nodes in other machines are scaled down by some factor s. The approach is evaluated experimentally, showing that it reduces the amount of communication while (essentially) preserving the accuracy.
Accept (Poster). rating score: 7. rating score: 6. rating score: 6. rating score: 6. <BRK>### SummaryThe paper proposes a way to impose trust region restrictions via projections when doing policy optimisation in Reinforcement Learning. The projections have a closed form and enforce a trust region for each state individually. The authors propose three types of projections based on Frobenius, Wasserstein distances and KL divergence. They compare them to the existing methods (PPO, PAPI) and provide some insights about their behaviour. I would like to see more discussion on what the numbers tell us. If your method properly imposes trust regions, but the results are comparable to PPO, does it mean that approximate trust regions are okay? The problem the authors consider is important, and the proposed solution is reasonable theoretically and practically. Why is having this beneficial? I think, there should be a clear description of what a  differentiable projection layer  in more details.<BRK>The authors explore the use of KL, 2 Wasserstein, and Frobenius norm in order to derive trust region projections in DRL. The topic is relevant and novel   specially given the prevalence of TRPO and PPO in RL in recent years. I would like the paper to better motivate  why these three (KL, Wasserstein, Frobenius) were chosen. the rationale to select the right one for a given problem. I also find that the discussion on entropy control, although interesting on its own, somehow distracts from the main message of the paper.<BRK>The authors try to solve this issue by introducing the closed form derivation of trust regions for Gaussian policies with three different types of divergence (or distance). (p.7, Experiments) For our experiments, the PPO hyperparameters are based on the original publication (Schulman et al., 2017). Successive policy updates seem to improve the methods in a way that the mean is well bounded during updates, but its explanation and justification are difficult to understand. I don t know if the authors will keep working on this direction, but I think it will be interesting to see the performance of *off policy RL* with the proposed method. To leverage those benefits, we embed this entropy control mechanism in our differentiable trust region layers. However, as all current approaches do not use a maximum KL constraint but an expected KL constraint, thus the monotonic improvement guarantee also does not hold exactly.<BRK>However, it seems that the authors only applied these layers to the PPO algorithm. Can the proposed layers also be applied to other RL algorithms (beyond PPO/TRPO)? ### [Original Review]This paper considers trust region methods in deep reinforcement learning (DRL) and proposes differentiable trust region layers, a type of differentiable neural network layers to enforce state wise trust regions exactly for deep Gaussian policies via closed form projections. Please find both the original review and the comments on the revised paper below. The proposed approach is flexible and general, and can in particular handle different trust regions (in KL, W2 and Frobenius norms, for example) and can be applied to existing RL algorithms as a complementary component. However, in general, the empirical performance improvement shown in this paper is not very significant. Overall, I decide to maintain my original rating.
Accept (Poster). rating score: 7. rating score: 6. rating score: 6. rating score: 4. <BRK>The paper presents an approach that enforces IGM through an appropriate selection of a neural network architecture that transforms the IGM principle into constraints on the output range of an appropriate neural network. Overall, I find the paper to be well written, and is well motivated. While I believe the paper presents good contributions to MARL, I am not an expert in this area, so, I will mark my review with low confidence since I am unable to offer constructive pointers on possible improvements etc.<BRK>##########################################################################Reasons for score:  Overall, I d vote for acceptance to the paper. However, the main concerns of mine lie in the details of the proposed model(see cons below). ##########################################################################Pros:  Overall, the paper is well written. The motivation of the proposed method is straightforward and sound. The proposed methods give a practical solution for that. Although the proposed method conducts a few experiments, I still suggest the authors conduct the following ablation studies to enhance the quality of the paper: Why do the choice of layer and the number of heads are not aligned on matrix and StarCraft II tasks?<BRK>The authors show the novelty of the proposed method through theoretical proof and experimental results. This paper is well written, but there are two significant concerns:1. The authors argue that QTRAN does not perform well in complex domains because it uses soft constraints, but QPLEX needs to demonstrate this more precisely. If the authors use a unified environment, the experiment results will be more enjoyable.<BRK>This idea is to follow Individual Global Max (IGM) principle. While this paper seems to be a reasonable contribution, a bunch of critical issues exist in the experiments so I vote for rejection. Note that the Qatten paper shows performance on these hard/super hard tasks.
Reject. rating score: 4. rating score: 4. rating score: 4. rating score: 6. <BRK>The main idea behind their approach is to perform metric guided penalization on top of a prototype based cross entropy loss. Strengths:The starting motivation and direction of the paper are clear. For the rebuttal, it would be interesting to discuss and compare the submission to hyperbolic hierarchical prototype approaches. Opinion post rebuttals:After reading the rebuttal and the other reviews, I remain of the opinion that more work is needed before warranting acceptance. 2017.Compared to recent works on prototypes in hyperbolic spaces with hierarchical knowledge, the paper provides limited novelty. As the authors state that moving the prototypes to non Euclidean spaces is future work for this submission, the other recent works make the paper come across as somewhat outdated. Empirically, the paper provides a comparison to multiple hierarchical alternatives on four datasets. The use of four datasets is appreciated, as it provides a clear picture of the strengths and limitations of the approaches.<BRK>But,1) Out of 4 datasets, the proposed method works better than others on only two datasets for which in one case the addition of proposed term L_{distortion} has helped, but it actually degraded the results in the other dataset. From paragraph 1 in page 1, it is inferred that the cost of a class confusion is determined by the actual consequences e.g.cost of confusing a streetlamp with a crossing pedestrian. The notion of "cost" and "similarity" are not the same but used interchangeably in the paper. Its performance on the other 3 datasets may be improved if the temperature parameter is adjusted to compensate the various number of classes. Of course, same can be said about tuning hyperparameters of the proposed method.<BRK>The google colab link provided in the supplementary was not accessible. ### Additional feedback, comments, suggestions for improvement, questions for the authors, and a **recommendation (accept or reject)**. The missing property is the identity of indiscernibles. The authors provide a good motivation for why the incorporation of a cost matrix is useful. However, could the authors explain why they then assume that the matrix $D$ is symmetric? The current version of the paper has some minor mathematical mistakes. ### Relation to prior work: Is it clearly discussed how this work differs from previous contributions? The statement on page 1 bottom about the cross entropy is incorrect.<BRK>The latter is a problem solved as linear programming or similar, and not a closed form formula  3) it is not clear what the role of \mu is and why can be set to 3 irrespective of the scale of metric Dpage 7: The experiments show small, but consistent improvements of the suggested method over standard cross entropy, and improvements versus most competitors in most casesI have read the reviews of others and the author s response. However, my acquaintance with the previous literature in this subject is partial compared to the acquaintance of other reviewers, so It may well be possible that they are in a better position than me to see the incremental nature of the proposed work.
Accept (Poster). rating score: 7. rating score: 7. rating score: 6. rating score: 5. <BRK>The experiment section cover a decently large set of experiments and performance are compared with a very large number of state of the art baselines.<BRK>It would be great if the authors could add a baseline experiment operating in the time domain with the proposed affine layer (Equation (8)).<BRK>nan<BRK>* The main issue with the FF is its computational complexity.
Accept (Poster). rating score: 7. rating score: 7. rating score: 7. rating score: 6. <BRK>  pros:      The paper examined the claims of IRM thoughtfully and critically. I applaud the authors  effort to make the results intuitive! cons      There is only one DGP used in throughout the paper.<BRK>Main comments:This paper studies a theoretical aspect of IRM and how will it fail. It would be great if the authors could give some explanation on this point. The analysis is two fold: linear regime and non linear regime.<BRK>The work considerably extends the results in the original IRM paper. Cons (minor issues):* I know the main contribution of the work is on the theoretical side, but a simple experiment or simulation for the nonlinear case to demonstrate the empirical consequence would be appreciated.<BRK>ECML/PKDD 2015.p.2 §6: whose joint distribution with the label is fixed for all environments  > this is a strong assumption which IRM, as far as I understand, does not makep.3 §3: invariant (causal) relationship  > I am a bit uncomfortable with the authors mixing the invariant and causal adjectives in this context. The theoretical analysis of IRM conducted in the paper does not require the notion of causality, which only brings confusion to the paper in my opinion.
Reject. rating score: 3. rating score: 5. rating score: 5. rating score: 6. <BRK>This paper studied heterogeneous graph embedding with graph neural networks. The authors proposed an approach which samples multiple different subgraphs containing a subset of relations and then aggregate the node representations from different subgraphs with attentions. However, the novelty of the proposed approach seems to be quite marginal, and I am not quite convinced by the proposed approach. I am also surprised by the very small standard deviation in Table 4, which is not consistent with results reported in existing literature on the tasks of node classification.<BRK>This paper aims to propose a new GNN for heterogeneous graphs, which is scalable to large scale graphs. The results on several benchmark datasets show the proposed approach is better and faster than baselines. 1.The description of the methodology is very vague. The formula in Eq.(2) and (3) do not help due to their simplicity. 2.The novelty of the paper is also limited, consider it is extending an existing algorithm SIGN to heterogeneous version. According to other simplified GNN paper, their performance is just comparable to their counterpart but not better, and usually the results are worse than the attention based one. 4.It seems the numbers in Table 4 is higher than the ones in the original paper, such as HGT.<BRK>This paper extends from SIGN (https://arxiv.org/abs/2004.11198) model to heterogeneous graphs. To extend to the heterogeneous graph, this paper proposes to sample relation graphs, by:(1) sample several subsets $R_i$ of relations; (2) sample relation subgraphs whose edges belong to $R_i$; (3) treat each subgraph as homogeneous graphs and perform neighbor aggregation (simply average). (4) Apply MLP on each node for node classification. I have several questions about the proposed approaches:(1) The difficulty of heterogeneous graphs is that each node might have different types of features. Intuitively, with different relation set, the semantic of the relation subgraph should be very different, but the authors seem to treat them equally. It would be better if the authors can provide some analysis on this, for example, for a given node, what is the variance of final node embeddings calculated with subgraphs of different relation sets. So which set the authors to use? Also, though the authors show superior experimental results, I have several concerns about experiment settings:(1) About feature initialization. If the authors want to claim their method is more scalable, it would be better to include the inference time comparison. Overall, I think the simplified procedure (direct neighbor average) over heterogeneous graph limits the usage of this model, and there s also some unclear part in experimental settings.<BRK>This is then used to classify the node. One way to apply SIGN to multi relational graphs would be to conflate all relation types into a single relation type. The authors show that this doesn’t work well. Do this several times, that is, sample subsets of R several times. Finally aggregate the representations coming from each of the application of SIGN to the sampled subgraphs. The authors then also discuss what to do if the nodes do not have attributes (features) and ways to improve the memory efficiency of the approach. They compare to existing approaches (such as R GCNs) and show that NARS (the name of their approach) is competitive with these existing methods. I want to be careful to lament a “lack of novelty” here, because it is still a good contribution to broaden the scope of an existing method to a larger class of problems. But this is based on one way of sampling subgraphs (based on first sampling relation types and inducing the graph based on this). Overall, I would tend towards accepting the paper but I would really like to see more analysis on different sampling strategies.
Reject. rating score: 3. rating score: 5. rating score: 6. rating score: 7. <BRK>The theoretical work is also weak with regards to the convergence rates of the proposed estimator as well. Other comments/questions:The authors should define the MINE estimator in this paper. This should have been a bare minimum and it is a mistake for variational approaches to ignore these works which have theoretical guarantees that many variational approaches do not have. Comparisons to other methods should also have been included. The methods the authors did compare to have weak (or no) theoretical guarantees for higher dimensions. Does regular L2 regularization help with the drift problem? [R1] Moon et al."Ensemble estimation of mutual information," ISIT, 2017. They investigate a few of the issues of this specific estimator and propose a regularization to help with one of them. Pros:The paper appears to be technically correct. Cons:There are some interesting ideas here but the paper feels unpolished. Several issues with the MINE estimator are presented and then two of them are discarded in favor of a focus on one of them.<BRK>The connection between the exploded outputs / bimodal distribution of the outputs and ReMINE is weak in the paper. iv) The paper states that MINE must have a batch size proportional to the exponential of true MI to control the variance. Yes, the variance of some mutual information estimator, like NWJ,  is proportional to the exponential of true MI, as proved in [1]. Yes, I agree with the author the drifting phenomenon is not the only problem that the proposed method solves. Also, the optimization of MINE will result in the bimodal distribution of the outputs, in which the statistical network has very distinct values for joint and non joint samples. Issues:i) The drifting phenomenon of MINE is a feature but not a bug, since the drifting term has no effect on the final MINE estimated value. For example, does it exist in the density ratio estimator in logistic regression or in JS dual lower bound? iii) The proposed ReMINE is motivated by the drifting phenomenon.<BRK>The work studies a neural network based estimator, referred to as MINE, for approximating the mutual information between two variables. The proposed method is novel and performs well compared to state of the art. For example, the paper is not self contained: the authors use terms like "statistical network" without providing definitions, and begin simulations without introducing MINE or giving a high level discussion of how it estimates the DV representation, so it would be very difficult to read this paper without having first read the original MINE paper. The paper lacks intuition. Some additional typos/comments:Pg 1: "...the value of each term in MINE loss IS shifting even..."Pg 2: I believe equation (3) should be a lower bound not an equality.<BRK>This paper attempts to answer the four questions raised from the mutual information estimator. To this end, this paper investigates why the MINE succeeds or fails during the optimization on a synthetic dataset. Based on the observations and discussions, the paper then proposes a novel lower bound to regularize the neural networks and alleviate the problems of MINE. Overall, the paper is easy to follow and new insights have been brought for the MI estimator and the downstream tasks.
Reject. rating score: 3. rating score: 5. rating score: 5. rating score: 7. <BRK>The main objective of this paper is to reduce the model stability, in particular, the prediction churn of neural networks. The paper proposed to use a interpolated version of global label smoothing and k NN label smoothing. The problem is interesting and the the paper is writen clearly. Currently, since the experimental results are questionable for me, I would suggest a rejection of this paper. However, the discussion and results on this respect seems insufficient. My major concern is about the experimental results. The additionally provided Table 3 makes me question the implementation of the experiments.<BRK>This paper condsidered the problem of churn reduction in DNN training,  and proposed to  utilize the k NN predictions to smooth the labels results. Theorectical  analysis  and empirical  comparison results were given to validate the proposed  idea. Overall, the paper is well written. My main concerns are about the contribution significance and practical effectiveness of the proposed method for general problems.<BRK>The paper proposes a k NN label smoothing approach in order to reduce the churn as opposed to the traditional label smoothing. The idea of improving the prediction churn is encouraging. The k NN label smoothing with theoretical guarantees for the bounds sounds interesting. However, the paper has serious drawbacks in the experimental evidence. The paper requires some proofreading. Hopefully the authors will address these concerns in the rebuttal period. However, none of the augmentations are experimented in the results section.<BRK>My enthusiasm for this paper remains fairly high after reading through the other reviews and responses: I think the results (in the main body of the paper) look strong. The discussion and theoretical analysis provide context for previous work and deepens understanding of, in particular, label smoothing. These flaws should be addressable by the authors. In an ideal world, it d be nice to have one more round of discussion with the authors about Table 3. This becomes even more important if the label smoothing is based on nearest neighbors in the original input space: designing a distance for, e.g., a mix of continuous and categorical features is tricky. The introduction does a nice job of motivating the problem of prediction churn, which I suspect will resonate with practitioners.
Reject. rating score: 4. rating score: 5. rating score: 5. rating score: 5. <BRK>Most significantly, I believe the derivation of the updates is not correct. Secondly, the paper does not compare to any baseline other than training the plain networks (that were not really designed to handle significant noise). Is there absolutely no other approach in the literature to tackle this problem? Minor things:  The motivation of the approach was not clear to me at the beginning. The introduction  refers to a missing robustness of neural networks and cites a paper on adversarial attacks. These attacks are, however, not addressed in the numerical experiments. It was a little unusual to me to avoid the problems of minimizing (1) by just designing parts of the matrix $D$. In which cases is the specific design with Fourier coefficients justified? Maybe the term "Theorem" is a little too strong for these results. Theorem 2 is interesting but unfortunately seems far from the practical case. After the rebuttal: I would like to thank the authors for their response and clarification.<BRK>The paper is generally well presented. However, a main issue is that the optimization algorithms for the l0 norm regularized problems (Section 3.1.2 and Section 3.2) are not correctly presented. See Comment 1 for details. Actually, as implied by Theorem 2, $|\boldsymbol{B}_i^{\rm T} \boldsymbol{B}_j|$ can be very close to $\|\boldsymbol{B}_j\|_2^2   \|\boldsymbol{B}_j\|_2^2   \frac{m}{n}$ when $n$ is comparable to or greater than $m^2$. In Section 3.1.1 and Section 3.2, to make it clearer, please change the Frobenius norm operator $\|\cdot\|_F$ to the Euclidean norm operator $\|\cdot\|_2$ whenever it is applied to a vector, e.g., in Eq.(8) on the right hand side of equality, and in Eqs. On page 5, the sentence in the first two lines should read as "We can see that Eq.(12) is minimized when $S$ consists of the indices of the $k$ largest (in absolute value) elements of $\boldsymbol{h}$". After rebuttal  Thank the authors for their detailed responses, which have addressed my concerns regarding Eqs.<BRK>Thanks for the efforts of the authors. Some of my concerns have been addressed. However, I think that  the paper should test SDL layer by plugging in any hidden layer. So it can be solved very fast when testing. Pros: The idea of using a structured dictionary is interesting, which produces a fast sparse coding. Cons: 1.The paper does not distinguish the corruption (e.g., additive Gaussian noise and blur) and the adversarial robustness. However, the statements in Section 1 and 2 are on adversarial robustness. 2.The experiments do not test the superiority of the proposed approach. It is especially when the denoising phase can be trained end to end with  deep networks. The paper only shows the effectiveness of SDL layer used as the first layer. Moreover, I think the image denosing task [Zhang, 2017 in the references] is more appropriate for testing the superiority of SDL layer. It is better to detail this issue.<BRK>*Summary*This paper addresses the problem of Deep Learning models  output instability to small perturbations in the input, first pointed out by Goodfellow et al 2015. In the numerical section they show how the method outperforms other state of the art methods when adding Gaussian noise to image classification and Reinforcement Learning*Positive points*   The paper tackles an important problem for ML with an interesting approach  Numerical testing on very different scenarios such as classification tasks and reinforcement learning. The authors numerically show the robustness of their method to Gaussian noise perturbation of images  The paper gives many details that allows reproducibility and is well written*Negative points*  The usage of denoising layers is not new in the context of image processing for restauration. It is clear that the paper would like to tackle the implications of denoising for image classification, but it is difficult to argue that Gaussian noise is representative of realistic perturbations that can trick an image classifier. Some typos:  sentence  The optimization problem (1) is non convex and non smooth, which is very difficult to optimize  after eq 2 might need a bit more explaining or a reference since people have been using intensively numerical solvers to minimize non convex and non smooth energies. The instability of DL models to small perturbations is important and needs to be addressed, but the paper does not show that it can tackle realistic scenarios.
Reject. rating score: 4. rating score: 5. rating score: 5. <BRK>This paper proposes a consistency score (C score) that measures the expected accuracy of a held out instance averaged over different training sample sizes, which can be useful for analyzing learning dynamics and generalization performance. There are many other possibilities. This seems to be the opposite of what authors are suggesting in Figs. If there s no such *real* benefit, then it s hard to justify the use of C score considering its high computational cost.<BRK>Summary:This paper formulates a consistency score (C score, which characterizesthe expected accuracy for a held out instance given training sets of varying sizesampled from the data distribution) to measure the regularity of an example. C score is also used to learn dynamics of different optimizers and consequences for generalization. It further provides proxies (in particular, the learning speed) for C score for practical tasks such as outlier detection. However, the novelty of the consistency score and the experiments on utilizing it for outlier detection are relatively limited. rather than 50 or other numbers? Since ImageNet is much more complex than CIFAR, it is hard to tell if similar observations (other than the examples given) hold. ################################################Post Rebuttal:I want to thanks the authors for their detailed responses. The addition of this paper to our existing understanding is not that much. Besides, diving a bit deeper into using C score to analyze the dynamics of different optimizers will be very interesting.<BRK>This paper proposes yet another score, the consistency score, to evaluate the importance of individual data samples built on held out performance. As motivated by the authors, consistency score is designed to tell regular examples from exception examples, measured by the held out performance. Similarly, the authors also lack a discussion of the necessity of designing such a new metric based on held out acc, which is hard to compute. Without this, the novelty and motivation of this work would be questionable. In all, I find this paper is well written with thorough experiments around the proposed metric. However, as mentioned above, it would be a regret that the paper lacks more in depth discussion.
Reject. rating score: 5. rating score: 5. rating score: 6. rating score: 6. rating score: 6. <BRK>This paper analyzes the ability of BERT model to learn good representations of sentences, through a purely empirical study, there is no other contribution of a new model or a new analysis technique. In the experimental analysis, phrases or words are swapped and the corresponding changes in the sentence representations are analyzed from all the hidden layers. I would rather see this kind of work in a workshop track for ICLR. A minor comment: only one dataset is considered in analysis. I would like to see the analysis sentences from the training set as well as many test sets.<BRK>The paper investigates the sensitivity of BERT representations to different kinds of permutations in the input sentence. The authors measure the l2 distance between representations coming from original and perturbed input. StrengthsThe idea of measuring a network’s response to input transformations is nice and potentially could be used to test different kinds of hypotheses. (To measure differences in what representations encode you can use, for example, PWCCA (NeurIPS 2019 “Insights on representational similarity in neural networks with canonical correlation”) or other related measures.) Hence the results stated in contributions 2, 3, 4 are not surprising. Maybe the most relevant to your work is the part showing how different words influence each other (e.g., rare words influence others more than frequent ones; the same analysis for POS).<BRK>This work can be regarded as a moderate reification and improvement of that thought (but it is still limited to existing scopes and methodologies). In my point of view, the main issue of this paper is, like many other works, there is no strong theoretical explanation for the phenomenon being investigated. In experiments, it is not clear whether the randomness of BERT itself has been deducted. The randomness could be caused by the dropout operation which may lead to the discrepancy on output even using the same sentence. I would recommend testing on other domains (e.g., biomedical or academic) that BERT never saw before (or structure less data that do not present a syntactic structure). Strictly speaking, the terminology “gram” in Fig.2(a) should be called "chunk", as “grams” are usually related with sliding window thus overlap with each other.<BRK>The results mainly focus on BERT (base cased), but some results of the other variants have the same trend. The rank correlations between distortion and the syntactic distance are higher in the deeper layers across all attention heads, but the correlations are somewhat weak (around 0.3). The main conclusion is that BERT (and its variants) builds its contextual representation by increasingly incorporating more phrasal units along with the layers. The paper presents a novel approach to study the behavior of Transformer models and the findings are quite interesting. The paper provides comprehensive experiments including many carefully designed experiments to reveal the key insights. The differences between the distortions among tree distance and the rank correlations are somewhat weak, or rather we do not have a baseline to compare against.<BRK>The first is to permute n grams of a predetermined order. This seems like it could be a generally useful means for probing networks both for NLP as well as possible for other domains (e.g., CV). The latter would involve running experiments with different probes and in the best case on different models to discover which method is the most discriminating. As a minor nite, the paper attempts to make a connection to neuro science. This would be better done if there was more of a clear and explicit connection between the techniques explored in the paper and existing neuro science work beyond just the fact the model is using probes and measuring network activity.
Reject. rating score: 4. rating score: 5. rating score: 7. rating score: 7. <BRK>The authors propose to encapsulate the update rule for a neural net into a look up table specifying weight changes for each combination of "pre synaptic" input to the weight, and "post synaptic" activation of the unit receiving that incident connection. This is motivated by a separation of timescales biologically, wherein learning rules might be evolved over long timescales, and then act within each brain over shorter ones. There is a nice discussion of related previous work, but it misses a few key items that, to me, diminish somewhat the novelty of this work. That s okay: being first isn t everything. But I think it is important to point out to readers what is new and better about this work vs previous work. a) The auto ML zero paper from Quoc Le et al.(arXiv 2003.03384). They use GD to learn plasticity rules. c) A few recent papers on bio plausible backprop type algorithms. "Towards deep learning with segregated dendrites." In Advances in neural information processing systems, pp. That could matter for comparing the training curves of accuracy vs. training time because a good initialization could give one learning rule an apparent advantage. And given the accuracy at t 0, it doesn t look like they are the same. 2) I like that the authors studied generalization of the learned rule between tasks: that is important (although, SGD also generalizes well). I m a bit less impressed by the performance obtained in the MNIST and fashion MNIST tasks.<BRK>The authors argue that this is a promising strategy for discovering the learning rules used by biological systems, with three main contributions: (1) they provide proofs that this approach does what we would hope it would do when applied to a single linear layer; (2) experiments demonstrate meta learning for simple non linear and recursive architechtures; and (3) the authors provide an argument that evolution could replace gradient descent as the method of searching over possible learning rules. Overall, I thought this paper was well written and provided interesting arguments and proofs. Most of the work in this area has been focused on feed forward networks, but as the authors emphasize, recurrent neural networks add a whole new dimension to the space of biologically plausible, local learning algorithms. As the authors admit, one can achieve good performance in a multi layer NN by fixing the random weights in the hidden layers and by training only the output layer. I would have really liked to see how the meta learned algorithm compared to fixed, random representations. The experimental performance of the meta learned algorithm on MNIST is quite poor (~80% test accuracy, Figure 3), so its unclear what is going on. The results of the adversarial robustness experiments are not surprising. Adding additive or multiplicative noise during training will also make the trained networks more robust. It think these experiments actually distract from the main ideas of the paper. It would have been better to more carefully explore whether the learning algorithm can learn more difficult functions. I think it is important to highlight the fact that the latter is an unsupervised plasticity rule   the meta learning algorithm has access to the target output, but the local plasticity rule does not. So while a plasticity rule trained on Dataset 1 does have some information about the Dataset 1 targets (by way of the meta learning updates), when it is applied to Dataset 2 it never receives any information about Dataset 2 s target, and is thus unsupervised. This is an important distinction between the two approaches.<BRK>This paper introduces a new method for meta training plasticity rules, allowing networks to learn new instances of a given domain quickly and efficiently. The method consists in implementing plasticity as an arbitrary function of the past few timesteps of local activity at a synapse (input and output). Experiments show that the method finds reasonably successful rules, that these rules generalize across (some) domains, and that the learning seems more robust to (some) adversarial attacks than plain gradient descent. While meta training plasticity rules is not new, I believe the method is novel and quite interesting. I also appreciate the experiments to demonstrate cross domain generalization and robustness to adversarial attacks. A possible caveat is that the experiments, though diverse, are still a bit limited. The paper only uses relatively small feedforward networks and recurrent networks with only 3 timesteps. In terms of "real" datasets, only MNIST and Fashion MNIST are considered. Similarly, the trained plasticity rule seems robust to a certain type of adversarial attack, but is it more robust to other forms of distortion   such as plain noise, deformations, etc. ?I suppose this is tolerable for an introductory paper. Minor comments:   Bengio  et al.1992.“On the Optimization of a Synaptic Learning Rule.”, used gradient descent to meta learn plasticity rules and should be included in the Related Work section. E.g.Which type of plasticity rule is used (network based or lookup tables)? Also in Figure 4, how exactly are these image generated   what s the criterion for a sufficient error? Speculative comment: IIUC, the networks in this paper are binary and can be seen as spiking networks. There s probably some interesting work to be done in this direction (for future work of course, not for this introductory paper).<BRK>Concretely, it shows that by using concepts from neuroevolution together with deep learning concepts, we can learn how to learn learning rules (i.e.plasticity rules) which can generalize across tasks and can train neural networks which are more robust to adversarial attacks than typical networks trained with stochastic gradient descent. •	The paper is very well written and anchored in a multidisciplinary literature. It has the potential of becoming a “must read” paper in the future. During the discussion phase, I would recommend to the authors to address the following comments:1. In the limit of time, try to perform experiments also on CIFAR 10/100. I believe that it would be interesting to see on CIFAR 100, the behavior of three types of learned plasticity rules: (1) plasticity rules learned on the simple datasets, (2) plasticity rules learned on CIFAR 10, and (3) plasticity rules learned on a subset of the CIFAR 100 training set. 2.Are you encountering problems with ReLU activation in recurrent networks such as exploding or vanishing weights? Does your approach work also with hyperbolic tangent? 3.I believe that it would help the paper clarity if you can add a table towards the end of the paper to summarise the main results in terms of accuracy, training time, etc. 4.Perform a proof read of the whole paper to improve the English usage and the presentation. For instance: typos (e.g.“rule, The next theorem”), unit measures for axis labels (e.g.figure 5 – accuracy [%]), etc.
Reject. rating score: 4. rating score: 5. rating score: 5. rating score: 7. <BRK>The proposal is based on a teacher student scheme, where the student does many steps of inner loop gradient descent, and the teacher does only a single update step where the single step is computed using the regions visited by the student. The authors propose variants of this setup for both Maml and Reptile. Experiments are conducted on few shot learning, long tailed classification, and meta attack benchmarks. Pros* The paper considers a variety of different benchmarks (other than simply few shot learning), including memorization in few shot learning, long tailed classification and meta attack. Cons* It is not straightforward to me what the benefit of the proposed method is compared to first order maml (fomaml) or reptile. This to me means that the second order term is being ignored and so the update is indeed first order. This could indeed offer some benefit but doesn t seem to offer the claimed benefit of "enabling...exploring long horizons in the inner loop" compared to fomaml or reptile as training with long inner loops seem similarly computationally expensive across all these first order methods. In general, I believe a better argument needs to be made about the benefits of the proposed method compared to first order methods such as fomaml and reptile, given that they all seem computationally similar in terms of cost. Additionally, I believe the setup of the experiments could be improved as stated above.<BRK>To avoid the multiple problems of back propagating the gradient through inner loops with many steps, the authors propose a teacher student solution that decouples the exploration (done by the student) from the actual computation of the gradient (done by the teacher). Finally, the metagradient is taken with respect to the initialization avoiding the backpropagation through the inner loop. In the experimental section, the authors apply the methods in three different use cases (few shot learning, long tail classification and meta attack)## CommentsThe paper is well motivated and the writing is clear. Why is iMAML included but not Reptile, when it is actually Reptile and not iMAML the one baseline that appears in the main manuscript. * At the end of page 4, the authors mention the importance of shuffling the N classes and then that their method performs better than MAML even without shuffling. Does that mean that all the experiments (for all methods) are done without shuffling?<BRK>This paper studies the problem setting of gradient based meta learning (GBML) and presents a new algorithmic approach that is computationally tractable. However, the memory footprint will increase linearly with the length of the inner loop. See the iMAML paper for further discussion about these aspects. **Assessment**The proposed algorithm and motivations are interesting. The experimental results are convincing. **Questions and Comments**(1) Having very long inner loops can lead to divergence in the bi level optimization setup, since the inner level parameters are no longer influenced by the meta parameters. This suggests that we either need early stopping (like in MAML) or some other explicit regularization (like in iMAML). This presumably would add significant computational overhead compared to MAML.<BRK>This work proposes a simple approximation algorithm to alleviate the issue. They use a student network to explore the search space of task specific models. Unlike MAML, the computation graph of the task specific adaptation is not tracked and this allows the number of task specific updates to be larger. Once the task specific adaptation is done, they simply take a convex combination of the task adapted parameters and the original parameters and evaluate the resulting model on the validation data of the task. The proposed approach seems to be applicable to different meta learning scenarios including few shot learning, unbalanced classification and meta attack. Pros:  The paper is well written and clear  They evaluated the algorithm on multiple different meta learning problemsStrong results on the unbalanced classification and the meta attack benchmarks Cons:  Although the approach is motivated by the look ahead optimization, it feels a bit unintuitive to take a simple convex combination of the adapted and the original parameters for the sake of skip connection  I am not fully convinced that the method alleviates the short horizon biases of the gradient based meta learning algorithm. The meta update still seems to favor a solution that is closest to the original model due to the skip connection.
Reject. rating score: 4. rating score: 6. rating score: 6. rating score: 7. <BRK>The authors propose a learning approach for developing heuristics to solve the capacitated vehicle routing problem (CVRP). The paper is clearly written and easy to follow. My main concern is related to generalizability of the proposed solutions. What is missing in this work is what the generalizability of the proposed methods to other combinatorial problems is. The approaches suggested by the authors, viz., partitioning the input, and rewriting the basic VRP solution by merging regions and recomputing routes, are also adopted by the meta heuristics developed and used in the commercial OR solvers. Another big drawback with the current model is that it does not address many constraints/considerations in real VRP applications that typical solvers address   different costs per vehicle, cost of missed shipment, route limits, dimension limits, alternate visits, etc. Even the more realistic fixed number of vehicles case is not considered in the proposed solution.<BRK># SummaryThe paper proposes an extended framework for neural combinatorial optimization of the vehicle routing problem. The neural solver is based on the existing literature and the overall method is trained, as it is common in these papers, via Reinforce. # CommentsThe interest in solving combinatorial optimization problems using ML/RL is high and the method is a timely advancement in this area, since it addresses one of the main issues in earlier work, which is how to scale to larger instances. It is therefore reasonable to transfer the concept. Is OR Tools run with a timeout or is it solved to optimality? Which CVRP model was used there? In Sec.4.2 (Analysis of Rewriting Strategy) I do not completely follow the metric used to compare both selection strategies. Could the authors clarify this? Also, the difference between both strategies are mostly small, which indicates that most of the performance benefits stem from the partitioning & rewriting in general and less from the learned selection strategy.<BRK>Summary The paper presents a hierarchical reinforcement learning approach to solve large scale vehicle routing problems (VRPs). A “rewriting agent” is responsible for dividing the customers into regions while a “generating agent” is responsible for computing the vehicle routes in each region, independently. The main contribution of the paper is the novel rewriting process that allows to decompose the problem into smaller subproblems, that can then be tackled with state of the art methods. 17.There are confusions at several points regarding heuristics vs learning algorithms: “....can be divided into heuristics and reinforcement learning (RL)”. RL based approaches are also computing heuristics to solve the VRP, in the sense that they are not computing exact solution of the problem. For instance, the multiple vehicle routing problem, where the rewriter would be responsible of assigning the customers to the vehicles. 20.Sec 4.3 “…this is the rare case in real world.” Do you have a reference? Questions to authors 6. What do you mean by unexplored? 12.Table 2: are these results averages over a number of runs?<BRK>An RL based method, called Rewriting by Generating (RBG), is proposed to solve large scale VRPs. It borrows the idea of the hierarchical RL agent, which consists of two parts: "Generator" and "Rewriter". In the generation process, the graph is divided into several sections and in each section, an RL algorithm runs to get the best route. The rewriter uses the attention mechanism to choose two parts of the merged routes, and then get a new solution for each part using the inner agent.
Accept (Spotlight). rating score: 7. rating score: 7. rating score: 7. <BRK>Both the idea of leveraging existing code and also the adaptive layer to capture long dependencies are interesting and the experiments look solid. The author proposed an attention mechanism to capture global information between nodes, and then a hybrid GNN layer encodes the retrieve augmented graph. This can further indicate which part of the retrieved information is more useful. Experimental results over a new C code indicates that the proposed method outperforms both IR and neural generation methods.<BRK>SummaryThis paper proposes a retrieval augmented method for generating code summarization. The model augments the initial graph representation of the input code based on the representation of the top 1 retrieval result. This paper reports human evaluation results. Ablation study shows the retrieval augmentation and the new hybrid GNN architecture is helpful. Weaknesses  The model is not evaluated on any existing code summarization benchmarks. Suggestion: it would be helpful to provide an input output example earlier in the paper.<BRK>Reasons to accept:* They collect and release a new challenging C benchmark for code summarization. * To my understanding, this work is not the first work combining retrieval solution with generation model for code summarization (e.g., Retrieval based Neural Source Code Summarization) so please modify some of the claims in the paper.
Accept (Poster). rating score: 7. rating score: 6. rating score: 6. rating score: 5. <BRK>This paper proposes to use an evolutionary search algorithm to search for better loss functions for the classification and regression branch of an object detector. The proposed way to reduce the search space seems to be effective. Cons:The baselines provided in this paper are little bit weak. Although the new loss function is able to improve the performance of all of the baselines, it is unclear if it would generalize well to state of the art results. It seems that the search is biased towards to the initial loss.<BRK>This paper proposes CSE Autoloss to deal with searching loss function for object detection with an effective convergence simulation driven evolutionary search algorithm. 2.Is it possible to search classification and regression loss together? It seems for now classification and regression loss are searched independently. CSE Autoloss employs monotonicity and convergence of classification loss to and a small verity dataset to easily filter out loss candidate, it speedups the searching process by 10x and reaches similar results with random research in Table 5.<BRK>4.In the experiments, there are some results to verify the generalization of the searched loss on different detectors and datasets. For me, this can not result in the optimal loss functions. Pros:1.This paper proposes a new framework to search loss functions for object detection. For me, it is the first attempt to discover loss functions for object detection automatically.<BRK>This paper proposes to automatically discover proper loss functions for object detection. Overall, this paper brings new ideas and experimental conclusions to the auto loss discovery in the object detection area.
Reject. rating score: 6. rating score: 6. rating score: 6. rating score: 6. <BRK>**After Author Response** I think the authors made a good effort to address the concerns and I have recommended to accept the paper. The proposed method is meant to be used in situations where the supervision signal is known to be noisy but it does not estimate parameters of the noisy channel   which is corrupting the supervision signal. ## WeaknessesThere are two problems I see with the analysis in the paper that gives me concern:1. Theorem 1 then crucially relies on this result for the ensuing convergence analysis. This is the first issue I see in the analysis.<BRK>This paper tackles the problem of policy learning under weak/noisy supervision. The authors present PeerPL, a unified framework that can train agents using behavior cloning under noisy/suboptimal demonstrations, or using reinforcement learning under noisy rewards. The authors instantiate this idea on both behavior cloning and RL (PG/DQN), and also evaluate it on the problem of policy co training. I am not convinced by the authors’ explanation in Section 5.1, as subtracting the reward with randomly sampled { 1,1} will not help exploration. The theoretical results are not particularly useful as they assume binary reward/actions, which is rarely the case in practical settings. For PeerRL, it would be nice to see results on tasks that are harder than CartPole, for example, Atari or other continuous control tasks.<BRK>While the authors claim to address the case where the rewards or demonstrated actions are perturbed by some arbitrary confusion matrix. Without resolving these issue, it is impossible to know whether the proposed methods will actually be useful in settings where we have noisy rewards or demonstration data. AREAS OF CONCERN:The main concern with this work is that the gains in performance observed with the PeerRL and PeerBC algorithms may not actually reflect the ability of these methods to correct for a specific type of noise.<BRK>This paper formulates a framework for reinforcement learning and behavior cloning from weak supervisions (i.e., noisy rewards or imperfect expert demonstration). It would be helpful for the authors to add some further discussion in this regard. The proposed idea is useful in practice as it increases the robustness of the learning process to imperfect supervision signals.
Accept (Poster). rating score: 7. rating score: 7. rating score: 6. <BRK>The information contained in this paper should be of sufficient interest to ICLR community. Finally, author conducted thorough experiment to evaluate the effect of ensemble strategy and choice of metric for error detection and OOD detection in ASR and NMT. * **Strength and Weakness**: * (Strength) Uncertainty quantification in structured prediction is an important but less explored topic. This work provided a much needed, information theoretic framework to both conceptually quantify and empirically compute uncertainty measures for different sources of uncertainty.<BRK>There are a few papers (some of which the authors already mentioned), that suggest different ways to estimate uncertainties in AST and MT. This paper investigates uncertainty estimation for autoregressive structured prediction models. Overall this paper is clearly written and investigates highly relevant topic. → "The results also show that uncertainty based rejection works better for ASR than NMT."<BRK>This paper proposes two different measures of knowledge (epistemic) uncertainty in structured prediction with an autoregressive model and discusses how to compute their approximations. The paper proposes two ways to combine the ensemble of autoregressive models, EP or PE. Experimental results are provided on these two ways of combination. The main contribution is the proposed reverse mutual information (RMI) as a measure of epistemic uncertainty in structured prediction. It is not clear why this is the case.
Reject. rating score: 4. rating score: 5. rating score: 5. rating score: 6. <BRK>This paper introduces a taxonomy of OODs and proposed an integrated approach to detect different types of OODs. Motivated by this observation, they combine multiple existing OOD detection methods to detect various types of OODs. But I have the following concerns: 1. Lack of discussions about some important related work. Based on the current description of experimental settings, it is hard for me to evaluate the reported results. But there are some other OOD detection methods which also achieve state of the art results, such as [1][2][3]. 3.The proposed approach needs a lot of hyper parameters (4 attributes, 12 combinations, the weights of the binary classifier, etc) and it is unclear how to tune these hyper parameters and how they would affect the results. The current ablation study is limited I think. I think previous works like ODIN and Mahalanobis all assume that OOD inputs are far away from the in distribution. Thus, I keep the same rating and think the paper is not ready for publication. And I think the classifier trained on CIFAR10 may have correct predictions on some of those images. "Deep anomaly detection with outlier exposure." 3.I am wondering whether the analysis for the simple two dimensional dataset could be applied to high dimensional datasets. In the high dimensional space, their conclusion about which method detects which type of OOD may not hold. [2] Liu, Weitang, et al."Energy based Out of distribution Detection." It is hard for me to understand what they exactly do in their proposed method. The current experiments performed are not enough to demonstrate the effectiveness of the proposed method. The old experiment results (Table 6, 7, 8) are not convincing since the authors train a binary classifier as an OOD detector using a subset of the test OOD data, which is not realizable in practice.<BRK>  Paper Summary: The paper presents the idea of fusion of attributes from existing sota ood detection methods to achieve higher detection performance. Figure 1 suggests the "tied distribution of all training data" is different than the combination of "class distributions". "Hybrid discriminative generative training via contrastive learning." The relation between five types of OOD with three criteria for OOD categorization is not clear. (2018).A simple unified framework for detecting out of distribution samples and adversarial attacks. the total number of figures can be reduced by eliminating some and combining others. I suggest authors either report both or use threshold agnostic metrics like area under precision recall curve (AUPR) or area under receiver operating curve (AUROC) for reporting as in the Table. I can t find an explanation and/or discussion on the final detection score and it s hyperparametere. The results from the Mahanalobis technique [7] does not match the original paper. If authors did not use a subset of ood samples for tuning, it should be reported in the paper. Strengths:   interesting taxonomy of ood samples and the following conclusion for integrated detection score. Weaknesses:   limited on contribution  no discussion on final detection score and its hyperparameters. comparison with more recent techniques including Outlier Exposure [1], Self supervised reject classifier [2], Geometric self superivised learning [3,4], and contrastive learning [5,6] are missing in this paper. ICLR 2019[2] Mohseni, Sina, et al."Self Supervised Learning for Generalizable Out of Distribution Detection." Using self supervised learning can improve model robustness and uncertainty. "Deep anomaly detection using geometric transformations." Advances in Neural Information Processing Systems.<BRK>##########################################################################Summary:The authors explore the different kinds of outliers and show that the methods previously proposed detect different kinds of OOD and not a single one can detect them all. The authors propose an interesting study of the different kind of outlier on synthetic data which  illustrates well the different characteristics of the outlier types. The authors then propose to combine different methods to increase the OOD detection rate. For each dataset, samples from other databases are introduced as outliers and must be detected. The combination method yield better detection rates than baseline methods in almost all configurations. ########################################################################## Reasons for score: The main idea of the paper is simple : combine different OOD detection metrics to increase the detection rate on different types of outliers. The proposed method indeed increases the OOD detection rate for almost all the experimental settings tested by the authors. It would be interesting to show that the method also increases the detection rate of outliers inside a given database. This could be done by reporting the classification rate of the DNN in an abstaining scheme : if the OOD metric is greater than a threshold, the sample is not classified (rejected). The author do not justify their choice of the combination method. Computing all the OOD metrics can be computationaly expensive, is it necessary to compute them all ? Are this combination of metric the best ?<BRK>##########################################################################Summary:This paper introduces a novel taxonomy for OOD outliers. The authors analyze current OOD detection approaches and uncover their limitations. They propose to fuse several existing approaches into a combined one and extensively evaluate it on various data sets (CIFAR,10, SVNH, MNIST, STL10, ImageNet, etc.). The authors make several key contributions: The introduce a novel OOD taxonomy, analyse current OOD detection approaches on a toy data set, propose an integrated OOD detection approach, which shows a superior performance in their extensive evaluation. ##########################################################################Pros:* Introduction of a sound and helpful OOD taxonomy* Limitation analysis of state of the art OOD detection algorithms* Proposal of a new integrated approach to detect different kind of OOD inputs that unifies the advanatges of underlying algorithms. ##########################################################################Cons:* The demonstration of the limitations of current OOD detection algorithms is solely empirical (based on a toy data set). * Similarly, a sound theoretical derivation for the proposed integrated approach is lacking. * Further toy data sets beyond the two half moon data set would be helpful to better understand the implications of all algorithms. ##########################################################################Questions during rebuttal period: Please address and clarify the cons above
Accept (Oral). rating score: 9. rating score: 7. rating score: 7. <BRK>The paper is well written. Readers can easily follow the thought process of the authors. For example, Figure 2 shows the relation of l2 loss and phase loss with respect to target energy, indicating the importance of penalizing phase loss in the end to end system. 2.Strong results. I am excited to another example of applying domain knowledge to an end to end model. The model includes two novel components: the neural warp network compensates the errors from geometry warp, the temporal convolution works as a post processing module to account for reverberation and other effects.<BRK>This paper presents a neural network based model to generate binaural audio given a single channel audio and positions of source/listener & their angles. (1) it is unclear why DTW based warping is required. If there is only delay between left and right, just having a shift is enough isn t it? (2) The use of hyperconvolution is an interesting idea. Comments:   This paper claims that it works in real time but no information about speed such as real time factor & hardware specification are provided. Sampling rate information is not explicitly provided in the experiment section.<BRK>The paper is about a method for synthesizing binaural audio from a mono recording of a single speaker s speech. The new title is a bit better and I think it may be OK since the goal is to perform a moving source simulation for single speech sources. The authors may consider possibly a better name: "Neural binaural synthesis from mono speech" which emphasizes that the synthesized target is "binaural speech" from a single speech recording. So, I suggest to emphasize that "autoregressive sampling" is not performed in the paper to avoid misleading the readers. One wonders if using a larger STFT window size would improve its results. In this paper, visual information is replaced with the spatial conditioning information.
Reject. rating score: 4. rating score: 5. rating score: 5. rating score: 5. <BRK>Based on these attacks, the authors further develop a method to measure the robustness of a network or an adversarial defense. Despite being conceptually interesting, this paper is working on the thing that many studies have done. The related work section is not on the point, in my opinion, the core problem ( unforeseen adversaries ) of this work should be  adversarial attacks that are designed to break the commonly employed defenses , but not  adversarial attacks that are visually meaningful, and occasionally break the commonly employed defenses .<BRK>However, judging by the implementation, the rain, snow, and fog attacks are too simplistic. From the paper, it is not clear how well these attacks approximate the respective type of perturbations in real world scenarios. Therefore, the proposed set of 6 attacks is more the author s heuristic than something that can be used for the evaluation of computer vision systems, e.g.autonomous vehicles. ###### SummaryThe paper proposes a novel benchmark for the evaluation of the model s robustness against unforeseen adversaries. It is not clear how to reimplement these attacks.<BRK>As a baseline framework, it is hard to say that the six adversarial attacks are sufficient to evaluate robustness against unforeseen adversaries. In general, the result that adversarial training is not robust to unforeseen attacks is not surprising and has be been discussed a lot in literature. In this framework, the novel measure is normalized with the performance of adversarial training. ## Disadvantages  The proposed measure mUAR is quite heuristic, e.g., the choice of the epsilon parameter.<BRK>(In addition, most of the attacks seem to be a marginalimprovement of existing ones.) How is this verified? Within theirframework, the authors propose four novel adversarial attacks toconsolidate our understanding of test time adversarial ML attacks. In thisparticular context, the authors observe that attackers are not limitedto induce minimal Lp norm perturbations and therefore it is important tounderstand how robust existing defenses are against attacks that divergefrom these constraints while still being realizable.
Reject. rating score: 4. rating score: 6. rating score: 7. <BRK>These tasks were used to demonstrate the benefits of representations that are disentangled, but here they are used to validate the learned representation. ### SummaryThis paper proposes a non conventional approach to learning disentangled representations through finding interpretable directions in the latent space of a pre trained Style GAN (Karras et al., 2018). To apply this technique to existing (real) images, an encoder network is trained on the generated images to learn a mapping  from images to disentangled representations. However, the “pre trained” line in Figure 11 in van Steenkiste et al.(2019) only contains a random selection of representations learned by VAEs _without_ considering their disentanglement. One reason for this is that it performs about average in all cases and has relatively large variance (if averaging across the main hyper parameter). ### Pro’s / Con’s / JustificationThe paper is reasonably well written, although I should note that it violates the required style guide for formatting paragraphs. On the one hand, I like the approach as an alternative to VAE based approaches and I think that it is valuable to connect the field of disentangled representation learning with the field of controllable image generation. For example, currently it is unclear to me how many training steps were sed to obtain the results in Figure 2. This is problematic, since it leaves it unclear to what extent the evaluated approach applies to other GANs. ### Post RebuttalI have read the other reviews and the author s response. The reader is required to compare the boxplot in Figure 1 to the violin plots in Figure 13 in Locatello et al.(2018).Other than this being far from ideal and only allowing for a rough comparison, it is not clear to me that these results can be compared since Locatello et al.(2018) reports an average that additionally includes six different regularization strengths for the VAE based methods. Hence, to improve over VAEs in this regard, it is crucial that the proposed framework does not suffer from this same issue or least consistently performs better compared to VAEs. * The abstract visual reasoning experiment is flawed, since the comparison to van Steenkiste et al.(2019) figure 11 considers a random selection of pre trained VAEs for which it is unclear whether they were disentangled or not. I would also like to see the authors aggregate results across methods for interpretable latent discovery and compare that to an aggregate over possible VAEs (and regularization strengths). I do think that there is significant value in a systematic analysis of GAN based approaches applied for disentanglement in this way, as it could serve as a useful benchmark for existing (VAE based) approaches to compare against.<BRK>This paper proposes a method to learn disentangled representations. The idea is to use an existing GAN generator and use an existing controllable generation algorithm (such as ClosedForm or GANspace) to find a set of “important” directions in the latent space. These subspaces spanned by these directions could already represent disentangled factors of variation. The main contribution of the paper is to propose learning a mapping from input to this subspace (that inverts the generator). Pro:The empirical evaluation shows that the unsupervised controlled generation methods are able to find a set of directions in the latent space that represent disentangled factors of variation and can achieve on par performance on standard disentanglement benchmark datasets. If I understand correctly the paper only proposes to invert the generation process from the disentangled subspace (which is learned by existing methods) to the image. (Though I think the empirical finding that existing controllable generation algorithms also work well for disentanglement is also interesting) The writing seems fairly unpolished. For example, it took some effort to understand that the authors are defining a new basis with columns of A, and the vectors w_i are defined under the coordinate system of the basis. These should be formally defined and clearly stated to avoid ambiguity. As another example, it is unclear what “sources of randomness” means.<BRK>The proposed approach can be summarized in 3 steps: (1) Training a GAN (in an unsupervised way) (2) Find  ‘k’ new basis vectors in ‘W’ space (3) Train a new encoder on the synthetic dataset {(z,G(z))} with loss that encourages the encodings to correspond to the subspace spanned by the basis vectors. Furthermore, the authors propose a new approach for identifying meaningful basis vectors which the authors claim that it is the generalized version of ClosedFrom. [3] Locatello, Francesco, et al."Challenging common assumptions in the unsupervised learning of disentangled representations." international conference on machine learning. The related work section touches on some of these, but not enough in my opinion. As an example, I am personally not familiar with the GAN literature including the architecture of StyleGan decoder so I had to go and read [1] to see what is the difference between Z space and W space and what is the mapping. The revised version certainly looks better. In the methodology, it is written as “w’   w + a * n”. Is this just a change in notation, or ‘w’ here corresponds to the W space? I m still happy to recommend this paper for acceptance. While it is relatively simple and straightforward to implement, the idea of learning a disentangled representation from an entangled latent  is novel and makes sense. Regarding the experiments section,  I applaud the authors for doing the fairness and abstract reasoning experiments in addition to reporting standard disentanglement metrics. (I had to double check the results in [3] to make sure the results are indeed better). One key weakness of their approach is that it relies on training the encoder based on ‘generated’ samples. [Questions]  As far as I can see, this approach is not specific to GANs and can be applied to the latent space of VAEs as well. I’m wondering about the results from applying this approach to a trained VAE with a style gan decoder. Is it simply the empirical evidence regarding controllable generation on GANs? Could the authors elaborate on this?
Accept (Poster). rating score: 7. rating score: 7. rating score: 7. rating score: 6. <BRK>This paper tackles the problem of learning with noisy labels and proposes a novel method CDR which is inspired by the lottery ticket hypothesis. In this experiment, which one did you choose? I personally think this is an interesting and meaningful direction. The authors can regard this as future work to improve this paper. Overall, I think this paper makes sense in learning with noisy labels. I recommend to accept this paper, and hope that the authors can address the above issues carefully.<BRK>Based on this observation, this paper proposes to divide all parameters into the critical parameters and non critical ones, and perform different update rules for the two types of parameters, in each iteration. 2.This paper proposes a novel method with an interesting idea. The view of updating different parameters by different rules is quite novel for learning with noisy labels. I would suggest the authors to carefully check the notations used in the paper. So I prefer to accept it.<BRK>As the deep models fit training data with clean labels in the early stage of training, the authors propose a novel method to identify those more important parameters for fitting clean labels. I think this paper is interesting and makes sense. Different from other complex methods for learning with noisy labels , this work discusses that standard cross entropy loss can achieve competitive performance with early stopping. The idea of this paper is novelty and meaningful. I suggest that the authors can emphasize it or change it.<BRK>This paper proposes a method for deep learning with noisy labels, which distinguishes the critical parameters and non critical parameters for fitting clean labels and updates them by different rules. The method is easy to implement and the empirical results are promising. Experiments on both simulated and real world datasets show it reaches new state of the arts results.
Reject. rating score: 5. rating score: 6. rating score: 6. rating score: 7. <BRK>The paper studies the problem of offline Meta Reinforcement Learning (RL). In this problem, N separate RL environments are considered which are drawn from a specific underlying distribution. Therefore, the algorithmic contribution seems somewhat lacking in my opinion.<BRK>Summary:This submission studies the meta learning problem in RL under offline settings. Based on my main concerns regarding the assumptions on reward function and soundness/correctness of experiments, I lean towards a rejection. The main contribution of this submission is not in solving a specific real world RL application problem as the cited references. Cons:1.Regarding the reward re labelling trick, the assumption that reward functions for all training environments are known limits the applicability of the proposed algorithm for real world problems.<BRK>Summary: The paper proposes an extension of VariBAD to the offline setting. The test environments are not extensive: all 4 test environments used in the paper are quite simple and 2 out of 4 are even similar.<BRK>Summary In this paper, the authors tackle the problem of offline meta RL, where one observes trajectories from environments drawn from a distribution of environments, then using these past observations finds a policy which will work well on new environments drawn from the same distribution, optimizing both in the face of uncertainty about the environment and about the random transitions. The authors propose a method based on VariBAD which achieves good experimental results. Major Concern: My major concern is that the evidence does not fully support the author’s central claim: “quickly maximize reward in a new, unseen task from the same distribution” where this distribution is described as varying over both the reward and transition functions. Although I do believe that the author’s method addresses varying reward structures, they have provided no evidence that the method addresses varying transition dynamics. Therefore I would ask the authors to clarify the scope of their contribution, and perhaps address what challenges stand in the way of offline Meta RL where the state transitions vary too.
Accept (Poster). rating score: 7. rating score: 7. rating score: 6. rating score: 6. <BRK>To show that current methods do not handle reward progressivity, the authors introduce two domains, _Exponential Pong_ and _Ponglantis_. The paper shows that returns learned separately on decomposed rewards can be composed to get the original return. The paper then presents spectral Q learning and spectral DQN, with experiments on the domains presented earlier as well as experiments on 6 Atari games. + The domains proposed for testing these rewards are clear. + The effectiveness of spectral DQN on the two domains introduced in the paper is clear. ## Negatives/ Questions  One question that does not seem to be satisfactorily addressed is whether there is a commonly used benchmark domain or one which was not specifically engineered for progressive rewards where value based deep reinforcement learning would fail or not perform well unless it was using Spectral DQN. Another useful ablation could be to remove the monte carlo mixing and show how unstable the updates get. It is motivated by a different problem, but since a possible logarithmic mapping might mitigate the problems that come up with progressive rewards their method might be a possible solution to be compared against. ## SummaryOverall, I find the idea in this paper clear, simple, and effective. There are some additional questions and comments that if addressed would make the paper a more well rounded submission.<BRK>This work describes and addresses the issue of _reward progressivity_ in reinforcement learning, where as the task progresses the scale of the reward changes. The authors present a handful of experimental results demonstrating that their proposed method outperforms two other reward re scaling baselines when reward progressivity is an issue and maintains good performance in more standard tasks. ### ClarityThis paper is exceptionally clear. However, comparison to other forms of reward decomposition is lacking, both experimentally and in the discussion. In addition, the added discussion around discounting is also valuable; and the simplification of the algorithm and added ablation experiments improve the quality of the contributions. Indeed, the authors point out the 2 main baselines do not consider _intra task_ reward variability. If this is indeed the intuition, then it would be very useful to actually demonstrate some sort of accuracy trade off induced by reward progressivity. It would also be valuable to discuss whether reward progressivity is ultimately an issue of discounting. ### SignificanceIf we take the authors  claims at face value (which I am still inclined to do even though the paper lacks more detailed validation), this paper presents a simple technique to handle reward progressivity. This contribution is significant from a practical standpoint but perhaps only when solving tasks with this particular reward structure. For this to have broader signficance, it would be necessary for the authors to provide a more detailed characterization of the potential pitfalls associated with the technique s heuristics. **Pros**  Adds a simple technique for balancing losses associated with rewards of different scales. Overcomes extreme examples of the problem it is designed to address. Exceptionally clear and well written.<BRK>#######################################################################Summary:In this paper the authors propose a new RL method, spectral DQN, in which rewards are decomposed into different frequencies. This decomposition allow for the training loss to better balanced on certain tasks   in particular those with progressive rewards. The new method is shown to perform well on specially constructed tasks with extreme reward progressively, as well as on a selection of standard Atari tasks. The paper is well written and clearly presented. This would seem to be a contribution to that body of work. While I believe the selected experiments are sufficient to demonstrate the claims in the paper, they do not fully explore the capabilities of the method and the intuitions that motivate it. It would have been good to see some more thinking here (even if it means experiments outside of Atari)(1) It would seem that this approach would work for a parametrizable class of reward functions   why not test that? (2) Similarly for failure modes(3) Frequencies needn t be geometric, how might other choices have performed2. I d like to have seen a slightly richer discussion/motivation of the mixed Monte Carlo update. What else was tried?<BRK>This paper proposes an extension to DQN, more generally applicable to value based deep RL systems, that encodes the return using a thermometer encoding with exponentially sized bins. This enables returns of vastly differing magnitudes to be learned without hurting performance. The authors propose an algorithm for learning these encode returns, including the use of a variance scaling term to speed up learning. Overall, I enjoyed reading the paper and appreciated the clear exposition, which is sensible throughout. My main concern is whether this is solving a real problem, or a hypothetical one. The experiments don t support the former case, while I would argue that solving the problem hypothetically would require a more thorough development. Training on all Atari games with a single network, for example, might be a better case. I have a few technical questions for the authors:4.1: Why use a thermometer encoding versus a binary encoding? I.e., write the reward in binary, and encode its bits. 5: "We do not simply set ..." I would have expected some ablation studies here, regarding the role of the parameter. Also, you should include published DQN results (and if possible, other value based methods) as I would expect these to be worse for games that have rewards of different magnitudes. 8: "the phenomenon of reward progressivity". Distributional RL algorithms replace the L2 loss with probabilistic losses.
Reject. rating score: 3. rating score: 4. rating score: 4. rating score: 8. <BRK>In the recent literature there has been a rise in the number of papers which attempt to verify neural networks. More specifically, for image classification networks, the problem is to prove that the output of the neural network does not flip for small perturbations to the pixel values. For a robotic setting, the problem is often safety and convergence to some goal state. The authors in this paper present an adversarial attack model on neural networks, which is deemed correct by some verifier. More specifically , given a neural network which can be shown to be robust to adversarial perturbations around some input, the authors exploit numerical errors in the computations to attack the network. This is due to the approximation errors introduced by using floating point numbers. It s a step in the right direction, if the verification of computer vision task was a well defined problem. Since it s not clear what to verify in the first place, the use case of this paper  is not a very convincing one in my opinion.<BRK>The resulting perturbed image is going to have adversarial examples very close to the boundary of the region considered, so small floating point errors might result in returning incorrect results. Main thoughts:The problem that the author discuss is very well highlighted and explained. On the other hand, in terms of importance, I would rank it more as an interesting observation that an actual critical problem. Page 2: "any sound verifier for this class of networks must reason about the specific floating point error characteristics of the neural network implementation at hand." Minor questions:In Figure 1.a, it seems like for the first 4 graphs, the dotted lines which I assume implies what the difference should be are lines with slope 1. Shouldn t there be a slope dependent on the corresponding gradient coefficient?<BRK>This paper focuses on the floating point unsoundness of neural network verification procedures, and shows that the results from such verifiers can not be trusted. To drive home the message of the paper, the authors take MIPVerify (which doesn t ensure FP soundness) and shows that they are able to construct adversarial examples for the cases that are returned as verified by MIPVerify. So I am split on the paper. From a formal methods perspective, the discovery of the paper is not surprising as Floating point computations are known to be important (this is one of the reasons that SMT solvers put a lot of emphasis on FP).<BRK>The paper presents a method to find adversarial inputs for neural networks in regions where the networks can be "proven" not to admit any such adversarial examples, practically demonstrating the unsoundness of a "complete verifier" as well as an "incomplete verifier". While it was already obvious to me that "verifiers" that assume floating point arithmetic is the same as real arithmetic are unsound, the paper is a service to the community in that it also makes this very obvious to informed outsiders who may not have already questioned the validity of robustness verification research that does not model round off and even ignores it in its own implementation. The related work section does a good job of surveying the state of the art as it relates to floating point soundness. Perhaps there could be a short discussion of challenges that different approaches face to become sound with respect to floating point semantics. For example, it is not so clear how precisely binary search is used to find α and δ simultaneously.
Accept (Poster). rating score: 7. rating score: 6. rating score: 6. rating score: 6. <BRK>**Summary**: The paper combines ideas from two previous works (MVAE and MMVAE) to propose a new multimodal formulation of the ELBO for VAEs. The approximate posterior consists of a mixture of subsets, with each subset a product of approximate posteriors for each modality. By combining MVAE and MMVAE under one framework, this may provide insights to researchers in this area. Could non scalable models handle the datasets in this paper? The authors provide experiment details, primarily in the appendix, for reproducibility. I found some aspects of the motivation to be unclear. It seems that this is a result of wanting to consider the case where some data modalities are missing.<BRK>This paper formulates a multimodal ELBO as a mixture of product of experts. The idea is simple and appears to improve on baselines derived from a mixture or a product of experts. At a high level I think the paper is fine, simple to understand, and the results are straightforward and good. Some remarks:P1:* First sentence talks about generalization, but it s not really substantiated in any of the experiments. It just seems thrown in there without a lot of support. * I have to start with that I disagree that VAEs are self supervised models (or at least I don t gain anything by saying they are self supervised generative models). I m happy to be convinced otherwise, but it might not be worth it for this paper. * So there s a lot of 2^M for the power set over modalities, but do you really use the null set?<BRK>The key idea is to optimize over all possible subsets of modalities (their powerset), even if only a subset of the modalities are observed. Quality: The paper was good quality, as it was clearly written and easy to read. Significance: There is rather limited amount of work on generative modeling in multimodal applications, so this work addresses that gap. Pros: This was a nice work which drew interesting connections between prior works (MVAE and MMVAE), provided explanations for each method’s strengths and weaknesses from a theoretical viewpoint, and proposed an approach which bridged the two. Cons: It s not very clear to me how the PolyMNIST setting counts as having different modalities when the only thing that s changing is the background image. For the other datasets (e.g.MNIST SVHN Text), the separation is more clear.<BRK>Different from PoE (product of experts) and MoE (mixture of experts), MoPoE (mixture of product of experts) explores a more general way by mixing more experts where each expert is the product of a subset of all modalities’ posterior. In this way, as illustrated by the author, PoE and MoE can be seen as specific cases of MoPoE easily. The proposed model achieves competitive results compared with PoE and MoE. In my opinion, it is difficult for these two advantages to happen at the same time. It will be better for the authors to explain this in the paper. 3.The samples generated in Fig.14 need to be discussed. 4.Regardless of whether Lemma 3 can be proved, a verification test may be a good choice. 5.Although as the author stated that MoE is a special case of MoPoE, the loss function in Eq.9 and MMVAE model (Shi et al., 2019) (the first equation on page 5) are different.
Reject. rating score: 4. rating score: 4. rating score: 6. rating score: 6. rating score: 7. <BRK>The authors propose to use many techniques to push the limit of neural quantization, which shows reasonable improvements in some datasets. + clear presentation. + easy to read and followMy main complaint is that this type of combination is better for a technical report rather than a standalone paper. In more detail, the authors claim four proposed components: AdaQuant, Integer programming, Batch norm tuning, and two pipelines for neural quantization. However, the problem is, are these four components all original or just adapted from existing works? The authors are failed to present the relation of the proposed components with the exitinging ones. For example, in my understanding, the AdaQuant is only an adapted version of AdaRound, but the discussion about it (in Section 3.1) is too superficial. Furthermore, the experiment part is not convincing and I think can not back up the claim. With so many "proposed components", the missing of thorough ablation study is a big problem.<BRK>This paper proposed a set of methods for post training quantization of dnns. What is the definition of "compression ratio"? Since AdaQuant is the major claim, the authors should provide more discussion on how they dealt with this increased complexity. The authors claim that AdaQuant avoids overfitting, but the reason does not seem to be clear. There is no clear explanation of how AdaQuant increases the generality of the quantized model, and the discussion about the sample size (B) is hard to understand (why there s infinite solution when B << N?how B>  Ck^2/(HW) is derived for the convolution case?) Also, it seems that the "per channel" quantization method is utilized in this work, but the formulation in (2) seems to be for "per layer" optimization. Without clear explanation and justification about it, the proposed IP formulation does not make sense. The authors mentioned that deltaP should be additive and sum up to the total benefit. 3) Batch normalization tuning  Unfortunately, there is a very similar idea proposed by [Sun et al., NeurIPS 19]. Also, there are several suggestions to improve understanding of readers. Currently, the ablation study looks very confusing. It is not clear which of the pipeline options (light, advanced?) It would be desirable to expand the coverage of neural nets as much as the prior work did. Currently, the proposed methods only utilized "per channel" quantization.<BRK>This work presents a quite comprehensive multi step scheme for post training neural quantization that does not rely on large datasets or large computational resources. The work is has significance in the domain of post training neural quantization, especially in cases where only a small calibration set or limited resources are available. It would be interesting to also think about accumulator quantization. AdaQuant seems like a rather straight forward step from AdaRound by combining with it some related works. It is not clear how the BN error compensation differs exactly from related work. Some details were missing, for example, it is up to the reader to guess how many bits were used in the accumulators. Some spelling mistakes, e.g., “Optimizing Quantization Pipline”Overall: An engineering oriented paper with some lack of testable hypotheses and analysis of some parts of the methods, but the 4 bit results justify publication. Edit: I have not seen author reply and further reading of the paper has not clarified the main issues found by all reviewers.<BRK>The paper introduces a series of techniques to quantize neural networks, and how to combine them:* Layer by layer quantization where weights can change as needed (rather than to the nearest quantization error). > acronym not previously introduced (unless mistaken)* Section 4: IP acronym should be introduced in the integer programming section. * Tuning batch norm weights by re computing statistics. * Integer programming to determine the precision required at every layer. * All components in the proposed method are straightforward. CONS* The organization of the paper can and should be easily improved (see below). Fig.2 is introduced much earlier than it is first referred to in the text, in section 5.2, where it is not explained either. Additionally, the colors for the last two lines are too similar. The experiment of Fig.1  should be introduced with some level of detail (there is none outside the caption). The caption is a bit confusing: what are the "the last layers after"? It can be inferred from earlier sections what this refers to but it should be explained clearly. Also, I am not very familiar with quantization papers, so I might have missed relevant baselines, but they seem hard to compare. For instance, the authors omit [Nagel et al 2020](https://arxiv.org/pdf/2004.10568.pdf), which seems to do better at similar quantization levels, but I am not sure the results are directly comparable. The authors should discuss this better. Compression ratio in the plots should indicate %? * "Early work by Lin et al.(2016) used [a] convex optimization formulation which results ~~with~~ [in] a simple greedy compression scheme."<BRK>In particular, the authors focus on sub 8 bit quantization and propose a novel integer linear programming formulation to find the optimal bit width for a given model size. These include(i) AdaQuant in which the parameters are quantized layer by layer to match the full precision output,(ii) a batch norm tuning approach to re adjust the statistics to the quantized model, and(iii) an advanced pipeline for cases where backpropogation can be performed. I think the approach in the paper is pretty interesting, and specially the integer linear programming solution. Overall the paper is strong however, please note the following:  Page 3 last paragraph: it seems there are errors in the results for calibration data. 2/ The analysis requires specifying the rank of W and in particular the relationship between M and N. In particular, note that the matrix W can at most have a rank of min(M,N) and as a result the rest of the linear equations for calibrating the data would be redundant. This needs to be taken into account in your result. Figure 4 (a,b): Please provide the BOPS for the mixed precision results. This is a good result but please note that other work in the literature (arxiv:2001.00281) reports 72.91\% for INT8 quantization of MobileNetV2 (this comparison is actually missing from the paper). It is immediately not clear if the lower reported accuracy is due to the weaker FP32 baseline used or if it is an inherent problem with the method (most probably it is the former but it would be to show this).
Accept (Poster). rating score: 7. rating score: 6. rating score: 6. rating score: 6. <BRK>Summary Authors applied reinforcement learning framework to the problem of task oriented dialog. In particular, they used the option framework to represent the connection between the dialog policy and the natural language generation. Theoretically, they showed that synchronized updates to the low level and high level policy may never converge, yet asynchronized updates guarantees convergence. Authors also used a discriminator reward signal to cope with sparse reward (dialog success rate) and better representation of the human evaluation. Empirical results on MultiWoz 2 and 2.1 shows improvement over other state of the art techniques. Summary:+ Appealing theoretical contributions+ Empirical results are encouraging+ The use of discriminator for reward shaping in addition to task success rate is interesting   Writing and explanation can be improved. There are few places (see details) that authors have assumptions in mind but do not provide those assumptions until later. The original option framework assumes given options. Given the ending of the paper, I interpreted that the set of dialog acts (i.e.options) are learnt automatically but I could not find this to be communicated explicitly. Questions:Did you look into the quality of dialogs specifically? In some of our recent experiments we found out that those automatic metrics do not necessarily correspond to great user experience. Details Abstract: In o gur work  > In our workP5: "oracle dialogue state": What is the oracle dialog state and how is it calculated? The world oracle conveys the meaning of being absolute truth which sounds a bit unexpected. The same comment is applicable to oracle database (DB) results. If yes, do you still name the output of the DB as oracle results? Would be great to tell your reader earlier.<BRK>This paper attempts to model task oriented dialogue system using hierarchical reinforcement learning between the actions policy and natural language generation system. The paper is well written, easy to follow, and adequately motivated. Pros:Hierarchical RL formulation with options for joint action policy and natural language generationThe paper proposes asynchronous updates between dialogue policy and NLG to theoretically guarantee their convergence to a local maximizer. It also proposes using a language models as a discriminator model for reward assignment to further improve the comprehensibility of generated responses. Experiments show significant performance improvement over the baselines with both automatic and human evaluations. Also, the results show that continuous representation of the action policy in HDNO performs better than the discrete representation in the LaRL, which is very interesting and informative. Cons:The use of an oracle dialogue state and an oracle database search result. This is a major drawback of this work. Recent work in this space are now considering imperfect dialogue state and the corresponding database search results. The evaluation/comparison using updated official evaluator may be missing for some of the more recent work e.g..,GPT 2 (Budzianowski and Vulic 2019)Structured Fusion (Mehri, Srinivasan, and Eskenazi 2019) SOLOIST (Peng et al.2020)DSTC8 Track 1 Winner (Ham et al.2020)SimpleTOD (Hosseini Asl et al.2020)Questions:What do you mean by “Distinguished from a conventional modular system, we additionally give a context to NLG to satisfy the option framework.”? You mean the dialogue context is equivalent to the state space in HDNO formulation? What kind of model is used for the language model discriminator? Are the results in Table 5 based on the updated official evaluator similar to Table 1? If not, then the comparison in Table 5 is not apples to apples.<BRK>Summary: This paper proposes modeling the hierarchical structure between dialog policy and natural language generator with option network and train it with HRL. It also introduces a discriminator modeled with language models as an additional reward, which further improves the learning procedure s comprehensibility. Pros: 1.This paper proposed formulating dialog policy as a high level policy over dialog act and NLG as a low level policy and train the word policy module using HRL. 3.It is interesting to see that when formulated as latent factors, the distribution of dialog acts still shows a clustering pattern, which manifests the model s interpretability. The ablation study shows that this reward can be useful (though not significant). Cons:The proposed HRL approach is a direct application of the option framework on a task oriented dialog system. It is similar to the hierarchical structure in the open domain dialog system. Introduction paragraph 2: When introducing end2end models, there is a lack of citations of recent e2e models including DAMD, SimpleTOD, SOLOIST, etc.<BRK>The paper proposes a  HRL/options framework based method to learn a dialog policy over learned latent dialog acts which can then guide the lower level NLG. This along with a regularization reward using language model the paper aims to improve comprehensibility. The paper looks at a very relevant problem in dialog research. The ability to use RL along with SL and being able to use RL without compromising on comprehensibility. 2.The paper proposes Options framework based method for using the hierarchical structure in dialog to learn the dialog policy and NLG in a hierarchical fashion. 3.They provide a training that guarantees convergence to local maxima. The paper starts with the motivation of handling comprehensibility. It would helpful to place it more clearly where the contribution of the paper lies in the related work. HRL in general has been used previously for goal oriented dialog, using language models to regularize RL models has been used and pertaining using SL is widely used. 3.Relevant to some of the above points. Some discussion on the current design choices and why making the proposed methods features to some of the other baselines is not a way to achieve some benefits is not the right way to do it. It s ok for the proposed method to be one particular way, but that discussion would be useful. 4.Clarification on the task setting: Is it the case that the agent s current utterance does not decide what the next user utterance is? If that is the case, that should be made more explicit. The complexity of learning options would be way different in the two different settings. To see if this is due to some new addition by the paper or is generally present. 6.There are several parts to the method and there are I assume several differences in the architecture etc with baselines etc. It would be really useful to have an ablation study to disentangle which piece of the training or method is contributing to what and how in the performance measure. Otherwise for example it is not clear to me if the improvement in Blue compared to LaRL comes from the extra reward using the language model or from the options framework. what part of the performance is coming from pretraining (especially if using VAE type is novel, then quantifying that is important with and without VAE type SL), etc. As the paper points out, the success rate alone is not enough. Human evaluation is costly and could also have bias like the paper points out. After author responseThe authors have responded to most of my questions/concerns satisfactorily.
Accept (Poster). rating score: 8. rating score: 7. rating score: 6. rating score: 5. <BRK>Recommendation:I strongly recommend to accept this paper. Most of the paper is a really nice and gentle discussion of this idea.<BRK>This is potentially the main weakness of the paper. Is it on a graph? Why are they introduced? The authors have made a good effort in their revisions to improve and clarify the exposition, and rectify the other comments made by the reviewers.<BRK>The paper also proposed a new generalisation bound, although did not compare with other generalisation bounds. In particular, many definitions rely on globally defined constants such as r and t, which has not been made clear to the reader.<BRK>Empirical observations on LeNet and Wide ResNet trained on MNIST and CIFAR showed adversarially trained or noise trained networks did exhibit curvature of the decision boundary, showing finer structure than previously known.
Accept (Poster). rating score: 7. rating score: 7. rating score: 7. rating score: 7. <BRK>The authors apply MFTMA to DNNs trained on CIFAR with label noise to analyze their behaviors between generalization and memorization. This claim is convincing. Another claim that this is not caused by a vanishing gradient effect is plausible, too. I m sure these results give some insights into understanding generalization and memorization by DNNs. In the experiment of rewinding individual layers, the three FC layers are left untouched.<BRK>### Summary:This paper investigates memorization in deep neural networks (DNNs). Through empirical analysis, they show that i) generalizing feature are learned initially and that memorization happen later in training mostly in the top layers ii) we can mitigate memorization by rewinding the top layers parameters to earlier values. Finally, they demonstrate that gradient descent initially ignores noisy example and focus on correctly labeled examples. This paper makes interesting observation regarding memorization of deep network, it performs a good empirical study which provide enough evidences for the different claims. ### Pros:  As stated above, the paper makes interesting observation regarding memorization of deep network. It performs a thorough empirical study. In addition, it would be good to further explain Fig 1.<BRK>I find three particularly interesting results in this paper:  later layers seem to be responsible for memorization, while early layers seem to converge last but consistently learn "generalizing" features (although this may not be true for other architectures)  increasing the dimensionality of the network to induce double descent _decreases_ the manifold dimensionality of the last layer. This is consistent with overparameterization making everything smoother/flatter and more easily disentangleable in the last layer. Although the results of the paper do not hinge entirely on it, the reliance on MFTMA limits the interpretation somewhat: while an interesting tool, it s not clear to me that it allows us to make strong statements about the geometry of neural networks. I presume that the figure shows a different minibatch of Xs for these two columns; I would highly recommend not doing so and using the exact same inputs. Having to zoom in and out to be able to read figures properly hurts accessibility and legibility, which detracts from the quality of the paper. In Figure 5A, it s not immediately clear that the X axis are individual layers, the log(nabla) label should be on the colorbar rather than on top of the figure.<BRK>The paper empirically studies the reason for the phenomenon that deep neural networks can memorize the data labels, even the labels are randomly generated. The findings of the paper are interesting. It shows the heterogeneity in layers and training stage of the neural net:i) Memorization occurs in deeper layers; rewinding the final layer to the early weights mitigates memorization. iv) Near initialization, the gradient is dominated by unpermuted examples. In general, this paper carries well organized experiments. The modified version improves clarity.
Accept (Poster). rating score: 8. rating score: 6. rating score: 6. rating score: 6. <BRK>The paper is easy to read and technically sound. The algorithm presented is efficient to the point that it can be used for large networks and datasets. And the insights on the differences between a FCN and a CNN are quite significant. I however believe that the paper would be stronger with the addition of a small experiment, namely, comparing the results from MNIST using Fashion MNIST. This would be significant for pruning. Have you considered estimating the volume of the regions around the training data points?<BRK>This submission proposes a new method that can extract a linear region from the network function such that the region contains a specific data point. It only needs the computation of a forward computation. In the experimental analysis, the paper compares FCN and CNN by checking test performances, the extracted function, and coefficients. The submission devotes a lot of analysis to the difference between FCN and CNN. I think most of the difference is because the CNN has many more linear regions and more complex function surface, which are harder to simplify, comparing to FCN. This might be useful for studying the robustness of these neural networks. 3.More CNN architectures might make this analysis more interesting. For example, the residual links in a ResNet may have some relation with these linear coefficients.<BRK>The paper is well written, and well presented    the push of most of the details on tropical algebra to the appendices is appreciated. I am happy to accept that TRM can approximate network s computation over a finite set of inputs and I agree with the authors that counting linear regions alone might not tell us all that much about complexity of the mappng. However, I am not sure there is enough in this paper to convince me that TRM evaluated on training data contains enough information to tell us something useful about generalisation capabilities of the underlying model . I think that might be the reason why performance of TRM based approxiation for MNIST is similar to the original network and drops from 71% to below 42% (the latter happening to be approximately the accuracy of a linear model on CIFAR10). Figure 2 seem to provide more evidence that TRM might only capture information about simpler representation.<BRK>Summary: This paper studies the role of linear terms in the network performance using nontrivial tropical algebra inspired algorithms. In fully connected networks, such a modeling seems to work well on the test data. The paper argues that the number of linear regions may not be the right metric for expressiveness and generalization. Cons:1) While the general motivation is nice, the paper makes some strong claims without enough justification.
Accept (Poster). rating score: 8. rating score: 6. rating score: 6. rating score: 4. <BRK>Summary: Training free NAS is a promising direction. This work demonstrates that two theoretically inspired indicators: the spectrum of NTK and number of linear regions, strongly correlate with the network’s performance, and can be leveraged to reduce the search cost and decouple the analysis of the network’s trainability and expressivity. + On NAS Bench 201 and DARTS search spaces, the propose method’s results are extremely promising. For the latter space, the proposed method can search within 30 minutes (on CIFAR 10) and 4 hours (on ImageNet), while the search models’ accuracy remain among the most competitive few. Although the authors claimed that the two chosen indicators are strongly correlated with the network’s test accuracies, I’m not sure how true that actually is. The authors might need to discuss more or to tone down further.<BRK>This paper introduces a searching framework of neural architectures ranking the candidates with two different metrics: the spectrum of NTK and the number of linear regions in the input space. These two metrics do not require the training of neural networks lightening the computational burdensome. Overall, this paper proposes new metrics that can be used in the NAS search. NAS Bench 201: Extending the Scope of Reproducible Neural Architecture Search. The author provides two new different metrics: a condition number of NTK and cardinality of linear regions to select good architectures besides the validation loss. Free from the training allows their method to free from computational burdensome allowing to stack an equivalent number of cells for both the search and evaluation phase. (However, NAS BENCH 201 results are competitive.) Table 4 seems $R$ plays more significant role than $\kappa$.<BRK>Also, the novelty is a bit limited as the metric seems to be a direct application/extension of recent deep learning theories. The metrics to measure trainability and expressivity are inspired by recent progress on deep learning theory. Although it’s more expensive than training free, it’s practically acceptable. Specifically, The trainability of an architecture is evaluated by the spectrum of its NTKs, and the expressivity is evaluated by the number of linear regions in its input space. 3.The proposed NAS method can complete the search much faster than previous NAS methods. I encourage the authors to update the paper with the results provided in the rebuttal, especially the explanation of the novelty and the results for "Proposed metric with existing search algorithms". This makes the novelty of this work a bit weak.<BRK>This work studied an interesting topic of training free Neural Architecture Search (NAS). It utilizes two training free indicators to measure the performance of a network without training it. However,  as the main contribution, the two measurements of the training free indicators (trainability and expressivity) already be proposed by previous works. The writing quality of the paper is good enough2. On the number of linear regions of convolutional neural networks. 2.The differences between the proposed pruning based NAS and previous work are not clear. What is the key novelty of the proposed pruning strategy? 3.According to Table 3, it seems that the TE NAS couldn’t find the optimal neural architecture like P DARTS and PC DARTS, which confirms the limitation of this training free search framework.
Reject. rating score: 3. rating score: 5. rating score: 6. rating score: 6. <BRK>This method trains a deep graph generation model in a differentially private manner, by injecting Gaussian noise to the gradients of link reconstruction module, thereby claiming to guarantee edge privacy, while ensuring utility via a structure learning component based on a variational generative adversarial network (GAN) architecture, to enable structure oriented graph comparison to the original. The motivation of the work is sound, yet the work does not deliver on its main promise: while the claim is made that link privacy is protected, there is no experiment to justify this claim. These results are not presented, and the privacy parameters ε that lead to them are not discussed. Thus, it is hard to know how much utility has to be sacrificed for the sake of the privacy gains mentioned. The fact that learning under differentially privacy can be surprising successful, and thus constitutes an attack on differential privacy, has been established in previous work [1]. This paper should reflect more thoroughly on what privacy means in the proposed setting, how it is shown, and what utility it corresponds to. On the other hand, there have been efforts to define explicit guarantees regarding link reconstruction [2, 3], which this work does not take in consideration. Incidentally, the paper claims that graphs lack efficient universal representations, citing [Dong et al., 2019]. It is not clear how the cited paper, which studies a form of universal graph representation, supports this statement. Besides, there is work explicitly dealing with efficient universal graph representations [4], based on similar principles to those studied in the cited work. Last, previous work [5] has already established, to a higher degree that it is done here, that any kind of structural identification attack can effectively be prevented using random edge perturbation, even while important properties of the whole network, as well as of subgraphs thereof, can be accurately calculated on the perturbed data. Given these results, it is unclear what this work adds to what has been already established. KDD 2011.[2] k isomorphism: Privacy preserving network publication against structural attacks.<BRK>The paper considers the problem of learning graph properties with differential privacy (DP) constraints on edges. The problem is well motivated in the paper with potential applications in social network learning where interactions between users (nodes) are sensitive. An interesting observation of the paper is that it is enough to preserve privacy when training the generator network since the inference is only based on the generated network. My main concern about the paper is its novelty. It seems to combine results from different areas. I hope the novelty can be further explained in the response. In the experiments, it would be better to report delta values for the privacy guarantees for each experiment. 2.It seems the exact nonprivate counterpart of DPGGAN is not included in the experiments. It would be nice to have results on it to see the exact influence of privacy. It would be better if the authors can decouple the influence of privacy and network structure. 3.It seems unnecessary to include the whole proof of DPSGD in the appendix since it is almost identical to the original paper. Including the sensitivity analysis would be enough to me.<BRK>This work consider the problem of link privacy when releasing models that are trained on graph data. It achieves this by making a generative graph model based on a VAE differentially private. Comparison of algorithms with related work and details of the mechanism could be expanded to articulate novelty and significance of the work. I.The paper is well written and nicely merges ideas of DP + VAE + graphs for edge privacy. II.It would be good to see the main body of the paper articulating what the difference is between papers that use DPSGD from Abadi et al.as is and this paper. That is, what exactly had to be changed for graph data. III.Experimental results could include accuracy of classification tasks of graph network models as it seems baselines in the paper use non private generated networks only for downstream tasks. It may suggest that accuracy of generated networks on these tasks is already not optimal and the utility impact of DP is smoothened as a result. Please consider adding what is different from DPSGD mechanism; what was the challenge in applying DPSGD in this setting. The use of word "probably" is worrisome. 3.Lemma 1: please articulate how s and C depend on each other or relate, if at all. Does Algorithm 1 make use of s? If not why Lemma 1 was needed.<BRK>The global network structure should be effectively preserved; 2. The link privacy should be rigorously protected. This paper looks at the secure release of network data with deep generative models. I think the problem this paper looks at is both interesting and fundamental. Most of the current DP works focus on estimating one specific property of the dataset s underlying distribution, e.g., mean of the distribution. However, in many real world applications, the tasks can not be known beforehand, and a potential solution is to release synthetic datasets under DP constraint. As far as I know, there are few papers in this area of privately releasing synthetic datasets with deep generative models. Meanwhile, experiments are carried out on real world network datasets, and it has shown that for many structural statistics, the algorithms have competitive performance. I have little background knowledge of graph generation, and I can not give many technical comments:1. I will be appreciated if the authors can provide the results when $\varepsilon   \infty$, which I expect to be better than the case when $\varepsilon   10$. Does it mean your algorithm outperforms the current best algorithm in IMDB when there is no privacy constraint? 2.I think it is also interesting to include some experiments on synthetic datasets. For example, if the objective is to estimate the number of triangles in the graph, I expect the privacy to incur more loss when the underlying graph is complete compared with a sparse graph. I am not sure this paper has made many theoretic contributions.
Accept (Poster). rating score: 8. rating score: 7. rating score: 7. rating score: 7. <BRK>The authors consider the problem  of differential  performance across subgroups commonly present in classifiers, and propose to mitigate it. This approach is demonstrated in several datasets including the ISIC skin cancer data. The appendices are also very  thorough and the code is organized well. There is sufficient detail to help readers reproduce the results. 2.Relations to the  area of group fairness may be helpful. The goal there is also  to ensure parity of predictions among groups, and various methods are used. It may be possible to extend this approach to that area, and the idea of data augmentation may be very appealing there. A small discussion on this can be quite informative to the community. And is the gap same as the metric of interest for SGDRO? It may be helpful to define the experimental metrics explicitly. 6.Have you tried  any preliminary experiments with greater than 2 subgroups per class? 8.How well  will  this method work when there is considerable imbalance in the subgroups considered?<BRK>The paper focuses on data augmentation in cases when the classifier performance is worse on specific parts of the data. The solution proposed by the paper is quite intuitive: first, given the subgroups in a class, a CycleGAN model is used to learn different versions of the same training example, each corresponding to a different subgroup. Once the augmented versions of the examples are available, the the classifier is then trained with additional penalty terms, ensuring that the predictions are consistent across different versions of the same training example. Empirical results show that the proposed method does reasonably well as compared to the competitors. The proposed method is quite intuitive and simple to implement; and the empirical results are quite encouraging. However, that may not always be the case in the real world. Consider for instance the problem of fair classification where subgroups (e.g., socially salient groups) might span multiple classes. In such cases, even the proposed method would also have a problem. How does the proposed approach expected to perform in such cases?<BRK>Machine learning models are trained to optimize performance on the entire training set, and can often exhibit inaccurate performance on a subgroup. Such inaccuracy often results from the model’s dependence on spurious features. This paper proposes model patching — a two step method to avoid this problem. The first step learns inter subgroup transformations where an example from one subgroup is transformed into examples of all the other subgroups within a class. The second stage uses these transformations as controlled data augmentations to learn a classifier that is robust to subgroup specific variation. The second stage uses the original data and the augmented data from stage 1 and minimizes a subgroup robust objective plus a self consistency loss. Finally, the authors apply the proposed framework on the real world ISIC skin cancer dataset and found that it improves robust accuracy by 11.7% compared to the other methods. Strengths:  Extensive experimentation: I thought that the experiments were sufficient to demonstrate the effectiveness of the proposed approach. I thought that the use of CycleGAN and SGDRO is well motivated and appropriate for this setting. Weaknesses:  No overlapping subgroups across classes. I am not sure whether the proposed framework can be easily generalized for the setting when the subgroups are overlapping across multiple classes. 2.If I expand the square term given in theorem 1, there will also be a product term in addition to the consistency loss and CycleGAN loss. 4.Subsection 4.2.2 does not report performance for the datasets — CelebA and MNIST. In summary, this paper proposes a model patching based two step method to design a classifier that is robust and reduces performance disparity across different subgroups.<BRK>This paper additionally introduces a novel objective (SGDRO). CAMEL shows preferable performance on various data, including a real world dataset of skin cancer classification. The proposed method is straightforward and sound. 1.The authors conducted extensive experiments with various datasets, including a real world dataset of skin cancer classification. 1.CAMEL can be combined with heuristic augmentation (e.g., rotations) to improve robustness. The datasets used in the experiments are too simple to show the effectiveness of CAMEL and SGDRO. Specifically, the number of subgroups in each class of all datasets is at most 2. On the other hand, especially SGDRO assumes that subgroups have structure. A more complicated subgroup setting should be considered. 1.When the number of subgroups in each class and the number of classes is limited as the experiments, one can treat $\mathcal{Y}\times\mathcal{Z}$ as target and use ERM. This should be a strong baseline. 1.I concern the scalability of this method. # FeedbacksThe paper will be improved if generated images of CycleGANs are presented to visually show how CycleGANs change subgroups.
Reject. rating score: 4. rating score: 5. rating score: 5. rating score: 6. <BRK>This paper proposes a risk sensitive algorithm with function approximation in reinforcement learning. This paper is trying to extend the results in [Fei et al.2020] to the function approximation setting. The main contribution of this paper is to provide theoretical guarantees for the proposed algorithms. ConsMy major concern is the novelty in this paper.<BRK>The paper considers a finite horizon episodic RL problem where the objective is risk sensitive using the entropic risk measure. The organization of the paper is pretty clear. What’s proposed for general function approximation is mostly a conceptual algorithm.<BRK>In this work the authors studied the entropic risk RL problem and derived a regret bound of the entropic value iteration algorithm via the risk sensitive optimism in face of uncertainty approach. While I understand this paper s main contribution is theoretical and appreciate the efforts for deriving that.<BRK>##########################################################################Summary:The paper studies risk sensitive reinforcement learning with the entropic risk measure and function approximation. A meta algorithm based on value iteration is first proposed, then the paper proposes two concrete instantiations, one for linear function approximation and one for general function approximation. I think this paper considers an important topic: risk sensitive reinforcement learning, and provides a first step in establishing theoretical foundation under that setting with function approximation. The technical tools used in this paper seem to be based on a combination of ideas from Fei et al.(2020), Cai et al (2019), Russo and Van Roy (2014) and other papers.
Reject. rating score: 4. rating score: 5. rating score: 6. rating score: 6. <BRK>4) The paper proposes to adapt the learning rate based on the observed loss values. I think this modelling choice should be discussed and justified in the paper. **In its current state, I recommend rejecting this paper. **## Major Comments1) The quality of the empirical evaluation is questionable in my opinion. The paper says that “Since the methods […] are not able to generalize, we do not compare them here”. Since they aren’t meta learned, there is no notion of generalization for this method. 3) I am missing a discussion of the computational and memory cost of the proposed method.<BRK>In general, this paper addresses an important problem of LR schedule and the suggested method is easy to follow and the experimental results are promising. However, there are several issues that need to be addressed to improve the work further. 1.The novelty seems quite limited – it appears to be a straightforward application of meta weight net to the problem of LR scheduling. 2.Analysis on the learned optimizers is lacking, such as the convergence and speed. 3.It is not clear why the learned optimizer can be transferred to new heterogeneous tasks.<BRK>Summary: The paper proposes to parameterize learning rate (LR) schedule with an explicit mapping formulation. Justification of rating: The paper solve a practical problem that is not handled in the existing literature. Strengths:+ This work proposed to parameterize the LR schedule with a MLR SNet. + The meta learned approach allow the learned model to be applied to unseen data. + The paper provide comprehensive experiment to validate the efficacy of the proposed model. Is this also observed in any other dataset? In the "Formulation of MLR SNet", it states that the input $h_{t 1}$ and the training loss are preprocessed by a fully connected layer $W_1$ with ReLu activation function.<BRK>In this work, the authors use an LSTM to meta learn learning rate schedules. Understanding learning rate schedules is an interesting problem in machine learning and the authors seem to provide a useful blackbox learning rate scheduler. In the SM, they discuss that they have to choose 3 MLR SNET for transferability. How many runs are needed to meta train the LSTM? It would be nice if the authors could add tables reporting the accuracies, it is not clear from the plots how good/bad the performance of MLR SNET is.
Reject. rating score: 4. rating score: 5. rating score: 6. <BRK>One of the proposed techiques is flawed and need to be revised, while the other one is a trivial extension of the standard REINFORCE algorithm. ## Questions for the authors  Why should one prefer this framework over the mean variance one, a part from sampling issues? The paper is readable and easy to follow in all of its parts. It is not clear why one should prefer the EQUM framework over the classic Mean Variance one. In particular, the gradient they consider is equivalent to ignoring the gradient of the squared expected return term in the classic mean variance policy gradient, hence it is clearly not equivalent. In section 4.3 the authors implement the EQUM Policy Gradient, directly extending the REINFORCE estimator.<BRK>The paper proposes a policy gradient style RL algorithm that optimizes an expected quadratic utility, a commonly used objective of risk management in finance and economics. The key idea here is based on the observation that when using the quadratic utility function, the use of mean variance RL methods can be shown to optimize the utility of the agent. To this effect, the paper considers the use of expected quadratic utility maximization in the policy gradient. The paper implements two variations   policy gradient and actor critic with EQUM framework. I like the idea of using EQUM motivated by economics as the objective function. While I like the paper, there is clearly some room for improvement. The justification of \psi as \beta/2\alpha is not well justified.<BRK>In this paper the author proposed a new mean variance algorithm whose policy gradient algorithm is more simpler than other SOTA methods and it has an unbiased gradient. Instead of formulating the problem as an traditional mean variance constrained problem, the authors utilized quadratic utility theory and formulate the problem as variance minimization problem with a mean reward equality constraint. In general, I find this paper well written with ample discussions of state of the art mean variance RL algorithms. However i do have several questions about this paper:1) How does the per step variance discussion in Section 3.2 relate to the proposed method? How does one set \psi instead of treating it as a tunable parameter.
Accept (Poster). rating score: 7. rating score: 7. rating score: 6. rating score: 5. <BRK>This paper presents a scalable data poisoning attack algorithm focusing on targeted attacks. The technique is based on gradient matching, where the intuition is to design the poisoning patterns such that their effect on the gradient of the training loss mimics the gradient as if the targeted test image is included in the training data. The algorithm proposed in this paper is practical and general, making it a realistic poisoning threat to modern deep learning systems. The presentation is clear and the theoretical justification is intuitive and easy to understand.<BRK>It seems surprising that the related work section claims that poisoning attacks, unlike backdoor attacks, do not require access to test data. The proposed approach works by perturbing the clean poison set to introduce a gradient direction which mimics the victim training their model on a targeted mislabeled set. ### Post rebuttal updateI thank the authors for their response   this helps. To add to it, the authors further evaluate on Imagenet and achieve strong results. **3.Writing**  I enjoyed the writing in the paper. It would be nice to know how severe this is.<BRK>This paper proposed a simple yet effective approach for data poisoning attack targeting a few "clean label" victim images, using the idea of gradient matching (cosine similarity maximization) between the gradients of adversarial and clean losses. The attack results are significantly better than the compared poisoning attacks, and the authors show effective attacks on the ImageNet dataset as well as Google Cloud AutoML with the poisoned data. Overall, this paper shows some new insights and sets new benchmarks for targeted data poisoning attacks, with practical threat assessment on ImageNet datasets and Google Cloud AutoML, which I deem as a significant contribution. It will be more meaningful to control the effective budget/target and check the resulting accuracy of different number of targets, in order to understand whether gradient matching is scalable to multiple target setting.<BRK>The attack is formulated as a bilevel problem which is then solved with a (fast) heuristic approach based on aligning the gradients of the inner and outer objective functions. How do detection methods for backdoor attacks work against the proposed attack? (2) the authors should clarify the threat model, and clearly distinguish poisoning availability attacks (bilevel data poisoning) vs poisoning integrity attacks. (3) the authors should revise their sentences on the complexity of data poisoning (previous clean label targeted attacks like poison frogs are not as complex as bilevel data poisoning attacks). (this might be explained in the supplementary material, but it is important for understanding the whole results). This might make it difficult for the paper to reach a broader audience.
Reject. rating score: 3. rating score: 5. rating score: 6. rating score: 9. <BRK>This paper studies the optimization and generalization properties of a two layer linear network. The authors claim that this makes the convergence rate slower. cons:My overall concern is the significance of the results. In Appendex E, it is shown that the scaling considered in the paper and the NTK scaling lead to the **same** gradient flow dynamics.<BRK>This paper proves the convergence rate of gradient flow for training two layer linear networks. The authors may try initializations with different imbalances and plot the convergence curves to demonstrate the results in Theorem 1.<BRK>This paper analyzes the convergence of gradient descent optimizing overparametrized linear nn, and proves a exponential convergence rate. A sketch of Montanari paper about the property of $\hat \Theta$ can be discussed in the appendix. 3.Regarding the thm, it would be definitely sufficient for the conference if anything can be suggested with RELU activation. 5.In appendix, it s also great to prove that under a certain initialization, what is the expectation/value of high probability of the imbalance singular value. 6.How does amount of data affect generalization bound?<BRK>The results seem interesting in the deep learning theory literature. The main task of this paper is to  present a novel analysis of overparametrized single hidden layer linear networks, which formally connects initialization, optimization, and overparametrization with generalization performance. (2) The stationary point of the gradient flow is sufficiently close to the min norm solution in the linear case.
Reject. rating score: 5. rating score: 6. rating score: 7. rating score: 7. <BRK>Comments:I think the paper could be better organise and not focus so much on the RUDDER framework. This reward simply represents how much I deviated or not from the expert trajectories between time t and time t 1. Therefore presenting the method as a trajectory matching imitation learning method would be way simpler for the reader. Indeed understanding how events are computed is crucial because they define the intrinsic metric used to compute distances between trajectories. This has been for a long time the main problem of trajectory matching methods compared to distribution matching method. It seems also that the number of events should not be too large for the alignement method to work. In addition such alignement methods may have problems in stochastic environments unless the events are really well defined. Could the authors expand on this (deterministic vs stochastic environments)? At the moment, it is not ready for publication.<BRK>Experiments on two artificial tasks and a Minecraft task demonstrate that the presented method performs advantageously than two baselines (DQfD and BC+Q learning). +) The paper is overall clear and well written. +) Reused from the previous RUDDER paper though, the author provides a comprehensive theoretical analysis on how the proposed reward redistribution could not alter the optimality of the original problem, and how can these results adapt to the new configuration with demonstrations. Some of them are minor while others could be crucial to the acceptance in this round. ) Compared to RUDDER, the redistributed reward is computed by weighting the return with the differences between the trajectory similarities of two consecutive states. However, only very few of them are compared in the experiments. I do believe most of my concerns have been addressed. The authors are expected to further revise their paper to make it more self explained.<BRK>This paper presents strong research and is very well written. Sub tasks are an important element of the approach, but the authors did not even cite the most classical papers on hierarchical reinforcement learning. In particular, learning Sutton s options was not a trivial task in the past. The authors should explain why sub task learning seems to be trivial in this paper. I would ask however what would happen if the agent lost the key? The authors should say a bit more about this limitation. Could it be mitigated? If not so, this limitation mustn t be hidden. Also, assume that the goal state can be reached from each of these 3 states, but one has to follow a different path from each of these states.<BRK>The approach is shown to work well in toy tasks in multi room navigation scenarios and a final sparse reward task from the MineRL competition. The paper is clearly written and the introduction, reasoning behind the novel method are succinct and prior work and new contributions are clearly separated. All with their own requirements which will partially render them inapplicable to the tasks but they could nonetheless be linked under related work for a more complete picture. There is a lot of introduced structure: small set of events, independent training for events and finally use of the consensus strategy for switching between individual policies. A possible baseline would be to use the manually chosen set of events, train policies for all possible events independently and train high level controller to switch between these policies and solve the task. This baseline would use the same information about the event space and independent training, but would need to train more agents and also the high level controller. In the current text, this is hinted at but not actually executed. Similarly, if accepted the additional space could be used to show success in the final task during training.
Accept (Oral). rating score: 8. rating score: 7. rating score: 7. rating score: 7. <BRK>The authors target the unsupervised reinforcement learning problem. An opposite idea from the existing approaches by maximizing state entropy is adopted to minimize state entropy. It is interesting that such an idea has achieved good performance in unstable environments. The parameters or sufficient statistics are also applied to the policy. The motivation is clear and verified. It is generally a good paper. It is surprising that the exploration is achieved in the long term even minimizing state entropy. What if there are some patterns underlying the exploration events but only part of the  unstable  environment?<BRK>The paper discusses the relevance of the approach in environments that are said to be "unstable" (with an attemptto formalize this concept). The paper also shortly presents an application of the algorithm for imitation learning. Experiments are presented in a variety of environments. Major strengths:+ The paper is globally clear and well written+ The qualitative results showing and analyzing emergent behaviour are stimulating+ Implementation of the free energy principle in high dimensional spaces is known to be challenging and thispaper contributes towards understanding how to do it (yet the precise formal articulation between SMIRL and theFEP remains to be done, and this does not seem to be the primary objective of the paper)+ The discussion of the complementarity of within episode SMIRL and across episode novelty seeking intrinsicrewards is interesting+ The shortly described application for imitation learning is promising+ The are many and varied environments in experiments Major weaknesses:  The environments chosen for experimentation are such that minimizing surprise aligns very well with eitherproducing "interesting" behaviour or maximizing an external reward. This problem of trivial solutions to surprise minimization approaches has been called the Dark Room problem inthe FEP literature.<BRK>         **Summary:**This paper proposes a new intrinsic objective for RL agents: surprise minimization. Let me list some strong points:* The idea is simple, novel and well motivated. The paper positions this new intrinsic objective with respect to variants of novelty maximization objectives and brings a new perspective. * It’s well written and organized* The algorithm is tested on a relevant selection of environments and against state of the art algorithms using intrinsic objectives. * Some descriptions of the results are missing. * It would be interesting to discuss how it plays out in natural agents. In natural agents, surprise minimization must also be model based. If not, why so? * It seems to me that this approach could potentially tackle harder problems, but the paper is limited to a simplification of the tetris game, planar humanoid variants and Doom.<BRK>The idea at a high level is almost the opposite of intrinsic motivation RL approaches, which encourage novelty seeking behaviors. The proposed method instead aims to minimize surprise or state entropy. Pros:  The problem formulation is interesting and novel. Intrinsic motivation is well studied, but this problem considers the setting where the environment is unstable rather than static, which requires new methods. The paper is written well and is clear. The authors evaluate on many domains, highlighting the diversity of settings in which the approach can be applied. In Figure 3 left, it might be a problem that with minimal episodes, SMiRL does worse. The approach is reasonable and experiments show the value of the method in unstable environments. Post rebuttal response:The authors addressed most of my concerns so I continue to recommend acceptance of the paper.
Accept (Spotlight). rating score: 8. rating score: 8. rating score: 7. rating score: 6. <BRK>4.Was there an attempt to mix masking strategies during pre training? I think it would be interesting to address this issue given how closely related it is to this work, and the good performance the cited papers demonstrate using it. Would be interesting if you could make this vocab available or add a small sample in the appendix. 3.Throughout the paper there is an assumption that contiguous words are considered for masking. This was not immediately clear in the beginning of the paper (I realized it only in Section 3.2 with   “What about contiguous spans of more than two tokens?”). Have you considered extending this work to non contiguous spans?<BRK>This paper introduces an approach for masked language modeling, where they mask wordpieces together which have high PMI. In addition, showing how performance is affected by the amount of pretraining data is very useful, and the experiments range from small scale to large scale. Table 4 is a nice addition   it s interesting that the MLM loss is not predictive of downstream performance. I would be interested in seeing if this reduced the variance of the fine tuning process. That might be something the authors could include for the camera ready, maybe in the appendix. I believe the paper should be accepted.<BRK>Summary: This paper presents a masking strategy for training masked language models (MLM). The proposed strategy builds on previous approaches that mask semantically coherent spans of tokens (such as entire words, named entities, or spans) rather than randomly masking individual tokens. They show this gap remains if they use a larger pretraining corpus (54GB), as would likely be the case with a large scale pretraining experiment. They do additionally compare to outside models (AMBERT, SpanBERT, RoBERTa) and show their PMI based models outperform or perform similarly to these models. Overall, this paper introduces a solid theoretical basis for existing and new methods for training masked language models.<BRK>Summary:The paper proposes a variant on the MLM training objective which uses PMI in order to determine which spans to mask. I d have liked to some more analysis/discussion of the linguistic consequences of this new objective. E.g., can you say more about how often the model sees unigrams being masked and how the distribution of these unigrams differs from what would be seen if we did random masking? By way of analysis, the authors also make an argument that token level perplexity is not correlated with downstream performance.
Reject. rating score: 5. rating score: 5. rating score: 5. rating score: 7. <BRK>Normal RNNs/LSTMs update their hidden states at every time step. I also like the idea, and preliminary results for jumpy planning. Method: The paper proposes a method to update hidden states in an event driven manner. 4.Related work: I think, many of the important references are missing.<BRK>Summary: This work presents Jumpy RNNs, a recurrent network that learns to take variable length steps based on time scales of the data. The key concern about the paper is the lack of rigorous experimentation to study the usefulness of the proposed method. 2.Despite the paper stating that there have been some recent work on Neural ODEs (Chen et al., 2018), the paper does not compare with them. 3.The linear dynamics of hidden state is too simple and can only handle constant hidden state dynamics or the one with constant slope. Although the authors mention that the hidden state linearity does not translate to linearity in output space as the learned decoder can be an arbitrarily complex non linear function, but the experiments are only based on constant or linear hidden state/output dynamics.<BRK>To train this model, the authors propose to use a greedy supervision to determine optimal time intervals. Even though non uniform time step RNNs have been studied in many literature, this proposed training supervision method seems novel. +ves: + Overall, the paper is well written. In particular, the proposed model and its training methods are clearly explained. The authors addressed the novelty of this paper. The experimental results on toy tasks are convincing. I increased my score. The experiment is clearly not fair for a standard RNN.<BRK>The paper is an enjoyable read due to its clarity and presentation. ## Strengths  The proposed approach is simple and intuitive. ## Post rebuttalI thank the authors for their hard work, and for incorporating my suggestions into the paper. While the experiments were not conducted on common benchmarks or real world data, I think this paper is well written as a proof of concept, does not overclaim and should motivate further work on similar ideas.
Reject. rating score: 3. rating score: 5. rating score: 6. rating score: 6. <BRK>This implies that the proposed method is not learning to ignore the spurious features at all. It might seem from table 1 that C VIRMv1 is performing well   it has 46% accuracy on the test environment after all. The authors claim that the results in table 3 show that C VIRM with EIIL is performing better than IRM with EIIL, but the data does not support the claim. In its current form, I cannot recommend this paper for acceptance. ## Issue No 1 ## The proposed solution is arguably not learning at all. This is not just speculation, I have run experiments using a randomly initialized network on these benchmarks to confirm that they do indeed get ~50% +  2% accuracy on both train and test set. Does this mean the two methods proposed in the paper are bad? The BIRM formulation of the problem seems technically correct and the authors are tackling and important and interesting problem.<BRK>Summary:In this work, the authors consider the problem of continual learning with distribution shifts. Significance: The problem considered in the paper is interesting. The authors should have run experiments allowing all methods the same run times and then shown that IRM based methods are still not able to perform better. ReferencesJaved et al."Learning causal models online"Creager et al."Environment Inference for Invariant Learning" g) Current experiments do not reflect the true potential of the method proposed by the authors: The current experiments do not do a good job of reflecting a continual learning setup. Simple modifications on existing IRM based methods can outperform the method proposed by the authors. I make some suggestions on how to improve the experiments. In Arjovsky, the authors use two environments only.<BRK>This paper extends the idea of invariant risk minimization (IRM) initially introduced by Arjovsky et al.(2019) to the setting of continual learning in which environments are observed sequentially rather than concurrently. This extension is implemented under a variational Bayesian and bilevel framework and the optimization is solved using a variant of the alternating direction method of multiplier (ADMM). The authors demonstrate the superiority of the proposed methods on variants of Colored MNIST. Without them, it is hard to judge whether or not  the proposed method really generalizes out of distribution in a sequential manner. The section of Continual IRM by Approximate Bayesian Inference is not friendly with readers unfamiliar with ADMM, which makes that part hard to go through.<BRK>## SummaryThe paper proposes a generalization of the invariant risk minimization objective to the continual learning setting, where environments are observed sequentially. An ADMM strategy for the solution of the resulting bilevel problem is proposed. In extensive experiments on smaller MNIST like datasets, the method is shown to perform favorable to recent approaches for continual learning. ## Explanation of RatingThe main strength of the paper is that it attempts to tackle an important and open problem using a reasonably principled approach. The statement about convergence rates in the strongly convex / convex setting are a bit puzzling, as they do not apply to the problem at hand.
Reject. rating score: 4. rating score: 5. rating score: 7. rating score: 7. <BRK>This paper introduces a latent variable model for high dimensional stochastic time series. The model is akin to a VAE with RNNs that incorporate time series data. Using importance waiting and the standard ELBO does not seem to add to the paper. If the comparison cannot be made for some specific reason, the authors should explain why. Typo in the final sentence of the first paragraph of the experiments section.<BRK>Pros.1) the paper tackles a relevant problem. what about dynamic Bayesian networks, hidden Markov models, contiuous time Bayesian networks, and many other ...2) the paper states the interest is related to high dimensional time series, but the data sets used for numerical experiments, in my humble opinion, are not as such, they are of very small or moderate dimensionality, in terms of number of variables.<BRK>The authors adopt a Bayesian perceptive on the smoothing problem for time series living a latent space and irregularly observed. In particular, the random evolution of the process in latent space is clearly accounted for in the paper by embedding an SDE into an RNN.<BRK>So, the authors should define it in the paper so that the readers know what it is. How many dimension are used for states and observation data? When the underlying latent state is an ODE, the stochasticity of the time series could be modeled via rich conditional observation distributions.
Reject. rating score: 4. rating score: 5. rating score: 6. rating score: 7. <BRK>## SummaryThis paper proposes an approach to program synthesis that aims to incorporate information from different modalities, focusing on combining input output examples with natural language specifications. They formulate it as a form of constrained optimal program synthesis. **Results** The results are a little hard to interpret. In addition to this, the authors should present in Table 1 data for OpSynth with a routine that does check for consistency, such as the beam search approaches. The optimality criteria is in the form of maximizing the conditional probability of a program $P$ given a natural language description, as specified by a conditional distribution that is learned from data. ## OverviewOverall:  The use of program analysis methods within neural program synthesis is promising. The paper needlessly combines different concepts, and doesn’t clearly disambiguate what is it’s contribution. ## Questions  OpSynth finds 75.5% of the optimal programs. The formulation of optimal multimodal program synthesis in these terms. Is this performance on a test set? In particular, there are potentially very large gains to be had in efficiency.<BRK>The paper shows an interesting and novel combination of neural and constraint based synthesis that despite using a relatively simple neural model, manages to improve over more involved neural synthesis techniques. This is a significant point in the space of such algorithms and an important research result. There are a few areas for improvement of the paper, but overall it is well written and shows interesting results:  The technique is described very generally, but there was only one dataset and one task for which it improved the results over the baselines. Also n_k is not the same as n_k in the examples.<BRK>Summary A method for synthesizing programs from a combination of natural language specifications and formal specificatios (or, more concretely, input/output examples) is presented. Experiments show small but noticeable improvements over baselines on a dataset of regular expression synthesis benchmarks. * ( ) Lack of comparison with highly related work that combines neural generation approaches with deductive methods to simplify the search space. which check if the partial program generated so far is compatible with the examples. In the regex space, this is naturally applicable to left to right generation approaches by checking if the partial regex generated so far matches a prefix of the input example. I have doubts that this _helps_ readers to understand and due to the space limitations, it doesn t make things exact either (e.g., it took me a while to understand that $\Phi^+$/$\Phi^ $ implicitly consume $\mathbf{z}$; and the "definitions" for $\Phi^{+, }$ in Appendix B/Fig.9 are not precise as they just map to some undefined underlying regular expression language)Recommendation I think this is a borderline paper.<BRK>The work presents a technique for synthesizing program from a combination of NL and input output examples. It is guaranteed to find the best possible program under a trained NL >AST model that satisfies the given examples. The technique assumes a particular kind of NL >AST model from the literature and a pruning mechanism for infeasible partial programs, integrated into best first search. On a recent dataset of multi modal regular expression problems, the technique significantly outperforms both purely neural, purely symbolic, and prior multi modal baselines, although it only slightly outperforms its own ablations. Other approaches to optimal program synthesis exist, but they focus on manual cost functions. In theory, best first search could be integrated into other methods, but this is the first work that presents such a method end to end. The paper is written clearly and with plethora of examples. However, space constraints are understandable and unavoidable. The ablation experiments are also quite insightful. The main gains in the approach seem to come from the choice of ASN as a semantic parsing model and the feasibility pruning, not from the best first formulation. This is a fixed order of expansion, the same one as TranX. Could you formally state and prove the statement of optimality wrt the semantic parsing model as a theorem?
Accept (Poster). rating score: 8. rating score: 6. rating score: 6. <BRK>To do this, the paper proposes the use of a FIPP objective, which consists of the linear combination of reconstruction and transfer loss terms. * The proposed method is quite efficient because it can be represented as a few matrix operations.<BRK>This paper proposes an approach to align word vector spaces based on a dictionary of pairs (e.g.translations) that retains "common geometric structure" (inner products within each vector space for a subset of word pairs). In essence the approach applies preprocessing to each vector space, finds a projection of one vector spaces to the (lower dimensional) vector space minimising a combination of a reconstruction and transfer loss, and then aligns the two using weighted procrustes to resolve rotational symmetries. Is it crucial to use the weighted variant here? Instead, Artexte 2017 and Smith 2017 showed (independently) that procrustes provides an analytical solution to the optimisation problem proposed in Mikolov 2013 under the orthogonality constraint.<BRK>While the paper might not be so impactful, it might still be a pretty nice and quick to compute BLI baseline for any future developments in this area. This is some inaccurate argumentation imo. The paper is clearly written and easy to follow, and the main results across three tasks (bilingual lexicon induction + 2 downstream tasks) following a standard evaluation setup demonstrate that the method can often outperform its competitors.
Reject. rating score: 4. rating score: 5. rating score: 5. rating score: 6. <BRK>##########################################################################Summary:This paper proposes a semantically adaptive upsampling approach for layout to image translation. The only difference from CARAFE is that the proposed method uses the semantic label maps rather than the feature maps to predict the upsampling kernels. 2.The novelty of the semantically adaptive upsampling is limited. ##########################################################################Reasons for score: My main reason for rating this paper as below the acceptance threshold is that the noveltly is limited (see the 2nd point in Cons for explanation), and there is not a large improvement on the quality of the synthesized images.<BRK>Many of the reviewers share similar concerns regarding the technical novelty of this work. **Weakness**  Exposition  The exposition of the proposed method can be improved. The proposed upsampling layer is called “semantically aware”. Technical novelty   	My main concern about the paper lies in its technical novelty. 2016.The work most relevant to this paper is the CARAFE [Wang et al.ICCV 2019], which can be viewed as a special case of the dynamic filter network for feature upsampling. However, I have concerns about the high degree of similarity with the prior method and the lack of comparison with CARAFE.<BRK>The proposed method is able to aggregate semantic information in the layout input and adaptively conducts class specific upsampling in the translation process. 5.In Table 3, it seems that the semantic consistency with layout input is worse than TSIT and LGGAN. [a] B.De Brabandere et al., Dynamic Filter Networks, NIPS 2016.<BRK>Comparison results with different upsampling methods and state of the art methods guarantee the effectiveness of the proposed methods. (+) This paper is well organized and the description of the proposed method is easy to understand. The paper is well organized and easy to follow.
Accept (Poster). rating score: 8. rating score: 6. rating score: 6. rating score: 6. <BRK>Recommendation and Rationale:I strongly support acceptance because this paper contains much needed fundamental work on theoretical underpinnings of modern meta learning. Honest discussion of limitations and good intuition is provided for applicability of the work. Writing is clear enough, although the paper is dense.<BRK>### SummaryThe authors of this paper prove that the optimal learning rate for MAML is negative under mixed linear regression and nonlinear regression with overparametrized models. ### Recommendation / JustificationI vote that the paper is marginally above the acceptance threshold. * I think the clarity could be improved, especially in section 3. * It would be nice to discuss more the practical implications of the results or whether there still exists a divide between theory and practice.<BRK>Two more notes:  1. This would be useful to do in both linear settings (see e.g.the experiments in Denevi et al.(2019)) and for standard meta learning tasks. For future versions of the paper, I encourage the authors to consider adding such results.<BRK>2.Presentation of the main results: I would suggest the authors formally state the results in theorems or propositions. However, there are some concerns regarding the practical relevance and presentation of the results. It is helpful to be explicit about the loss function in the main text. How could you use the results in this paper to provide some guidance?
Reject. rating score: 4. rating score: 4. rating score: 5. rating score: 6. <BRK>In terms of novelty, this paper has a lot of promise. Update:I thank the authors for providing additional data, however the additional data is insufficient for me to recommend acceptance. * The training approach for a network without multiplications seems like a novel contribution in its own right.<BRK>In this paper, the authors consider the task of adversarial/robust learning with respect to neural networks. In this direction, they consider a neural network that is built out of ell_inf neurons.<BRK>This paper discussed a technique to obtain a neural network with verifiablerobustness guarantee. A recent work on L2 norm verifiable training is [3]. Despite that the evaluation of the proposed algorithm is relatively weak, Iappreciate this approach and I am overall positive with this paper.<BRK>**Summary:**The contribution of the paper is threefold: First, it proposes a novel variation of AdderNet (Chen et al.2020) that ensures the network is always 1 Lipschitz with respect to $\ell_{\infty}$ norm. Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition. My main concern is that the experiments are not comprehensive enough.
Reject. rating score: 5. rating score: 6. rating score: 6. rating score: 6. <BRK>For example equation (7) is just pointed to another paper which I checked another paper for mins and did not find equation (7)  and did not try more as the cited paper is very very long. The paper does not seems differ such existing separation. The paper in the end contributed a non amortized inference method, but the discussion of the paper is very entangled. c) As the author pointed out, the proposed method is not scalable  as it is per data point and thus it will have limited application impact. e) there is only one set of experiments in total and on an traditional denositing task which is very limited.<BRK>This paper proposes an evolutionary optimization framework for training vartional autoencoders (VAEs) with discrete latents. In contrast to the standard VAE paradigm, the proposed TVAE approach does not require an encoder for amortized inference given the input. The paper is well written and easy to follow. The work proposes an interesting alternative for optimizing VAEs which does not require an encoder network for amortized inference. From reading the paper it is not clear how the size of the pool may need to be varied as the nature of the task or the number of latent changes. Why only denoising experiments?<BRK>**summary** the paper proposes a novel approach to training variational autoencoder models, based on non parametric form of truncated approximate posterior. Method is applied to denoising tasks for images. The authors for this reason choose to apply their approach to denoising applications, which I am not familiar with and can t evaluate the advantage of their method. It seems plausible that applying more evolutionary steps can take care of this but it would be useful to see experiments with varying number of  evolutionary steps. This can be made more efficient by adding temperature annealing.<BRK>The authors consider the setting of training a VAE with discrete, Bernoulli distributed latents and a continuous, Gaussian distributed output. Optimisation of these parameters amounts to a discrete search problem, which the authors tackle using an evolutionary algorithm. I think the motivation of the paper is well established, given the number of papers that have investigated the training of modern generative models with discrete latents. The proposed method does have limitations, which the authors acknowledge. However, these limitations I do not think are overwhelming. The main experimental result of the paper is denoising on a single image. One other thing I would say about this paper is that identifying the approach as a VAE almost feels inappropriate.
Reject. rating score: 4. rating score: 4. rating score: 5. <BRK>This paper proposes an invariant causal representation learning paradigm in the nonlinear setting. Based on a conditional factorial assumption, they proved identifiability up to a linear transform. The novelty of the paper seems to be in the generalization of the IRM framework to the nonlinear case which is interesting to me. In the iVAE paper, they assumed the latent variables to be conditionally factorial, while here the authors assume the potential causals (unobserved variables) are independent. The synthetic data experiment is not convincing at all.<BRK>This paper proposes a method for learning invariant (nonlinear) data representations and classifiers, using data from multiple domains. As it is formulated, Theorem 4 sounds like a theoretical identifiability result, and as such is not rigorously established by the proof given in the paper. In order to apply identifiable VAE, the method needs to assume that any two latent variables are independent conditional on the outcome variable and the domain index (Assumption 1). But what about latent variables that are direct causes of the outcome variable?<BRK>The paper proposes Invariant Causal Representation Learning, which seeks to learn representations for downstream tasks that are based on only causally invariant latent variables so the representation is robust to shifts in the test environment. Finally representations are learned from the observed variables to the causal latent variables of the target and then from these variables to the target.
Accept (Poster). rating score: 7. rating score: 7. rating score: 7. rating score: 6. <BRK>Hi,First I want to thank authors for putting this together, I enjoyed reading it. It seems like some notational clarity can help the paper, for example what is \Phi in the definition of \epsilon_T? In general I enjoyed reading the paper, and I believe the idea is novel, a good step toward multi task learning and well written.<BRK>I am a bit concerned that the gains we are seeing here have more to do with image representation learning than the structural assumptions employed by the algorithm. The result is multi task and meta RL algorithms that operate from images and in the meta RL case adapt to changes in the dynamics of the environment. It is not clear to me what new insights are gained from this extension. Recommendation:BorderlineI think the strong point of this paper would be the theoretical contribution of connecting bisimulation to HiP MDPs.<BRK>See detailed comments below. The theoretical results focus on assessing the value function errors for fixed abstractions/dynamics structure rather than on the errors in learning the abstractions/structure themselves (and learning these components seems to be one of the primary concerns here). Why is it natural to use a gradient based algorithm in this setting? Is there any particular theoretical or empirical evidence that justifies this term? ### UpdateI have increased my score to 7 after reading the authors  response and the updated paper. "Sample complexity of multi task reinforcement learning."<BRK>Overall, I believe this paper is well written and well organized. These results are applicable in multi taks learning settings. The authors derive the error bounds over between the Q value parametrized by the learned $\theta$ and that of the actual parameter $\theta^*$. The authors then apply these results to the transfer learning settings where one applies the learned parameter $\theta$ to a different but somewhat similar environment parametrized by $\theta_i$.
Reject. rating score: 4. rating score: 6. rating score: 7. rating score: 7. <BRK>SummaryThis paper aims to provide a unified principle for multi source transfer learning under label shifts. The proof seems correct via combining existing single domain DA theory and the theory regarding Wasserstein distance. The main theorem (Theorem 1) assumes that we can get the label information in the target domain, which is not realistic in many DA problem settings (e.g., UDA or multi source UDA in this paper). In many DA problem settings, we have to use pseudo labels to replace with true labels in the target domain, which should be analysed in the proposed theorem. Besides, there are some misleading conclusions in this paper. 2.The proposed algorithm has good results after using the label distribution ratio. If the quality of pseudo labels is low, can we still obtain good adaptation results in the target domain using the proposed method? If not, how does the quality of pseudo labels affects the performance of DA methods? However, it is still unknown if the Rademacher complexity (regarding deep networks) will converge to zero when we have infinite observations. 4.The motivation of this paper is unclear. 5.The proposed method can only be used to address three transfer scenarios (presented in this paper)?<BRK>This paper has made a good attempt to provide a unified approach for unsupervised domain adaptation. Another feature of the proposed approach is that it deals with target shift without assuming that conditional distributions are identical, a more realistic assumption for real world problems. Results show uniform improvements in the range of 2 6% over methods compared in various tables. The paper can be improved by providing comparisons with recent UDA and domain generalization methods from Balaji, Sankaranarayanan (CVPR 2018, NIPS 2018) and Balaji and Feizi (ICCV 2019). Given that one of the problems that is considered is UDA, I am not sure why the authors have not compared their approach on the Office dataset that is used in UDA papers. arXiv preprint arXiv:1904.06487] has considered the problem of small source. Since the authors consider the limited target labels problem as a one of the cases, comparisons with this paper should also be provided.<BRK>Summary:The paper is concerned with label shift in multi source transfer learning setup. In particular authors look into target shift without assuming conditional distributions to be the same in source and target. They propose a unified frameworkthat can be used for learning with no target labels and limited target labels. They show that the performance on target depends on how well we are able to estimate the ratio of label distrib between source and target, and the gap between real and estimated ratios,. 1) The structure of the paper is strange, it is impossible to just read the paper and get enough information on any of the subsections,  everything is in the appendix. Related work is missing (see appendix), the theorem with bounds (that is used through the paper) appears without at least a sketch/idea of how authors got to it and where wasserstein distance comes from. Since it is not intuitive, and since it is not clear how these bounds compare to DANN bounds (https://arxiv.org/pdf/1505.07818.pdf or Ben David bounds for that matter), it is really hard to believe in the algorithm authors derive, even though it seems to give good empirical improvementsAlso K section in appendix is essential for understanding and should be moved to the main paper. I think it is most common to assume p(y|x) is the same, not p(x|y) as you show. what exactly is "label partial unsupervised domain adaptation"<BRK>In this paper, the authors focus on the label shift problem in multi source transfer learning and derive new generic principles to control the target generalization risk. They propose a framework that unifies the principles of conditional feature alignment, label distribution ratio estimation, and domain relation weights estimation. A WADN algorithm is proposed for 3 multi source label shift transfer scenarios:  learning with limited target data, unsupervised DA, and label partial unsupervised DA. The proposed WADN algorithm is validated on different scenarios on common benchmark datasets (Digits, HomeOffice, Amazon Review), and results indicate that it can outperform related SOTA methods for these scenarios. Although the paper is well written, and all the key concepts are described in some detail, I found some parts of Section 2 difficult to follow. In terms of the organization, the authors present their review and analysis of the SOTA literature in Appendix A (no in the main paper). Therefore, it is not immediately clear in the main paper how their framework and algorithm are motivated by challenges in literature. The experimental section should be expanded to compare results on different backbone networks. Tables 2 7 shows the lower bound (source or source + tar), but does not show upper bound results, like when training a DL model on the source and target data that are labeled (in the UDA scenario). It seems like their code is not made available, so there is a concern that the results in this paper would be very difficult for a reader to reproduce.
Accept (Poster). rating score: 7. rating score: 7. rating score: 6. <BRK>*********Summary Of The Manuscript:*********This manuscript focuses on the problem to protect the users/humans from unauthorized facial recognition systems. Therefore the current rating of the paper will be 7 in reviewers  opinion because of fairly consistent work and practical usage. Novelty:There are mainly two points are novel which are presented very well and are as follows:++ The author designs a custom black box adversarial attack on facial recognition models where the proposed algorithm changes the representation of the features in such a way that it will preserve the image quality. I believe that the standalone contribution of the manuscript is to create such an attack to hide the user s identity.<BRK>The same for probe images. 1.Summarize what the paper claims to contribute. The paper claims to contribute the following three:(1) design a black box adversarial attack on facial recognition models. (2) interrogate the performance of the proposed method on commercial black box APIs, including Amazon Rekognition and Microsoft Azure Face, and against the existing data poisoning alternative, Fawkes. (3) release an easy to use webtool, LowKey. This helps with reproducibility. Digital user privacy is a hot topic to be further studied especially in the context of practical application purposes.<BRK>This paper presents a method/tool, i.e., LowKey, to protect user privacy which leverages adversarial attacks to pre process facial images against the black box facial recognition system in social media, yet the processed facial images remain visually acceptable. Overall, this is an interesting work addressing the user privacy protection against commercial facial recognition system.
Reject. rating score: 4. rating score: 4. rating score: 4. <BRK>#### **Assessment**The foundational concepts underlying this paper are strong and the authors make good points framing the potential advantage of using preference based rewards over what is termed a “handcrafted reward”. Finally, there are some major concerns about the suitability of the simulated environments to adequately evaluate the contributions of the proposed methods (more in the “Weaknesses” section below) in light of the applicability to real world dynamics. The paper is firmly grounded in relevant literature. It rather stands as a statement of what should be an objective of the paper, demonstrating how to build preference based agents. Section 3 mentions the development of a platform. Are the differences meaningful (ie.do they provide unique policies)? Within the healthcare space there has been some notable work done to address reward design for RL approaches. What are the relevant parameters of variation within the simulated dynamics? How are patient physiologies varied? It’s not clear from the formulation how the returns or observations from the environments are used to compute the rewards under this framing. Provided the lack of variation in the reported results over the different data sets (namely in Figures 2, 3, 4, etc), it appears that there’s no variation nor differences between the training and validation sets. How does performance change based on the “accuracy” of these estimators? Missing from the paper is a discussion about how these preferences would be obtained and integrated into the development of learning a policy.<BRK>The paper is generally well written and easy to follow. Despite the technical contributions listed above, I would suggest rejection due to the following concerns. The approach proposed is similar and the innovation seems marginal. I suggest the authors include a comparison of methodology and, if possible, experiments comparing their approach with state of the art preference based RL algorithms in the revision. 2.Lack of implementation detailsThere are a few details I didn t find reading the manuscript, for example, how to query human responses on line 12 of Algorithm 1, what is the guideline for human experts to rate different treatment plans, implementation details and hyper parameters used in the agent learning algorithm. The lack of such details makes it difficult for other researchers to reproduce the results shown in the paper.<BRK>What would be an intuitive explanation of L in this algorithm? The authors know the related literature, and a number of experiments are presented to show different aspects of the methods. The work is promising. This is a relatively comprehensive evaluation. I am not happy with the way the authors split their manuscript into the main paper and the appendix. The current version of the main paper is not self contained, and it does not provide minimum explanation of the methods. Writing is great, and the authors can write beautiful sentences, but the overall structure has some flaws. It is not entirely clear what is the new contribution in this paper. This important component of the algorithm is unclear to me in the paper. Could one derive a better handcrafted reward for these experiments? Sufficient evidence should be provided that the handcrafted rewards used in comparisons are not trivial. What are "hill" equations on p. 3?
Accept (Poster). rating score: 7. rating score: 6. rating score: 5. rating score: 4. <BRK>Summary:The authors propose a novel method for learning prototype representation for relations which abstracts the essential semantics of relations between entities in sentences. The learned prototypes are learned based on an objective with clear geometric interpretation and have been shown to be interpretable and robust to noisy from distantly supervised data. The results show that the proposed approach gives state of the art results for few shot, supervised relation extraction.<BRK>Summary:This paper proposed a prototype based relation learning method which infers a prototype vector for each relation type to help train a better text encoder. And authors introduced several loss functions including statement statement similarities and statement prototype similarities. Experimental results over both few shot and supervised scenarios indicate that these additional training losses improve the performance significantly. Furthermore, the author provided a dataset to indicate that the proposed method can effectively reduce the noise from distant supervision. Overall, the paper is interesting and the experiments are extensive and promising. #####################Main concern:Besides promising results shown in the paper, there are some concerns about the concept of "prototype" in this paper: 1. A brief comparison will be useful in the related work for completeness.<BRK>This paper presents a pre training method for encoders (i.e., BERT) of relation extraction, leveraging distant supervision data. The main idea is to introduce a prototype embedding for each relation in the distantly generated data. Some analysis are also conducted to demonstrate the effectiveness of the proposed model. However, I have several concerns regarding the motivation and experiments of the papers as follow. Also, how does the relations in the distantly generated data differ from those in the few shot and supervised experiments? The performance gap between the proposed model and the baseline MTB seems minor too. Also, Soares et al., 2019 seem to report a different performance on SemEval than those indicated in this paper (i.e., the best score in there is 89.5).<BRK>This paper proposes a method for learning representations for relation statements as well as classes (prototypes) for relation extraction tasks. I think the key insight is that the relation statements and classes (corresponding to relation types) can be learned jointly using contrastive training objectives. In the experiments, they first train this framework on distantly supervised data constructed from Wikidata + Wikipedia and then fine tune it on several downstream tasks: FewRel, SemEval, and a new dataset they created focused on identifying false positives in distantly supervised data. Overall, I think the proposed approach is quite reasonable and also seems to work well in the evaluation tasks (learning prototype embeddings instead of taking the average of instance embeddings and adopting contrastive losses between relation statements and prototypes). I think this point really needs to be clarified and can be a weakness of the approach, especially in few shot settings. My main concern is the pre training data used in this paper can be different from what has been used in (Soares et al, 2019   they didn’t use Wikidata and only consider Wikipedia and the links) and it makes the comparisons unfair.
Reject. rating score: 4. rating score: 4. rating score: 4. rating score: 4. <BRK>UPDATE: After reading through all other reviews and responses by the authors, I share the concern that the theoretical justification of the paper is lacking as the connection between the truncation error and the improved algorithm performance is not rigorously proven. Motivated by the proof, the authors then derive a new accelerated method with a faster rate of convergence than the original Nesterov s method, which is shown to be more stable than the original Nesterov s method when the step size is large. Strengths:  The paper proves the convergence rate of Nesterov s method to the ODE proposed by [Su et al, 2014].<BRK>summary:This paper proposes an accelerated method that has a high order truncation error $O(h^4)$ to the ordinary differential equation $\ddot{x} + \frac{3}{t}\dot{x} + f(x)   0$ obtained from Nesterov s accelerated method by (Su et al., 2014), while Nesterov s method has $O(h^3)$ error. This implies that the iterates of the proposed method converge to the trajectory of the differential equation faster than those of Nesterov s method. The reason why we care "large" step sizes seem insufficient, while Nesterov s method is stable for normal step sizes (e.g., $1/L$). Numerical experiments are limited.<BRK>Inspired by the ODE analysis of Nesterov accelerated gradient method by Su et.al., the authors propose a different discretization of the ODE by Su et al.The truncation order of this scheme is of a higher order, thus the authors claim that the proposed algorithm is more stable and, therefore,  will converge with larger steps. Probably, a significant experimental evidence will help here. Second, for a new scheme convergence of iterates $(x_n)$ to a solution and the convergence rate $F(x_n)   F(x_*)$ should be proven explicitly, they do not follow automatically. Based on this, I cannot recommend this paper.<BRK>Review:  This paper refines the the truncation error analysis for discretizing the ODE to obtain accelerated optimization method. The truncation results include higher order term. + Overall, the paper is clearly written. Two simple examples do not provide that much evidence here. Suggestions for improvements: It will significantly strengthen the paper if the authors can provide more theoretical justifications for the claim that their proposed method is faster and more stable.
Reject. rating score: 5. rating score: 5. rating score: 5. rating score: 6. <BRK>Both consistency and complementarity are well investigated. The difference lies in the format of input data and how to measure these properties. In view of this, the novelty of this paper is reduced. (3) This paper focuses on multi view learning. But the experiments only involve two views. (4) In experiments, only s^{1], s^{2} and the concatenated representation are used for clustering or classification.<BRK>The authors also provide a novel objective function to explicitly disentangle the multi view data into a shared part across different views and a (private) exclusive part within each view. Also there are other types of mutual information estimators that maximize similarities between two views. If not, how do you ensure that the mutual information maximization does not results in a degenerated solution. I think it is also worth to compare the proposed methods with the prior works (for example, Gonzalez Garcia et al.NeurIPS 2018). Based on my current understanding and the above comments, I currently recommend the paper as "marginally below acceptance threshold".<BRK>This work aims to achieve disentangled representations for multi view data. However, I have the following concerns on this work:1. the author presented the three conditions to be necessary conditions for disentanglement and argued them to be  strict conditions . In my opinion,  Information theory can be a candidate perspective to perform your proof, e.g.[1].2.The proposed model achieves disentanglement on s and e with the product of the expert. 3.The compared baseline methods are not up to date. It is also good to compare this method in the reconstruction visualization part.<BRK>The goal of this paper is to define multi view disentanglement in an unsupervised manner. The authors follow the above rules to design a VAE based model and demonstrate favorable results on image clustering and classification tasks. Clear definition of representation disentanglement: as the prior works do not clearly define all constraints, which are ought to be valid for representation disentanglement, the prior models might not able to disentangle the representations into disjoint representations. This paper clarifies the four conditions and formally define the problem with four mutual information terms. 2.As beta VAE is also designed for unsupervised disentanglement (although it is single view), how beta VAE performs in the tasks demonstrated in Table 1 4?
Accept (Poster). rating score: 7. rating score: 7. rating score: 7. rating score: 6. <BRK>In this paper, the authors apply the technique of  learning  the sparsity structure [1] in the context of Neural Recommender Systems. Overall, given a very practical solution to the exploding users and items situation in a RecSys setting, I recommend an accept for the paper. the operator on the entire matrix? A parameter compression technique can indeed prove to be very effective in such situations.<BRK>The paper proposes PEP (Plug in Embedding Pruning) to reduce the size of embedding table while incurring insignificant drop in accuracy. The related work is well summarized into Embedding Parameter Sharing and Embedding Size Selection methods and the motivation for the current approach is well explained. The paper draws inspiration from Lottery Ticket Hypothesis. Similar to LTH, the paper shows that the initiation strategy can make the training process faster and stable. The results show an impressive 97 99% parameter pruning via PEP. As for the computation cost, PEP results show an additional 20 30% time cost compare with base models. The results are well summarized and analyzed.<BRK>The paper is written very clearly with very strong experiments (multiple datasets/recsys models/recent baselines), showing ~99% parameter reduction while maintaining the same or even better performance. It s likely that I d change my score based on the author s reply, e.g.I d be happy to raise the score if the authors provide a clear explanation/intuition for the idea. Overall the paper is in a good shape now.<BRK>This paper proposed a novel approach to reduce size of the embedding table while not to drop in accuracy and computational optimization. 2.Experiments baseline with classical datasets and models show promising results on recommendation ranking problem.
Reject. rating score: 3. rating score: 4. rating score: 4. rating score: 5. <BRK>Straightforward application of the algorithm could violate the  effectiveness  and  efficiency  goal. The paper has not discussed how to tune them for a given task. (3) The same question can be asked about how the initial point of hyperparameters is chosen, and the paper does not discuss that. The CMA evolution strategy: A comparing review. Since the paper emphasizes the importance of HOZOG in the high dim scenario, some of these stronger baselines need to used.<BRK>EDIT: **post rebuttal. The fact that using trust regions could benefit HOZOG, or that HOZOG is a better strategy compared to TurBO and REMBO, should be demonstrated empirically. The authors seem to be aware of this as they provide larger versions of the figures in the appendix. ** The paper is generally well articulated and easy to follow. **Missing baselines. In that case, non GP, scalable models have been proposed (e.g., SMAC) or adaptations directly targeting high dimensional BO problems (e.g., TurBO).<BRK>EDIT: **post rebuttal (could not add a comment readable by authors). I agree with the other reviewers that some more baselines for high dimensional benchmarks are required, and was not convinced by your rebuttal to those requests. I also think the aspect of non convexity and local optima touched by Reviewer 4 warrants some discussion in the paper. Is BOHB really that simple compared to HOAG? just an idea. The description of the experiments is a bit lacking in certain regards.<BRK>The proposed method (called HOZOG) is based on zeroth order hyper gradients, and is empirically demonstrated on several benchmark tasks to compare favourably against state of the art hyperparameter optimisation approaches in terms of simplicity, scalability, flexibility, effectiveness and efficiency. This paper will likely be of great interest to some members of the ICLR community and might become an impactful contribution for machine learning researchers and practitioners alike. That being said, I see some issues with the generality of the method as well as the presentation of the paper. As a result, I overall recommend acceptance of this manuscript, although not too enthusiastically. ### Post rebuttalI thank the authors for their response, addressing some of the issues/questions I had raised.
Accept (Poster). rating score: 7. rating score: 6. rating score: 6. rating score: 5. <BRK>The paper deals with learning reusable hierarchical skills in sequential decisions. The authors introduce a partially amortized model based on four parametric functions (the representation module, observation module, latent forward dynamic, and task reward module) and an additive loss function composing these modules. The overall algorithm consists of learning the four modules jointly and to plan using MPC in the latent skill space. Overall the paper is original and sufficiently empirically motivated for acceptance.<BRK>The paper proposes combining model based RL with high level skill learning and composition through hierarchical RL, into a single reinforcement learning framework. More specifically, the proposed approach leverages planning and composing skills in the low dimensional, high level representation, and learn low level skills conditioned on the high level skills. A mutual information objective is used to learn low level policies conditioned on high level skills, and this was shown to improve sample efficiency as the low level policies do not learn to ignore the high level skills they are conditioned on. On the transfer experiments, what guarantees do we have that transfer will always be beneficial? And what guarantees do we have that it will always be more beneficial than using Dreamer?<BRK>Summary:This paper presents a model based RL approach that i) learns a high level plan generator that produces a sequence of latent continuous variables as skills for a task through CEM, and ii) a low level policy conditioned on the skills amortized policy training. The approach is shown to be more sample efficient than a recent model base RL approach (Dreamer), and is able to learn to solve a new (but related) in simple walking tasks relatively faster as well. The main objective of the approach seems to be improving the transferability of skills learned by RL as stated in the main text and in the title. Is it the high level skill plan? If so, a transfer setting where the low level policies are fixed and only the high level plans are updated / fine tuned would be far more convincing.<BRK>In theory, this would allow the planning method to operate in an abstract, high level space and delegate low level action selection to policies trained on simulated rollouts. If I understand the paper correctly, the result of the LSP learning algorithm is the skill policy and a (fixed?) Refer to section 3.2 for a definition? Section 3.2.: it would be good to relate the MI objective to previous methods. My rating is mainly influenced by the experimental section, as well as several minor issues with the overall presentation. This is a good result, but some relevant details on the tasks and plots are missing, which make the achievements hard to interpret (see below). My interpretation is that it means that you use a learned model to train a policy rather than using the model during evaluation.
Accept (Poster). rating score: 7. rating score: 7. rating score: 7. rating score: 6. <BRK>Summary:* This paper proposes perceptual adversarial robustness, an adversarial truing against the set of all imperceptible adversarial examples. Through a perceptual study, they approximate human perception with a neural neural   “neural perceptual distance”, and test the robustness against 5 threat models including L2 and Loo showing state of the art robustness even without training on them, showing generality of the model considered. I think the authors could have elaborated more on this. * The authors emphasize a lot on the NPTM, which is definitely relevant. Nevertheless, the PAT (Perceptual Adversarial Training) seems mostly unsuccessful against the PPGD and LPA (and Fast LPA) attacks, and this may * I am a bit confused by the overall narrative of the work.<BRK>This paper proposes a threat model called Neural Perceptual Threat Model (NPTM) and under NPTM, they develop novel perceptual adversarial attacks: perceptual Projected Gradient Descent (PPGD) and Lagrangian Perceptual Attack (LPA). Also, they propose Perceptual Adversarial Training (PAT) which achieves good robustness against various types of adversarial attacks and even could generalize well to unforeseen perturbation types. However, I have some concerns: 1. Could the authors explain why AlexNet is the best proxy for human judgments of perceptual distance? "Adversarial robustness against the union of multiple perturbation models."<BRK>This work proposes a new form of adversarial training, supported by two proposed adversarial attacks based off a perceptual distance. The choice of perceptual distance (LPIPS), is computed by comparing the activations of (possibly different) two neural networks with respect to a pair of inputs. This is in contrast to adversarial training with "narrow" threat models, which fail to be robust to at least one other threat model in the set. Overall I believe this to be a good paper. 3) I think its interesting that PAT * performs worse on PPGD and LPA, than on other threat models (that presumably the PAT * model has not seen). This could mean that PPGD and LPA adversarial examples are "harder" in some way. Do the authors have more explanation/intuition about this?<BRK>This paper studies the adversarial robustness of deep neural networks against multiple and unforeseen threat models. The writing of this paper is clear. Based on a perception similarity metric, new adversarial attacks and defenses are studied. Thus the paper is comprehensive. Although the authors have conducted human evaluations to prove that this metric correlates well with humans, I still doubt whether it can reflect all potential threats.
Reject. rating score: 4. rating score: 5. rating score: 6. rating score: 7. <BRK>This paper comes up with a novel scenario where the unlabled data are available as well as labeled data in the continual learning scenario. The novelty of DistillMatch is an incremental modification of previous work. It is reasonable to make use of it when this assumption is true. 3.It is true that this assumption should be utilized when available. With the given information, it is hard to tell whether the comparison is fair. GD depends on internet crawled data, is it replaced with the unlabeled data since it is available in the experiment setting? With the above said, I suggest the author to list clearly the objectives, replay buffer sizes or even pseudo code for each of the compared method and their own method in a table, which will help the reader identify what major component in the proposed method is making the contribution.<BRK>This paper investigates a semi supervised continual learning (SSCL) setting and proposes a new method called DistillMatch for this setting. And then, they develop the DistillMatch method combining knowledge distillation, pseudo labels, out of distribution detection, and consistency regularization. However, there are some downsides that should be considered before its publication. (1) In abstract the authors claim that they can significantly reduce the memory budget (of labeled training data) by leveraging unlabeled data (perhaps with large volume). (2) From a methodological viewpoint, the proposed DistillMatch method is just a combination of existing methods (listed as in above). So where is the novelty of this "new" method? In summary, I think this semi supervised continual learning setting is interesting, but the proposed DistillMatch method can not persuade me that this method is a novel significant contribution to this problem. So at present time I believe there is much room for the authors to improve their method before publication.<BRK>  Summary:This paper proposes class incremental learning with unlabeled data correlated to labeled data, and a method to tackle it. R1 and R4 seem to have a similar concern. The proposed method is inspired by state of the art class incremental learning, semi supervised learning, and out of distribution (OoD) detection methods: local distillation [Li and Hoiem], OoD detection [Hsu et al.], consistency regularization and pseudo labeling (or hard distillation) [Sohn et al.], and loss balancing based on class statistics [Lee et al.]. (2) the scale of experiment is too small. I am okay with the lack of novelty on the proposed method. Extending continual learning to the semi supervised setting is natural, given that the extension to self taught learning has already been considered in [Lee et al.]. I believe both semi supervised and self taught learning are realistic in some cases. I also recommend to provide real world scenarios that the proposed task (correlation between labeled and unlabeled data exists and no memory for coreset is available) is useful in practice. 2.The proposed method is not novel, which is essentially the combination of state of the art methods in relevant tasks. But I do not discount this much, because this work would be valuable as the proposed task is interesting but not investigated before. **After rebuttal**I d like to thank authors for their efforts to address my concerns. It would also be interesting to see the performance of the proposed method in the setting.<BRK>The paper presents a novel semi supervised continual learning (SSCL) setting, where labeled data is scarce and unlabeled data is plentiful. The proposed framework is built on pseudo labeling, consistency regularization, Out of Distribution (OoD) detection,and knowledge distillation in order to reduce the catastrophic forgetting in the proposed setting. Positive aspects:    the definition of a realistic, semi supervised setting for continual learning  a novel approach for continual learning in order to cope with  catastrophic forgetting   the proposed approach is memory efficient, since it does not need exemplars to replay past tasksNegative aspects:  the OoD implemented in this paper rejects the unknown samples. I mean, do you make a study of error propagation of pseudo labeled data? 6.Would be interesting to test your approach in a real world scenario, i.e.robot navigation.
Reject. rating score: 5. rating score: 6. rating score: 6. rating score: 6. <BRK>These involve a noise generator, a loss to enforce consistency, as well as a stochastic variant of adversarial training. It is not clear to me what ball this is since the authors are considering multiple perturbations. I assume it is at least one of the perturbations being considered, or is that incorrect? 2) Similarly, in the algorithm, the authors generate adversarial examples (x_adv) by sampling a random attack. I could not find what set of attacks were being sampled from, or what the sampling distribution is (I checked the appendix as well). However, the adversarial examples are generated from some unspecified set of attacks, which implies that the set of attacks actually depends on the generator somehow. Is this supposed to be the classifier loss on the augmented samples? 4) The consistency loss involves clean, adversarial, and augmented posterior distributions. Why would we want to do this over random sampling?<BRK>This paper addresses a timely issue in adversarial robustness   efficient training of robust models against multiple adversarial perturbations. Overall, this paper provides very detailed evaluations involving multiple datasets, attacks, baselines, and robustness metrics. I find the results convincing and important, and also find sufficient novelty in the proposed training method. The strengths (S) and weaknesses (W) of this submission are summarized below. W1.The adversarial consistency (AC) loss is never defined explicitly. Based on the ablation study,  it seems that the role of SAT and MNG is to reduce overfitting in robustness to encourage generalization, rather than optimization over the worst case scenarios. Although the authors showed improved robustness over unforeseen attacks, the authors should also discuss how the proposed method can generalize to different attacks beyond Lp norms.<BRK>In this paper, the authors propose a novel meta learning framework that explicitly learns to generate noise to improve model robustness (against multiple types of attacks). Overall, the paper is well written. The experiments could be expanded. 1) There is a significant amount of work about using generative models to build adversarial examples. Since there is multiple threat models, I am assuming that it is selected at random between l_1, l_2 and l_inf (like SAT). However, I don t see any study on this in the paper. Also, it is not clear which value was used for the experiments. E.g., RST_inf should reach about 59% robust accuracy with 200 epochs of training (with 30 epochs it only reaches 55%). I d assume that if the model was only trained against l_2, then there might be an optimal value for beta that is different that the one from Fig.2.In general, it would be interesting to see on MNG AC does if different subsets of threats are used. 9) In Table 5, MNG AC achieves 35.1% against all l_inf attacks, but only 33.7% against AutoAttack. Details:A) It would helpful to the reader to have the epsilon values written on top of the different tables.<BRK>Their method combines adversarial training with an adversarial noise generator. They additionally improve robustness by regularizing model features between the standard image, the adversarially perturbed image and a perturbation of the image created with an adversarial noise generator. + The authors attack their models with  a range of attacks that to the best of my knowledge are state of the art. Especially why the MNG was trained the way it is was a bit unclear for me. 4.RecommendationI think this paper is an accept but as I don t work with adversarial examples I am not at all confident in that assessment. So I am definitely leaning towards accept but the opinion of a real expert would be highly appreciated as I feel not at all qualified to assess the validity of papers on adversarial examples.
Accept (Poster). rating score: 7. rating score: 6. rating score: 5. rating score: 5. <BRK>In this paper, the authors show that transformer protein language models can learn protein contacts from the unsupervised language modelling objectives. However, I have a number of concerns. The main contribution of the paper is that attention maps built in Transformer based protein languages learn protein contacts and can be used for protein contact prediction. However, from the plot in Figure 10a, it is not totally clear that the probabilities are well calibrated. In my opinion, this is not enough to reproduce the results in this paper.<BRK>Furthermore, I have some questions regarding the methodology employed by the authors. **Major comments*** The main metric employed by the authors is the precision of the top L (protein length) contact prediction for a given range (P@L). I wonder why the authors do not also consider recall at L as an accompanying metric for reporting the results. Why is that the case? This creates some information leakage. It s unclear from the results presented in the paper whether it is an issue or not   how does contact prediction precision / recall change as sequence similarity to the ESM training set drops? However no results on this are presented. I would be very eager to see the comparison of 3D structure accuracy inferred with ESM predicted and Gremlin predicted contacts.<BRK>In this manuscript, the authors present a method for predicting residue residue contacts within protein structures using the attention layers learned by transformer language models. The paper is clearly written and easy to follow. 4.How are sequence depths in Figure 3 calculated? This paper is also very application specific and may not present new machine learning methods of general interest to the ICLR community. The existence of previous language model based contact prediction methods reduces the novelty of this work, especially given that the model used here is from Rives et al.2019, who already look at contact prediction. Additional specific comments follow below. 4.What should interest the general machine learning community about this paper? Reporting results on the CASP data would help to make this comparison.<BRK>## SummaryThe paper shows that Transformers trained unsupervised on millions of protein sequences learn information about protein contacts by using attention maps for contact prediction. The paper is mostly clearly written and discusses server interesting ablation experiments. However, two recent papers that appeared on arXiv before the ICLR submission deadlines also use Transformers for protein contact prediction. Both publications appeared on arXiv at least one month before the ICLR submission deadline and are not clearly discussed in the paper. 2.The introductions discusses existing work on Transformers for protein languages models. This is not new (see Rives 2020 and Vig 2020) and does not fit well to the rest of the paper, which is about contact prediction.
Reject. rating score: 4. rating score: 5. rating score: 6. <BRK>Summary:  This paper presents a randomized second order smoothing certificate for providing robustness guarantees against adversarial attacks. By additionally using the gradient estimation of smoothed classifier, the proposed method has been shown to outperform the existing randomized smoothing certificate in practice. The addressed topic of randomized smoothing certificate is of significant importance and interest to the society of adversarial learning. S2.The certificate radius of the proposed method is novel as far as the reviewer knows about. Based on the current bound in Equation 2, it is hard to evaluate the theoretical gain of the proposed method in robustness. W3.The paper is poorly organized and presented.<BRK>2.The proposed second order smoothing is novel. This paper aims to improve randomized smoothing by incorporating the gradient information of the smoothed classifier, which is novel. Cons:1.The evaluation metric is not standard. Standard randomized smoothing and the follow up work use certified accuracy as the metric to evaluate the certified robustness. It s better for the authors to also use certified accuracy as the metric for a fair comparison. What s the impact of the parameters on the results?<BRK>The authors then show estimation of the gradient norm is dependent on the dimensionality of the data, and provide alternative methods that do not have a strict dependency. I  am confused about the positioning of this work. 2.I found it difficult to interpret the experimental results. It would be useful to plot  these results in terms of certified accuracy vs epsilon, as has become standard in this line of work. "Certified adversarial robustness via randomized smoothing."
Reject. rating score: 3. rating score: 5. rating score: 6. rating score: 7. <BRK>I recommend datasets such as FEMNIST [5] for studying personalization. [1] provides a Mapper optimization algorithm in Figure 4. Generalization analysis in [1] is for the mixing parameter learned from data (i.e.adaptive), that is why there is no dependency on it. I agree that the convergence analysis is new, but [1] also has two more algorithms. Regarding the number of parameters, based on eq.(1), personalized model is a convex combination of **two** models. This paper considers the problem of personalization in federated learning. The proposed method is equivalent to the Mapper algorithm from [1]. The empirical study is not conducted carefully:  Allowing for a separate local model for personalization increases the number of parameters which is not accounted for  Reported performance of the global model trained with FedAvg on MNIST and CIFAR10 is overly pessimistic.<BRK>This paper studies the personalization aspect of the federated learning problem. The authors propose a new framework in which they replace the common global model in the original federated learning formulation with a convex combination of the global model and a local model. They later introduce an adaptive optimization algorithm for their formulation and provide generalization bounds as well as convergence guarantees for both strongly convex and nonconvex settings. The paper has a complete story. It is completely possible that I am missing something here. But I would appreciate it if authors provide a clarification on this.<BRK>In this paper, the authors propose a variant of FedAvg that not only produce the global training, but also a mixture of the local model and the global model, which is called personalized model. In overall, I think the paper is technically sound, making reasonable contributions. In the exerpiments in Figure 2, it is still unclear to me how the "local validation" dataset is sampled, even after I read the appendix. The most important thing is, on each worker, does the validation dataset shares the same subsets of labels as the local training data? My concern is that, if the local validation data is close to the local training data, and is not sampling from the global distribution, then this criterion is too beneficial to APFL, and not fair for the global models.<BRK>I do spend lots of time and carefully read the paper. All comments listed below intend to help authors improve the quality of the manuscript. I hope comments are helpful and even the critiques are not discouraging your endeavor in the following. First of all, the manuscript proposed an adaptive personalized federated learning algorithm, where each client will train their local models while contributing to the global model. The convergence analysis and generalization bound for the algorithm are conducted. The optimal mixing parameter is the theoretical one.
Reject. rating score: 5. rating score: 5. rating score: 7. rating score: 7. <BRK>The paper proposes a number of improvements to the standard behavior regularized offline RL paradigm introduced in BRAC (Wu 2019). The paper combines these improvements to propose BRAC+ and presents empirical demonstrations showing favorable performance. Strengths:  Given the many improvements over BRAC suggested, the paper is organized well, and generally easy to follow. However, the "Initialization" section suggests the behavior policy is actually a Gaussian. ways to do this? Using the reasoning of the paper, shouldn t this limit be larger for states which appear more in the dataset? Moreover, the underlying problem here appears to be an issue with the form of KL regularization. Perhaps a solution is to use a different regularizer? The experimental results are favorable for BRAC+, but they are not especially compelling. Moreover, experimentally, if the claim is that BRAC+ is better on multi modal datasets, it would be nice to show performance on other (more multi modal) datasets in D4RL.<BRK>In the original paper, this is implemented by fitting behavior policy with a density model and then using this density model to estimate a divergence between these distribution. In overall, this paper introduces a simple and well motivated set of enhancements. The paper is well written and easy to follow. The approach is evaluated on a subset of tasks from a widely used benchmark for offline RL, D4RL. The approach is competitive with the state of the art methods and  outperforms the original BRAC and CQL, a recent state of the art approach, on several tasks. How sensitive is the approach is the choice of a number of mixture components? 3) table 3 raises some concerns regarding generality of the approach since some of the crucial hyper parameters are task specific;4) each ablation in figure 1 is performed on a single task that raises concerns regarding generality of the conclusions. However, if the aforementioned concerns are properly addressed, I will reconsider my rating.<BRK>**Accept / Reject**: I would lean slightly toward acceptance for this paper. The authors present several techniques for improving offline RL algorithms within the BRAC framework. For the most part, these improvements are also technically correct, or at least fairly well motivated. Likewise, the experiments could be improved to provide a more direct comparison and evaluation of these techniques (see above). However, in practice, many RL algorithms are prone to small, important implementation details. **Weak Points**: As mentioned above, this paper is not substantially novel; it proposes a few practical improvements for an existing framework/algorithm in an existing area (offline RL). This should probably be discussed. It should also be noted that 3/4 of the ablation experiments are on the hopper environment as well. Experiments:  “regularizor” —> “regularizer”  Unclear what is interesting/new in the performance vs. KL threshold section. As such, the issues with previous works and the improvements from these techniques should be clearly described and demonstrated. The authors appear to have implemented the basic BRAC framework along with their proposed improvements. The results for other methods in Table 1 are from Fu et al., 2020 and other recent papers.<BRK>This paper proposes two major improvements over BRAC to address the key challenge of offline RL: out of distribution action sampling. The second improvement is to use a state dependent weight for divergence regularization, which is learned automatically via dual gradient descent. The paper is clearly written. Although the method is improvements of an existing approach (BRAC), the improvements are non trivial, novel, and show promising results. I have three minor questions and suggestions:1) Intuitively, I think that KL divergence between policies might not be the best measure to prevent OOD action samples. However, it is these single modal datasets that improvements are more desired. To improve the paper quality, I suggest that more analysis and discussions about why BRAC++ did not work well on single modal datasets are included. If page limit is a concern, it is OK to add additional figures in the supplementary material. Thus, I recommend accepting this paper.
Reject. rating score: 5. rating score: 5. rating score: 6. rating score: 6. <BRK>While authors wrote quite detailed response, I did not find it convincing enough. > Reasons for score:I vote for rejecting this paper. Perhaps the threat model can limit the attacker to all possible perturbations under the certain volume budget, but that would be quite different from the idea in this paper. Given that algorithm in 3.3 is perhaps the biggest contribution of this work, I would have expected there to be at least some baseline to compare with.<BRK>The budget generator is trained jointly with the certified defense to change the shape of the perturbation set while maintaining the volume to improve certified error. On motivation: the paper could use some more justification or motivation for why we would want to change our perturbation radius during training to maximize certified performance. On related work and comparisons thereof: The authors seem to be unaware of the ICML 2019 publication "On Certifying Non Uniform Bounds against Adversarial Attacks" by Liu et al., which has studied the problem of certifying non uniform bounds which maximize the volume (exactly the same type of bound studied in this work).<BRK>This paper proposes to change the perturbation budget for adversarial attacks to a non uniform setting where differet input pixels have different perturbation budgets. The idea that not all parts of the input should be treated equally makes sense and is well motivated.<BRK>This paper address the problem of training robust neural networks with non uniform perturbation budgets on different input pixels. In practice, a perturbation budget generator is introduced to generate the context aware perturbation budget (i.e.conditioned on the input) for each pixel of the input image. Extensive experiments on MNIST and CIFAR10 demonstrate the proposed outperform SOTA method under various uniform perturbation budgets. Are the final results sensitive to these hyper parameters? How about the performance by using IBP?
Reject. rating score: 3. rating score: 6. rating score: 6. rating score: 6. <BRK>The authors describe a QD RL algorithm to solve continuous control problems with neural controllers. Furthermore, the authors state that QD RL selects agents from a Pareto front or from a Map Elites grid. This paper is weak in estimating its performance in a clear way. Descriptions of the methods and technical details of the proposed study are incomplete. Overall, the contribution of the paper is not significant.<BRK>Reasons for score:The Ant Maze results are promising and the paper is clear and well written. I ll expand on these points in the following paragraphs. I hope the authors will have such comparisons for the rebuttal phase, or if these comparisons are not suitable, I d be interested to hear out their arguments. Other comments:  "Besides, evolutionary methods are very valuable when nothing is known about the function to optimize." Please also report the standard deviation for steps UPDATE: The authors did a very good job at answering my questions and the new experimental results are very much welcomed, hence I m updating my score from 4 to 6. Given that this is a highly empirical paper with relatively little novelty in the key idea, more comparisons would be necessary to justify increasing the score further. While I sympathize with the lack of computational resources and access to implementations, taking some extra time to implement and run those comparisons can be done.<BRK>Without such a wider comparison that would confirm the main claim of the paper it is hard for me to recommend this paper for acceptance as the experimental evidence for the main claim is not sufficient in my opinion. The presentation of results is a bit hard to follow... it s a bit hard to put finger on what is exactly the issue. If this is something that generally done in QD literature an explanation why this is the way would do good. The use of RL is alluded to in the second paragraph of section 3, but, taking into account that combination of RL and QD is the core claim of this paper this combination in other works deserves better description and not just a mention. I sympathize with the lack of computational resources, which makes it very hard to compete, but if a 1 to 1 comparison with competitors if at all feasible, it would be worth it. Since experimental evidence was my main concern and now there is more of it, I am upping my score from 4 to 6   "Marginally above acceptance threshold". How did this difference occur?<BRK>Overall, I agree with the authors that combining QD with pg operators is novel   however, I am still not fully convinced that it is significant enough for a full paper at ICLR. This remains my primary reservation that prevented a higher score for the paper from my end. The paper makes good use of visualizations and flowcharts to communicate the ideas clearly and succinctly. Further, these are some key advantages of a gradient free EA that the method is being compared to. Without comparisons to a more representative sample of methods outside this family, it is difficult to quantify the significance of the results presented in the paper. Overall, the paper presents an interesting method. ########################### Post Rebuttal ##################I have read the other reviews and the author s responses. I thank the authors for conducting the additional experiments and integrating the feedback from the reviews.
Reject. rating score: 3. rating score: 4. rating score: 6. <BRK>The authors formulate the problem into the feature robust optimal transport (FROT) problem. The authors propose two solving algorithms, one based on the Frank Wolfe method, and one based on linear programming. *Cons*:Note that the first point is the main contributing factor for my rating. I checked the proof of proposition 4. However, this is also not obviously true for me. With its similarity to group lasso, FROT might have more interesting applications. 3.Presentation of the introduction can be improved. Also, it would be helpful if the author can list their contributions in a more organized way. While  high dimensional  appears in the abstract, introduction, and conclusion section, I didn t find the correspondence in the main text.<BRK>The proposed framework FROT   feature robust optimal transport   seeks to select feature groups to both speed up OT computation for high dimensional data and make it more robust to noise. The exposition is generally clear. My main concerns are limited novelty and lack of extensive experiments. The paper draws the contrast between prior work SRW that yields a discriminative subspace via dimensionality reduction, and offers a dual perspective to use feature selection instead. Traditional entropy regularized OT regularizes using the entropy of the transport plan $\Pi$, whereas FROT regularizes using the probability distribution $\alpha$. Currently, the optimization for the group selection is done independently of optimization that produces the features, for instance by the choice of a pretrained network in the semantic correspondence application. There should be more extensive experiments applying FROT to more tasks and compared with additional baselines. Similarly for the semantic correspondence results in Table 1.<BRK>The motivation is similar to that for feature selection: where perhaps only few of these groups of features are critical/sufficient for OT purposes. So it can also be understood as joint feature group selection with OT. The resulting convex problem is proposed to be solved using FW, whose details are presented (including convergence). Given SRW and other robust/min max OT works, and multitude of feature selection/group lasso works, the novelty seems restricted. Even in terms of optimization, it seems a straight forward application of FW. If so, perhaps a case of FROT which uses exactly same input as SRW must be included for a fair comparison (along with the FROT with all layers). It is will nice to clarify this. why not fix or validate ? (more like fig5 in appendix)
Reject. rating score: 5. rating score: 5. rating score: 6. rating score: 6. <BRK>Summary: The paper  presents a data augmentation framework for zero shot cross lingual transfer learning. .Given that, it would be useful to see more discussion/experiments comparing the two types of augmentation strategies especially that the self training step can leverage augmented data without labelsExperiments:The paper provide a lot of interesting ablation and analysis. However one of the key questions that I couldn t get an answer to is what is the value of each source of data and could they be used in any different way. On a related note, it looks like the specific order in which the datasets are used is important as shown in the experiments but it is not very clear what is the intuition behind that choice and whether other choices were considered or tried. Edited after authors responses:I would like to thank the authors for the detailed response and the changes they have made to the paper. Regarding contextual data augmentation (Kobayshi et al., Wu et al., etc.): Thanks for pointing out that these method use the labeled data to finetune the LM to make sure that words are replaced with other "label compatible" words. Note that the comparison is not intended to necessarily show that the proposed method outperforms these baselines. Also , it is not clear that  these methods would require labeled data in the target language for the finetuning or not. For example, can the source labeled data be used for the finetuning step? However, there has been several methods to address this in the literature by using unsupervised word alignment  (e.g., Yarowsky et al., 2001; Ni et al., 2017) or attention weights from NMT models (Schuster et al., 2019), heuristic approaches (e.g., Ehrmann et al., 2011) or co learning alignment and tagging (Xu et al., 2020). A detailed comparison and discussion of the trade off between the performance of each method and the resources they require would make the paper much stronger<BRK>This paper proposed a new data augmentation framework for low resourse (and zero resource) cross lingual task adaptation by combining several methods (entropy regluarized training, self training). The authors conducted extensive experiments on three cross lingual tasks, demonstrating the effectiveness of XLA. In addition, the authors compared different choices in the XLA distilation stage and claimed that the gain from XLA is beyond the model ensemble effect. According to the experiment results, the XLA framework has a remarkable gain over previous methods. However, it s not clear which component actually contributed to this gain. For instance, the author use "model distillation" to define the label selection procedure, yet, this term is another technique in machine learning literature. Lastly from a methodology perspective, I think the algorithm seems like a combination of existing methods and its novelty is incremental to me. I hope the authors can edit the paper more carefully with a clarification on their novelty.<BRK>The authors present an unsupervised data augmentation framework for cross lingual NLP. I still feel like the XLA framework is quite involved and it would have been good to understand which components of this framework are crucial to its success which is why I do not increase my score. Response to author s replies:I am impressed by the detailed response and the changes they have made to their paper and I am happy for this paper to be accepted. The key contribution in this paper is how they get reliable labels by simultaneously co training three student models and using them to filter examples for training each other to avoid confirmation bias. Their method, called XLA, combines self learning with co learning and filtering. I also think that the explanation of the architecture is not as clear as it could be. Each of these explanations are presented separately, and neither the algorithm not the figure are fully explained. I think the algorithm should go in the appendix and be fully explained. Even with these limitations, I think this paper makes enough of a contribution to warrant being accepted. Why not report the supervised results ie. Like the Conneau et al 2020 paper? I know the main point is the zero shot performance, but it is an interesting data point anyway especially as you have already included the baseline results for this. It would be a nice extra contribution to the paper if features such as sentence length were indeed included.<BRK>Overall, the paper is clearly written. The proposed method is intuitive to understand and is novel in crafting augmentation texts using masked language model of BERT and relabeling these vicinal examples. Experiments on cross lingual NER, NLI and paraphrase detection tasks demonstrates the effectiveness of XLA, outperforming  XLM R methods by a large margin and setting up new baselines for future cross lingual models. However, as the paper focuses on data augmentation in NLP, it should discuss about data augmentation methods in mono lingual (e.g.English) and cross lingual NLP (if there exists), pointing out their advantages/disadvantages under the paper s setting. As the paper crafts examples with and without vicinity separately for the target language, which part contributes more to the improvement of XLA over XLM R? Pros:  The augmentation algorithm is clear and results are promising. Cons:  Lacking comparisons and discussions with other data augmentation methods in NLP.
Reject. rating score: 6. rating score: 6. rating score: 6. rating score: 7. <BRK>While the intuition behind the objective can sound plausible from the discussion that follows, I am still curious what problem formulation is this objective solving? Since the paper is advocating the use of NPM objectives, is it possible to plot the value of the proposed NPM objective in the Transformers as a function of the Top L precision? This will strengthen the claim in proving all the gains are actually coming from the objective and not other possible factors in training the model. ** after rebuttal: thanks for addressing the comments. I have revised my score based on the discussion.<BRK>I would be very concerned if this is also what is done for the Potts models as it would significantly harm their performance. The rest of the paper is more clear. The model is trained with a pseudolikelihood loss across all sequences within individual families. This is a new and clever idea. Please provide an explanation. This seems to be a significant drawback of the method. The main drawback is that the authors exclusively evaluate the model on contact prediction and do not demonstrate convincing performance.<BRK>Could this method be applied to generate Potts Model parameters to be fed into other structure prediction models (e.g.AlphaFold). This is a well motivated problem. 5.References for important proteins with shallow MSAs. In particular, this paper focuses on using MSAs for contact prediction as a down stream task. Is it random subsampling? The author response clarified some key questions and the updated paper incorporated some of the feedback. Personally, I think this approach is interesting, but the paper needs further experimental validation for acceptance. This claim is only partially supported for very shallow MSAs. I would like to see the authors further examine this and show that this is not the case.<BRK>Overall comments  The paper introduces an amortized optimization framework to learn a mapping between an input sequence and the coefficients of the corresponding Potts model   a standard method used in computational biology to model protein sequences. I suspect the 1000 Potts models are still cheaper to train, but if not that could be one strength of your approach to mention. I would have expected that increasingly larger Meff would be associated with monotonically increasing performance (as is the case for the Independent Potts model curves in blue). Appendix C3  Did you look at the performance gap on HTH?
Accept (Poster). rating score: 7. rating score: 6. rating score: 6. rating score: 4. <BRK>This paper shows that knowledge graph augmented question answering and recommendation system models are so graph structure/content change invariant that by using a simple heuristic or more sophisticated reinforcement learning based approach, we can change the knowledge graphs without significant change in the model performance. The paper is also very well written. Weak points:  I suggest the authors make their arguments about the invalidity of model explanations more clear by providing some explanation samples.<BRK>The systems are recommender and QA systems which use an underlying knowledge graph (KG) such as ConceptNet. Previous work has suggested that the KGs are important for good performance, and moreover that the use of KGs lends the system a degree of interpretability. Overall I have a lot of sympathy for the motivation of this paper. The other problem is that the presentation in the paper is poor. Part of that is down to the non native English in parts (which is not the fault of the authors), but part of it is also just down to sloppiness in the presentation. This evaluation would be morepersuasive with a larger sample (even 20 or 30).<BRK>The paper presents an interesting finding that some of the existing KG augmented models, such as those for QA and item recommendation, may not actually capture or leverage the semantics in KGs, and their performance improvement cannot be attributed to the usage of additional knowledge. I think this finding is of some significance. Cons:1) In my opinion, perturbing KGs by randomly or heuristically changing existing edges is not well motivated. Why?I think this is an interesting and noteworthy finding, but no in depth analysis is given. after rebuttal  Thank the authors for their response which addressed my concerns.<BRK>I am not convinced by the motivation of this paper. This is under the assumption that similar performance in predicting the correct answer (e.g.accuracy) will lead to similar quality of explanation. There s also a discrepancy between the authors  motivation and their evaluation.
Reject. rating score: 5. rating score: 5. rating score: 6. rating score: 8. <BRK>The paper does not contain a theory part, but wherever possible, equations are provided to illustrate how the method works. Concerns: This paper includes a detailed empirical evaluation of the proposed BNN training scheme. The major concern is that the proposed low cost BNN training scheme can cause a nontrivial accuracy degradation (2.25%) as shown in Table 5. Besides, the baseline networks (BinaryNet and ResNet) used for comparison are out of date. The authors can mainly improve the paper s strength by prototyping the proposed BNN training scheme on an embedded CPU and measuring real world performance and power.<BRK>This paper proposes a novel method to make the training Binary Neural Networks with low memory and low energy by modifying the backpropagation and forward process. The paper targeted one of the problems in Binary Neural Networks and provided experiments as well as source codes to the proof of efficiency. In figure 2, the legends should be put in the figure (not in the caption). It is better to follow. In conclusion, the paper addresses the novel idea for the training improvement of Binary Neural Networks in low memory and low energy. However, there are many concerns aforementioned.<BRK>The experimental results show some improvement memory footprint reductions and energy savings vs standard approach. The experiment results seem that the proposed method achieves good performance in memory footprint reductions and energy savings. Comments:(1)	There is a difference in the memory consumption in Table1 and in section 5.1 (1.67 MiB or 1.41 MiB ?). The authors may check this. (2)	As for the perspective of overall design, it’s better to emphasize the trade offs between the importance of different variables to the overall training and the choose of the data type.<BRK>The foundations for the method are presented in great detail in a formalized manner and provides sufficient elements (i.e.experiments) to assess the validity of the proposed approach. Although there are certain aspects that could be improved, such as including a table outlining in a clearer manner the contributions of the authors in this context. However, I would like to know if the authors have made an ablation study to assess whether or not the use of batch normalization would have an effect on the accuracy of the proposed models.
Accept (Poster). rating score: 7. rating score: 7. rating score: 6. rating score: 5. <BRK>The method works by modelling the training dynamics and adapting the learning rate to optimize performance on the validation set. **Overall, I recommend this paper for acceptance. ** I have some minor concerns which are detailed below, but generally the clarity was excellent throughout and the approach seems novel. It seems that there are various "hidden" hyperparameters in addition to the ones stated. "*    The problem in a,c can be solved in closed form but how is the step size for descent in b chosen? Is this a tuned hyper parameter? It  would be nice to see at least standard error bars so we know the variance in performance of the proposed method, and how it compares to the baselines. I think an interesting addition to the empirical results would be something like "computation to performance" for each of the methods. (To be clear, *I am not requesting such a result*)<BRK>Summary:This paper uses Bayesian optimization (BO) to dynamically tune the learning rate during the course of training of DNNs. Strong points:  I think this is a very interesting and practical application of Bayesian optimization (BO). The way BO is used for learning rate selection in every stage is very clever, especially the trick of reverting back to the checkpoint model parameter before the evaluation of every LR, which resolves the non stationarity issue due to evolving model parameters. I wonder whether the proposed algorithm still has an advantage after taking into account the additional computational cost due to BO. In the experiments, the number of BO iterations is selected as k 10. Both [1] and [2] also use an exponentially decaying kernel to model the evolution of the learning curve, and hence early stop some evaluations of hyperparameters (learning rate).<BRK>The auto method presented in the paper is based on Bayesian optimization, but with an modification to make it less expensive. The idea proposed in new and the experimental results demonstrate its effectiveness. Using a forecast model to reduce running time is an interesting idea, thought the forecast model is an exponential time series forecasting model. 2.The paper demonstrates the effectiveness of this method by pretty solid experiments. The method itself introduced a few hyperparameters including k, $\tau$, etc. The paper focuses on learning rate. In practice, there are many other hyperparameters that have impact on performance.<BRK>Experiments are very large scale and show that particular instantiations of the proposed algorithm achieve noticeable speed ups and can also improve final performances in some cases. Weaknesses:  The method has several hyperparameters, which potentially limits its significance and applicability. Comparisons with other methods and experimental protocol could be improved. Besides, there are also several choices related to the BO part of the method that feel somewhat arbitrary to me (choice of kernel function for the GP/acquisition function). The paper reads overall fairly well, but I think it could have been organized better. There is no ablation of the BO part of the method. I wonder if the BO part of the method adds that much. I have doubts about the comparisons with other methods. These points can be clarified in the rebuttal phase. Therefore it is not clear to me concerning what they consider the various quantities as `"optimal".
Reject. rating score: 4. rating score: 6. rating score: 6. rating score: 8. <BRK>Overview: The authors examine deep learning from the perspective of kernel methods and demonstrate that convolution layers in these architectures can make DNNs a form of composite kernel learning. Hence the paper tries to address some important and relevant problems in the field, however, I m not fully convinced as to whether their procedure is  any more interpretable than existing methods or extracts features optimally. Quality and Clarity: While the work provides sufficient details to understand prior work and the method itself, the key contribution section of the paper needs more work. Novelty: There are many works that focus on understanding neural networks and learning features for downstream prediction from the perspective of kernels. The novelty in this work is limited to allowing gating functions to adapt during training, such that the learnt gates can perform better than random gates.<BRK>This paper builds on recent work characterising deep neural networks in terms of Neural Tangent Kernels and Neural Path Features. A recent paper (Lakshminarayanan and Singh, NeurIPS 2020) provided a new perspective on Neural Tangent Kernels for Gated Neural Networks, by decomposing the network into independent paths. While this work seems to be an interesting and promising line of research, I think it is fair to say that we will need to wait to see if it really does provide a paradigm shift in how we understand deep learning. The recent work of Lakshminarayanan and Singh (NeurIPS 2020) seems to add to this and this paper provides a relevant follow up up that and as such is likely to be of interest to the ICLR community. Specifically, they show that the value of the neural tangent kernel matrix tends to a constant multiple of the neural path kernel matrix as the width of the network goes to infinity. Firstly, the analysis is extended to certain ResNet and Convolutional architectures, showing that in both of these cases we can relate the neural tangent kernel matrix to the neural path kernel matrix using a result analogous to Theorem 5.1 in (Lakshminarayanan and Singh, NeurIPS 2020). Secondly, they provide an interpretation of the neural path kernel as a composite kernel composed of layer wise kernels, giving rise to the title of the paper. I was not very familiar with the work on neural tangent kernels and encountered (Lakshminarayanan and Singh, NeurIPS 2020) for the first time when reviewing this paper. Theorem 5.1 (in both papers) relates the neural path kernel to the neural tangent kernel by showing that the neural tangent kernel for a network in which the gates have been fixed tends to a constant multiple of the neural path kernel as the width of the nerwork goes to infinity. For any fixed gating structure, there is a relationship between the neural path kernel matrix and the neural tangent kernel matrix for a network with that gating structure (i.e., one in which we are only learning the neural path values).<BRK>#### General CommentsThis paper establishes close relationship between CNN and FC DNN with a composite kernel method. Specially,  this paper shows that architectural choices such as convolutional layers with pooling, skipconnections, make deep learning a composite kernel learning method, where thekernel is a (architecture dependent) composition of base kernels. This interestingly indicates that  standard deep networks have in built structural properties that may explain their success before training them. #### Specific Comments(1) It would be more interesting if some superiority of deep learning relative to kernel methods can be provided. (2) Lakshminarayanan and Singh (2020) has developed a neural path framework in the NTK regime.<BRK>Paper proposes and extension of neural path framework to include composite kernels which comprise of a) FC networks (Hadamard product of gram matrices), b) residual networks (sum of products of base kernels), and c) CNN max pooling layer. Furthermore, they also include learnt gates instead of static initialized random gates and show learnt gates perform better. Paper is well written with main technical contribution being theorem 5.1 which shows for infinite width case $w \rightarrow \infty $ the NTK is independent of the weights. It also presents experimental result on MNIST and CIFAR for four proposes regimes of (Definition 5.1) that models are robust to combinatorial variations in layers and inputs. This results in novel makes an important theoretical contribution towards understanding of why DNN with composite kernels perform well in practice.
Reject. rating score: 4. rating score: 5. rating score: 6. rating score: 6. <BRK>This work presents a strategy for improving exploration and efficiency of RL by leveraging the graph structure of an episodic experience buffer. This strategy combines goal oriented RL with structured exploration. Grammar is a minor part of this issue, but the major clarity issue is that it s simply quite hard to understand what exactly the authors are proposing/doing in this work. It s very unfortunate, because it seems like there might be some interesting ideas here, but it is very hard to say because so many details are not clear. How do you go from a set of trajectories to a useful graph representation? In addition, it is difficult to form an intuition around the logic of the algorithm   for instance, how is the generated goal related to the task goal; or, does the multi episode graph assume that the task goal is the same across episodes? The result of this confusion is that I would **not** be able to implement their algorithm based on the understanding I gain from the paper. It s entirely possible that fixing the clarity issues would result in a strong paper, but, unfortunately, it does not currently seem suitable for publication. In particular, this work is focused more on graph based exploration. Certainly, this is not the first paper to recognize that trajectories of experience can reveal the environment s structure and that this structure is conveniently represented as a graph. ### SignificanceAgain, clarity issues make it difficult to assess the significance of the work.<BRK>This paper proposes a new framework, GSRL, to handle the sparse reward challenge and better leverage past experiences. Specifically, it formulates trajectories as a dynamic graph, and generates hindsight like goals based on sub group division and attention mechanism. The authors provide theoretical analysis to show the efficiency and converge property of their method. Are they embedded as representations (e.g., averaging all state representations within one group) or just identified as one hot group ids? If a group is selected, is it true that the goal is sampled randomly from this group, or with some priors. If the boundary of a graph consists of too many states, I wonder whether the groups divided from it are still too large, as there are only three groups. + It is better to clarify which episode the s_{last} comes from. However, according to the "Attention Strategy" in Section 4.1, s_{last} is appended to each group since the beginning of each episode. So is s_{last} here not from the "future" episode? Please correct me if I misunderstood your illustration. + Regarding Appendix E.4, is a method regarded as more sample efficient if it can explore more unique nodes and form a larger group given a fixed number of episodes? + Not sure experiments are enough. There are some recent works [1, 2] addressing similar problems and tasks.<BRK>**Summary**This paper proposes graph structured reinforcement learning (GSRL), which consists of two key components: (1) goal generation, to choose what goals a goal conditioned agent should follow during an exploration episode, and (2) value estimation, to prioritize experience from highly related trajectories according to local graph structure during value/policy updates. For (1), the algorithm maintains a “state transition graph”, essentially a graph of the observed state transitions in the MDP. Florensa et al.. Automatic goal generation for reinforcement learning agents. This “optimal goal” that should be generated can be trained in hindsight by looking at the best trajectory in the next episode (where the best trajectory terminates at a state that is estimated to be the closest to the goal) and finding in it the reachable state from the current episode. The paper presents several theoretical results: (1) to motivate the need for directed exploration, (2) showing that the exploration goals should come from the boundary. Rather than constructing a graph to determine the boundaries/frontier, they use either a discriminator or a density estimator. It would be worth discussing these works in relation to GSRL, and perhaps even including some of them as baselines. There are several ablations that can be included to further gain intuition about GSRL (see questions below)  Some parts of the paper were confusing to read. In Section 3, the goal oriented RL framework was introduced, with both discrete and continuous action space. The motivation and proposed ideas are promising, but I have several concerns about relevant related literature and ablation experiments which would make the paper much stronger and convincing. How does the method perform without using the attention strategy on the groups to pick $C_{ATT}$ first, then obtain the optimal goal within $C_{ATT}$? The “Impact of Group Selection” shows a curve with “no group selection”, how exactly was this done? 2.Ablation for the value learning strategy: compare updating from transitions drawn uniformly from the replay buffer, without using $\mathcal{D}_{related}$. Along with the above, would allow you to show that the various components of GSRL are necessary2. **After rebuttal responses**:The authors have tried to address my concerns about the baselines and clarifications on some of the details of their implementation during the rebuttal period.<BRK>This paper introduces Graph Structured Reinforcement Learning (GSRL) framework, able to balance exploration and exploitation in RL. Actually, GSRL builds a dynamic graph based on historical trajectories. Then in order to learn from sparse or delayed rewards and  be able to reach a distant goal, it decomposes the main task into a sequence of easier and shorter tasks. An attention strategy has also been proposed that is able to select an appropriate goal for each one of the easiest tasks. Experiments have been conducted on various robotics manipulation tasks showing that GSRL performs better compared to HER and MAP algorithms. The idea of constructing a dynamic graph on top of the state space is really interesting. In this case, the agent is able to explore efficiently all the state space as the graph is expanded at each time step. Despite the possible merits of this approach on  various robotics manipulation tasks, the applicability of GSRL on other sensitive tasks is not guaranteed. For instance, could GSRL be applied to the task of helicopter controlling where  the safety of the agent is critical? It is not clear how does it defined (Eq.2)?It is not also obvious  for the reader how do you update the attention mechanism (see Eq.4).I recommend to the authors to give more details about the definition and update of attention strategy and not treat it as blackbox. Moreover, at Eq.3 the state with the highest value among the states of C_ATT is selected as goal. Finally, despite the fact that the paper is well written, the notation is cluttered in some cases.
Reject. rating score: 3. rating score: 3. rating score: 5. rating score: 7. <BRK>Experimental results on three semantic parsing datasets show that the phrase level transformer is better at capturing the local information than the original transformer. *Strength of the paper*:  This paper proposes to use phrase level information to capture the local dependencies of the transformer architecture and the experimental results show that semantic parsing could benefit from such local dependencies. I like the idea of trying different granularity representations for the transformer.<BRK>This paper extends Transformer to integrate representations of ngrams for semantic parsing. The experimental results show that this modification leads to marginal improvement on three benchmark datasets of semantic parsing. There is a thorough evaluation which displays the model performance, and visualizes the attention alignment and the similarity of the phrase representations. Although in semantic parsing, there is no previous work that utilizes local context information with a Transformer, there are many similar architectures in machine translation.<BRK>The paper modifies the self attention mechanism in transformers to function at the phrase level, rather than at the token level, as a means to improve alignments between input phrases and logical form predicates for Semantic Parsing tasks. The datasets used are quite old and performance on these datasets is quite saturated. Might be useful to evaluate on newer semantic parsing datasets such as TOP or other SQL based datasets such as WikiSQL or SPIDER. The citation format is quite terrible in the paper. The other models should never have been evaluated on the test set.<BRK>The paper proposes an improved version of the transformer for the task of semantic parsing, called PhraseTransformer to overcome the limitations (i.e., failure to capture local sentence context effectively) of the transformer. ##########################################################################Reasons for score:  Overall, I like the idea and I would like the paper to be accepted. 2.The proposed approach is also very interesting, i.e., captures the meaning of the phrases or n grams in the context of the whole sentence. 3.Reasonable experiments have been provided to prove the efficacy of the proposed approach. Still, I will not recommend rejecting this paper only because of this. And how these representations are put back to generate predictions at the word level.
Accept (Poster). rating score: 7. rating score: 6. rating score: 5. <BRK>In this paper, the authors proposed a method to learn efficient representations of discrete tokens. They took a two step approach: in step 1, they learn "full fledged" embeddings for a subset of anchor tokens. This two step approach reduced the overall number of parameters. The sparse matrix T can also encode domain knowledge (e.g.knowledge graphs). In the experiment section, the authors showed that their approach has good performance on several language tasks, with far fewer parameters. I find the main idea plausible and ingenious. 2) they use a sparse T matrix to relate other tokens to anchors which again has reasonable prior: the meaning of a word can be efficiently defined by a few good chosen anchors. To be specific, there can be a variety of knowledge (like related, is a subset of, analogy, etc.). Another question is about training. Their method does not reduce the theoretical complexity (still linear w.r.t.vocab size, as T must have at least one element per row), but in practice the reduction (which mostly comes from savings of dimensionality) is quite obvious.<BRK>Hence the paper proposes to only store a few anchor/latent vectors (the matrix is A with |A|<<|V|). All label vectors are expressed as linear combinations of a  few  anchor vectors. To train this end to end, we need a transformation matrix T such that T*A   E (E is V\times d embedding matrix). This design admits multiple ways of initializing the anchors A. And the authors perform experiments with both frequent token vectors and random anchor vectors (both have their merits, random seems to be a robust choice). The authors provide a statistical interpretation of their approach using a generative formulation to the embedding vectors in terms of the latent vectors (using a Indian Buffet Process membership matrix Z). The idea seems a little similar to Compositional Embeddings (Shi et.al.2020, Ginart et. 2.There are other sparse embedding methods like SNRM (Zamani et.al.2018) and SOLAR Sparse Orthogonal ...(Medini et.al.2020) which might be comparison candidates at least for IR tasks.<BRK>This paper introduces a row rank approximation of embeddings using “anchors”. It also proposes a probabilistic interpretation of their method as a non parametric Bayesian dictionary learning model, which can be inferred by optimizing the small variance asymptotic objective. ii) The initialization for basis vectors is extremely important and should be updated through training. iii) Experimental results look reasonable in this paper. Normally in a recommendation system, the vocabulary size can be the number of users, which is at least 100M. Overall, this paper proposes a practical solution to cut embedding storage. But the Bayesian interpretation is not persuasive and there are still missing pieces in the experiments.
Accept (Spotlight). rating score: 8. rating score: 7. rating score: 7. rating score: 6. rating score: 6. <BRK>This paper shows a formulation of regularized Markov Decision Processes (MDPs), which is slightly different from that of Geist et al.(2019).Then, the authors propose a novel inverse reinforcement learning under regularized MDPs. I think it is sufficiently significant to be accepted. Still, I have the following questions. 3.The authors claim that the solutions provided by Geist et al.(2019) are intractable in the Introduction. Finally, are exp, cos, and sin the meaningful regularizer?<BRK>This work considers a regularized IRL setup, where instead of the entropy regularization used in maximum entropy IRL, an arbitrary convex regularizer $\Omega$ is used. I hope that the following questions can be resolved during the discussion period as some things are a bit unclear to me which is preventing me from providing a more detailed analysis of the work. What exactly are you using for $t(s,a,\pi)$?<BRK>########################################################################## Summary: This paper examines the problem of regularized IRL. 5.It s unclear how this is different from the theory for GAIL which is also in terms of a general regularizer. The paper is a bit notation heavy and that makes it hard to follow. The mathematical foundation is rigorous and the experimental results are promising.<BRK>Pros:1. this paper studies an interesting problem called regularized inverse reinforcement learning, which is novel to me and brings me some knowledge. In summary, this paper studies a novel problem, regularized inverse reinforcement learning. Cons:1. in the experiments, much comparison is internal comparison of proposed methods.<BRK>This paper proposes a new method for regularized inverse RL. The Shannon entropy is a special case of such a policy regularizer. I believe that the paper is technically correct (and I appreciated that the derivations are well explained in the appendix). Could you comment further on this empirical result? I think it would be nice to include an algorithm box as a summary of the proposed algorithm. The paper might benefit from some further proofreading for typos and small grammatical errors such as missing articles
Accept (Poster). rating score: 8. rating score: 7. rating score: 6. rating score: 4. <BRK>The paper presents a dataset for autoformalization (semanticdisambiguation) of informal Latex STEM documents. The SMGloM glossary and the MiKoMHrepository are used as parallel sources, and the MMT system connectinga number of formal systems and foundations is used for dataaugmentation. The pretrained model is thenfine tuned on the smaller training data. My overall impression is that this is an important step in theautoformalization program [1]. Some detailed remarks:p5: Disamiguation  > spell checkp5: def 4.1 "We call S ∈ L fully disambiguated" >I would not call the text fully disambiguated without types ofvariables. Would larger GPT models help? Would unsupervised learning like in Wang 20 be useful in some context here?<BRK>This paper addresses a variant of the style transfer problem   that is, transferring formal latex expressions to less formal descriptions that can be followed by mathematician. 3.The authors propose a new dataset that is likely to be useful for the community of researchers that work on this problem. The problem definition was not clear to me. 2.I had a similar problem with the description of the dataset. Yes, there is a formal description (just as there is a formal description of the task), but the lack of examples leaves the description at a very abstract level   I could not understand what exactly should be expected in the dataset. Overall, despite the challenges, I think the paper can contribute an interesting and practical new task to the research community   both at the task definition level and in providing an actual dataset. I recommend that the authors try to solve the above issues in the final version, but I am leaning towards acceptance.<BRK>This paper proposes a new task, disambiguating an informal math expression in LATEX by associating its tokens with concepts in a predefined formal math library and determining its abstract syntax tree. The drawback of the current benchmark is the lack of training and evaluation data. We do need a larger and high quality evaluation set to validate any actual progress on this problem. From what I understand, our best evaluation protocol should be checking if S_F belongs to STEX(S_STEX), which is not used in this work. After reading other reviews and authors  responses, I upgrade my score to 6.<BRK>Take for instance, formal and informal. The main finding of this work is that pre trained transformer models outperform more traditional fully supervised translation systems on this task. The authors are clearly knowledgable on the topic, and discuss and cite a great deal of the literature and libraries relevant to the problem. Even off the shelf methods can of course be part of an important contribution when the authors show that they have pushed the field further with an important result (say, GPT) but I do not feel the evaluation in this case supports that conclusion. #### RecommendationBecause the presentation makes it difficult to fully grasp the problem setting, precisely what is being learned, precisely what is failing, it is difficult to recommend the paper for acceptance. It also seems that some of the errors pointed out (like those involving ellipses) would likely be remedied by additional synthetic data.
Reject. rating score: 3. rating score: 4. rating score: 4. rating score: 6. <BRK>  Summary:This paper aims to tackle the out of distribution generalization problem where a model needs to generalize to new distributions at test time. The paper demonstrates the experiments on two tasks (i.e.semantic segmentation, image classification), but the proposed conditional network is different and does not have a unified architecture. The Conditional Batch Normalization (CBN) mechanism has been used in some generalization relevant tasks like domain adaptation detection [1] and few shot learning [2]. I think more ablation studies to demonstrate the effectiveness of the proposed conditional network and choices of the “metadata” or additional information in a dataset are needed.<BRK>##########################################################################Summary:This paper propose a method for leveraging additional annotation by using an auxiliary network that modulates activations of the main network. ##########################################################################Reasons for score: ves: + This paper tackle the problem of out of distribution generalization which is crucial for machine learning applications. + The idea of leveraging additional information to enhance domain shift generalization is interesting and make sense to me. + The proposed conditional networks is practical. cons:  My main concern about the paper is the novelty. The paper seems the incremental work based on Conditional Batch Normalization, which limits the contribution. Overall, the novelty and contribution of this work are marginal.<BRK>The paper intends to tackle the problem of out of distribution generalization through conditional computation. In 4.2/Methods tested: you state:  “the conditioning network is trained to perform image classification for the type of cancer (tn).” Can you give more detail on how that was done? Imagine the model is presented with an image from a new city, which might have features that are visually similar to cities from the training set, but has very different coordinates. The experiments claim that the conditional network improves performance in certain tasks, namely semantic segmentation and image classification .<BRK>### Summary:This submission proposes an approach to modulate activations of general convolutional neural networks by means of an auxiliary network trained on additional metadata to a dataset. The specific goal is to improve out of distribution (OOD) generalisation. This change might be beneficial for the points mentioned above, too. However, there are some concerns and questions outlined above which I believe need to be addressed / adapted in order to accept this paper. I believe a more explicit / clearer discussion would improve the quality of the paper quite a bit.
Accept (Poster). rating score: 7. rating score: 6. rating score: 6. <BRK>**Recommendation:**I gave the paper a score of 7, and recommend acceptance. A nice decomposition of suboptimality. I am wary of the comparison of upper bounds done in the paper.<BRK>The message of this paper is that naive policy evaluations common in current (deep) RL algorithms, can lead to a dangerous overestimation of the value function. What was the \alpha in the experiments? Overall, the paper is an interesting read and its message is well presented and supported.<BRK>The second paper is an example where model learning agents keep track of the uncertainty in their learned transition and reward functions and use pessimism to fill in uncertainty. The other issue with the paper is its organization and writing.
Reject. rating score: 3. rating score: 4. rating score: 5. rating score: 6. <BRK>Even if all the assumptions are right the suggested solutions for training a "robust" model is very ad hoc and unlikely to be robust. Clarity:The paper is poorly written and most parts of the paper are difficult to follow. The loss function being minimized is not the same between different epochs.<BRK>Particularly, I think section A2 could be  improved, including a more detailed description of d_x(\theta, f) and providing more intuition for r(\theta, A(f,x)). The paper s goal is to show that if we can make assumptions on the knowledge of the spurious features that may exist in the dataset, we can better analyze the error on potential test sets. but there are subtle differences in how the different works the authors cite define these terms.<BRK>Cons: The paper seems to interchangeably use spurious correlation, confounding factors and dataset bias. It is still unclear to me how to select them in practice, and how this affects the performance. There are some typos in Section 4.2 regarding f_d and f_p, for example, in the first sentence, f_b should be f_p.<BRK>The analysis leads to a set of solutions linking to established solutions. ################################################Reasons for score:In general, I like the flow of the paper. However, I have some concerns / questions regarding its assumptions. Also, several typos need to be fixed. Features are usually mixed together.
Reject. rating score: 3. rating score: 4. rating score: 6. rating score: 9. <BRK>The data augmentation is done on the validation set. This means there is no longer a validation set! The authors are able to improve upon the TAPE baseline. However, the lack of rigor, novelty, nor impressive results means this paper does not meet the high bar for an impactful paper at ICLR.<BRK>Additionally, data augmentations show better results on all tasks, beating the TAPE baseline significantly. It is interesting to find out whether they do well on protein language modeling tasks. However, there are a few glaring weaknesses in the paper. (3)I have a few worries about the augmentation strategies used. This is simply wrong.<BRK>### Summary  This paper experimentally investigates the effects of data augmentation for protein deep learning. For data augmentation strategies, they used the Random dictionary, Random Alanine, Global/Local Shuffling, Sequence Reversion, and Subsampling. This is a very well known fact in the deep learning community.<BRK>4.I am not seeing the problem of data augmentation done on the validation set that anonreviewer1 is suggesting. There are of course problems that will indeed need to be addressed in further work but there are also important insights in the paper. This is probably the first paper that tries a wide range of data augmentations for protein sequences, and tests these augmentations on a wide range of benchmarked tasks. 2.The design of the data augmentations is creative.
Reject. rating score: 3. rating score: 4. rating score: 4. rating score: 5. <BRK>Problem motivation not very clearI felt that the paper does not provide good enough context into the problem and had to piece it together from related work cited (e.g.[1], [2]). In this case, it may not be optimal toupdate the encoder and decoder simultaneously. In other words,the result of the encoder can be optimized in advance in direction in which the decoder trains well.<BRK>1: Although the adaptation of latent optimization to cVAE is interesting and can improve the performance, the idea or the contribution is not significant enough, especially considering many existing VAE variants (including those already discussed in the paper). The paper may need some more coherent experiments to be more persuasive.<BRK>The paper is well presented and the method easy to follow. * There are some language errors, e.g.“the training of decoder”. * All images, notably Figure 5, should be in vector format.<BRK>In fact the method of He at al, can be considered as a generalization of the proposed method, with minor differences. The paper is well written and easy to follow. My main concern with this paper is novelty. Even in this case, I think the comparisons with existing work is not thorough.
Reject. rating score: 5. rating score: 5. rating score: 5. <BRK>The numerical experiments are extensive which include discrete and continuous control tasks. Strength:   The paper is easy to understand and provide informative discussion on related methods. The point probability distance is simple but appears to work well. The point probability distance actually depends on the action taken at that iteration.<BRK>In fact, the parameter beta plays a role that is similar to a Lagrange multiplier, if the new distance measure is introduced as a constraint. The authors explain the shortcomings of KL divergence and the solutions obtained with other methods but they do not provide a sufficient discussion how and why their simple approach overcomes those concerns.<BRK>I believe that these ideas may be central to the paper. (page 4)I believe that this is an interesting work, though possibly still showing only preliminary results.
Reject. rating score: 2. rating score: 4. rating score: 4. rating score: 5. <BRK>The authors claim that they introduce a concept of sample robustness based on the Lipschitz constant of the label map. It might be used to compare different datasets. It is nice to see the details for each model and the robust samples for each dataset. Cons:  The novelty of this work is somewhat limited. Moreover, there is no theoretical or empirical justification regarding why this definition of sample robustness is useful. Despite that I really appreciate the efforts that the authors have put on the experimental section, I think that the contribution to the community is somewhat weak. This paper just uses the so called sample robustness to select a subset of the training sample and then reports the performance of model trained with that subset on some test data. In fact, from the results, it is still not clear how sample robustness can help choose the right training set and improve the model performance. There are some typos.<BRK>Summary:   This work introduces the concept of sample robustness – based on computing the pointwise Lipschitz constant of a data point – and use it to empirically analyze the effects of training on least and most robust training subsets on the performance for different models. The paper is structured well, and the problem has been motivated well in the introduction. The related work and experiment results sections are quite thorough, and the empirical results are interesting. It would have been more satisfactory had some supporting theoretical results been provided for the proposed notion. The grammar and quality of writing can be improved at many places. But then, what is the benefit here of training on a (least/most robust) subset of the training data? In Table 4 as well, downsizing x drastically increases the loss on the complete test t. However, in Table 6 (KNN), choosing the 40,000 most robust training samples gives the best accuracy on t. There is no explanation provided as to why this is the case. In practice, we do not have access to the test data so we cannot process it. Post rebuttal:I have read the authors response and appreciate the effort made to improve the paper. But I think the results still need additional work, especially from the theoretical front.<BRK>Overview: The authors introduce sample robustness, a pointwise measure of the sensitivity of the label map to perturbations in the feature space. They do so by taking the pointwise Lipschitz constant of the label map and normalizing it by the label norm. The authors show that training on samples that are / are not sensitive helps on a test set that is / is not sensitive. It seems like the conclusion of the paper is that if the goal is to get good performance on the standard test set, you should just use the original data without taking into account sample robustness. There are gains for specific subsets, but the paper doesn t really do a good job of convincing me why I should care about this. It seems like the end goal should be performance on whatever test set I originally collected, and not a subset where you remove particular hard or easy examples. Finally, the writing style is generally convoluted and requires substantial effort to parse. Please use proper left quotes for Latex.<BRK>This paper proposes a novel concept of sample robustness, which is used to find tailored training(sub)sets and hyperparameters depending on the robustness distribution of the test(sub)sets for boosting model performance. The motivation is straight forward and the proposed concept of sample robustness is inspiring. However, the works in this paper do not match with the announced contribution. Hence, I vote to reject. Reasons for scores:1，this paper announces that the novel concept of sample robustness can be used to choose training subsets and boost the model performance. In addition, some simple theoretical analysis about sample robustness are also given. 2，The main works of this paper is based on empirical studies. 3，this paper does not analyze the pros and cons of proposed technique based on the concept of sample robustness.
Reject. rating score: 4. rating score: 4. rating score: 5. rating score: 5. <BRK>The authors propose a metric learning and clustering method based on the idea of learning the metric from the context. 5.Why are different metrics used for the two data sets (Rand Index and NMI)? In summary, I believe this idea is incremental and possibly has some potential, but the authors have not done justice to it in this paper. Review update after rebuttal: The authors have addressed some of my concerns in the rebuttal and the modified version of the paper. They have added results on more real datasets and explained some aspects that were unclear in the first version. Their proposed method seems to perform worse than the baselines in some scenarios. Though this is not bad and it is important to show negative results, I did not see any discussion or insights into the poor performance.<BRK>This paper proposes Attention Based Clustering (ABC) that learns latent representations to adapt to context within an input set. They use ideas from the metric learning literature and the Siamese network on how to learn compatibility scores, and the transformer architecture and the Set Transformer on how to use context to make decisions. 1  Transformer already being used as the Set Transformer to improve clustering (Lee et al., 2019b). 2  Also they mentioned that their model uses context to output pairwise similarities between the data points in the input set. 3  The paper also claims that the ABC model is agnostic to the number of clusters. The known eigengap method (von Luxburg, 2007) is used to find the number of clusters. Clustering is a very old problem and there are  known datasets from different domains to evaluate a new approach.<BRK>This paper proposes a method for producing representations for clustering that take into account global trends in the dataset, rather than considering each pair of instances in isolation. They claim to achieve competitive clustering performance on omniglot, which they attribute to the use of these contextualised embeddings. * Both the algorithmic and theoretical work described by the paper appear to be technically correct. * The experimental evaluation could be improved. Does this mean $d_x   d_z$? * In the circles experiment, are multiple datasets (i.e., "instances") synthesised in order to train the embedding block? * It is said that the "pairwise" baseline, the embedding block is removed, but also that it is a metric learning method. ## Other comments* Siamese networks were proposed by Bromley et al.(1994), not by Koch et al.(2015).## After Author ResponseMy concerns are only somewhat addressed by the authors  response. While the results on extra datasets are encouraging, I still think there is limited novelty in the proposed approach and some of the important details remain unclear. In particular, terminology seems to be used inconsistently, which results in ambiguity when describing variants of the model used in experiments.<BRK>The paper proposes a new method   Attention based clustering (ABC) that incorporates context to learn a metric in the form of an embedding and kernel similarity layer (predefined). An off the shelf clustering algorithm (e.g.spectral clustering) is used to cluster the similarity matrix (number of clusters pre specified or inferred). Given that ABC’s results are surpassed for the Omniglot task by methods leveraging context for clustering, it would help to have a real world task where the approach taken (learning the similarity metric followed by off the shelf clustering) is the best. This would help better motivate the proposed approach compared to previous approaches in the literature. It would also help to have further experiments on other datasets to increase the range of validity of the experiments.
Reject. rating score: 4. rating score: 5. rating score: 6. rating score: 7. <BRK>Prior to this work, the work of Zhou et al.tries to achieve the same for DP SGD and they analyze their algorithm for many variants of adaptive gradient descent based method. The difference with Zhou et al.is that they do not gradient norm and the generalization property of DP. I will keep my reviews limited to the current submission. My first objection to the paper is that gradient descent requires computation of gradients over the entire dataset. The paper considers Polyak Lojasiewicz s condition, a more general form than strongly convex loss function. For example, Feldman et al.(STOC 2020) and Zhou et al.show an excess "population loss" of order $O(1/n)$. On the other hand, if I understand the paper correctly, the current submission only give excess empirical risk. Further, Feldman et al s two algorithms achieve optimal excess loss while running in nearly "linear" time, which is not the case in the current submission. There might be some interesting idea in this paper, especially, looking at the dynamic schedule for PPML. However, I fail to see the strength of the result in the view of prior work as the comparison done in the paper is between apples and oranges. There are some typos that I found.<BRK>Concerns:1)	While there is some improvement with the dynamic schedule vs. fixed schedule, the improvement does not seem to significant (ref.Table 1).2)	Results hold only under PL condition (which is a more general condition than strong convexity) and smoothness. The results look correct. Questions:  Maybe relaxing the assumptions might be a way to strengthen the paper. Is that possible? What is the advantage of using the momentum method in terms of the utility upper bound (ref.Table 1).It looks like (within constant factors) vanilla GD with dynamic schedule works as well as using the momentum.<BRK>Summary Gradient Descent and related variants are the defacto standard algorithms for optimizing empirical risk functions. However, this still leaves the question of what privacy schedule is best open, since any schedule whose privacy budgets sum up to R achieves the privacy objective. In this paper they compute the optimal privacy schedule from an accuracy perspective via a novel analysis of the convergence of private gradient descent on loss functions that satisfy the PL condition, which turns out to be exponentially decaying noise (increasing privacy budget for each round). These results extend to a privatized variant of the momentum based gradient descent algorithm, although the dynamic privacy schedule has less improvement there. Pros  improves over previous soa analysis of private gradient descent (wang 2017) by $log N^2$ factor  Novel analysis for an important practical problem that also yields intuition via the notion of noise influence / propagationCons   Grammatical Errors throughout: e.g.sentence fragment page 6 “because our bound saved a factor of log N and is thus tighter, as compared to Wang”  Overall the clarity of the writing and explanations, both in terms of prefacing the technical content, and readability could be improved. Comments  why does the utility bound for non private GD in table 1 have a factor of $R$ (privacy budget) in the denominator?<BRK>The problem studied is interesting and important for the privacy community   private gradient descent is an important mechanism for private convex optimization, and one of the only mechanisms for non convex optimization. 2.The authors do a good job engaging with prior work, identifying gaps in related work (i.e., theoretical justification for dynamic noise schedules) and putting their contributions in that context. 2. log(N) improvement may be an artifact of the analysis, and not an inherent advantage of dynamic schedules. Since we are dealing with upper bounds, I wonder if it is just an artifact of the analysis of Wang et al., rather than an improvement offered by dynamic scheduling. Are there other ways to remove this dependence other than dynamic noise?
Reject. rating score: 3. rating score: 4. rating score: 4. rating score: 5. <BRK>This paper presents an approach to low dimensional embedding of data, with an emphasis on image datasets. The main drawback of the paper is that the empirical results compare only to PCA. Several more modern methods for low dimensional embeddings exist (e.g., T SNE, UMAP), and it would be appropriate to compare to these.<BRK>Summary:This paper proposes a subspace indexing model with interpolation (SIM I) for dimension reduction. 5.Figure 1 is not clear. The main contribution of this paper is to propose an interpolation technique to combine several subspace indexing models to determine a more promising model for low dimensional embedding. 2.The novelty is limited. In Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition, pp. This is very limited.<BRK>The authors propose an approach towards computing  globally non linear, low dimensional embeddings of high dimensional data. In particular, they consider a sub space indexing model with interpolation (called SIM I) which consists of two steps. Indeed, the experiments considered here, also only consider data sets of moderate sizes when compared to modern standards. First of all, there is a very rich literature on "local PCA" which is not acknowledged and the practical performance of corresponsing earlier methods is not considered / reported.<BRK>### Summary: This submission proposed a subspace index method with the additional interpolation (smoothing) step, i.e., SIM I, first to build a piecewise linear affinity aware subspace model under a given partition of the data set; next is to interpolate between several local linear subspace models by using the “center of mass” on Grassmann manifolds. This seems fine, still raising some concerns for using PCA as the foundation step for non linear low dimensional embedding methods. ### Reference:Related works from non linear dimension reduction and manifold learning are discussed in this submission.
Reject. rating score: 5. rating score: 6. rating score: 7. <BRK>The proposed method showed a competitive performance with the previous approaches and showed a faster adaptation speedStrengths* This paper showed that a simple approach can achieve a competitive performance on meta reinforcement benchmarks* The presented method showed faster adaptation compared to previous approachesWeaknesses* Motivation for the proposed method is not clear. If the training method is an important component, there should be at least one ablation for this detail. I still have a concern that it is not clear what s the main finding in this paper. Specifically, whether architecture is important or objective is important. * Experiments do not contain ablation studies for the proposed method. I believe this is because the paper lacks a thorough discussion or empirical studies to clarify this question. ### After the author feedbackI appreciate the authors for more control experiments and additional discussion in the manuscript. Two differences of this work from LSTM based RL^2 are 1) neural network architecture, and 2) training objectives.<BRK>## Response to the author feedback:We appreciate the authors for the extra effort to demonstrate the abiltiy of FLAP by adding more experimental results and discussions. The key point that distinguishes FLAP from other meta RL algorithms is the prediction of the policy weights. The author needs more theoretical analysis to prove the validity of this method on OOD tasks. In such cases, the adapter can be confused and the performance of FLAP may deteriorate. This paper makes an assumption that the Adapter Network can converge on the train tasks. Both $\mathcal{D}$ and $\mathcal{T}$ refer to a set of tasks in different parts of the paper.<BRK>The solution in this work is conceptually simple. However, the other idea of this work is to use an ``adapter network  to predict, from a tuple of transition $(s, a, r, s )$ the corresponding $w_t, b_t$ for this task. This work demonstrates empirical performance on a set of meta learning tasks that is competitive. The main suggested improvements are further discussion for related work, clarifications and experiments and/or discussions of limitations of this approach. It would be helpful to discuss whether this is important for this approach. Universal value function approximators.
Accept (Poster). rating score: 7. rating score: 6. rating score: 6. <BRK>DG most commonly refers to evaluating a model trained with photo cat only can generalize to carton cat. This study still focus on one domain (one style of images). Though there were several datasets for domain generalization, I think the view of this benchmark is different and it can be a starting point for another direction. The benchmark introduced in this paper is very interesting.<BRK>This paper addresses the problem of model robustness to subpopulation shift. Results in sections 4.3 and 5.1 appear entirely unsurprising (though admittedly, this could be hindsight bias). The authors have performed an excellent work with the comprehensive experimental setting proposed in the paper.<BRK>They propose to create large scale subpopulation shift benchmarks  to assess how models generalize beyond the diversity of the training examples. The problem is very important and the paper well written. The main application presented is images although the authors claim that the approach could generalize to other ares like nlp. Grouping similar classes may not be possible in many application and a discussion about it could have been helpful.
Reject. rating score: 4. rating score: 7. rating score: 7. rating score: 7. <BRK>This paper discusses several protocols including data augmentation, point distribution, loss function, ensemble scheme, and testing models, which serves as a kindly reminder that the training protocol matters. As claimed, earlier work like PointNet++ can still achieve comparable performance to more recent methods. Moreover, the authors investigated a new projection based SimpleView method by converting point clouds into depth images, achieving SOTA performance without pretrained CNNs. Although such detailed reviews over the training protocols are helpful, the authors failed to propose any novel methods or evaluation metrics (notice the SimpleView is also a commonly used methods in other multi view framework), resulting in relatively inadequate novelty and originality. Besides, it only focuses on the classification task, which cannot convincingly conclude the representation ability of different methods.<BRK>This paper studies the factors that are related to point cloud classification but independent of model architecture. The paper is well written, rigorous and easy to follow. I think this paper would be a good contribution to the community. (2)For performance in Table 6,  do you train the the state of the art method with the same protocol for SimpleView? After discussion:After reading the author s response as well as the opinions from other reviewers, I will stick to my original rating.<BRK>It is also surprising that factors beyond architecture design can make such a difference in the evaluation. I think following the same evaluation protocol can be very helpful for subsequent research. These messages are worth to be known by the community. Since the SimpleView does not operate directly on point clouds, it might have distinctive failure modes compared with other models. It might be helpful for designing point cloud processing models in the future. The author(s) addressed my concerns and I think it is a good paper for the community.<BRK>Post rebuttal reviewAfter reading the authors  responses and other reviews, I would like to stick to my original rating to accept this paper. This paper provides important findings of the source of performance improvements in the domain of point cloud classification. In my mind, It is totally okay to not propose any new method/architecture but it would be nice to see a solution to the revealed problem. The paper’s findings/claims are mostly based on experiments, so it would be nice if the paper can release its code to be used by the community and validate if it is really true that the efforts put in neural network architecture are not useful in terms of performance in point cloud classification. It might be good to combine the third contribution in the introduction with the second one?
Reject. rating score: 4. rating score: 6. rating score: 6. <BRK>### summaryThis paper introduces capsule network for object detection. To solve the issue of capsule network when applied to large scale detection problems, this paper develops deformable capsules, a new  prediction head SplitCaps, and a dynamic routing algorithm, SE Routing. 2.Why KL in Sec.3.2? This paper is insightly as it studies several important issues of capsule network when being applied to large scale visual tasks. Note that the network weights are initialized from CenterNet. 3.Lots of reference are missing. However, there are no ablation studies properly studying them. Are they simply borrowed from original paper or some published work?<BRK>The paper proposes to use the capsules to perform object detection on COCO. And together with the SE routing methods, the novelty should be sufficient for this venue. Nonetheless, I believe this work is a good step towards more interests and advances in capsules, and will be of interests to the audience of this venue. I agree with the authors  novelty claims.<BRK>Pros:It is indeed that capsules have many promising attributes but they are not quite easy to be exploited in object detectors. This paper has addressed many problems existed when applying capsule architectures for the object detection task. The writing is also good. In particular, I am confused about the details of deformable capsules. It seems that the authors only mention the problems they will address and the techniques they proposed to address problems.
Accept (Poster). rating score: 7. rating score: 7. rating score: 7. rating score: 6. rating score: 4. <BRK>To explore the efficacy of this method, they experiment in 4 different conversational semantic parsing tasks, which are each different enough to demonstrate the usefuleness of their approach. Contribution: modified pre training method The good: My overall impression is that this work makes sense, the paper is clearly written and flows just fine, and that the authors demonstrated the efficacy of their proposed method. The fact that the 4 baselines are different enough makes your claim convincing. The bad: I feel that this paper is really lacking a driving motivation and the cohesive story is a bit weak/lacking. Shouldn t the discussion/conversation be more about contextualization methods? I think the paper could be significantly improved if the conversation is shifted towards discussion of contextualization, rather than "CSP is a task. You can generate some data and use it". You have an interesting story in your hands, but you are not using it! How is your contextualization method different from other attempts? That said, I still think the paper has good and interesting work, and I do think it should be accepted, but I would really encourage the authors to consider making some changes on this note because it will make the work much more interesting, thought provoking, and impactful. Clarifying question:* With regards to encoding the Turn Contextual Switch   are you encoding the changes from user to agent, or just the changes from user to user, or both? This is the only section in the paper that I found to be lacking a bit, and maybe a sentence or two more could add clarification for me.<BRK>The authors argue that general pre training methods such as BERT do not have enough inductive bias for semantic parsing. They use this pre trained model on 4 semantic parsing tasks and show that their pre training is indeed helping the SOTA models by establishing new SOTA for 3 of the 4 datasets. Reasons for score: This is a clearly written paper with the objective of making conversational semantic parsing better. The authors present a reasonable idea, do extensive experiments to show its merit on variety of tasks. Pros:1.2 new objective functions (TCS and CCS) specific to the task of semantic parsing. The paper was easy to follow with clear objectives. Well done ablation studies and analysis. Cons:1.Data synthesis steps are ad hoc. Why were only 435k dialogues synthesized? It would be great to have a more detailed study of how the synthetic data looks, what is the effect on model performance. 3.While the authors chose SOTA baselines for their task, stronger general pre trained models (BART, T5 etc) might beat this method easily. If you simply drop in the network to something small like geo query, does it help a general seq2seq model? 2.Beyond semantic parsing tasks   does this help in sentence classification too?<BRK>This paper proposes a pre training approach to improve the performance in conversational semantic parsing. The idea is to use the training data to learn how to generate contextual representations by combining the now commonly used masked language modelling pretraining objective (MLM) with two additional objectives, named column contextual semantics and turn contextual switch. I found the paper easy to follow and the results convincing on the whole. 4 datasets and different parsers were used and in each case the proposed method improved the results, and in 3 out of 4 a new SOTA was reached. However, it is important to note that the objective propose needs labeled training data in order to learn the alignment between the utterances and the queries. Thus they allow us to exploit labeled data for other versions of the task, rather than exploit unlabelled data for the semantic parsing. Having said this, I am not arguing that training data concatenation or multi task training will work better; but I think such a comparison is needed. Beyond this, the other aspect of the paper that needs to be improved is the technical clarity. This is particularly important as how one does the decomposition of q is likely to matter to the results. While I think Review5 raised some interesting discussion points which should be included in the final version, I still think the paper has merit, even if larger scale pre training would have improved the results.<BRK>The paper proposes to pretrain contextual semantic parsing models on synthesized data with two new training objectives: Column Contextual Semantics (CCS) and Turn Contextual Switch (TCS). The CCS objective predicts correct database operations based on corresponding columns in tables. The TCS aims to predict the labels of conversational turn switch patterns categorized based on differences in meaning representations between dialogue turns. My decision is just between marginally accepted and marginally rejected. I like the idea of pretraining with CCS and the empirical results show that the proposed approach outperforms all baseline systems on three out of four benchmark datasets. Pros:  1.The paper finds out that CCS and synthetic data works in pre training, despite the prior work finds synthetic data is not useful in a standard supervised setting. 2.Overall, the paper is well written. I can easily follow the technical details of the proposed methods. 3.This paper provides comprehensive experiments to justify the key contributions of this paper. I suggest adding experiments of TCS only to Table 6. 2.It is desirable to have an ablation study to investigate how effective is each grammar (with/without follow up context free grammar) for pretraining. 2.Why does the system compare with the baselines also trained on 10\% training data of SOA?<BRK>[Summary]In this paper, the authors proposed a pre training strategy for Conversational Semantic Parsing (CSP) tasks. The pre training is run on top of any existing LM (i.e., in this work RoBERTA has been used), and uses three additional loss functions to inject the CSP inductive bias into the LM: Column Contextual Semantics (CCS), Turn Contextual Switch (TCS) and Masked Language Modeling (MLM). The results are presented in four well know datasets for CSP: SPARC, COSQL, MWOZ, and SQA. [Pros]  the proposed pre training strategy is novel  the performance of the proposed pre training strategy is effective [Cons Edited]  the paper claims that "However, existing pre trained LMs that use language modelling training objectives over free form text have limited ability to represent natural language references to contextual structural data. Moreover, there is a substantial human bias in the construction of the synthetic data, for instance, to create these data probably a human would need to read way more than 500 samples, or even with 500 samples, a human can pretty much guess the distribution of the data, especially for a grammar based generation as SQL  the comparison made in the paper are over existing model instead over existing pre training strategy, or larger models. I suggest showing more examples. [Reason to reject] The claim of the paper is not supported for lack of comparisons with different, larger and using different pre training strategies, LMs. Moreover, the community should be encouraged to create as general as possible pre trained models, instead of task specific ones, and especially pre trained models that use real and unlabeled data.
Reject. rating score: 4. rating score: 6. rating score: 6. rating score: 8. <BRK>It seems like this paper is producing a special case of CEB for gaussian assumptions on that distribution family. In addition, the empirical contributions are decidedly not novel. Evaluation:Overall, this is a fine paper   the introduction is especially well written and I appreciated the inclusion of all the information plane images of training trajectories.<BRK>A common complaint when reading ML papers is that they don’t discuss related work enough, so this paper was refreshing. UPDATE 2Following discussion among the reviewers and especially a summary of experimental results by Reviewer 3, I m lowering score back to a 6.<BRK>Quality and Clarity: The paper is clearly  well written. Pros:  1) The work presents a very rigorous analysis and discussion of multiple competing IB objectives and discusses the implications of each of these. What are the implications of this for downstream application of the IB? Should IBs with surrogate objectives only be used for compression or also for prediction tasks like VAEs? 2) I would have also like to have seen empirically how the surrogate objectives can generalise across domains that are not images?<BRK>I don t have specific criticisms for this paper. This paper provides several surrogates for the Information Bottleneck (IB) and Deterministic Information Bottleneck (DIB) loss functions that are more friendly to optimization. This paper is well written, fully prepared and contains a large amount of results that are of wide interests of researchers working in this topic.
Reject. rating score: 4. rating score: 5. rating score: 7. rating score: 7. <BRK>However, in the remainder of the paper, there are several technical issues/confusions, outlined below (p.s., please number the equations):1. But the plots and the discussion about Fig 5 for example suggest that EWN and GF leads to different asymptotic solutions, which is not correct.<BRK>cons:The theoretical results are based on very strong asymptotic assumptions, which are not justified properly.<BRK>The authors show two main results. 3.Most of the paper is clearly written. 9.I think that the authors should provide more context to the pruning results in Section 6. The authors should comment on this.<BRK>I think SWN and EWN proposed by this paper are interesting, and it is surprising that they introduce opposite implicit biases.
Accept (Poster). rating score: 7. rating score: 7. rating score: 7. rating score: 6. <BRK>The paper presents a novel approach for denoising score matching, where the Annealed Langevin Sampling has been substituted by Consistent Annealed Sampling, which adds more stability to the process. The paper is in general clear and well written. Can it be extended to the case of general noise (a noise model which could be also learnt)?<BRK>The article deals with generative models based on “Annealed Langevin Sampling“ rather than a GAN. Authors proposed to denoise the last Langevin samples to reduce the gap in performance with Adversarial Network. The paper is really easy to read with good illustrations and supporting experiments. In order to gain in comprehension, especially for people new to ALS, it would have been great if authors have proposed an illustration (and comparison) of the samples evolution along Alg 1 and Alg 2 .<BRK>This paper tackles the problem of generative modeling by using Langevin dynamics to sample from the denoising score function. The paper introduces different improvements over Song and Ermon (2020). Second, it is empirically shown that running a denoising step on the generated sample leads to an improvement of the FID score. 3.Regarding using an adversarial denoising.<BRK>Furthermore, the submission introduces a hybrid adversarial score matching model that demonstrates improvements in terms of FID on simpler architectures. Taking EDS on the last Langevin step diminishes the impact of CAS (doesn t bring unambiguous improvement in FID scores in the experiments), otherwise very interesting finding both theoretically and algorithmically, and substitute for ALS. The effect of the hybrid model is also not persistent and depends on the architecture used. Given all the above, I am still leaning a bit towards accepting the paper as it covers an interesting finding relating to the ALS. Although the CAS effect on performance is limited by the EDS, score matching models are of broad interest for the ICLR community.
Reject. rating score: 3. rating score: 3. rating score: 4. rating score: 5. <BRK>In this paper, the authors propose generative domain adaptation approach called EMTL. Update: Thanks for the authors  response. The paper is well organized and easy to follow. Moreover, there are some technical flaws, which need to be further clarified. While it is a practical and nice point of motivation, the paper misses one important research line on federated learning that is specially proposed for privacy issues. In this sense, I am not convinced by the claim on the privacy preservation. (3)	The authors highlight no access of source data in the adaptation phase. It should be \thetha_m^(0), right? or using p^m(y|x)? If the latter is used, does it mean y_i is updated in each iteration? (8)	The proposed synthetic dataset is very naïve. It is more convincing to test on these well known datasets.<BRK>Summary:This paper proposed a generative domain adaptation (DA) approach under covariate shift. Source Hypothesis Transfer for Unsupervised Domain Adaptation. [2] The proposed adaptation algorithm does not require accessing the source data at the adaptation phase, which has some practical potential. A thorough analysis is lacking. Discussion on the benefits of these settings is highly expected. [2] ExperimentsSince it is an empirical paper, I am most concerned about the empirical results. [a] The current results are rather limited. The author only evaluated on UCI and Amazon review dataset. Both are simple datasets and linear models can achieve good results. I would like to see an empirical and theoretical discussion on the high dimensional dataset (such as the image)[b] The notation in the EM algorithm is rather confusing and difficult to follow. Eq (9) is not correct, $p(x)$ should be a continuous function (not discrete). This discovery is really important and interesting.<BRK>Update after author response: I appreciate the clarifications, but given the lack of comparisons or discussion to related prior methods (at least Liang et al 2020 or some alternative equivalent), I cannot recommend acceptance at this point. The authors did not submit a paper revision as well. Understanding Self Training for Gradual Domain Adaptation. #########################################################################Summary:This paper tackles the problem of unsupervised domain adaptation, where there may be privacy constraints on the source, so we have access to a source model but not the source data. Is there any prior work that suggests that this is not the case? Overall, this is a promising and interesting idea, and with more work would be good to publish. This seems to be a key point of the paper, since experimental results are not better than baselines, but the argument is that those don’t preserve privacy. This, and related methods such as entropy minimization, has been used in the context of domain adaptation (Shu et al, Kumar et al), also in the context of privacy preserving domain adaptation (Liang et al). If there is only covariate shift, then P_m and P_t should be identical. There seems to be an attempt to explain this in Section 4.2, but in the case of no covariate shift, the Bayes classifier on the source   Bayes classifier on the target, so it suffices to simply train a model on the source.<BRK>This paper proposes a novel method for Unsupervised Domain Adaptation (UDA) when the source domain s privacy should be preserved. **Pros**  Unique motivation for UDA and privacy preserving. The method by Song et al.utilized a framework of federated learning and encryption. Although the adaptation phase does not require the source domain data, a probabilistic function $p^m(y|x)$ should be available. The reviewer just concerns if model inversion attacks, such as (Fredrikson et al., 2015), violate the source domain s privacy. It is reasonable to compare ETML to DANN since both methods have conceptually similar characteristics: matching the data distribution and learning the posterior probabilities of the label given a sample. Experiments on a single real dataset are difficult to convince about the generality of UDA performance. **Additional comment after rebuttal**Happy to hear that the authors plan to upgrade their draft. Since the submitted paper is not updated, the reviewer keeps the first rating but also looks forward to read a revised version in another conference or journal.
Reject. rating score: 5. rating score: 5. rating score: 6. rating score: 7. <BRK>Even if $r d$, we still have the theorems valid? The authors should highlight key technical differences in the proof due to the sparsity setting, to make it more convincing about the novelty of this paper. 3.Here is a question for the authors to confirm. 1.Frankly, the technical novelty over the reference Zhong 2017 (Recovery guarantees for one hidden layer neural networks) seems not fully clarified.<BRK>Cons:  The authors prove the above results by adapting the proof of Zhong et al 17. The learning algorithm assumes knowledge of the sparsity mask.<BRK>I am not sure whether the underlying assumption trivialized the problem. The authors develop a theoretical validation of the improved generalization error of the lottery ticket. The results are clearly explained, and the paper is well written. Similar to many theoretical results for neural networks, several assumptions have been made.<BRK>The same comment also holds for your comment after Lemma 1. I am willing to revise my score if the authors give constructive feedback to my concerns. Numerical experiments are interesting and complement pretty well the theory of the paper.
Accept (Poster). rating score: 7. rating score: 7. rating score: 7. rating score: 6. <BRK>Summary:This paper tackles the problem of improving exploration in deep RL for procedurally generated environments, where state of the art exploration techniques typically fail. In the proposed approach, called RAPID, each agent generated episode is evaluated with respect to its local exploration score (for the given episode), global exploration score (across all previous episodes), and extrinsic reward obtained. Episodes with high scores are stored in a replay buffer, and a policy is trained via behavioral cloning on batches of state action pairs from this buffer. I have some concerns and questions (detailed below), but overall I recommend acceptance. Pros:* The approach itself is simple and easy to implement. * The paper is clearly writtten and well motivated. * It seems strange to me that single state action pairs are stored in the replay buffer, rather than keeping all state action pairs from an entire episode together. Using a goal reaching continuous control task would be more relevant: e.g., Swimmer in a procedurally generated maze.<BRK>Summary: This paper focuses on building reinforcement learning agents for procedurally generated environments. In particular it presents a method RAPID to intrinsically reward previously seen *good* exploration behaviors by attempting to imitate them in the future. Pros:1.Procedurally generated environments provide a rigorous test of an agent s ability to generalize as exploration strategies learned cannot be reused as a whole. To counter this, they operate on the hypothesis that high rewarding trajectories are more likely to contain more generalizable exploration behaviors. 2.Given that the idea itself is simple (yet effective!), I appreciate that most of the main paper is dedicated to experiments   with each set of experiments designed to answer a particular research question. Cons: I have a couple of primary concerns mostly dealing with the underlying assumptions made. The global score, being effectively count based, alleviates some of this, but it is likely that this form of intrinsic that is based on extrinsic reward will likely fail in such scenarios. It would strengthen the paper to see positive results in such an environment. Regardless, this paper provides a valuable contribution is studying exploration in procedurally generated environments   with the core idea of rewarding historically good behavior being one well proven in the past. The motivation is clear and the experiments well designed   proving that there are indeed environments where this idea works on, within the limits of the assumptions made.<BRK>The local score is computed per episode, it is the fraction of distinct states visited during an episode, the global score keeps track of the exploratory effort of the agent over the whole training procedure. The authors provided many explanations and figures to illustrate the behaviour of their algorithm. I think the key insight of this paper is using the local score as a reward for exploration and using it with imitation learning. I was actually surprised by the performance of the local score, as it is a per episode metric it seemed to be a weak signal and that would be hard to learn from it. From a global view, the agent should explore the regions that are not well explored in the past", unfortunately while similar states may be encountered they will likely not be equal, a metric defined using raw counts will often not be useful. "From a global view, the agent should explore the regions that are not well explored in the past" This sentence hints at the notion that should be more clearly discussed: generalization. Overall I lean towards acceptance as I think that the paper presented a good idea with some solid experiments even though I have doubts regarding how applicable it can be to more complex environments.<BRK>This paper presents an exploration method for procedurally generated environments, RAPID, which imitates the past episodes that have a good exploration behavior. The authors use the weighted sum of these two exploration scores and extrinsic reward as a final episodic exploration score. This paper is well written, but I have following concerns and questions about the paper. Moreover, the paper also includes experiments on problems with continuous state space. In order to show the effectiveness of the ranking buffer, in addition to show with/without results as shown in Table 1, the results when using the replay buffer without considering the ranking should be compared together. In the experiment, the explanation is mainly based on KeyCorridor, but there is no explanation for what task it is. This paper contains experimental results for various tasks, but It is unclear the contribution of this paper and the reason why the proposed method is effective in very sparse and procedurally generated environments. It seems necessary to explain intuitive motivation more clearly in the paper.
Accept (Poster). rating score: 7. rating score: 7. rating score: 6. rating score: 5. <BRK>In this manuscript the authors derive a bound on the rademacher complexity of neural network models which can be written as a funciton of the MARS norms of the weights in the network. They use their regularization for transfer learning of ResNet 101 and EfficientNetB0 from ImageNet onto the set of smaller image classification tasks. Overall I vote for acceptance. This is an interesting contribution to the literature, providing both a theoretical insight and an experimental test that this theoretical insight is relevant for applications. Pros: 1) Well structured paper with interesting results2) Theoretical results are well justified to be more helpful than existing bounds.<BRK>This paper studies regularization for neural network fine tuning, motivated by limiting deviation of the final model from the initialization states. The provide a generalization bound that utilizes a novel Rademacher complexity term built on the layer weights and their deviation from the initial weights. Using this objective, the authors provide several fine tuning benchmark experiments and demonstrate competitive performance. The analysis appears to be general, without any particularly strong assumptions. Two different norms are considered with corresponding algorithms and experiments.<BRK>The work proposes a Rademacher type bound for the fine tuned models based on the distance between the fine tuned weights and the pre trained weights. Since the distance term shows up in the upper bound on the generalization gap, the authors further propose to adopt it as the regularization term to boost the generalization performance of the model during the fine tuning process. Some experiments are also done to show the effectiveness of the proposed regularization. The function class F_*, by definition, depends on the pre trained weights W_j^0. The randomness of the hypothesis class F_* destroys almost all the derivations the authors are currently using in their proof.<BRK>This paper proposes new regularization methods for fine tuning deep neural networks based on matrix $\infty$ norm distance. Therefore, more evidence might be needed to support their claim. Moreover, the authors empirically show that enforcing a hard constraint on the weights by projected methods throughout the training process is more effective in regularizing neural networks than widely used strategy of adding a penalty term to the objective function. The authors claim that Figure 2 shows that the PGM methods behave as the theoretical analysis predicts and the penalty based approaches are not able to influence the model capacity as much as the constraint based approaches. This statement is inaccurate in several ways.
Accept (Poster). rating score: 7. rating score: 6. rating score: 6. rating score: 5. <BRK>This work adds a SCL (supervised contrastive learning) loss term during the fine tuning stage of RoBERTa. The results show that the model with the SCL term and cross entropy (CE) achieve better GLUE scores than the classic baseline that only uses CE loss, especially when the numbers of supervised training data are small and the data is noisy. Originality: The SCL is not novel because it comes from computer vision but this is the first paper I have seen that successfully applies SCL in NLP tasks. Significance of this work:If the authors can really show that the improvement comes from SCL, it may become a popular tool in the fine tuning stage. You can report it as CE+CE. In the introduction, you cite several studies that mention the limitations of cross entropy loss. Could you show the lambda value for each downstream application? In addition, could you show the final tau value(s) as well?<BRK>Summary* For the fine tuning of pre trained language models, the authors proposed a supervised learning method that combines cross entropy loss and contrastive loss. The proposed method outperforms cross entropy loss in few shot learning tasks and noisy datasets generated by English German and German English translation. Strong points* The proposed loss function is reasonable and the effect of supervised contrastive learning was not reported for NLP applications before, the experimental results are valuable. * The paper is well organized and well written.<BRK>The paper proposes a new training objective for fine tuning pre trained models: a weighted sum of the classical cross entropy (CE) and a new supervised contrastive learning term (SCP). d) This method excels in the few shot setting, at least compared to the CE baseline. There are 4 sets of experiments:1) When training on the full datasets, results are quite modest (+0.4 increase in accuracy on average over 6 GLUE tasks). 4) Finally, the authors look at domain shift; they fine tune a model on SST 2, then apply few shot learning on other sentiment classification datasets.<BRK>The supervised contrastive loss uses a normalization summation over the batch examples, with a temperature hyperparameter \theta. The empirical results cover nicely three scenarios: (a) the impact of adding the SCL loss, in the presence of all the fine tuning data; (b) the impact of adding the SCL loss in few shot learning scenarios; and (c) the impact of SCL in the presence of training noise (induced via back translation through German, using a standard WMT trained MT model). One one result that seems to hold strong is the one for few shot learning, which appears to support the main hypothesis of the paper. I have a few suggestions that could be seen as minor, and a few observations that are major.
Reject. rating score: 5. rating score: 5. rating score: 6. rating score: 8. <BRK>The authors consider self play in zero sum discounted two player Markov games with compact state space and finite actions. The assumption would rather be that \sigma   O(1). Thus the link with Fictitious play is not completely clear. My main concern is that the different assumptions made are a bit ad hoc (sometimes the assumption relates directly on the sequence of policies generated by the algorithm).<BRK>However, I am quite surprised by very important related work missing in the paper. Also, the Lipschitz regularity assumption being made is important enough that it is good to add it in the abstract, as the abstract feels misleading otherwise. And the importance/restrictiveness of this assumption is ideally discussed in more detail in the introduction. This is an important topic and the question studied is an important step to take given that we don t know whether Fictitious Self Play is guaranteed to converge in Markov games.<BRK>The authors proposed a novel entropy regularized policy optimization method for both agents. They proved the sequence of joint policies converges to a neighborhood of a Nash equilibrium at a sublinear rate. The results seem to be rigorous. Since the algorithm is new to the literature, it would be expected to see how it performs compared with other baseline methods in experiments. Equation (2.3) is not entropy regularized. I did not see any discussion in the paper to address this issue or discuss how the neighborhood can be shrunk to a smaller region.<BRK>This paper studies the problem of learning to play a Nash equilibrium in two player, zero sum Markov games. The main result is that as long as the game satisfies Lipschitz and Concentratability properties for each player when the other plays optimally and the policy updates are sufficiently accurate then play converges to a Nash equilibrium. I like this paper quite a bit. 2) I’m not quite clear how to interpret the convergence guarantee in Theorem 4.5. 3) I’m intrigued by the observation at the end that this algorithm is Hannan consistent under stronger assumptions.
Reject. rating score: 5. rating score: 5. rating score: 6. rating score: 6. rating score: 6. <BRK>Summary:  The paper presents a promising idea to build interpretable models by combining discriminative and generative approach. The results shown in Figure 6 of the appendix don t have a thorough caption to illustrate the findings. They should be placed with the main text before the references. Major  •	The paper an interesting and potentially important idea. •	In the Evaluation section, for celebA, the authors identified principal components that correspond to attributes like gender, smiling.<BRK>This paper describes a computational method to construct ideal counterfactuals and isosurfaces via invertible CNNs, and uses it to reveal biases in three different datasets. The use of directional derivative to construct ideal counterfactuals is interesting. 3.The human study is a plus, where the stimuli are based on counterfactual interpolations created by the proposed method. The authors may come up with a clearer presentation. 2.The descriptions about saliency maps are less relevant to the main idea, further confounding the reviewer. 4.The human study may need to conduct another set of control experiments to show that only original training images (not counterfactual interpolations) are $\textbf{less}$ helpful for uses to identify CNN patterns and biases.<BRK>Update after revision I thank the authors for their work on this paper. The second reading was more pleasant. I however still believe that, if not benefitial to the user, the complexity of the method can be a drawback. Weaknesses I have identified several weaknesses of the work that justify my recommendation:  the (lack of) clarity of the text. The single example provided in the main text is appealing but this requires more evidence to me.<BRK>2.I found the structure of the paper confusing and lacking in clear elicitation of contributions. The authors have done a reasonable job at addressing my concerns and I have increased my score from 5 to 6.<BRK>For example, the description of the main method is severely lacking, and is lumped into a few short paragraphs (section 2). I am still unsure about the development of isosurface section. The appendix (Fig.9) also shows that the subjects thought both the proposed method and the baseline were equally good. Overall, the paper has some good ideas and interesting analysis but falls short on clarity and fully convincing the reader about the results. I am marginally inclined for it to be accepted.
Reject. rating score: 2. rating score: 4. rating score: 4. <BRK>3.There is no baseline on any of the standard dataset, not strong supervised baseline on the datasets paper proposed.<BRK>It is a small scale dataset which is used in this paper. Cons:1.Though this paper presents a really focused contribution on training with small dataset, one can see that the paper lacks of in depth analysis on either the target task or the proposed algorithm.<BRK>It is clear that the method is more successful compared to learning from scratch, but I would have liked to see how much worse it is compared to fine tuning of pre trained models. The experimental evaluation in this paper is woefully inadequate. The lower layers are trained to classify the smallest size crops.
Accept (Poster). rating score: 7. rating score: 7. rating score: 7. rating score: 6. <BRK>The paper derives several algorithms for various objectives and value functions, and empirically investigates the deterministic versions. The benefits of doing this are illustrated by some experiments, and the deterministic versions of the new methods are compared with reasonable competitors (DDPG and ARS) in other experiments. The paper is clearly written for the most part, with the exception of some parts of the related work that are overly terse (i.e., the connection with UVFAs could be expanded). I was disappointed to see that only the deterministic algorithms were implemented and analysed. Even if the stochastic versions of the algorithm are only demonstrated in a simple linear setting, that would be better than just not investigating them at all.<BRK>On page 2, in the background section: the discounted state distribution, what you wrote is not a distribution (doesn t sum to 1). I think that this is meant to describe an off policy setting where we are collecting data from $\pi_b$ but want the policy gradient for $\pi_\theta$. Notes:  PVF: to me this acronym is strongly synonymous with Mahadevan s proto value functions (PVFs), circa 2007.<BRK>From this point of view, I agree that optimizing $J_b$ directly is an interesting question, despite the fact that the exact gradient for $J_b$ may be less similar to the gradient of $J$ compared to the usually used approximate gradient of $J_b$ that drops the $\nabla_\theta Q$ term. \+ They discuss a lot of related work. The discussion in the paper is much improved compared to the original version. 6.Provide additional feedback with the aim to improve the paper. However, thisis not the RL objective. They propose new policy gradient theorems for the $V(s,\theta)$ and$Q(s,a,\theta)$ cases (but I believe these to be theoretically flawed). It is the limiting distribution as$t \to \infty$, which is a stationary distribution. I think it would also be importantto show compelling evidence that including the s input helps in learningbetter $V$ and $Q$ functions. Perhaps there are also other ways to bettershow the advantage of the method. Another option may be to change the problem setup, so that the new policy gradient theorems would be more sound. \+ The paper is clearly written.<BRK>I see in the experiments that using PSSVF, policy is converged but am not convinced about it. Having this in mind, my question is that have you tried to find the best set of hyperparameters for ARS and DDPG as well as your proposed method? which is not convincing without showing results. However, as an initial step, PVFs seem interesting and could be beneficial in terms of learning generalized value functions. Overall, I liked the idea presented in the paper and would like to see what their next step would be. I believe the most important weakness of the paper lies in the experiment section.
Accept (Poster). rating score: 7. rating score: 7. rating score: 5. <BRK>This paper introduces a "new" inference method for autoencoder type models, where the encoder is taken as a gradient of the decoder with respect to a zero initialized latent variable. The paper is very well written and interesting.<BRK>The main idea being that existing generative models with an encoder are “redundant” in that the decoder itself has the ability to compute the gradient with respect to a latent vector, z, which itself can be thought of as the “encoding”. Most of my concerns have been addressed in this version. How stable is this for different datasets? Update on the revised manuscript  I have read the new version of the paper and it reads a lot better.<BRK>This paper proposes a new type of generative models with a new inference method of latent variables. Specifically, the gradient of latent variables with respect to zero vector is taken as the inferred latent variables. Pros: the proposed method is easy and straightforward to implement. Overall, the method proposed in this paper is new and promising. However, given the current unclear formulation and lack of strong experimental results, I recommend a rejection.
Accept (Spotlight). rating score: 8. rating score: 7. rating score: 6. rating score: 5. <BRK>Some constructive feedbacks:While the Morse theory itself seems to be interesting, the way it is incorporated with deep learning can be better justified. The MST algorithm is not a functional, is non deterministic, and changes with iterations, so the converging property of the objective function is not clearly studied. The proposed method essentially highlights segmentation accuracy in geometrically rich regions.<BRK>The paper focus on the segmentation of images in which the correct topology of the segmentation an important role plays. While DMT  is not my area of core expertise, I believe that the proposed use Morse structures for the end to end training of a network is novel and a very interesting idea.<BRK>The paper introduces an elegant, yet not fully novel, idea: use a discrete morse complex to define a new loss for image and volume segmentation. Overall, the paper is a variant of Topoloss which has slightly better results.<BRK>## Summary and contributionsThis paper tackles the problem of topological correctness of segmentation of fine structures. This would be very interesting. The result seems to be similar to an increased weight for FP,FN and TP pixels or a decreased weight for TN pixels. This seems to be an open problem and thus speculating about it should be founded in experimental data.
Accept (Poster). rating score: 9. rating score: 7. rating score: 6. rating score: 5. <BRK>This paper proposes a simple but highly effective approach to improve differentially private deep learning, by projecting gradients to a small subspace spanned by public data. The experiments convincingly show that this approach leads to strong performance improvements, and I predict that this simple technique will be applied in many subsequent works. This is easily shown to produce an unbiased estimate of the gradient, which is crucial for performance.<BRK>Paper proposes gradient embedding as a potential solution to boost the privacy utility trade off in differentially private machine learning via gradient perturbation. Main idea is to project the gradients into a lower dimensional subspace before adding noise, so that the curse of dimensionality in DPSGD can be somewhat dealt with. Also, the requirement for a public data is much more acceptable in the provided scenario, where it is only required to get anchor gradients.<BRK>Summary: The paper focuses on training deep learning models under differential privacy. The main contribution is a technique based on  projecting the gradient into a non sensitive anchor subspace and then perturbing the projected gradient (and the residual). Now to reduce the dependence on dimension the authors use a simple idea: reduce the dimension of the gradients using an embedding and then add noise into the low dimensional gradients. They assume the presence of non sensitive data for this purpose. Strengths:1)	The problem of reducing noise in differentially private ML is an important practical problem. 4) How crucial is the choice of picking the right auxiliary dataset.<BRK>This paper studied how to use the information that the gradient lies in a low dimension space to design more accurate differentially private SGD algorithm. Specifically, the authors proposed a new method which is called Gradient Embedding perturbation method. 2.In the experimental part, I think it will be better to have a comparison with the above two papers to see the superiority of the method in this paper. And I want to see more comments about the number of public data to get the anchor subspace.
Reject. rating score: 6. rating score: 6. rating score: 7. rating score: 7. <BRK>The paper discusses the theory behind parallelization of Monte Carlo Tree Search, and establishes some necessary conditions for excess regret to vanish asymptotically. A different algorithm, BU UCT, is introduced in the paper which satisfies, like WU UCT, the necessary condition. The algorithm is evaluated on 15 Atari games, showing performance comparable or exceeding that of WU UCT and other parallel MCTS variants. I find the paper an interesting and easy to follow. Parallel MTCS (UCT) variants are indeed not analysed theoretically well enough (and in fact sequential UCT analysis is not flawless either) and any contribution in this direction is very welcome. This paper spots the importance of proper estimation of visit count and value gap (theorem 1). My main concerns with the paper are:1) somewhat imprecise definitions2)  superficial discussion of cumulative vs simple regret, with citations missing3) the theoretical analysis of WU UCT, limited to depth 2 and nontrivial to extend to unlimited depth, is a minor contribution since behavior of bandit algorithms in trees can be very different than in this special case4)  limited empirical evaluation, on one domain and with settings to chosen for that particular domain. In game settings in particular, the discounted MDP is often *not used* because the reward of individual actions is zero, and the value of a rollout is determined by the final action (win/loss and by what score) only. with the same formula but different discussion. Bibliographic references cite papers on flat bandits rather than trees, and it would be hard to connect cumulative regret for MBA to the formula provided. Simple vs cumulative regret  was analysed  theoretically quite well (https://arxiv.org/abs/1207.5536 AAAI, https://arxiv.org/abs/1408.2048 UAI) and I believe these works should be cited, and the choice for cumulative regret justified in their light. * the theoretical analysis for depth tree 2 is not obvious to extend to a more general settings, and thus its importance is questionable. * BU UCT introduced to highlight importance of design based on the theoretical analysis in the paper, analysed on a single domain with conditions tuned for that domain compared to algorithms chosen by paper authors. Performance numbers in this setting indicate that the domain for empirical evaluation is chosen well in favor of the proposed algorithm, but not necessarily that the algorithm is better than others. There are may be settings in which BU UCT does not perform as well. I would prefer to either see insights (detailed analysis) why other algorithms failed or underpeformed when BU UCT succeeded on selected problem, or an empirical evaluation which gives the impression of being unbiased.<BRK>This paper builds on WU UCT and further explores effective parallelization of MCTS. These conditions are then used for theoretical analysis in simplified settings (tree of depth 2). Furthermore, the paper includes interesting analysis, motivation and results on Atari games. Weak points:  The paper (as well as the previous work of WU UCT) claims that the virtual loss used in ALphaGo [Masteringthe game of go with deep neural networks and tree search. modifies value, while the innovation here is increasing the visit counts. This is not true, virtual loss (as used an described in AlphaGo and followup papers) modifies visit counts in the very same way WU UCT and BU UCT does! The theoretical framework / claims is very limited, and follows easily from the bandit settings. It has limited connections to the MCTS settings   I appreciate though that the authors are very open about those limitations. The necessary conditions really just say "one needs to have the same/similar visit counts (N) and value (Q) as if one ran sequential MCTS". While the theory is not amazing, the motivations are solid and this paper brings some interesting insights. Additional feedback:Please see the virtual loss comment and fix the corresponding text and description, as it not correctly represents the previous work.<BRK>The paper looks at the issue of parallelising Monte Carlo Tree Search (MCTS). While existing techniques for doing this are well known and have been studied in practice, this paper makes theoretical contributions to understanding what properties a parallelisation scheme should have in order to minimise the performance losses inherent in parallelisation. Based on this, they propose a new parallelisation method and provide results showing it outperforming existing techniques. The paper provides a sensible background to previous approaches. While it is rather brief due to the paper size, the important previous work is described at an understandable level. Much of the research mentioned in here has already been mentioned much earlier in the paper   often in greater depth. It would be better to merge anything that only appears in this section into the introduction or preliminary material. The results are first stated and explained in clear language with intuitive explanation of why they should help the search. This is later formalised in terms of regret. The main theoretical result in the paper have proofs in the supplemental material but this is not stated in the main paper   e.g.a reference to appendix C.1 is needed just above or below Theorem 1. The new algorithm proposed is inspired by the theoretical results developed. As a result, the novel algorithm does not have any theoretically guaranteed behaviour, but the presented results indicate that it works well in practice. i.e.are statistics gathered and then the average only backpropagated when N have been seen (and if so what is N) or does aggregation only happen if several workers are currently exploring the same node? As the paper acknowledges the results are tentative rather than comprehensive. The setting doesn t appear in any way deliberately biased, but there is only a single setting of the algorithm considered. Given that the paper also presents both theoretical results and a novel technique, a slightly weak results section can probably be accepted.<BRK>The authors consider a general family of parallel MCTS algorithms, and  derive some properties which algorithms in this family must satisfy in order to converge to zero excess regret on any MDP. They then analyze various parallel MCTS algorithms against these properties and propose a new algorithm (BU UCT) which satisfies them. Empirical results are given on Atari, showing BU UCT outperforming baselines. I was unable to verify the key claim that all of the prior parallel MCTS algorithms are indeed members of the family of algorithms considered.
Reject. rating score: 2. rating score: 4. rating score: 4. rating score: 5. <BRK>In this paper, the authors present an exploration of learning reward signals and how various reward signals can be adaptively used to provide a single reward feedback channel for a reinforcement learning process. I would urge the authors not to present statements about work that they will continue to work on or work towards in the future. These forward looking statements do not serve the purpose of the current work. A great deal of real estate in this paper is committed to background information on related work reinforcement learning and reward shaping which does not serve this paper. This paper feels incomplete to me, and I would like to see this research continued toward a more full submission.<BRK>It is worth exploring more flexible and adaptive mechanisms for reward shaping. Weaknesses: 	The question of how the shaping signals are selected / designed is not addressed directly. When I started reading the paper, I expected this to come from transfer (from similar domains), but in the experiments it seems to be hand designed. There is no standard error on any of the results. Furthermore it’s not clear in Fig.1 that the selected shaping signal is any better than the shaped reward.<BRK>However, one aspect of PBRS, that is introduced in this work, is how to select among multiple shaping signals in an adaptive manner that is also dependent on the current context, in a way that can overall improve the performance of the agent. "Dynamic potential based reward shaping." This would strengthen the technical aspects of the paper that could be quite improved at this stage.<BRK>The paper presents an approach to select the best reward shaping potential signal out of multiple available shaping potentials. This way the effect of the proposed approach could be highlighted, showing how quickly/slowly the proposed approach was able to ignore the negative of good potential function since its not really helping with the learning. The paper is quite unclear. Would that not slow down learning in this case?
Accept (Poster). rating score: 9. rating score: 7. rating score: 7. rating score: 5. <BRK>They also show that NAR models need deep decoders because they need to handle reordering. I think this is an important paper which establishes very strong AR baselines for future NAR work in the field. They also show that certain factors like knowledge distillation need to be applied to both AR and NAR systems. Cons:  One issue I had with the presentation of the results was the selection of different formats and language pairs for different experiments. This might raise questions of whether the authors are randomly subselecting or selecting favorable subsets. I would have liked to see all experiments done on all LPs. this is a very subjective statement and I would tone this down. Section 2.2.2: "Denote respectively by E and D the numbers of encoder and decoder layers."<BRK>The authors advocate for fair comparison between autoregressive (AR) and non autoregressive models (NAR) in non autoregressive machine translation (NAT) research. Pros:  The paper is very well written and easy to follow. Limitations of previous works have been well explained, and the motivation behind their work is clearly evident. The experiments are rigorous. I believe these findings can be valuable to the non autoregressive MT and machine translation community in general. Can the authors comment on why this could be the case? Can this  be because of variations in the distilled data used, or some difference in model training. I believe that 1 BLEU point is too big a difference to be accounted for as a noise signal. I find findings by this paper very interesting and potentially valuable to the concerned research community, hence I recommend its acceptance.<BRK>Summary: Traditional NTM is done with a large encoder and large autoregressive (AR) decoder. They claim that this matches the speed of NAR models while keeping the performance of traditional AR models, making it a better choice in the design space than any NAR models. Pros:1.The results are fairly compelling. 2.The point made about knowledge distillation is also valuable. According to the authors, it "measures speed when translating in mini batches as large as the hardware allows. However, this is emphatically *not* the purpose of NAR models, which are *specifically* for the low batch size case. This could be improved by reducing these to key points: (a) encoders are parallelizable (b) T is a significant factor in NAR models, and is around 4 10 for good performance (c) AR decoders are more robust to using fewer layers than NAR decoders, etc. I think this could be improved by converting it into a table with a "NAR" section and an "AR" section, and each row being a model.<BRK>** Summary **In this paper, the authors present a throughout study on the deep shallow architecture for autoregresstive machine translation model (AR) and non autoregressive model (NAR). (2)	NAR is not as fast as AR when using S_{max} evaluation (i.e., the inference in batch model, where all GPU memory should be utilized). (3)	Knowledge distillation is helpful for both AR and NAR. The experiment results are solid and reproducible. ** Significance **Although this is a well written paper with solid results, I still have the following concerns:1. The following terms might be helpful to you:a. The comparison of their training/validation curves to verify whether the improvement is due to better training/generalization. What if we try more combinations? i.Fix the encoder layer as 6/12 and vary the decoder layers {1,2,4};ii. I think you can briefly summarize the discovery in the paper with examples attached in the appendix.
Accept (Poster). rating score: 7. rating score: 6. rating score: 6. rating score: 5. <BRK>The proposed method yields significant improvements for the linear classification protocol using ImageNet while the improvements for downstream transfer learning tasks such as object detection is marginal. This is an unnecessary overloading of notation. *Novelty*To the best of my knowledge this is a novel approach. It is also surprising that using pseudo pseudo labels to correct the wrong assumptions of pseudo labels is working reasonably well. *Clarity*The paper is well written and easy to follow. *Evaluation*Authors use their loss function with MoCo and MoCo v2 and report relatively small improvements over MoCo v2. It is not possible to form an opinion on how statistically significant these improvements are.<BRK>This paper proposes to add a new consistency loss term to the momentum contrast (MoCo) framework for self supervised visual representation learning. As another limitation, the overall gains are small and it is not clear that they necessarily make MoCo+CO2 the top method. For optimal hyperparameters of this combination, experiments show small but consistent accuracy gains over the baseline MoCo in multiple scenarios: classification and semi supervised learning on ImageNet, as well as classification, object detection, and semantic segmentation on PASCAL.<BRK>The idea of consistency regularization is simple and low cost, but achieves significant improvement on the popular MoCo baseline. Mean teachers are better role models: Weight averaged consistency targets improve semi supervised deep learning results. It is acceptable to me that CO2 is evaluated with MoCo. The improvement of CO2 over MoCo is 2.9% while only 0.5% for MoCo v2. C3: This paper is closed to Mean Teacher [Tarvainen & Valpola, 2017]. If not, $Q(i)\log Q(i)$ works like a regularization term as minimizing $Q(i)\log Q(i)$ may encourage a sharp $Q(i)$.<BRK>(1) This paper proposes a new use of an existing technique (consistence regularization) to a new problem (unsupervised contrastive learning). Given that the technique is basic and well known in semi supervised learning literature, the proposal of its simple use (with no extension) bears little technical novelty. (2) In my opinion, the proposal could be a good practice for unsupervised contrastive learning, but its contribution may not be sufficient enough to make this work as a legitimate full paper on ICLR as one of the top premier ML conferences.
Reject. rating score: 3. rating score: 5. rating score: 5. rating score: 7. <BRK>The paper analyzes joint scalings of the parameter initialization and the learning rate, with respect to the limit of infinite width, in the context of two layer neural networks with stochastic gradient descent and binary logistic loss. To the left of this boundary, it is the NTK behavior as expected in [1]. This covers the neural tangent kernel (NTK) and mean field (MF) scalings, as well as others. Again this has to be done very carefully. I first thank the authors for clarifying the meaning of Condition 2. That is, there is a non zero probability (w.r.t.the randomness of weights) that the claim in the paper for a fixed $x$ fails. The paper should execute the proof very carefully.<BRK>This paper proposed a general framework to derive different stable limiting behaviors of the dynamics of two layers neural networks, under different parameterization of the hyper parameters. For certain choices of hyper parameters, this recovers the mean field limit and the NTK limit. This paper also proposed certain properties of the limiting dynamics and showed that using these properties as the classification criteria, there are only a finite number of distinct models in the limit. For this purpose, the authors advocate the IC MF regime, such that the limiting dynamics are stable with respect to all the conditions that the authors proposed. However, I believe that two more important criteria for good training algorithms are optimization and generalization efficiency, which are not discussed by the authors. There could be some regimes of hyper parameters that do not satisfy the stability condition, but has good optimization and generalization efficiency. For example, if some simple stability property is violated, the algorithm cannot generalize well.<BRK>This is an exciting program, and it looks like the ideas are very good. A (very significantly) revised version of this paper could, I believe, bring much insight to our understanding of neural nets. There is a lot of potential with this paper, just not realized in terms of exposition. For instance, it is not clear what is mean by "finite": does it mean there is a finite limit (what should be expected in principle), that it is bounded from above (what it seems to mean sometimes) or that it is strictly positive (what it appears to mean sometimes)?<BRK>Summary:Main topic of the paper is to study various infinite width limits. The paper notices different scaling used in NTK limit and MF limit and proposes a general framework for studying the limit behavior depending on the scaling. This allows the authors to define more general dynamically stable models in the infinite width limit. Model modification “Initialization corrected mean field limit”(IC MF) that satisfy all identified property of finite width network evolution  Demonstration of IC MF limit approximating finite width network the best among all other possible models. While settings are limited to make theory tractable, there is some empirical validation as well as capacity to broaden the study to different infinite width limits. I believe there are interesting novelties to be shared among ICLR participants who are interested in deep learning theory. The paper makes predictions on scaling limits and finds a limit that could agree with finite networks better (IC MF).
Accept (Poster). rating score: 7. rating score: 7. rating score: 5. <BRK>The authors seek a mechanism to train a spiking neural net to duplicate the function of a non spiking one. This is desirable for energy efficient inference, although the training process becomes challenging due to the discrete nature of the spiking process. They then determined parameters for the modified ReLU that would minimize the deviation between these activation functions, and computed the minimum conversion error (for converting ANN  > SNN). This scales with the square of the threshold voltage for spiking, divided by the simulation time. Next, the authors evaluated their procedure on several different image categorization networks. Nice performance was obtained in all cases: better than using a normal ReLU, or other comparison activation functions, in the "target" ANN. Overall, this is a reasonably nice piece of work.<BRK>Strength: (1) This paper proposes a layer wise optimization method for ANN SNN conversion. I appreciate the theoretical analysis of how to minimize the conversion error. (2) This work significantly reduces the simulation time since long simulation time is usually required for converted SNN to reduce error. I hope the authors can provide more comments on this. The threshold RELU is not defined in the paper which may cause confusion.<BRK>The findings are quite interesting but I still believe that the paper is not well written: the equations are interesting but the explanations between the equations are often unclear. ## SummaryThe authors suggest a relationship between a leaky relu and a spiking integrate and firing neuron model. The method is tested on CIFAR 10 and CIFAR 100, and compared with some other methods for converting ANNs to SNNs. ## Critical reviewThis topic is potentially important since spiking neural network are gaining popularity. But this paper is clearly badly written and it is extremely hard to understand, both in the math and in the text. I don t think it would help the progress of the field to publish the article in the current form. I was therefore hoping to see an empirical study of the difference between the SNN and the ANN: do the activity of the spiking neural network match the activity of artificial network? Since I had not understood the basics of the paper, it was impossible for me to understand the later section about the conversion error.
Reject. rating score: 5. rating score: 5. rating score: 5. rating score: 7. <BRK>Further, the authors show empirical evidence for the correlation of the pruning robustness to the generalisation ability of networks, based on the paper by Jiang et al., 2019 and dataset (updated with additional models) provided in the paper. The authors provide a nice discussion and lots of empirical evidence. Nevertheless, none of these methods improve generalisation when applied on top of already trained network. There are several failures, that make me believe that more work can improve the paper:  The goal of the paper is to show a measure that will perfectly predict generalisation   but according to the experiments it can be outperformed by other measures on the presented dataset. The theoretical justification seem unclear to me: what is the goal of introducing the generalisation bound (moreover even Appendix does not have details of derivation of the presented formula) if the authors notify themselves right away that it is vacuous. The justification is to give an intuition of how pruning connects to generalisation. Section5.4.3 attempts to analyse casual connection between existing measures, that seems to me unclear by motivation as well   one wants to see causal connection to generalization, not other measures. I would suggest to reject the paper, since the idea feels not being worked through enough. Unfortunately, I still find my concerns about motivation for the metric valid, which together with the rather weak performance creates a problem for this paper.<BRK>The problem of understanding generalization in deep neural networks is a fundamental problem of deep learning, and the results obtained by the authors present a potentially interesting perspective on the study of generalization in deep neural networks. On the other hand, there does not appear to be any bound provided for the case of “magnitude pruning”, and it is not immediately obvious why such a measure should lead to a generalization bound. Establishing a bound in terms of the “magnitude pruning” measure would be an interesting contribution, as handling such non random modifications has been a challenge in the community. Finally, the choice of measuring the prunability of a network as a proportion requires more careful justification in the context of magnitude pruning. However, in the “magnitude pruning” setup, with the parallels the authors draw to Occam’s razor and compression ideas, it is the absolute number of parameters which is more theoretically relevant than the proportion of parameters which can be eliminated: having 1 million parameters where half can be eliminated is still more complex than only having 100,000 parameters (where none can be eliminated). With the addition of networks of different depth in the set of networks used for evaluation, the empirical methodology also seems appropriate for the “magnitude pruning” method and the parallels the authors draw to compression. Overall, my opinion remains mostly unchanged, and I share similar opinions to reviewer 3 and 4 that although the proposed idea is interesting and intriguing, the paper is not quite ready at this point. I would like to see the authors present either: 1) stronger empirical evidence for the importance of their metric or 2) a more solid theoretical foundation of the measure they propose.<BRK>In the present work, the authors tackle the highly debated (and sometimes confusing) problem of finding a good simplicity/complexity measure able to predict generalization performance of deep networks. The experimental settings and the evaluation methods for this new metric are inspired by recent extensive studies on deep networks performance. The authors are able to show that prunability is in fact associated with good generalization and seems able to capture some non trivial phenomena (double descent), but they also find it to be inferior to pre existing (margin based) measures. In my opinion, the idea behind this complexity measure makes a lot of sense, but: 1) it is already used explicitly in dropout, so I don t see how it could inspire the development of better training heuristics. 2) It is not easier to measure that most other complexity measures (in the random version). Therefore, I think that this works lacks at least one strong point that could motivate the inclusion of this latest complexity measure into the generalization debate.<BRK>Nevertheless, I feel that it d be nice to have those plots in the paper too (no pressure to produce those plots during rebuttal). ## Paper summaryIn order to understand why deep networks generalize well, this paper proposes "prunability" as an empirical measure that can be predictive of the generalization. Specifically >"prunability is highly informative of generalization across all of our evaluation metrics  outperforming random perturbation robustness, the training loss itself and the Frobenius norm measures" seems to contradict later observations that on some evaluation metrics they are all just as good as each other on adjusted $R^2$. (Certainly not the same as what the submission suggests, but I think worth citing.) ** They do this by showing that there is little causal connection between prunability and all other measures except the flatness based random perturbation robustness measure. Predicting the generalization gapin deep networks with margin distributions. **Update:** The authors clarified all my questions very well. Although I feel a bit lukewarm about the added plot (in that the double descent phenomenon is only somewhat weakly reflected by their empirical measure), I ve increased my confidence score from 3 to 4 to appreciate their efforts in addressing my concerns. 3.The paper is honest and rigorous in terms of the values it reports: prunability is not the best of all metrics, and the paper is transparent about it. These empirical observations are rigorous and would be valuable in understanding the generalization puzzle. Hence, I think this is a good paper worth publishing.
Accept (Poster). rating score: 7. rating score: 6. rating score: 6. rating score: 6. <BRK>## SummaryThis paper studies the stable points of gradient descent ascent with different step sizes. Roughly, this paper s main result is that for any fixed point for the GDA dynamics, that point is stable for GDA with a large enough timescale separation if and only if it is a local Stackelberg equilibrium. This result is quite intuitive since a similar result has been proved by Jin et al.2020.(the timescale had to go to infinity). (I guess there is a difference, but I think this is related work that should be addressed, particularly Section 5.2)### Questions/comments on the appendixI think there is a typo that should be fixed on page 22: you recall that $A \oplus B   A \otimes B + B \otimes A$, which is, I think, incorrect. The results may interest the community. (see my section on questions/ comments)The experiments are not really related to the theory. (see my section about experiments)I have some technical concerns and questions (I would be glad if the authors can answer them; see my section on questions)The conclusion is overstating the results of the paper. ## Overall reviewOverall, this paper is a good paper that should be accepted if the authors fix some statements in the conclusion section and answer my questions about the theory. Some arguments should be added (since $A$ is a real matrix, the non real eigenvalues are complex conjugates) to justify that the only problematic case is when $A$ has $0$ as an eigenvalue. I am quite novel with these notions, so my questions are: Am I missing something? (see for instance [Letcher 2020])  Also, the value of $\tau$ depends on the neighborhood. So it seems that you may have an infinite number of critical points, and the value of \tau to globally (max of the \tau for each critical point) only has local convergence to local minimax may be infinite. In Assumption 1, you suppose that $(w,\theta)$ is an equilibrium.<BRK>Prior work has already established that making the learning rate of one player infinitely larger than other player s learning rate alleviates the cycling problems of GDA and makes game theoretically meaningful equilibria the only asymptotically stable fixed points. The main contribution of this work is that it proves that we can get the same stability guarantees while keeping the learning rates of both players finite. This is crucial for practical applications where using unbounded learning rates in not an option. 2) The proof techniques used for this separation results are, to the best of my knowledge, significantly different and elaborate than the ones used in prior work (Jin et al., 2020). 3) The theoretical findings are complemented with empirical evaluations both on small min max problems and on complex ones like training GAN architectures. Cons:Theorem 28 in the arxiv version of Jin et al.2020 does not explicitly reference the existence of a finite time scale that satisfies their inclusion results. However,  it is clear from the proof of Theorem 28 on page 24 that such a finite time scale separation exists even though they do not provide an explicit formula for it. Of course the result of the authors gives a more direct construction of the threshold $\tau^*$ by reducing the search for it to an eignenvalue problem. Neither proof approach gives particular intuition on how this time scale can be found in a computationally efficient way. However, I am willing to substantially increase my score if the authors address the above concern.<BRK>The main result of the paper states that a strict local minmax point is a stable critical point of t GDA for some large enough t, and that any non strict local minmax can be made unstable by s GDA if we choose s large enough. Major issuesMy greatest concern is that the first part of the main result, that a strict local minmax is stable for t GDA with all large, but finite t, is already known (Jin et al.2020).Specifically, the proof of Lemma 40 in (Jin et al.2020) shows that for all large enough finite t, the Jacobian of t GDA only has eigenvalues whose real part is smaller than 0, which then implies the stability of t GDA for a finite t. From what I can see, the reason why Jin et al.2020 stated their results in terms of infinite timescale separation is because they did not have a uniform bound on how large the timescale t should be, and therefore in general it can be made as large as possible (but finite). The proof in the current submission has exactly the same feature: for every game, there is a finite t that makes t GDA stable, but in general this t can be made arbitrarily large. In the same vein, the converse statement also more or less appeared in (Jin et al.2020); see the proof of Theorem 28, p.24 25 therein. Due to the above, I cannot see the claimed novelty of providing the first finite timescale separation for GDA, hence my rating. The authors claimed that "On the empirical side, it has been widely demonstrated that timescale separation in gradient descent ascent is crucial to improving the solution quality when training generative adversarial networks." I believe this is an overstatement of what we currently know about GANs; see https://arxiv.org/pdf/1711.10337.pdffor a comprehensive empirical study of the effects on the timescale for GDA, which is not as conclusive as the authors stated. F(x_k) here should be a vector valued mapping but the authors seem to view it as a function. I believe these are solid contributions and should be valued. On the down side, I d like to point out that the "practical implication" in this paper is a bit of stretch since the ImageNet experiments are run with RMSprop, whereas the analysis of this paper is highly specialized to GDA. Of course, studying adaptive algorithms in min max games is exceedingly hard and well beyond the scope of this paper. This is directly verifying what the theory is saying, and hence feels more valuable to me.<BRK>The paper studies  the local asymptotic stability of a specific class of solutions points, referred to as strict local minmax equilibria (or differential Stackelberg equilibria), in the case of Gradient Descent Ascent Dynamics with a finite time scale separation. The time scale separation (\tau) is being captured by the ratio of the step sizes between the min and max agents respectively. Recently, Jin et al.showed the set of asymptotically stable critical points of gradient descent ascent coincide with the set of differential Stackelberg equilibrium as the time separation goes to infinity. The paper shows that an infinitely large separation is not needed and some finite but large enough separation suffices. Although this is positive, the results are not particularly surprising given the prior work. The writing of the paper could also be significantly improved. One issue that I had reading the paper is that at times and especially in the introduction the treatment of (asymptotically stable), stable, unstable fixed points seem to be a little ambiguous.The paper only formally defines locally exponentially stable equilibrium in the preliminaries which is a notion that is not used in the introduction. The paper seems to state that any critical point that does not satisfy the definitions of differential Stackelberg equilibria lack game theoretic meaning. This seems like a strong statement. Maybe I am missing something here? [1] has recently shown that alternating GDA with fixed time separation does not converge in the case of bilinear zero sum games but is instead recurrent with the min max equilibrium being stable but not asymptotically stable. I think that due to the tight match between the two settings a thorough discussion is needed. The theoretical results as well as the prior work by Jin et. al are supportive of arbitrary large \tau.
Accept (Poster). rating score: 7. rating score: 7. rating score: 6. rating score: 6. <BRK>The weakest part of the approach is the image model. The previous work section is adequate for contextualizing the paper. The authors evaluate the model on four different tasks, and compare against reasonable baselines. If this is not what the authors are claiming, they should clarify this in the main paper.<BRK>(The authors also include some brief discussion of a GNS model of 3d structures in the appendix, but it seems very different from the model for characters, and I m not sure what the common thread is.) Minor issue: I noticed that the authors appear to have used the ICLR 2020 style files instead of the ICLR 2021 style files.<BRK>This paper introduces generative neuro symbolic modelling, advertised as a probabilistic programming framework in which the distributions are modelled by neural networks. This is a very exciting idea, and well past its time. Thus, while the sales pitch to the paper is very exciting, compared to that, the content itself, which is really just a model of one problem, is a bit disappointing. The section in the appendix detailing the future work on 3D object modelling sounds very promising.<BRK>Summary: This paper presents a generative neuro symbolic model for learning the task general representations. Moreover, the symbolic representations and symbolic module in the proposed method are quite trivial. I would like to raise my scores if the concern is properly addressed. I appreciate their efforts in addressing my concerns and improve the paper. The current version is good enough to be accepted and it also compares with other approaches thoroughly.
Reject. rating score: 4. rating score: 5. rating score: 6. rating score: 7. <BRK>Pros:The paper is well written. The idea is clearly presented. The experiments are clear to understand. The results presented demonstrate that the approach can either match or improve learning when compared to relevant baselines. Cons: Details about the architecture choices used for the baselines are not provided in the main text, which makes it difficult to understand the experiment setup and its results. Drawing out the differences between the baselines and their approach would be useful to help understand the contribution of the work. If so, could the authors describe how this was used in the baseline agents? In this case, what sort of z’s can the encoder produce? 4.As a baseline, it would be interesting to see the learning performance of a simple RL^2 agent on the domains, provided they take in the problem ID as input?<BRK>Summary of the authors claim\The authors claim end to end learning of exploration and exploitation policies can result in a poor local minima. By minimizing the mutual information between the problem ID $\mu$ and the task embedding $z$ which is called the information bottleneck term, the exploitation policy tends to be independent from the task irrelevant information and can reduce the risk of being trapped at a poor local minima. From these experiments, the readers will understand that the existing methods are implemented appropriately and how much the proposed method works well on the common benchmark. Although the authors emphasize that the decoupling of the exploration and exploitation is important in Section 1, Section 4.1 and also in the title, the actual proposed algorithm has coupled loss functions. Thus it is also true for the proposed method. Therefore, it seems for me the statement not only explains the reason why the existing methods sometimes does not work, but also explain the reason why the reinforcement learning is difficult. The condition of Proposition 1 is not clearly stated. In the proof, the authors assume “$\lambda$ approaches 0”, but this is not clearly stated in the condition. It seems for me theoretical analysis does not support the actual proposed method which uses the bottleneck in the optimization. Instead, as I wrote above, the authors emphasize the importance of the decoupling too much. However, based on the theoretical analysis and the experimental results, I think the authors should emphasize more the usefulness of the bottleneck. I also wonder how much the performance of the proposed method is affected by the mapping of the problem ID.<BRK>The authors explain the problem of coupled exploration and validate it through a toy example. To overcome this issue, the paper introduces DREAM, a meta algorithm decoupling exploration and exploitation. In the second step, DREAM learns an exploration policy that is "compatible" with the embeddings generated by the exploitation policy. DREAM outperformed the state of the art algorithms in several experiments. The paper is well written and easy to follow. The idea is clearly explained and justified. I have only a few comments. Despite being supported by your example (Sec 5.2), it is not clear to me whether this is a general problem of coupled exploration. To the best of my understanding, this seems very similar to what done in the mentioned approaches. The exploration policy is also trained to maximize the reward while enforcing the RNN state to be similar to $f(\mu)$. More mildly, even their approach aims to learn to generate trajectories providing information about $z$. The main difference between the two approaches resides in the fact that their exploration policy is trained to also maximize the reward, am I correct? Minor commentsProposition 1: could you explain in more details the need for ergodicity? You mentioned that you could remove the "ergodicity assumption by increasing the number of exploration episodes". Could you clarify this sentence? The number of exploration episodes is a term that does not appear in your current analysis since the reasoning seems to be "in expectation". I didn t check the Appendix C.2<BRK>An exploitation policy learns to maximize rewards that are conditioned on an encoder that learns task relevant information. Then an exploration policy learns to collect data that maximizes the mutual information between the encoder and explored states. Overall, I lean towards accepting the paper, though I am not as familiar with the meta RL literature to have much of an informed opinion about what relevant benchmarks or approaches are. The paper was well written and well motivated, and while the experiments were simple, seemed to highlight the problems that the paper was addressing. It makes sense to separate out exploration and exploitation and I appreciated the inclusion of tasks that helped motivate this point. Furthermore, the paper provides a theoretical analysis of DREAM showing that the decoupled policy maximizes returns. The paper only considers exploration in the context of meta learning but of course exploration is a central problem in RL and several approaches have studied it outside of Meta RL. The paper would be improved by discussing such approaches (for example intrinsic rewards such as empowerment [1] or surprise [2]) and/or evaluating how well these approaches compare to DREAM when trained alone and combined with vanilla algorithms. Questions:1) How was the decay rate for epsilon chosen in Figure 3? Could the authors provide more insights into this?
Reject. rating score: 3. rating score: 4. rating score: 5. rating score: 5. rating score: 6. <BRK>The authors propose a VAE type generative model approach to characterize the hidden factors, with a divided focus on the global and local reconstructions. The claim is that the learnt hidden representations are disentangled (which is not defined clearly) using two reconstruction terms. The setting of the problem adopts the graph VAE setting in [1,2] (which I think the authors should mention in the related work), and the ELBO & local aggregation (convolution) approaches used in this paper are relatively standard in the generative modelling and graph representation learning domain. Apart from the limited novelty, which would not have affected my evaluation if it solves the problem as claimed, I have several major concerns about this paper:1. The notion of disentanglement is not well defined in the first place. In the VAE setting where the hidden factors are stochastic, does disentanglement refer to independence? Given the high expressivity of deep learning models, the local factors can easily manage both tasks, or the global factors are merely enhancing the signals of the local factors. [2] Xu, Da, et al."Generative graph convolutional network for growing graphs."<BRK>The writing is well and easy to read. However, it does not meet the condition of acceptance from my point of view. Some important related work is missing. Also, they do not make a performance comparison with the methods above in experiments. Arxiv 2020  Disentangling the global and local generative factors graph representation learning is important. However, the authors didn t explain the definition of “Global” and “Local” factors clearly. It would also be better if they can show an example of global/local factors when generating graph. The experiments are missing. I have some concerns as follows. How can this method prove that each factor is necessary for the generative process? Based on the above reasons, this paper can have much more improvement.<BRK>Specifically, the authors propose a GL Disen model based on graph VAE architecture to jointly learn global and local representations for a graph. The global information is shared across the whole graph while the local information varies from patch to patch, corresponding to common and local factors, respectively. 2.The proposed method generalizes disentangled VAE into graph data to disentangle common factors from the local ones. 6.As shown in Table 1, though the proposed method outperforms other GNNs, it does not always compare favorably to kernel based methods such as GCKN. I will be happy to improve my scores if authors can address the above questions. I have updated my score considering the paper has improved its quality after the revision (adding more experiments/baselines, comparison with the literature, etc.). Thus, I am also wondering how the authors implement Graph VAE in the rebuttal phase and whether the improvement of their proposed method over Graph VAE is really from disentanglement or the differences in the autoencoder.<BRK>In this paper, the authors proposed a disentanglement learning based approach for unsupervised graph level representation learning. The extensive experiments and analysis  show that our method achieves the state of the art performance on the task of unsupervised graph representation learning. The paper is well written and the disentangling factors can benefit the unsupervised graph representation learning. 2.The performance of this work is good compared with the state of the art baselines. The idea is not very novel. For example, two important assumptions 1) a  global and local factor for graph analysis 2) local latent factors are independent. Those two assumptions actually have been explored in unsupervised learning tasks. It seems that migrating this idea under graph is straightforward. In Conclude,the authors propose a VAE based learning algorithm to disentangle the global graph level information.<BRK>In this paper, the authors proposed to disentangle the global level information from the local level one to reduce the effect of the irrelevant information. Overall, I like the idea of applying unsupervised disentangled learning to graph level representation learning. It is common for graph representation learning methods to be tested on other tasks, such as graph similarity/distance computation and graph level clustering, in order to draw a general and convincing conclusion. KDD 2020.3.The paper mentioned that the global and local latent generative factors are sampled from their respective posterior distributions.
Accept (Poster). rating score: 6. rating score: 6. rating score: 6. rating score: 5. <BRK>This paper empirically demonstrated the efficacy of DialoGraph on non collaborative negotiation dialog tasks, and the model is evaluated automatically and human evaluation. Training to control for the pragmatics of the negotiation dialogues has been less studied, and it is a crucial and interesting topic to build a logical dialog system. 2.This paper is well structured and well written. 3.Introducing GAT and ASAP to model strategies and dialog acts is novel, and the interpretation of their results is interesting. 4.This paper also provides soundly experimental results and comparison with other SOTA models. The dialog task problem setting seems unrealistic. A tuple of utterance, dialog act, and strategy at turn $I$ is given, and based on the previous tuple sequence the model predicts the next response, dialog act, and strategy. However, in reality, the user’s dialog act and strategy are hidden. I agree this paper followed the task as previously defined, but it seems awkward. 3.HED+Transformer vs. DialoGraph: Those two models show similar experimental results. According to the experiment configuration, the HED+Transformer used 6 decoder layers, whereas the DailoGraph used 2 graph layers. Those two models were fairly comparable with respect to the number of parameters? Or, the proposed method based on GAT+ASAP does not effective as much as Transformer, because both are basically based on attention mechanisms? Does it solely depend on the language model (i.e., the decoder)? 4.Please address and clarify the weak points above.<BRK>This paper deals with the problem of natural language generation for a dialogue system involved in complex communication tasks such as negotiation or persuasion. The proposed architecture consists of two encoders: one for the utterance and the other for dialogue acts and negotiation strategies. The decoder is an RNN that converts the encoded vectors to the output utterance. Each utterance is first passed through BERT to get an utterance level encoding. The sequence of utterance encodings is then passed through an RNN to generate a conversation level encodings. The negotiation strategies and dialogue acts in a conversation are represented using a node edge graph, where the nodes are one of the N different strategies/acts and there exists an edge from node a to node b if an utterance with strategy A precedes any utterance with strategy B. The proposed architecture is evaluated on the CraigslistBargain dataset and compared against Zhou et al.2020.The paper is very clearly written and the experimental work has sufficient detail to ensure reproducibility. The main contribution and the novelty of this paper is in the use of graph neural networks for encoding dialogue acts and negotiation strategies. This choice was mainly because it helps with better interpretability of predictions and this is demonstrated anecdotally in section 5. The proposed model shows better performance in three different metrics when compared to sota from Zhou et al: (1) prediction of dialogue acts and negotiation strategies, (2) on the downstream task of dialogue generation, and (3) human evaluation to quantify the quality of generated language. There are a few aspects of the paper unclear to me and could use more insight from the authors. I am not sure if the model evaluation should entail predicting negotiation strategies which are in itself predictions of a different model. How were the train/test/dev splits done? 2.If I am not mistaken, it seems like the model uses predictions from time step (t) to predict and generate for time step (t+1). 3.I think it would be clearer if you were to use a single letter variable for strategies. 5.How many conversations were used in human evaluations?<BRK>This paper proposes a end to end dialogue system that leverages Graph Attention Networks to model complex negotiation strategies. The main contributions that the author claims are to model negotiation strategies through a GNN and using these learned strategies to predict future strategies and generate a response leads to better negotiations. The end to end model contains a traditional hierarchical encoder to obtain contextual representations along with a structure encoder that is designed to model strategies and dialog acts and obtain structural representations. The decoder is a simple GRU that produces the response by conditioning it on the contextual and structural representation along with the previous word. The aspect of generating a response based on the prediction of negotiation strategies and dialog acts is an interesting approach. 2.The results from Table 1 show that transformers perform comparatively or even better than DialoGRAPH. What is the overall gain on using the GNN if the transformers match their performance? 3.The results in Tables 2 & 3 are hard to interpret. Prior research has shown that BLEU and other automated metrics are not good enough to evaluate the performance of dialog systems. How many participants were recruited? Was any ablation done with this to determine the optimal number or how this split affects the outcomes? was this done through a human annotation process? 4.What decoding strategy was used? Section 2.4 has inconsistencies in the usage of notations. 2.Can the bolding of scores in Table 1 be made more consistent and highlight only those scores that are higher and lower based on the metrics being represented After reading the authors response  I thank the authors for answering all the questions that been raised by the fellow reviewers. Looking at the responses and changes made to the paper, I have increased the score from 5 to 6 after the authors clarified the issues I had with the paper. Overall this paper demonstrates the effectiveness of using a GNN for negotiation dialogues. I feel that this approach can be applied for any non collaborative dialog settings and the claims of interpretability make this approach better.<BRK>* Quality:The motivation to introduce pragmatic information into negotiation dialogues is clear. The model is straightforward and effective. But the idea of directly applying Graph Attention Networks is not super exciting. * Clarity: The paper is in general written clearly and easy to follow, with a few points to improve listed below. And what s the cut off threshold for the edges in Figure 3? 3.In section 4, "(several results are in bold if they have statistically insignificant differences)"It s not clear which results are significant and which are not from the tables and the descriptions4. I d appreciate more dialogue examples in the appendix. 5.How hard is it to optimize the graph networks? Is the training stable? * Originality:The model is not very novel as Graph Networks have been used a lot in dialogue models. * Pros:1.The interpretation of the learned strategy graph is useful, but that section needs a bit more work in clarity. 2.The use of Graph Attention Network improves the negotiation dialogue system. * Cons:1.The model is applied on one negotiation dataset only and requires both strategy and dialogue act annotations, which may not be available in other datasets, making the model not so generalizable to other tasks. 2.From the results, it seems the major performance jump comes from using the embeddings from pretrained language models (transformer, BERT, etc)3. 4.It seems there is no connection between the "strategy structure encoder" and the "dialogue act structure encoder"? Maybe adding a connection between these two modules would give a better performance since dialogue act and strategies can be related? Also, are they related at all?
Reject. rating score: 3. rating score: 4. rating score: 5. rating score: 5. <BRK>The paper also further investigates several additional properties and provides empirical studies. Concerns:(1) Without going into the details, the main problem of the paper is that it only gives upper bounds on the sample complexity. When comparing the upper bound of two algorithms, the comparison does not show which one is better. (2) The paper proves the result only for a specific non attention model. Without further justification, it is very unclear why this is enough to represent other non attention models. The theoretical results are not sufficient to support the claims made in the paper about attention models.<BRK>The theoretical analysis reveals that using neural networks with self attention can have lower sample complexity than standard 2 layered neural networks when the data is in fact generated by a two layered neural network with concentrated self attention weights. The paper goes for too many results at the cost of readability. 3.There are also a number of other grammatical mistakes that should be fixed to improve readability. 3.Better sample complexity of attention model comes from the assumption that the true generative model has concentrated self attention weights, i.e.most of the attention weights are concentrated in a few attention masks. All these assumptions make assumptions on not only the true weights (the weights of the NN generating data) but on the weights of all NNs in the model class. If this needs to hold for all w s (and not just the true w s) then this is a very strong assumption. Why use $\gamma^2$ ?<BRK>The authors did not rigorously prove that their model does not suffer from scaling issues. For two layer neural networks, under all the assumptions mentioned above, the authors proved that the model with a fixed attention mask has a smaller sample complexity upper bound compare to that without an attention mask. Questions for the authors:1. 3.The theoretical proof and experimental methods appear to be correct and reasonable. Cons:1.The authors seem to use attention somewhat like a black box. This makes the attention mechanism in the proof essentially the same as the known fixed function f. However, I think one of the core parts of the attention mechanism and part of the reason why it works is the learning process of the query, key, value matrices, which is ignored by the authors in this paper. 2.Similar to 1, the authors also hide other properties of the loss landscape in assumption A3. 3.The comparisons in this paper are about the upper bounds or lower bounds, but there is a lack of evidence about the tightness of these bounds. Therefore, it is somewhat unfair to do a direct comparison between the upper bounds.<BRK>The theoretical findings of the paper can further provide guidelines for designing attention models. The theoretical results rely on a few assumptions, which I am afraid are quite strong. 1.(A1) assumes the data is generated by the attention model plus sub Gaussian noise, which seems very strong to me. In other words, the sample complexity comparison (Theorem 1 vs Corollary 1) is not fair to me. 3.Why is the assumption (B1) hidden in the text but not listed in parallel with other assumptions? My feeling is that the author makes strong and hard to interpret assumptions, in order to apply the classical learning theory to show the results.
Reject. rating score: 5. rating score: 5. rating score: 6. rating score: 6. <BRK>2.In Table 1, MAP is a better method than ACNML for the MNIST dataset. 3.In Table 3, it misses the results of ACNML KFAC for WideResNet28x10 dataset, and it seems that the proposed methods don’t outperform other existing methods. The author didn’t use the experiment to show the accuracy of CNML distribution and ACNML distribution. Since in equation 18, the given bound of the approximation error increases with k, we don’t know whether this approximation is well bounded.<BRK>ACNML is a tractable approximation to CNML that leverages Bayesian inference by using an approximate posterior in place of optimizing over the dataset during inference. The analysis of the second order approximation in Section 3.2 though interesting does not seem to impact the proposed method (as implemented) and the experimental results.<BRK>This paper explores the application of an alternative inference scheme for learning well calibrated predictive models especially target to ut of distribution samples. Strong points:  The paper explores the use of alternative and well founded inference approaches. Empirical evidence about the advantage of the method is limited. It is based on very strong assumptions which do not hold on reality. Missing Eq.reference in Pag. I think this paper makes a novel proposal which deserves to be published.<BRK>Overall, a good paper which advances the state of the art on an important problem (uncertainty estimation and calibration) in neural networks. The paper is well written, easy to understand and the authors are well versed with the domain (the writing clearly demonstrates scholarship, knowledge and expertise). The algorithmic advances (Algorithm 1) are not that novel.
Accept (Poster). rating score: 7. rating score: 7. rating score: 7. rating score: 5. <BRK>## Weaknesses and Questions for authors:* Intuitive explanation as to how $f$ was designed is missing. Based on the results from section 4.5, this overhead may not be that much, but still needs to be quantified. The same goes for hardware analysis. * Hardware analysis of possible cost savings for reduced accumulator implementations in a MAC unit.<BRK>This paper explores to solve an often ignored issue in quantization: accumulation precision. The cyclic method proposed by the authors at the first glance is not intuitive. As the accumulation length increases, the events of overflow could rise sharply, and the training could fail without room for the cyclic method to optimize the slope k.(3) Some comments on the relation between the accumulation length and bit packing would also be helpful. The paper attempts to solve the overflow problem, although not perfectly, with a differentiable "failure" approach. Therefore, I recommend the paper to be accepted on the condition that the authors could address my comments fairly.<BRK>This paper presents a method (WrapNet) for the problem of efficient low precision inference. In results Table 6, why we are not given results that would be comparable to BWN QA and TWN QA. The significance of this work lays in the fact that this is the first paper that allows lower precision accumulators while still demonstrating quite good performance, even in the case of demanding Imagenet classification. Actually the performance is almost as good as with other low precision schemes, which use high precision accumulators.<BRK>Summary:The authors propose a scheme named WrapNet to further reduce the bit width of accumulation operations in the deep neural network inference process. The overall idea makes sense and the proposed method can reduce computational resources and speed up computing greatly in the accumulation operations. However, it is more like an empirical adjustment. Can you do some experiments for comparison and give a detailed explanation of the proposed method used in the DNN model without batch normalization. Can the proposed method be applied into the model from scratch given the estimated step size?
Reject. rating score: 3. rating score: 3. rating score: 4. rating score: 4. <BRK>They did not compare with the state of the art methods to predict the binding affinity. 1.There is no novelty in the methodology part.<BRK>However, for the energy only feature encoding the model developed in this paper always performs the best   it is important for the authors to explain why this result is of interest to the reader, since the performance using this encoding is always worse than the performance of multiple other models using the hybrid encoding. The authors present results for both  hybrid  and  energy only  models.<BRK>Considering writing "Protein Graph Convolutional Network" here, so that it fits with the abbreviation. **Recommendation**I recommend this paper be rejected. Is there something missing in this sentence? Should one of these have been the "edge matrix"? I m also not confident about how significant the reported results are.<BRK>However, the evaluation, results, and model development are weak. However, there are no baselines from previous literature, so it is difficult to place this work in the context of the field.
Reject. rating score: 5. rating score: 6. rating score: 6. rating score: 6. <BRK>***Summary:The paper addresses learning the ratio between two densities from their samples, with applications to outlier detection and covariate shift adaption. Third, "Computation" would be an important aspect of the proposed heuristic. A particular issue of such an approach (that the present work aims to resolve) is "train loss hacking," meaning that the empirical loss can become arbitrarily large and negative. For the experimental results  completeness, it also seems reasonable to add relevant details about implementing the proposed algorithm. ***Reasons for score:I think the paper is marginally below the acceptance threshold of ICLR. Learning the density ratio is a well motivated problem with an objective different from density or functional estimation. The paper attempts to resolve a particular issue for employing flexible hypothesis families in density ratio estimation with BR divergence loss. It is unclear to me how strong those theoretical claims are. Note that this assumption translates to $0<R<1/C$. It would be nice if the author(s) can address some of my concerns in the rebuttal. In terms of sample size dependency, the discrepancy upper bound is $\mathcal O(\max \\{ 1/\sqrt n_{\text{de}}, 1/\sqrt{n_{\text{nu}}} \\} )$. In particular, does any constant in the theorem, e.g., $\kappa_1$ or $\kappa_2$, has non polynomial dependency on $C, B_p, L$, or $B_{W_j}$?<BRK>The paper addresses an issue that arises in a particular formulation of the density ratio estimation problem. Namely, when one tries to directly fit a density estimation ratio, while minimizing Bergman divergence, it may be that overfitting causes the minimization problem to diverge to minus infinity. The paper suggests some form of regularization that uses a bound on the ell_infy norm of the ratio function. There are other lines of research that attack the general problem of density estimation, some of them very related. It seems to insist on a particular formulation of the problem and then find ways to make it work, but the writing is not clear and important related work is missing. What is train loss hacking and why is it unique to this formulation of the problem? What does that mean? 5.I thing the assumption that the ratio is bounded is a reasonable one.<BRK>##########################################################################Summary:The paper studies density ratio estimation (DRE), addressing the  train loss hacking  problems which often arise and hamper estimation when models are too flexible. The authors propose a new risk estimator for DRE, providing a non negative Bregman divergence estimator, with the non negative correction. Although the non negative correction idea itself is not new, as the authors mentioned in Section 1, using the prior knowledge for the non negative correction appears novel in DRE. I acknowledge these are highly technical results, but I believe more theoretical analyses of the approximation error would improve the paper. Based on the theories in Section 4, the current algorithm provides an estimator that has a vanishing estimation error. Following the previous question, Figure 3 in the appendix shows the proposed algorithm does not diverge to the negative infinity, but it is not clear if the Bregman divergence evaluated at the proposed estimator is converging to the  optimal  Bregman divergence value. ##########################################################################Overall, I recommend the weak acceptance. I may well have missed some points in my reading, so clarification is welcome.<BRK>STRENGTHS:  This work introduces a new family of non negative Bregman divergences. The proposed estimator is theoretical justified. The paper is well written, although quite difficult to follow without some theoretical background. Shall I therefore understand that estimating the density ratio through classification, using the cross entropy (BR_BKL), does not suffer from this the train loss hacking issue? While appreciate the estimation error bound for D3RE, can you comment on the error bound for the original Bregman divergences? Some recent applications of density ratio estimation that could have been worth mentioning as well as likelihood free inference approaches based on likelihood ratios.
Reject. rating score: 3. rating score: 4. rating score: 5. rating score: 6. <BRK>Summary: this paper claims to show that the “mathematical formulation” of SDEs is “directly comparable” with the formulation of GANS. I’m also confused by the analogy between sampling SDEs and GANs — one might as well draw analogies with sampling Gaussian distributions.<BRK>This paper connects SDEs and GANs and proposed to learn the drift and diffusions in SDE under the framework of GAN. The authors also show how to efficiently simulate the adjoint process and sample the Wiener process. I would like to know more on the properties of the proposed GAN formulation of neural SDE, and I suggest that the authors summarizing the efficient computation part into another single paper, and focus more on the neural SDE as GAN in this paper. Also, some generative results is preferred, if there’re proper settings (e.g.some video scenarios), rather than the prediction results in the table. And as a result, I think the current version is not ready for publication.<BRK>The paper is topical, and the theme should be of interest for the audience of ICLR. Novelty.Even if I found the paper interesting, I can not quite agree with the novelty statement. I found the statement (i) just simply misleading (see #2) and I recommend that you would consider revising this. The technical details related to (iii) are interesting. 2.SDEs are not GANs. 3.Practical impact not reflected in experiments. Throughout the paper you present Stratonovich SDEs rather than Itô SDEs which could be regarded more standard in most related ML publications.<BRK># General statementsThis paper introduces an interesting parallel between SDEs and GANs, and pushes the analogy to its practical implications as a way to learn neural SDEs. Still, I believe that there could be some improvements to do. (what is the value of P ?) Are they guaranteed to exist ? That would make the paper more self contained## Efficient computation* Section 3.1 (rough adjoint equation) is harder for me. I m ok with the adjoint equation.
Accept (Poster). rating score: 8. rating score: 7. rating score: 6. rating score: 6. <BRK>### 4.Related papers that you might like[1] You cite Stiffness: A New Perspective on Generalization in Neural Networks by Stanislav Fort, Paweł Krzysztof Nowak, Stanislaw Jastrzebski, Srini Narayanan (https://arxiv.org/abs/1901.09491) as measuring an amount of class specific clustering. ### 2.Strengths* This is a super interesting question and I really like the paper overall. * I appreciate that you looked at a large hypercube of hyperparameters to establish correlation with generalization* I also like the care you put into establishing causality* The fact that you tried a simple variance based measure is also really good, especially given that it is very predictive! ### 3.WeaknessesI think this paper is really good, I have nothing much to point out here.<BRK>(and not bimodal as in the example?). Perhaps the rebuttal will help. Any theoretical or empirical support to this? Overall, the authors have addressed sufficiently most of my concerns, significantly improving the manuscript. Contradicting evidence could spark interesting discussion / research in the community. Fig.3: Can you discuss whether you think the values returned by the measures (y axis) suggest the existence of actual intra class clustering, and support the main assumption of the paper? (e.g.in an appendix).<BRK>My intuition was that at the beginning of training, there are probably some neurons that is lucky and separate the intra class clustering well. And criterions based on those correlate well with model generalization performance. However, I think the current state of the paper is still a bit below the standard of ICLR, and there are a few ways that this paper could be potentially improved.<BRK>Re: figure 4, the question is why the performance of the green curve is so bad. Does this mean good models are just memorizing prototypes? The paper presents an interesting idea that also seems to be practically highly relevant based on the experiments. A large number of my concerns have been addressed and the quality of paper has improved significantly.
Reject. rating score: 5. rating score: 6. rating score: 6. rating score: 6. <BRK>In this paper, the author proposed a variational framework for relational learning that decouples relational property and absolute property among objects based on a pre defined probabilistic graphical model (PGM). The author also proposed the so called relation preserving data augmentation (RPDA) strategy to address the challenges for the resulting optimization. Overall, the paper is well written and easy to follow. Below are some of my concerns. 3.Experiments seem to be too simple. 4.Can the author clarify the relation between the proposed PGM based relation learning framework and causal learning?<BRK>This paper proposes a model to infer the relationship between multiple instances in a dataset by inferring a latent variable. The authors accomplish this by defining an optimization problem that optimizes the ELBO of the proposed graphical model. The paper presents a nice solution to some of the identification issues that can arise when inferring the latent variable, in particular the so called “information shortcut” when the model overfits to only learning the “absolute” property of the dataset, rather than inferring the shared latent traits. I think that the formulation that the authors present and the proposed  optimization model is quite interesting, however I have additional questions / concerns:It’s not clear exactly what the authors mean by a relationship. Why is there a latent variable causing b but not a? According to figure 1 z has no effect on a, so it doesn’t seem like it would describe the relationship between a and b? It would appear that the authors are proposing a model to integrate a latent factor model into a generative net, which is interesting, but does not come through in the text as it currently reads. It would seem that in order to interpret z as a relational variable we should be able to extract some kind of meaning from it? I found the experiments to be a bit underwhelming. It is unclear how the authors decided on the architecture, hyperparameters, and number of latent variables for the proposed model. In addition, it seems that some of these evaluations would benefit from comparison to more traditional methods. Overall, I think this is an interesting idea, but I would like to see the paper a bit more refined before recommending acceptance.<BRK>Relational learning is an important capability exhibited by humans of learning relations between objects. This work considers a fairly general setup for relational learning and addresses learning of the relation through a method based on variational inference. I am not familiar with the VAE literature; having said that, the method seems novel. The authors are encouraged to look into adding more sophisticated and unique examples, and further analyzing their method. Of course, given that the objective is a lower bound of the probability we want to optimize, and since no theory is provided on the quality of the approximation, it is important to have strong demonstrations of the method in which it is shown to have unique advantages compared to existing approaches. The idea of RPDA is interesting, and in many cases it could be applied at least via data augmentation. However, the example has b as the MNIST label. Presumably the causal assumptions should be correct for the model to work well? It would be helpful if the authors could comment on this.] The Yale face dataset in the supplement seems a better fit to the causal assumptions. This is obviously an important difference, but one could reduce the problem of learning _z_ from _a_, _b_ to methods that learn _z1_ from _a_ and _z2_ from _b_ separately. Yet this again indicates that the examples used in the paper fail to show the uniqueness of the proposed approach.<BRK>The paper proposes variational relational learning by learning relations between two inputs via variational inference on a probabilistic graphical model (PGM). The example shown in the experiments is rotational mnist, where b is a rotated version of a, and z should encode the degree of rotation. In section 3.3 the paper describes two sources of problems with naively optimizing based on the above approach: 1) deterministic mapping where a completely determines b, and no learning of relation is necessary, and 2) information shortcut where z can completely encode b. To me 1 does not seem like a big drawback since it is more of a limitation with the underlying data. The experimental results seem reasonable, although having setting beyond mnist would have been nice. The two methods described to handle the information shortcut issue are not super satisfactory in my opinion. The alternative of using an informative prior is not super clear to me, since it seems that with a powerful enough model p(b|a,z), scaling and shifting p(z) to fit a Gaussian will not prevent the model from completely encoding b as a function of z. I would be interested in seeing the experiments done using discrete z s, which seems natural too given the 5 discrete rotations considered in the paper.
Accept (Poster). rating score: 8. rating score: 7. rating score: 7. rating score: 4. <BRK>The supplementary material contains some analysis on the effect and sensitivity of the value of $\beta$ on the performance of the algorithm. This should be extended and presented in the main paper. Further, GVCL is augmented with FiLM to alleviate weaknesses of VCL and GVCL.<BRK>While the Film layers per se also appear somewhat ad hoc, their empirical benefits  particuarly when paired with the lambda elbo, are impressive and well put together. Criticisms:While I really enjoy the derivation of the beta elbo in the zero limit, I found the introduction of the reweighting terms in Sec.2.3 to be ad hoc and not particularly well justified. The authors might want to consider tweaking the title to sth that is closer to the paper s actual contributions. This by itself is an interesting contribution.<BRK>I need clarification what the authors are doing here. Experiments:The experimental evaluation is thorough and seems promising. I am no longer sceptical that the claims regarding the equivalence to EWC in case of beta 0 is correct. However, I am quite confused what exactly the authors do here and there could be a major mistake:From the paper, I am not sure if the authors a) compute Laplace’s approximation in the end, at the resulting mean of q, for any beta value?<BRK>The authors also acknowledged this point and claimed that this is due to the difficulty in optimizing GVCL with small \beta. Various experiments are performed, showing some level of advantages. Cons:1.The new perspective that online EWC could be viewed as a special case of the GVCL framework is lacking preciseness. However, as shown in experimental results, e.g., Table 1, GVCL alone performs worse than Online EWC in large datasets, which is really wired.
Reject. rating score: 4. rating score: 5. rating score: 5. <BRK>This paper proposes a MCM aware twin least squares GAN (MTGAN) model for hyperspectral anomaly detection. 3.In the experiment part, to make the proposed method MTGAN more convinced, the authors are suggested that using GAN based anomaly detection methods and background dictionary construction based method as the compared methods. The component of MTGAN is independent.<BRK>The authors observed that their proposed twin least square loss can alleviate the GAN s gradient vanishing problem: it would be nice to evaluate this phenomenon for the generation with GANs and inspect the quality of the generated images   Providing experimental results on the outlier detection datasets would be very helpful. [Summary]In this paper, the authors proposed the MTGAN framework, a GAN based approach to the task of anomaly detection in hyperspectral images.<BRK>Overall, I vote for 5: Marginally below acceptance threshold. The method is novel and the experiments prove the effectiveness of the method. My major concern is about the clarity of the paper, and the explanation of some formulas is not clear enough.
Accept (Poster). rating score: 7. rating score: 6. rating score: 6. <BRK>if coarser variants are better, what happens when the entire weight matrix is treated as 1 block (so you just learn a scalar weight)? what about learning a single scalar for each weight in the FFN (i.e.block size is 1)? This allows one to fine tune only a small number of parameters and end up with a model that performs quite well on all tasks at the same time, not much worse than fine tuning the entire transformer model on all of these tasks. I guess the "intuition" is driven mostly by empirical results, which I suppose is ok but may be worth digging into a bit more.<BRK>This manuscript presents a HyperGrid Transformer, which is engaged in learning a single model to account for multi tasks in NLP. The core idea of HyperGrid Transformer is to learn task conditional dynamic weights in a grid wise manner in the feed forward layers, where the weights are factorized in local and global components. However, the conducted experiments look nice, showing promising performance on GLUE/SuperGLUE.<BRK>The authors propose HyperGrid Transformers with a decomposable hypernet work that learns grid wise projections to specialize regions in weight matrices for different tasks. Pros:1.The idea to make use of decomposable grid wise projection is interesting. 2.The proposed method has been widely evaluated on GLUE/SuperGLUE tasks, and achieve good performance. When using a single model as baseline, how many layers are shared across tasks? ####update####The experiment results are not surprised, but strong enough.
Reject. rating score: 4. rating score: 5. rating score: 5. rating score: 6. <BRK>This paper proposes a new method to compensate for "hindsight bias". Do either of the domains chosen for the goal conditioned rl tasks have stochastic transitions that would lead to a need for UVD? The results in this paper we only shown for a single environment.<BRK>This paper proposes a method for solving (state distribution matching) imitation learning and goal conditioned reinforcement learning (GCRL) by training a value function both with standard TD learning and Monte Carlo (MC) learning. Overall, the paper studies an interesting connection between GCRL and imitation learning, and the idea of training a goal conditioned value function via MLE seems novel and intriguing. I have concerns that the contribution of the paper is a bit unclear, and the overall performance gains are rather small.<BRK>Summary: This paper focuses on goal conditioned policy learning in the environment with stochastic dynamics and tries to address the bias of HER. Experiments show that the proposed method outperforms baselines in both goal conditioned policy learning and imitation learning. Second, in section 3.3, "if the goal is within the time horizon of the density estimator....". Pros:*The proposed method is well motivated to solve a problem of HER for goal conditioned policy. Experiments will be more convincing if the proposed method can outperform the baselines on various domains.<BRK>The estimator is integrated with TD learning and actor critic algorithms for goal conditioned RL and imitation learning are derived. I do have some concerns which (currently) hold me from rating this paper higher:1. Unifying goal conditioned RL and IL under a common view of training an agent to reach desired states and leveraging density estimators to achieve that is an interesting approach. (c.f.point 2. in the review)
Accept (Poster). rating score: 8. rating score: 8. rating score: 6. rating score: 5. rating score: 5. <BRK>As an alternative, the paper proposes temporally extended $ \epsilon$ greedy exploration which maintains the simplicity and generality of $ \epsilon$ greedy while offering better . Clarity.This paper is very well written and clear, making it enjoyable to read. It sets up the shortcomings of prior methods and offers a simple solution. 3.Strong empirical results. Moreover, it is not clear what the tension or tradeoffs are between the properties.<BRK>##########################################################################Summary:This paper proposes a simple yet general approach for exploration in discrete action problems. The proposed approach, called ez greedy, combines randomly selected options with the well adopted e greedy exploration policy to achieve temporally extended e greedy exploration. The results are interesting, and the analysis aligns and supports nicely the narrative of the paper. The idea is simple (a generalization of e greedy) and the discussions nicely illustrate the main properties of an ideal generally applicable exploration method.<BRK>This paper proposes an easy to implement algorithm for the efficient exploration, which is a temporally extended version of \eps greedy. I appreciate this work for its motivation and the algorithm is simple to implement and not computationally expensive, which points to an interesting direction for future study. The conditions in Thm. If not, what approximately are the upper bounds for these heuristics? It would be better if the authors can support it with a theoretical justification or some quantitive analysis in terms of e.g., how the value of \mu affects the final performance.<BRK>This paper presents a generalized overview of temporally extended e greedy exploration. Although this paper presents a general analysis on temporally extended e greedy exploration, the presented ideas are too general. Thus, the presented results are not unexpectedly novel. Basic principle of temporally extended e greedy exploration is to apply the e greedy exploration policy for an extended period of time.<BRK>The paper presents an extension of \eps greedy strategy in order to increase the coverage of exploration in RL problems. The authors demonstrate that given certain conditions, the algorithm will converge in polynomial time for Q learning method. Overall, the paper provides a simple yet effective exploration technique for RL methods. I would recommend to add such analysis for online exploration into the paper. Minor:Table 1 is not referenced in the text
Reject. rating score: 5. rating score: 6. rating score: 7. rating score: 7. <BRK>Overall I vote for (weak) rejecting. Some crucial terms should may have to be mathematically defined in the paper, like  greedyness  for hyperparameter optimization, a term a was not aware of. Would it be possible for the authors to define the concept properly in the paper, or the provide a selfcontained reference? This is very subjective, I would recommend to highlight this contribution, and maybe to remove  we combined the above [...] with momentum decay  in the introduction. In particular the number of steps in the inner problem is provided, but what is the size of the hyperparameter searching space? Since authors rely on forward differentiation, one step of the proposed algorithm can be more costly than one step of the algorithms in the baseline. Is it trivial? Or could authors provide a reference? Scalable gradient based tuning of continuous regularization hyperparameters. Automatic differentiation in machine learning: a survey.<BRK>The experiments on hypergradient variance are a useful diagnostic for these kinds of algorithms. Tuning hyperparameters like this could be impactful if we scale it to state of the art models and optimization procedures/horizons. It’s not clear to me the sign of the hypergradient is a good test for convergence. Perhaps your algorithm may find this setting faster since there are about 100 runs for the hand tuning. I would want to see if there is some way to offer an improvement over the grid search. What happens if you start your training with the hyperparameters initialized at the best value from the grid search? Only a handful of hyperparameters are tuned   perhaps this is due to how the gradient calculation scales with the number of hyperparameters. This is because the paper makes useful contributions toward optimizing hyperparameters over long horizons, by motivating sharing hyperparameters via gradient variance and providing a forward gradient calculation skeleton. Is there a typo here?<BRK>The authors tackle the problem of HPO, focusing on the optimization of real valued hyperparameters for DNNs using a gradient based method. This novel method enables non greediness through the mitigation of gradient degradation and allows for long horizons. How does this work compare against tools like BOHB (not cited in this paper) [1]? Or else, since the hypervariance is somewhat defined twice, these two parts could be merged in section 3.3. No formal justification is provided for these two contributions which is a weakness of the paper. Minor remarks:   The abstract should explicitly mention that this is a Deep learning paper. It is usually a good idea to have the figures either at the beginning or at the end of a page rather than in the middle of the page.<BRK>This is achieved by calculating gradients wrt to the hyper parameters and using the sign of the gradients to indicate convergence. The paper is well written and structured and the suggested approach shows promising results. I only have two minor comments/ suggestions. Introduction:After first sentence: there is an empty space missing: ").This"Experiments:Could you compare your results based on some Bayesian Optimization based algorithm and compare to the computational cost? If not, why?
Reject. rating score: 4. rating score: 4. rating score: 5. rating score: 6. <BRK>In equation (4) (6),  the authors endeavor to take an expectation with respect to {(x_i,  y_i)}. If the problem raised in 4 can be resolved, I would very much like to change my rate. Also, because the theoretical analysis is an important part of this research, more theoretical studies on the properties of the proposed distance. \ Post rebuttal Thank you very much for the response, and I understood that some of the concerns I raised can be resolved empirically under appropriate conditions.<BRK>The authors proposed a distance measure to characterize the information space among neural networks. + the connection with generalization of neural networks is an interesting observation. It is not clear the contribution, both theoretically and empirically. missing definition of math notations: e.g.what is f_square in eq 14? I think it would require some significant rewriting before ready for publication. The contribution of the work is also a bit confusing.<BRK>Summary: The authors provide a practical distance measure among different neural networks. Empirically, they show several practical advantages of the proposed distances. In literature, there are already many works in studying the distance and geometry associated with the neural networks, such as Fisher information geometry (S. Amari) and Wasserstein information geometry (W. Li). In your paper, you mention that this new practical distance is also invariant under parameterization. 3.To obtain the data dependency definition of information distance, do you have to calculate an empirical version of it?<BRK>The paper studies an interesting and timely problem. That said, there are some concerns about the novelty and presentation of the paper:1. After going through other reviewers  comments and the authors  responses to those, I am comfortable with my original score. Similarly, it is not clear why the discussion on information transfer measures is necessary after (6).
Reject. rating score: 3. rating score: 5. rating score: 5. <BRK>This paper examines the positive unlabeled (PU) learning setting, and recommends the usage of the area under the lift curve, or AUL, as an unbiased estimate of the AUL under the fully labeled setting. I think this is an interesting proposition, supported by the experimental results. In addition, I have several minor comments. 3) There are a number of typos in the text. I note that it does not seem to change the conclusions that can be drawn from Table 1. I think it would be warranted to discuss this method as well.<BRK>The paper argues that AUL is a better metric than AUC under the PU (positive and unlabeled data) learning setup in the sense that it leads to an unbiased estimator in this setting, which is not the case for the commonly used and known metric   AUC. It is also argued that it leads to better performance than those methods which directly optimize an AUC based metric and computationally efficient to evaluate than methods which attempt to estimate the unknown parameters (\alpha, \beta in the paper). In the context of the paper, it should be formally defined in terms of quantities already metioned such as \alpha and \beta.<BRK>In this paper, the author proposed to use Area Under Lift chart (AUL) as a new optimization metric for positive unlabeled (PU) learning. The proposed AUL can be estimated unbiasedly from PU data, without the need to estimate the mixture proportions. Why not use the classification error to evaluate the learned models? From the theoretical analysis in the paper, we can only see that the AUL_PU is an unbiased estimator of AUL. Does this imply that AUL_PU has no advantages if the mixture proportion is correctly estimated?
Accept (Poster). rating score: 6. rating score: 6. rating score: 5. rating score: 5. <BRK>This paper presents a new non parametric method for few shot incremental learning. The aim is to perform few shots classification and regression while being robust to catastrophic forgetting when trying to learn new classes. Quality & Clarity:The paper is well written and the proposed approach seems to work well for few shot incremental learning. There are some minor issues with presentation and experimental results. Originality & Significance:The approach seems a good extension of incremental few shot learning based on prototypes.<BRK>This paper proposes a new method for incremental few shot learning based on feature quantization. Pros:+ The paper is overall well written. + Experiments are conducted on both classification and regression datasets. Both this paper and Xu et al.propose an incremental learning method based on vector quantization and compare it with prototype based classifiers. I ve read the rebuttal and the authors have addressed most of my concerns in the revised paper so I raise my score.<BRK>This paper suggests to use a generative model to address the problem of  few shot  incremental learning. The method is experimentally validated on 2 tasks: an incremental image classification task and an incremental regression task. On the other hand, the paper says nothing about the behavior of the method when the size increases. Below are some related references. Finally, I did not find the experimental validation very convincing, as far as the part on incremental learning is concerned. In this regard, I also noted that recent incremental learning methods were not cited in the state of the art. I invite the authors to look at the papers:  "Prabhu et.<BRK>Paper Summary:This paper proposes a nonparametric method in deep embedded space to address incremental few shot learning problems. Finally, this paper evaluates the proposed method on the classification and regression problem, respectively. Besides, I recommend citing more papers about this interpretation. The writing of this paper is not clear. The authors should give some visualization analyses to demonstrate their opinion. Meanwhile, the experimental results do not demonstrate the effectiveness of the proposed method sufficiently. EDIT: The authors  rebuttals have solved my concerns partially.
Reject. rating score: 4. rating score: 5. rating score: 6. rating score: 7. <BRK>This paper proposes a method called "Conditional Masked Language Model" for unsupervised sentence representation learning. My concerns on the intuition are most not solved. The experimental results are good overall, as the proposed method tends to give the best results across monolingual and multilingual benchmark datasets.<BRK> I appreciate the response from authors and the additional experiments. I do think the semantic search task adds value to the paper. However, the paper continues to be centered around the SentEval benchmark results. Presentation can be improved, especially the organization of the paper. This work proposes a self supervised training objective called CMLM (conditional masked language model) for learning sentence representations. CMLM performs well on the SentEval benchmark. Cons* Experiments are weak. The main contribution is not clear.<BRK>This paper presents Conditional Masked Language Modeling (CMLM), which integrates sentence representation learning into MLM training by conditioning on the encoded vectors of adjacent sentences. The paper further proposes a principle component based approach to remove the language identifying information from the representation while still retaining sentence semantics. Multilingual experiments are conducted, with interesting results on language agnostic.<BRK>The authors present conditional masked language modeling (CMLM), a new method for unsupervised pretraining, in which the skip thought notion of conditioning on neighboring sentences is adopted for masked language modeling. This is functionally equivalent to your flip flopping the order of the consecutive sequences. Please specify the value of the margin m being used in the experiments Choice of number of projections is also not motivated (and in fact contradicted by the ablation  experiment finding that 15 is better)  the motivation and contribution for XEVAL are great  the explanation of the dataset is lacking.
Reject. rating score: 3. rating score: 4. rating score: 5. rating score: 7. <BRK>Briefing:This paper proposes a new Bayesian metric learning method, robust to noisy labels. The paper introduces a variational formulation for incorporating the Bayesian framework to triplet loss training, with supporting experiments. Weak points(1) An essential reference [1] and other triplet ablations [2] seems to be missing. [1] Lin, Xudong, et al."Deep variational metric learning." Proceedings of the European Conference on Computer Vision (ECCV). 2018.[2] Duan, Yueqi, et al."Deep adversarial metric learning." (2) Experimental setup seems to be not enough. Note:The paper seems to miss the related works that must compare.<BRK>This paper introduces a Bayesian deep metric learning framework that is robust against noise labels. + Overall, the paper is well written and well organized. In my opinion, the main novelty of this+ves: + The idea of using the variational inference by Blundell et al.(Blundell et al., 2015) to derive Bayesian version of deep metric learning is interesting.<BRK>Directly applying the variational Bayes learning (Wang & Tan, 2018) in deep learning is challenging since it requires sampling from a distribution of the neural network parameters. The experimental results on several noisy data sets show that our novel proposed method can generalize better compared to the linear BLMNN (Wang & Tan, 2018) and the point estimation based deep metric learning (Hoffer & Ailon,2015; Lu et al., 2017), especially when the noise level increases. 2.Adapting the variational inference by Blundell et al.(Blundell et al., 2015) for Bayesian DML sounds good.<BRK># pros:  The proposed method appears to be first to soundly combine deep metric learning with a Bayesian approach (Bayes by Backprop) to estimate parameter uncertainties. The paper is clearly written# cons:  The method is not evaluated against the state of the art on the evaluated datasets, albeit the authors give a convincing reason for this, namely that the methodological novelty of the proposed approach warrants proof of concept results with a relatively simple neural network architecture# comments:  It would be interesting to hear in more detail what motivated the authors  choice of Blundell s approach to uncertainty estimation as compared to e.g.the Dropout  / Deep Gaussian Process based works by Yarin Gal et al.The abstract mentions "simulated label noise" as something the authors have considered, whereas the experiments mention "synthetic" and "realistic label noise", and conclusion refers to "simulated label noise" in the context of future work.
Reject. rating score: 3. rating score: 4. rating score: 4. rating score: 8. <BRK>They motivate the paper by the necessity of having better evaluation for OOD detection to be reflective of real world scenarios. The addressed problem is interesting and valuable for the field as many of the defined OOD datasets, and evaluation metrics may not cover many real world scenarios. They specifically addressed three scenarios, inputs that i) are irrelevant to the task ii) are from novel classes and iii) are from another domain (domain shift), which for the first 2 scenarios, they only evaluate them as unseen classes and not distinguish between them. Based on my understanding of the paper, they compare 5 OOD detection methods from the literature, suggest a few test datasets/scenarios and conclude using cosine similarity is consistently favorable for evaluation, and the choice of using confidence based methods in case of domain shift detection scenarios. 1.2.Why are these datasets considered reflective of real world scenarios? As an example, it is mentioned in the Domain shift detection [2] study in NLP, that is not the focus of the paper. (2019).”Q7: For the evaluation using fine tuning, the impact of the results on the seen classes (the current classes in the dataset), are not re evaluated. In many cases, fine tuning in a new distribution, contributes to reduced performance in seen classes or former distribution. ##########################################################################Reason for the score: S1: The paper is well motivated and addresses an interesting problem. ##########################################################################		 	 	 		References:					[1] Hendrycks and Kevin Gimpel, A baseline for detecting misclassified and out of distribution examples in neural networks, ICLR 2017 [2] Elsahar et al, To annotate or not?<BRK>Summary: The goal of this paper is to evaluate methods for detecting out of distribution samples in a more comprehensive fashion than prior work. I appreciate the effort to distinguish between different cases of OOD detection as well as the amount of experiments with a good selection of methods including some very recent ones. However, Hsu et al.(CVPR  20) also distinguish between two settings in their evaluation: semantic and non semantic shift. These results are unfortunately not discussed in this submission. I would not tie "novel" and "irrelevant" to certain sets of classes, but instead focus on the effect of "semantic closeness" on OOD performance as a more nuanced way of looking at semantic shift. The experiments on "detection of novel classes" (3.2) find that within the food dataset, four methods perform almost identically well (Baseline, Calib, MC dropout, Cosine), and outperform ODIN* and Mahalanobis. This contradicts the results in Hsu et al.2020, where it is shown that both ODIN* and a variant of Mahalanobis significantly outperform Baseline across different settings. Here it is chalked up to the similarity between in distribution and out distribution images. Was this not an issue? The final set of experiments on "detection of domain shift" (3.3) were confusing to me. Here also the goal is now to predict the classification error from the OOD scores, which departs from the setting considered for the first two sets of experiments.<BRK>The authors distinguish three different operating scenarios:+ Irrelevant inputs: in this case, the inputs  simply need to be rejected. + Novel classes: detecting inputs that are of the same overall category as that used in training, but from an unseen class of that category. + Domain shift, where measurement artifacts and other disturbances cause a shift in the measured p(x) even though the underlying inputs belong to known classesMultiple methods are compared, using different image datasets to compare OoD detection performance using the AUROC as the evaluation metric. + Shows which methods work under what scenarios. Cons+ The biggest concern with this type of work is lack of novelty. Fine tuning/pre training gives a strong performance boost for OoD detection (as shown in the work of Hendrycks et al 2019) and cosine similarity has been shown to be better than softmax for OoD detection (as shown in the work of Techapanurak et al.(2019); Hsu et al.(2020)).Further, there is no real discussion or insights into why certain methods work in some scenarios and not others. This results in a rather underwhelming paper for  the reader, and while there are useful takeaways from the paper, as it stands, it does not meet the bar of ICLR acceptance. Generalized odin: Detecting out of distribution image without learning from out of distribution data.<BRK>The results show that cosine similarity consistently outperforms other methods across all sub tasks. #### Reasons for ScoreI believe that this paper touches a subject of importance to practitioners and researchers, the performance of OOD detection methods in more realistic settings and with three different sub tasks inside the OOD detection task. The evaluations are made correctly, in a variety of datasets, ensuring the robustness of the conclusions that were made. There is also a variety of tasks inside out of distribution detection (new inputs, novel classes, and domain shift). The evaluation is made with a practical point of view, motivated by real world examples, that differ from purely academic benchmarks. Note that Ovadia et al.also defines corruption methods for evaluating dataset shift. Note that this might change which method works better, or even create new baselines, as ensembling can be combined with cosine similarity OOD detection. #### Questions for Rebuttal PeriodWhy were ensembles not considered as one of the evaluated methods? This is important since an OOD score is used in Section 3.3.
Accept (Poster). rating score: 7. rating score: 6. rating score: 6. rating score: 6. <BRK>This paper theoretically analyzes "label smoothing” (LS)  with PAC Bayesian bound and motivated from their analysis, proposes a new method: LORAS. In their theoretical analysis, they identify that the generalization error depends on the smoothing distribution. * The experiment results are convincing.<BRK>This paper proposes to improve upon label smoothing (LS) by adapting the noise distribution used in LS from uniform to a distribution that better represents the correlation/similarity between the candidate space (types in vocabulary). The approach also seems to improve model calibration. This matrix is parametrized via a low rank approximation which prevents it from collapsing to a diagonal matrix and become ineffective.<BRK>The paper proposes a label smoothing method upon the low rank assumption of the output dimension, especially when the output dimension is large. It will be better for the authors to highlight more about the theoretic result and the empirical algorithms.<BRK>The paper proposes LORAS (low rank adaptive label smoothing) that learns with soft targets for better generalizing to the latent structure in the label space, and the experiments on three semantic parsing datasets show the improvement over the no smoothing methods.
Reject. rating score: 5. rating score: 6. rating score: 6. <BRK>The idea looks interesting and the empirical results verified its effectiveness. The major weakness of this paper is the presentation. The exact learning setting is vague. The classification problem or the generation problem? 2.The technical contribution to PU classification is very limited. 3.On the other hand, what is the contribution to generative modelling with extra unlabelled data? The experiments were only run on small datasets MNIST, Fashion MNIST and CIFAR 10.<BRK>The topic of the work is interesting. They extend their comparisons in ways to increase the data imbalance problem and show robustness to even that. Overall, the paper seems original and decently significant, however, it would be a much stronger submission if some of the issues here were addressed. It would also be nice to see how well it performs on slightly harder datasets with larger samples.<BRK>Especially, both PU classification and conditional generation can benefit from such a joint optimization. I have some concerns about the details of this work. They are one hot coding in this work. The estimation is conducted (L L_0) times in each update. Multi positive and unlabeled learning.
Accept (Poster). rating score: 7. rating score: 6. rating score: 6. rating score: 5. <BRK>This paper proposes a framework for self supervised representation learning using causality. The proposed model is formulated by assuming that the Data generation schema is composed of two independent mechanisms (ie., Style and Content) and only content is relevant for learning the underlying task. Thus, the Content is a good representation of the data and the goal of representation learning could be cast as a content estimation.<BRK>The experimental results on Imagenet and Atari demonstrate the effectiveness of the method. Also, a new understanding of contrastive learning is provided. StrengthThe understanding of self supervised learning from a causal perspective is novel. Especially, the image data were generated from content and style and usually the downstream tasks such as object recognition depends on the content.<BRK>## General Summary:The authors propose a causal interpretation of the self supervised representation learning problem. Again, I think this is a bit misguided. ## ProsThe causal framework is well motivated, i like the separation in content and style factors. The interpretation of self supervised learning as invariant prediction with refinements is valid. To the extent of my knowledge the contribution is novel, I am not aware of any other work that connected contrastive learning with the causal framework. p.4 par.1 where you separate style from content.<BRK>The data is modeled as being generated from two independent latent factors: style and content, where content captures all information necessary for downstream tasks, and style captures everything that is affected by training augmentations. The main contribution is a specific regularizer for self supervised contrastive learning, motivated by the assumptions about the data generation.
Accept (Poster). rating score: 8. rating score: 7. rating score: 6. rating score: 4. <BRK>This paper studies point wise certification methods for the decision made on a test instances against data poisoning. Namely, for a given point, we would like to know: how many changes in the training set would still lead to the same produced label. This approach is indeed reminiscent of "Bagging" (Bootstrap Aggregating), but this seems to be the first work (along with concurrent cited works) that apply this to certification under poisoning attacks. Along the way, (for label flipping) there are some neat ideas also to improve the results and the efficiency. (1) The paper shows how to benefit from semi supervised learning by each of the sub models is still trained on the *whole* data set, while only a subset of the samples (i.e., those in a particular partition) would have the labels kept. I, therefore, recommend acceptance. And anyway the bounds of this submission are stronger.<BRK>Based on this idea, the authors propose Deep Partition Aggregation (DPA), a robust classification algorithm that first partitions the training data into k subsets, and then separately train a model on each subset. The paper derives theoretical guarantees in terms of when the prediction of a particular data point can be certifiably correct. The main advantage of the paper lies in the experimental part. Furthermore, by comparison with prior works, the authors show that the proposed DPA and SS DPA are better at defending against data poisoning attacks. One disadvantage of the paper is that the power of the attacker in this paper seems to be very weak. However, even with such a small fraction of change, the proposed DPA already suffers 0 certified accuracy.<BRK>2) Label Flipping Poisoning Attacks   where the input images are intact but only the labels are flipped. The algorithm for the general poisoning case (called DPA   Deep Partition Aggregation) is simple. For each of the K models, we can first train an unsupervised model on the entire training data (since inputs are not corrupted). The results on MNIST and CIFAR are very encouraging and SOTA as per the papers claims. There is no discussion about how this method tackles evasion attacks or what prevents it from tackling it. even the concept of using the model number as a random seed to avoid correlations seems to be prevalent in that paper. 5. might warrant experiments on more datasets<BRK>+ Provides some certifiable measure of robustness against general poisoning attacks**Cons:**  The extremely simplistic defense strategy that is utilized implies that the defense has very poor performance. On the CIFAR 10 dataset, the drop in clean accuracy is >20% but only around 9 samples out of 50,000 can be certified robust. Thus, while defense may claim to provide some certified robustness against general poisoning attacks, the results are not promising.
Accept (Poster). rating score: 7. rating score: 7. rating score: 7. rating score: 5. <BRK>*Paper Summary*The authors provide a novel interpretation of dropout regularization using Banzhaf interactions, a tool from game theory. Visualizations are also provided in this respect on a dataset for face analysis. *Cons** Some of the results does not read well, like Table 3 or Figure 4, but this is really minor and fixable*Preliminary Evaluation*I believe that the overall analysis provided by authors is complete and interesting, so I am akin to call for a full acceptance of the paper which I deem suitable for such a venue like ICLR.<BRK>Summary.This paper aims to explain dropout from the lens of game theoretic interactions. 2.I would recommend revising the title of the paper. Based on this understanding of dropout, an alternative regularization technique is proposed, which explicitly penalizes pairwise interactions between variables.<BRK>The title of the paper is a bit misleading as it seems to suggest that the paper is about using dropout in Game theory (i.e.solving problems in game theory using dropout). The paper introduces a new regularizer that explicitly minimizes the metric and claims that using this regularizer instead of dropout has some advantages.<BRK>Thus, I cannot evaluate the main contribution of this paper. The paper introduces a new perspective of game theory to understand dropout. 2.Experiments are conducted on various datasets to support the theoretic proof and the proposed interaction lossConcerns:1.
Accept (Poster). rating score: 7. rating score: 6. rating score: 6. <BRK>An uncharitable reading of this paper would be that it is just an architecture tweak on top of ERO, and while the empirical results help dispel this idea, I think a more explicit comparison would still be useful. The Atari subset used here is a bit unusual, particularly the choice to not use frame skip.<BRK>This paper proposes to use two encoders   one global that encodes across the current batch of experience replay sample, and one local that encodes each selected timestep. But this is dubious. The idea to have a global feature pooled from the sample transitions is unique. is not a satisfactory explaination.<BRK>The idea is to take into consideration the context, i.e.many visited transitions, rather than a single one, based on which one can measure the relative importance of each transition. So this difference does not indicate “how much the sampling distribution can help.”In the empirical study, NERS does not show a clear benefit from the learning curves. The method is novel and is potentially interesting to the RL research community.
Reject. rating score: 3. rating score: 5. rating score: 5. rating score: 6. <BRK>Summary The paper makes the observation that various non decomposable losses in machine learning can be rewritten as linear programs, whose constraints depends on the model output. This is the case for AUC, multi class AUC, F score, and to some extend NMF. The authors review these losses, and recall how they may be rewritten as LPs. Then, the authors propose to directly backpropagate through the LP resolution to minimize non decomposable losses, applied on top of deep architectures. For this, they propose to solve an approximate solution to the LP problem (a quadratic penalization of the constraint violations) using a modified Newton method. In particular:      Phi is not introduced beforehand p. 4, and the F score part is very hard to understand. the NMF section is very unclear, in particular as the authors use vague terms in their construction, such as "zero padding ensures a sxs matrix". I do not understand the role of tilde p in (6). The authors state that "each y has a neighborhood in which the Hessian is quadratic", which does not mean anything. I do not understand whether rho is chosen at every iteration, and what is its importance. They modify a LP by making it a "smooth almost everywhere"problem, which can then be solved using any methods, and backpropagated throughusing either unrolling, or the computed minimizer (by virtue of Danskintheorem), or the implicit function theorem. There is therefore not need to backpropagate throught tilde A^{ 1} b. It appears that all three methods are within statistical variations. NMF is a long studied problem, with many powerful methods to handle large inputs. As it it, the experiment proposed in this manuscript is not polished enough to be valuable.<BRK>This paper addresses the classical topic of directly optimizing non decomposable loss functions. Since these metrics can be computed via linear programs, it is sufficient to compute gradient through the LP solver. "using off the shelf solvers ... can severely slow down"   a) SOTA LP solvers are extremely fast, b) if there is a faster "solver" for a concrete LP, such as quicksort for the LP formulation of ranking, (Berthet, 2020) allows using it. To that end, the authors propose to use a particular method for solving linear programs. My objections to the paper fall into three categories### Conceptual confusion In this section, I want to clarify that gradients to LPs are a mostly solved problem and there is no dire need to resort to a new type of an LP solver   as the authors do. I will also contradict the claim "a simple closed form gradient is not available for backpropagation" made on page 4For clarity let s focus on three scenarios (in the notation of the paper):1) Taking derivatives of the **optimal value of the objective function** w.r.t the LP parameters $c$, $b$, $A$In all these cases, there is a simple closed form gradient. $x^*$ is a solution to the linear system given by the set of active constraints, and gradients of matrix inversion are easy to compute. This is an ongoing research direction with competing methods. In this special case, which is actually the most common one in practice   and also occurs in this paper, the situation is a lot easier. I find it imperative that the authors acknowledge this and compare to some of them.<BRK>In the linear programmings, the constraints are indeterministic at each mini batch, the number of constraints increases quadratically to the number of training samples, so some previous works are inapplicable here. So does the primal dual based forward pass and the corresponding implicit differentiation for a backward pass. Finally, the experiments emphasize the condition when the positive/negative examples are imbalanced, and demonstrates the superiority of the proposed methods to cross entropy loss and one previous AUC loss. 2.It is a good choice to use Newton s method to tackle down the problem for training. Determining the accuracy \epsilon seems to be critical for the proposed algorithm. It would be good if the author could have some more explanation about this. Besides, is that possible to make it a learnable variable through back propagation at the first stage? So it would be better to compare with these methods as well but not only vanilla cross entropy. 3.The application of nonnegative matrix factorization should be further improved. As indicated in the paper, it only works when the number of channels is limited. The author may remove this application and focus on the AUC and F1 score. D. Justification of the score:In general, this paper focus on an important problem, and the algorithm is discussed comprehensively. I will raise my score if these concerns can be addressed during rebuttal.<BRK>This paper shows how some nondecomposable functions can beinterpreted as solving combinatorial optimization problemsthat can be relaxed to linear programs. As this paper brings some new insights and directions hereI recommend for a weak accept, although some of the experimentalsettings and baselines feel incomplete (more details below). The AUC experiments in Table 1 outperform [Liu 2019],which also optimizes for the AUC. # WeaknessesThe biggest weakness I see is that the F score experiment inTable 3 has no baseline that also directly optimizes the F score,such as in [Fathony 2020] and other methods cited in the paper,and the non negative matrix factorization experiments arelacking quantitative results. This paper relies on relaxing the integer domain to be continuousand it s not clear how much this approximation impacts the derivatives. Some of the design choices seem arbitrary and unjustified, such asfocusing on the fast exterior penalty optimization at the startof Section 3 and then doing the forward pass with Newton s algorithmon the unconstrained problem from [Mangasarian 2004] in Section 3.1to solve the LP. My interpretation withoutthis is that both of these approaches are approximations and it s hardto know a priori which will perform better.
Reject. rating score: 3. rating score: 4. rating score: 5. rating score: 5. rating score: 6. <BRK>The immediate reward $r(s_3) 2$ and $r(s_4) 0$, after that it always goes to the absorbing state with 0 rewards thereafter. I like the idea of finding the max reward policy, but the entire framework seems to stand on a shaky ground. Consider an MDP with 4 states plus a zero reward absorbing state. The main problem is with Equation (3), where the second equality is wrong.<BRK>Summary: This paper proposes a new reinforcement algorithm based on max Bellman operator which trains the policy to optimize the maximum reward achieved in a trajectory, i.e., $R(\tau)   \max_{t\geq 0} \gamma^{t}r_{t}$. The authors analyze how the newly proposed max Bellman operator leads to an optimal policy. Experiments on a toy task and de novo drug design tasks show better performance compared to the considered baselines. I think the proposed idea is promising, timely, and impactful. Especially, regarding (b), the experiments only compare with the Bellman operator under MDP with cumulative rewards that do not align with the true objective. The proposed algorithm is fundamental and can be extended to many problems. This dismisses how one can also design MDP so that the Bellman operator can optimize the true objective of the problem.<BRK>##########################################################################Summary:This paper proposes a modified bellman equation for reinforcement learning that optimizes the maximum expected single step reward along a trajectory, instead of the maximum cumulative reward. This formulation is applied to the generation of molecules with optimized properties of interest. The proposed max bellman formulation seems to make sense in the context of molecule generation. What is the reasoning for not using that standard evaluation suite in the evaluation of this work?<BRK>Motivated by the de novo drug design, this submission proposed a new objective in reinforcement learning, i.e., to maximize the expected maximum rather than the accumulated reward along trajectories. In the experiments, the authors first showed on a simulated grid that when compared with Q learning, the proposed Max Q algorithm can achieve higher maximum rewards along trajectories. The authors also investigated theoretical properties of the proposed operator, and the results are well presented, which I also appreciate. In Section 2.2., could you provide more explanation on how the maximum reward along a trajectory is related to the the issue of synthesizability there? Also, can you add the input, i.e.(s, a), for $r$ in the proof to make it more explicit?<BRK>The paper then defines the corresponding varaint of the Bellman operator (the max Bellman operator) and proves tabular convergence guarantees by a contraction argument. The application to chemical synthesis shows that the new problem formulation is able to improve performance on a relevant task. This reflects the fact that I think the paper introduces an interesting problem and clean solution, but does not do a good job connecting to prior work and has a few issues with clarity especially in the experiments. **Questions for the authors:**1. Is there potentially a connection between the proposed maximum reward formulation (especially for chemical synthesis) and the learning to search approach to structured prediction problems (see e.g.[2])?2.Is there a connection between the proposed maximum reward formulation and optimal stopping problems? From what I can tell, they propose the exact same problem formulation and algorithm based on a modified Bellman equation as this paper. For example, it is not clear that doing something like using a time dependent reward that only has nonzero reward at the last state in a trajectory cannot capture most of the relevant generative problems.
Reject. rating score: 3. rating score: 5. rating score: 6. <BRK>###############################################################Summary:This paper provides a new method for estimating the generalization performance of neural architectures. This method used the sum of training loss as a criterion. The paper gave some intuitions about the method from the perspective of Bayeian model selection. And the paper didn t give some convincing reason to this method. The method has no theoretical guarantee, and its analogy with Bayesian model selection seems problematic. 2, The analogy with the Bayesian model selection is problematic.<BRK>This paper proposes a simple model free method to estimate the generalization performance of deep neural architectures based on their early training losses. The proposed method uses the sum of training losses during training to estimate the performance and is motivated by recent empirical and theoretical results. Cons  I am wondering how the proposed estimator can be used in recent gradient based NAS methods. Is it possible to use the proposed estimator for DARTS optimization (i.e.recent gradient based methods) and to speed up the optimization? I would like to know the scope of the application of the proposed method.<BRK>SOTL E is a variant where the sum of training losses begins to be computed after the first E epochs. As this is paper is proposing something that is fundamentally opposite to what has been studied widely thus far, it requires a lot more scrutiny. Then use this SOTL and SOTL E to determine the best network.
Accept (Oral). rating score: 8. rating score: 7. rating score: 7. rating score: 7. <BRK>The paper introduces a novel recurrent neural network architecture which approximately preserves the norm of the gradient irrespective of the number of unroll steps. ##### I firmly believe that based on those, it should be accepted. Proposition 3.2 and 3.3. 1.I appreciate the authors comments regarding the expressivity of the proposed architecture, as well as the demonstrations in Appendix B. While granted, this is somewhat consistent with practices common in the community, it makes one question how representative they are of different initial seeds.<BRK>Lastly, in the part of discussion, the practical significance of proposed coRNN should be emphasized with more words. There are two main pros in this paper: 1. Considering the whole structure of this paper, I argue that the clarity is clear and logical.<BRK>I was very curious to see how the authors implement this scheme in practice, however, the provided implementation revealed that the authors use an explicit scheme in practice. The performance of the proposed recurrent unit is state of the art. Further, the presented results are intriguing, and the paper is well written.<BRK>The paper proposes a novel RNN architecture (CorNN) to tackle the infamous problem of vanishing and exploding gradients in RNNs. The novel CorNN architecture is based on time discretized forced coupled damped nonlinear oscillators. For the gradient norm of CorNN analytical lower and upper bounds are calculated implying that CorNN avoids vanishing and exploding gradients. * figure 3: It would help to also plot this for other tasks. * figure 3 lines for other tasks would be helpful
Reject. rating score: 4. rating score: 5. rating score: 6. rating score: 6. <BRK>**I have updated this review after noting the authors’ detailed response. **This paper focuses on the problem of “Neural Text Degeneration”—where text sampled from a language model can either be too repetitive and bland or too random and nonsensical. The authors test whether this improves repetition and unique token coverage with greedy decoding in open ended generation. A small human study is conducted and the proposed method, ScaleGrad, is found to outperform MLE and Unlikelihood Training (UT). On Abstractive Summarization BeamSearch is used and again outperform MLE and UT. Analysis attempts to make comparisons across different decoding strategies, though coverage of different variations is limited. The authors argue that stochastic decoding is outperformed by ScaleGrad, though they note that trigram blocking still helps ScaleGrad. Results for stochastic decoding should have been shown across tasks. The following two paragraphs are obsolete, because the authors shared experimental results from a larger set of experiments. > The results in Table 1—which show the main metrics of interest on open ended generation—are missing two key points of comparison: ScaleGrad is only show with gamma 0.2, even though gamma 0.5 & gamma 0.8 are used for the rest of the experiments, giving us little idea of how these metrics change over hyperparameter settings. We did  the same scale hyper parameter search for UL. First of all gamma 0.2 is not shown, though at least gamma 0.1,0.3 are so it can be somewhat inferred. That is suboptimal, but this graph does not even go up to gamma 0.8, which is what is used in the Abstractive Text Summarization experiment! Furthermore, the number in Figure 1 (b) cannot be directly compared to other decoding methods, because they are an average of repetition metrics shown in Table 1. Worse, the data presented in Figure 1 (b) actually makes comparison impossible, which makes me uncomfortable about the universally positive results in Table 1.<BRK>The authors propose to modify a language model s token level distributions by rescaling the output probability of tokens that do no appear in the context ( novel tokens ). #### Clarity and significance  **ScaleGrad motivation**. **Unlikelihood discussion**. However, *the model is not at an optimum* if $p_<0.5$. It s unclear why this specific method (renormalizing over the novel set) is the best or simplest method for promoting novelty. Overall I m borderline on this paper: the authors do perform a lot of experiments and show improvements, but I m hesitant that scaling novel tokens and renormalizing the model s output distribution is significant.<BRK>This is a decent approach with a strong experimental evidence and it will be useful for text generation community in the future research. ## SummaryThis work proposes an effective modification of language model token level distribution during the training which prevents some forms of degeneration such as repetitions and dullness. In other words, this method changes softmax distribution such that unseen/novel tokens is being rescaled with a given hyper parameter $\gamma$ (eq.4). Authors conduct several experiments using different tasks such as open ended generation, image captioning and abstractive text summarization. The approach is based on the idea of encouraging the model to use tokens which were not observed in the previous context so far. I wonder how can one use MLE criterion on the token level without teacher forcing? 3.In section 2.2: "*thus reformalizing the probability distribution*", this wording reformalizing sounds a bit weird to me, but it is clear what authors had in mind. From my understanding it is possible since ScaleGrad emphasize novel tokens, and this softmax from scalegrad may be used in the UL loss, which may help even further! 4.The potential issue of UL (sec.5.4) does not look convincing. Would be great if authors can elaborate more about this. ## RecommendationOverall I vote for accepting this work as long as main concerns will be addressed/discussed.<BRK>The paper presents a  technique to encourage generating certain tokens (i.e.non repetitive ones) in text generation. The idea is to scale the softmax probability  for certain words (in the novel set) by a factor of gamma. The authors show how this affects learning by deriving the effect on the gradient. This make the method section clearer. (2) How much is the model discouraged from generating stop words like "the" or "a" (and how does this affect fluency)Pros: Well justified and simple method to solve a relevant problem in text generation. Lots of experiments, gains in open ended generation seem decent. Cons: Gains on summarization are marginal / non existent suggesting that this is not as large of a problem for more constrained tasks.
Reject. rating score: 3. rating score: 3. rating score: 3. rating score: 4. <BRK>** Strengths **The paper addresses a highly relevant and important problem. ** Clarity **Unfortunately, the clarity of the contribution is not up to the standards of ICLR conference. In general, I believe that when formal tools (like group theory) are applied to prove anything outside of their original domain (i.e.when we are using group theory to reason about compositional representations in machine learning), it is crucial to 1) clearly define all involved notions (not only mathematical, but also the ones to which mathematical tools are applied) 2) clearly motivate the application. I understand that a lot of work went into this article, and I hope that the authors won t feel discouraged by the feedback, but use it as an opportunity to improve the paper.<BRK>Post revision update Thanks to the authors for their revision. Unfortunately, I still feel the contribution and value of the work is not well communicated. See also Zadrozny (1992). The authors do not justify that *any* composition operator could be specified in terms of their definition of compositionality. * Other definitions are not clearly expressed either. The authors present a page of group theory definitions, but this seems almost certain to be unhelpful.<BRK># Overall reviewThis paper applies concepts from group theory to help find necessary and sufficient representations on the presence of compositionality in representations and on mappings between them. * The applications in the discussion section are somewhat opaque. It might be more informative here to say that the meaning of a whole sentence is composed from the meanings of the parts and the grammatical structure of the sentence. * "We will provide examples and look into more details in discussion section." # Typographic comments:* I think e in definitions 3.8 and 3.9 refers to the identity element of the group, but this should be stated explicitly (e.g.in the definition of a group).<BRK>Since this research addresses fundamental questions about the object of study, I would encourage the authors to clarify how the two propositions and their consequences inform our understanding of compositionality (i.e., what do we learn that we didn t before?). ##### Summary #####This work uses group theory to investigate compositionality. The results also look sound to me. This is a major concern since it certainly has a lot of potential.
Reject. rating score: 4. rating score: 4. rating score: 4. rating score: 6. <BRK>This is achieved by following a differentiable architecture search strategy in which an additional loss function is included to account for the domain shift. Specifically, the loss function aims to minimize the discrepancy between feature representations from the two domains. Weaknesses:Originality:  While I like the general concept of designing a NAS method specifically for domain adaptation, the proposed method lacks originality. This is fine, and the results are encouraging, but this seems to be a small contribution for an ICLR paper. (1) (2).While the discrepancy term of course depends on the architecture, it also depends on the weights w. It is therefore not very intuitive to my why the discrepancy term does not appear in the inner minimization problem of Eq.(2).The other point that I find disturbing in the methodology is the fact that, during the second training stage, a different DA strategy, relying on adversarial training instead of MK MMD, is used. As mentioned by the authors, one cannot optimized the target validation loss, because the target data is unsupervised. Experiments:  In the comparison with the state of the art DA methods in Tables 1 and 2, what architectures do these methods rely on? The set of baselines differs across the different experiments.<BRK>In the work, the authors aim at improving the transferability of domain adaptation models from the perspective of neural architecture search. Learning transferable architectures for scalable image recognition. It consists of two phases, in particular, the first phase searches a neural architecture for domain adaptation based on a famous differentiable NAS method named DARTs, and the second phase develops an adversarial training method for domain adaptation by extending MCD to a multiple classifiers version. Concerns: 1. My question is, by searching a differentiable neural architecture with the objective function defined in Equation 7, how can we guarantee it is suitable in the target domain and ensure a high target accuracy. The authors may want to search for a not so bad architecture first and then align the features across domains, however, the main purpose of this paper is the former but not the latter. Therefore, ablation studies are necessary to verify the effectiveness of the MK MMD loss, such as comparing the architecture of the proposed method with that of DARTs. 2.The technique novelty of the first phase of combining DARTS with MK MMD is somewhat incremental and limited. 4.As shown in Table 2, the results of other NAS models with the same phase II are extremely low, however, there is no much difference between the architecture of NASDA with that of other NAS models. Meanwhile, the author has better show some evidence to justify that NAS can find more domain invariant features than the classical architecture including VGG, ResNet, DenseNet. It would be better to conduct some experiments on these standard domain adaptation datasets and compare the proposed method with numerous baselines of these benchmarks.<BRK>**This work introduces a two step procedure for unsupervised domain adaptation; (1) neural architecture search for domain adaptation (NASDA) based on DARTS for neural architecture search and MK MMD for differentiable unsupervised domain adaptation (2) adversarial training with a batch of classifiers. The novelty of the first component is not explained well in the paper. The first component seems to be a simple combination of DARTS with the additional loss function, MK MMD. All descriptions on <section 3.2 searching neural architecture> are borrowed from the previous works, DARTS and MK MMD. I believe that the author should give more evidence to support the claims.<BRK>This work devises a two step process for searching optimal models for unsupervised domain adaptation. The work addresses an interesting problem of automatically finding suitable architectures that transfer to unlabelled datasets. The novelty is involved in the formulation of a protocol that combines a modified DARTS objective with a post processing step to get a suitable feature generator. Is it possible to extend PDARTS with the MK MMD regularisation term? Do authors have any insights in this direction? Establishing the independent effectiveness of Phase 2 and regularisation term will add value to this work. These would also serve as relevant baselines. Minor remark: I think the paper can benefit from one through proofread for grammatical corrections and notational consistencies. Similarly, if the convention of underlined numbers in tables refers to second best models that it should be made consistent across all tables.
Accept (Spotlight). rating score: 8. rating score: 8. rating score: 8. rating score: 7. <BRK>Compared to the models with a large entity memory whose retrieval is performed with Maximum inner product search, how is the efficiency of your decoding strategy? It s also capable of linking novel entities at inference time. This paper is clearly written. The experiment results are convincing.<BRK>### SummaryThis paper proposes to tackle the entity linking task using a sequence to sequence neural model, trained by producing unique entity names, in autoregressive fashion. The ideas in this paper are novel, the paper is well written and the empirical evaluation is well executed. Doing seq2seq entity linking is really a novel idea and is surprising that it works so well for several of the datasets presented.<BRK>The paper introduces a new method to retrieve entity by auto regressively generating unique entity name as a sequence of word pieces, instead of pinpointing the ID representing an entity. Could you elaborate on this? The paper is clearly written and extensively evaluated on three relevant tasks, entity disambiguation, entity linking, and entity retrieval. I have one big concern with the current format of the presentation.<BRK>The paper proposes a brand new approach for entity retrieval, which leverages an encoder decoder architecture to generate the target entity directly. Although the paper does not come up with new architecture or elaborately designed neural components, I believe this paper is worth reading for the community, including how this paper redefines the problem. Is it possible to generate all entities simultaneously as what GENRE does in end to end entity linking?
Reject. rating score: 3. rating score: 3. rating score: 4. rating score: 5. <BRK>Summary:This paper presents a graph pooling operator by first predicting scores on edges, then performing min cut to separate subgraphs, and finally construct coarsened graphs. Pros:1, The problem of defining pooling operators on graphs is important. 2, The experiment section is not that convincing. First, the performances are worse compared to other baselines.<BRK>This manuscript proposes a new pooling layer in Graph Neural Networks (GNN). I m wondering if it is possible to look at the distributions of edge scores and determine an appropriate $r$ value. Even though the manuscript explores an interesting and timely topic, their approach is not technically appealing and the explanations are not enough to thoroughly understand the authors  ideas. Where and how $L_{reg}$ is used in their proposed method?<BRK>The paper proposes a novel pooling layer for graph neural networks. Concerns:  There are several concerns, the main being about the novelty of the approach since the minCUT problem has been addressed already in a previous work (as the authors also mention). In this case, the authors should describe in detail how Eq.6 and the computation of S and C result in minimizing the graph cut to underline the difference from the previous work and show that this work is not derivative. The edge score in practice is computed in each layer using an attention mechanism on the concatenated representations of the edge’s nodes from that layer.<BRK>SummarizationThe authors propose a novel pooling layer based on edge cuts in graph, where a regularization function is introduced to produce edge scores via minimizing the minCUT problem. Strong points1) The paper is good writing and easy to understood. Weak pointsThe main weakness could be the model performance and there are not enough experiments to prove the efficiency of the proposed EdgeCut.
Accept (Poster). rating score: 9. rating score: 7. rating score: 7. rating score: 3. rating score: 3. <BRK>The paper presents a method called SaliencyMix. They improve a method that augments images by adding random patches from other images. The innovation is that they select these patches using a saliency map. This paper has an excellent discussion and critique of previous work. They discuss the existing work with a nice summary and then discuss reasons why selecting random patches can have issues. There is a clear argument for their method over selecting patches randomly. They make clear claims that this approach improves performance over randomly selecting patches. The paper is well executed so there is not much to complain about. The availability of the source code is not clear from the text. A potentially interesting analysis (but not required) is an analysis of the increased runtime in practice.<BRK>This paper proposes an improvement on the cutmix strategy of data augmentation, where the source patch is selected not randomly but based on saliency. Results show improvements w.r.t mixup and other related strategies on Imagenet, CIFAR 10/100 and also transfer to object detectionPros:  The approach is intuitive and makes sense (which is more than can be said for the baselines of CutMix and MixUp). I think this approach probably starts to get to the heart of why these previous strategies work: they are probably less effective ways of doing what this paper suggests. The results seem quite promising, and the improvements seem significant. I understand the author s reasoning, but I find it strange that it has that big of an effect. It is well known in the saliency literature that saliency has a center bias. I am not sure about the point of using CAM visualizations on augmented images. Perhaps a better visualization might be CAM visualization of the models trained with each kind of visualization on the unaugmented images?<BRK>*Summary and contributions:*This paper proposes a new data augmentation strategy to train image classifiers and object detectors. The paper includes an exploration of the design space of such approach, and multiple experimental results showing the empirical superiority of the proposed approach compared to existing data augmentation strategies. Although no previous work provides the experimental results presented here, the results are expected. This work is good A+B incremental work. * The method implicitly relies on having “simple images” with one dominant foreground object  like the ones in CIFAR and ImageNet. Ideally the paper would be more upfront on these assumptions. * Some of the saliency methods evaluated use training data, even the ones that do not have been tuned using additional data. The paper would benefit from a discussion of this additional information. *Relation to prior work:*Related work section has a reasonable extent. which is the main object of the scene ?). In particular CAM is discussed in section 4.3. *Reproducibility:*The overall algorithm is simple to understand and re implement. From a quick inspection of that paper it is not immediately clear to me how to transpose it for single image saliency. Section 2.3: See comments above regarding related work. BAsNet for example, was trained on 10k images. Why not simply include these (and their mask) as part of the pretraining when considering some of the baselines ? What happens if (due to quantization) two pixels have the same value ? This is clarified later in the text, but some context would be welcome in the mention here. Figure 3:  Add non augmented result bar as reference point. are identical  > are similarSection 4.1:  SOTA top 1 error: there are 20 methods that claim better results in https://paperswithcode.com/sota/image classification on cifar 10 and https://paperswithcode.com/sota/image classification on cifar 100 that claim better results.<BRK>This paper is based on an interesting observation that previous data augmentation tricks cut&mix may select regions that do not contain useful information. Instead, this paper use saliency models to detect the salient regions first and then cut and mix these salient regions in a source image. Also for most image classification dataset, the images are quite iconic, so the improvement on classification tasks are limited (as shown in Tab.1, C10+ and C100+). This might help with detection as it may train models to focus on the most discriminative part of the image, but recent works show that there is no direct correlation between between the performance of the same backbone on detection tasks and classification tasks. In addition, the author didn t provide analysis on what causes the 1.8% improvement on detection tasks (better on smaller objects?) So it s not clear how helpful this trick is. 2.While the method is simple, I expect either some mathematic proof or this method works well on various tasks. The paper didn t have any proof or statistical analysis. This paper didn t either show if the proposed method will work on more tasks (for example segmentation or GAN?the detection provided in this paper is using the backbone initialized from classification, during training faster rcnn it seems that the trick is not used).<BRK>This paper proposes a new augmentation method based on CutMix. So, they propose to use saliency maps to control the selection of mixed patches, which is called SaliencyMix. However, the experiments’ results fail to show the ability of the method, and some explanation is missed. The idea is simple and clear, the paper is well organized and easy to follow. 2.The experiments are comprehensive, including classification and transfer learning. The main concern is the effectiveness of the proposed method. According to the authors’ experiments, the improvement over CutMix is very limited on all datasets. Why other methods even worse than CutMx? 3.The authors use batchsize 256, lr 0.1 for CIFAR training, while usually batchsize 128, lr 0.1 is used in previous works (Cutout). And as described in [1], the learning rate should be increased linearly with batchsize. The novelty of this paper is too limited for ICLR. I really do not think a combination of CutMix with existing saliency detection method is a novel method. These main concerns are not addressed by the authors. So, my final recommendation is still rejection.
Accept (Poster). rating score: 7. rating score: 7. rating score: 6. rating score: 2. <BRK>Summary:This paper focuses on the known problem that current NLP models tend to solve tasks by exploiting superficial properties of the training data that do not generalize. In this paper, the authors propose a method based on product of experts that doesn t assume particular knowledge of specific dataset biases. In particular, the analysis showing that the weak learners do in fact adopt the biases which have been documented elsewhere in the literature is interesting, and the discussion of "how weak does the weak learner need to be" is appreciated (a few questions on this below). The authors do a good job of directly contending with the similar contemporaneous work in their paperAdditional Comments/Questions:Just a few thoughts that came up while reading...* The weakness of weak learner analysis is interesting. I imagine this is not something that can be understood in absolute terms, i.e., I would not expect there to be some level of weakness that is sufficient for all biases and all datasets. For research questions such as this ("is the model using the heuristic?") I always find it unsatisfying to think about performance gains that are in between 0 and 100.<BRK>The experiments are interesting. This is the exact scope of this paper. ## SummaryContext:The paper focuses on automatically detecting data biases learned by natural language processing models and overcoming them using a learning strategy. Problem:The authors identify and tackle issues of state of the art methods:  they are required to already know about a certain bias to be overcome. Claim:  A weak model can be used to discover data biases  The proposed method produces a main model that generalize better to out of distribution examples## What I liked the most  meta problem of automatically detecting and overcoming biases in neural networks is critical  well contextualized  relevant issues of state of the art have been identified  intro and related work are easy to read and understand  novel, simple and interesting method to tackle them  interesting figures  experiments are  interesting and well chosen## What could be improved1. "Clark et al.2019 Don’t take the easy way out: Ensemble based methods for avoiding known dataset biases" that you already cite ran some experiments in multiple fields (NLP, VQA, etc.). It is really difficult to understand for readers that are not familiar with the datasets on which you perform your study. even in the caption.<BRK>*Summary*: This paper proposes a method for training model that are robust to spurious correlations, building upon prior work that uses product of experts and a model explicitly trained on a dataset bias (e.g., a hypothesis only model). *Strengths*: A thorough study of using a limited capacity auxiliary model to train more robust models, which helps a final model ignore spurious correlations that are easy to learn. *Weaknesses*: The work is a rather straightforward extension of prior work. Furthermore, the authors only evaluate on 2 textual tasks I would have liked to see more experiments with spurious correlations in vision (e.g., VQA or the datasets used in https://openreview.net/forum?id ryxGuJrFvS), and other experiments on text (e.g., the TriviaQA CP dataset in the Clark paper). 3.While it’s true that the weak model empirically learns to re learn the same dataset biases targeted in prior work (e.g., negation correlates with contradiction), it’s somewhat unclear to me how well this method would translate to a setting with unknown biases. The MNLI / SQuAD examples are a bit artificial since we already have knowledge of the bias it’s possible that weak learners can pick up on spurious features that are “easy to learn”, which are the same ones that humans notice. This is another reason why I think more experiments would be useful.<BRK>Paper summary:The authors argue that they have proposed a method to train robust models to biases without having prior knowledge of the biases. Reasons to reject:1) The authors argue they have shown the model with limited capacity capture biases. 2) The main method proposed in this paper, is exactly the same method proposed in [2]. 3) About the third argued contribution on showing how the performance of the debiasing method change based on the capacity of weak learners, in [1], the authors included the discussion between the choice of weak learners on their impact. Given the points above, and since the main method in the paper is proposed in [2], the paper does not provide enough contributions to be suitable for the ICLR venue.
Reject. rating score: 4. rating score: 5. rating score: 5. rating score: 5. rating score: 6. <BRK>[Strengths]The experiments have good coverage on four popular datasets. [Weaknesses]It is unclear what the novelty is in the proposed method compared to (Han et al.2020).The primary difference is replacing the self supervised learning method (rotation) with contrastive learning. I appreciate that. Why does the proposed pipeline perform better than RS? Is it a counterexample of WTA? The paper should summarize RS s design and explain why it can not do end to end training, then explain what modifications are made to make this possible.<BRK>I think it is necessary to provide more insight into these differences before readers can have full confidence in the proposed method. On the topic of fair comparisons, Han et al.2020 seems to report stronger performance than is credited to them in Table 3. The paper is generally readable and the figures and tables are clear.<BRK>Although I still think the proposed method is reasonable and the empirical results are impressive, I agree with other reviewers that the paper should have done more analysis on why the proposed method works to convince the reviewers (and the future readers), and to give more insights on why the combination of losses (and the use of WTA) is a right way to go.<BRK>The current description would make the readers distracted to concentrate on the cores. It would have been effective to present the combination of RS [Han et al., 2020] + WTA hashing based clustering, whereby the source of the performance gain becomes clearer. But these are the side loss functions. Reasons for score: The proposed method shows significant performance improvement for the target task, but the manuscript seems not ready for publication to this reviewer in that: 1) it is unclear specifically what makes the proposed method better than the competing method [Han et al.2020], 2) the motivation of the proposed method sounds hand wavy (in particular, why should the multi modal extension be considered?), and 3) there is room to improve the paper organization further (it comes and goes).<BRK>Pros:+ The paper is easy to follow. Cons:  The presentation of contribution two is not convincing in two ways. There are many losses and design options making the practicality of the framework doubtful. I understand there is a comparison between different alternatives in appendix A, but only WTA is outperforming RS in the existing work by a large margin. All the other reviewers and I are curious why this is the case. Because the choice of K is usually 2,4,8,16, it is more like local ranking information within a few random dimensions, while RS in Han et al is comparing the ranking of x and y globally (across d dimensions).
Reject. rating score: 4. rating score: 5. rating score: 6. rating score: 8. <BRK>This paper proposes a new text to speech synthesis (TTS) framework that does not require attention mechanism at inference time. The authors show that naively predicting the duration in the unsupervised setting does not work very well, whereas combining FVAE makes it work significantly better. Finally, the authors suggest two evaluations metrics, 1. Unaligned Duration Ratio (UDR) and 2. ASR word deletion rate (WDR), to test the robustness of TTS systems. 3)FVAE is also not proposed by the authors Lack of some experiments I assume the biggest advantage of Non attentive Tacotron is the robustness in generation quality. Please elaborate more on how it can be formulated as conditional VAE framework. Rating:I consider it “not bad” paper, but I think the impact of this paper is not strong enough to pass the bar of ICLR because of the lack of novelty (as written in Weakness part).<BRK>Instead of an attention mechanism, a duration predictor is utilized to improve robustness, which is evaluated by two metrics, unaligned duration ratio (UDR) and word deletion rate(WDR). Reasons for score:The paper is well written. The experiment results of improvement in robustness are convincing. However, using similar ideas to improve the robustness of end to end TTS has been investigated. Cons:  This paper mainly focuses on improving Tacotron 2 with regard to robustness. There should be more experiments on the comparison between Non Attentive Tacotron and similar work, such as DurIAN, non autoregressive FastSpeech, etc.<BRK>This paper presents an approach based on the Tacotron model for speech synthesis, where the attention mechanism is replaced by a duration predictor. The paper also introduces two metrics to evaluate the robustness of the model. The experiments shows that the proposed model is on par with the Tacotron baselines in terms on MOS score and better in terms of the new metrics. The new metrics are very welcome, as the evaluation tools for TTS are limited. The paper is clearly written. Cons:  The novelty is limited as it is mainly an incremental improvement to an established approach. The authors should provide a brief description of the attentive Tacotron and clearly explain which parts they modified. It could also be a good spot to present the Tacotron approach. Overall, the paper s novelty is limited, mainly due to it s incremental nature, but the unsupervised training capabilities and the new metrics are  significant, so I put it just above the acceptance threshold.<BRK>The key contribution of this paper is replacing the attention mechanism of the Tacotron 2 with an explicit representation of token durations. The authors propose different methods toward that end. First, they introduce a duration predictor in the Tacotron 2 model architecture which utilizes the encoder features to predict the durations. The proposed non attentive Tacotron model achieves similar naturalness scores to Tacotron 2 which is very close to ground truth naturalness. More widespread adaptation of robustness as a criteria in addition to MOS scores is required. These objective metrics which can be computed for large datasets can be very helpful.
Accept (Poster). rating score: 7. rating score: 6. rating score: 6. <BRK>This paper provides a unique perspective on the implicit regularization effect of gradient descent that has been observed and studied previously. This work provides a very illuminating perspective on gradient descent through an extremely simple idea. The coefficient of the first term in the analysis scales with the learning rate and the number of parameters. Synthesizing all the information above, I think this paper should be accepted. I think the paper is rather well written.<BRK>The authors adopt a standard argument from the backward error analysis of Runge Kutta methods to show this phenomenon. The paper proposes a natural form of implicit regularization when gradient descent is performed to train neural networks. I favor the explanation of the predictions made in the paper and the geometric interpretation of IGR. The idea of backward error analysis is clear and comprehensible. In particular, the paper considers explicit gradient regularization which has been discussed and studied for a long time. Moreover, the gradient regularization seems to be very natural in the gradient descent. As the authors stated in Prediction 2.2, it is possible that the loss surfaces have nearly equally flat minima. Third, given the sophisticated interactions between different sources of implicit regularization, the paper is lack of careful discussion on other related implicit regularization.<BRK>## Edit after rebuttalI have updated my evaluation (from 4 to 6) based on the changes made in the manuscript and the responses by the authors. ## Summary of merits and concerns### Merits+ The paper presents a compelling mathematical analysis of the discrepancy between the actual trajectory of gradient descent and the gradient flow that determines it. + The sections of the supplementary material concerned with mathematical proofs and derivations (I have more carefully read A.1 and A.2) are very clearly presented+ The authors identify a good number of related articles. I would like to note that this denotes a relevant lack of transparency in the report of the results. However, since it is indeed included and discussed in the paper, I contend that the experiments should be more rigorous and the claims more careful.
Reject. rating score: 4. rating score: 4. rating score: 4. rating score: 4. <BRK>The paper did not dig further with the imbalance by investigating which subsets of weights are more useful, and did not give much explanation with experiments on why the imbalance exists. To prove this hypothesis, it goes on to define and study the "utility imbalance" of the weights and its changing with the pretraining, pruning, etc. I think this is a more important problem than its relation with network size. For example, in [2], the authors showed the imbalance usage within 3x3 convolutional kernels. 4.The paper only discusses the conclusion at [1] for the utility imbalance at initialization. I think some experiments could help better understand the cause of imbalance at initialization. The conclusions are kind of vague too.<BRK>e.g., "The weights is struggling to be utilized more, rather than SGD purposely differ the utility among the weights?" With that said, I remain concerned with many technical aspects of the paper, for example:* The scale of the networks studied. * I still don t understand why this particular definition of utility imbalance is well motivated. * I generally don t think that two dimensional loss landscape visualizations are informative since they discard an enormous amount of information from the full loss landscape. I also can t make sense of Figure 1. * Explain why this is called "utility imbalance"* Explain jhow the results in sections 2 and 3 related to each other and the larger narrative/takeaway that they provide. # Notes## AbstractWhat do the authors mean by "the pruning mechanism"?<BRK>In general, I liked the approach chosen by authors to study the effects of pretraining and weight utilization in pruning. However, I don t think that the utility imbalance measure is a unique mechanism for such analysis and one can choose other possible options. I don t think that this information is sufficient to obtain an accurate visualization. Therefore, I don t see much contribution here. Minor concerns: \ . some parts of the paper is relatively hard to follow (e.g.in Section 2, jumping from experiment setting to discussing results and vice versa);\ .<BRK>Overall: The paper is going in an interesting direction, but I find the paper to be unclear and I do not see how their chosen measurements lead to their claims. Thus it is only natural that it increases   I don’t see how this supports the claim that the loss landscape became sharper. Also, what does “each of the weights is struggling to be utilized more” mean? * Section 3 paragraph 3: what are the “assumptions for the heuristic”? There are also several points in the writing that are unclear. If it is just my misunderstanding, I will gladly read any explanations that the authors can provide. What does it mean for networks to have high utility imbalance given the definition you provided?
Accept (Poster). rating score: 7. rating score: 6. rating score: 6. <BRK>The paper considers the problem of solving differentially private empirical risk minimization. To reduce the dependence on dimensionality $p$, they propose Projected DP SGD (PDP SGD) that projects the noisy gradients to a low dimensional subspace computed from a free public dataset at each iteration. The theory indeed manifests the effectiveness of the proposed method. 2.The authors propose to reduce dimensionality by projection. The projected space varies for each iteration and is computed from a public dataset. I have read the authors’ rebuttal.<BRK>In comparison, our work studies both convex and non convex problems and our analysis applies for more general low dimensional structures that can be characterized by small γ2 functions (Talagrand, 2014) (e.g., low rank gradients and fast decay in the gradient coordinates)". They show that the algorithm can provide a better convergence guarantee, specifically, p reduced to log(p). They also conducted experiments to show the proposed algorithm outperforms the generic DPSGD especially at small epsilon with a small amount of public data. A minor point: Sec 4 said "Papernot et al.(2020) shows the accuracy of DP SGD is around 80% when ε ≈ 1", which I m not sure is accurate.<BRK>The paper proposes a new private SGD method by projecting the noise gradient onto a subspace, which is estimated using some public datasets. The proposed method can improve the utility guarantee by reducing the dependence of the problem dimension p. The idea of the proposed method is interesting, but the assumptions in the current paper seem to be very strong, and the evaluations in the current paper are not convincing to show that the proposed method is beneficial. It is important to have empirical evaluations on this.
Accept (Poster). rating score: 8. rating score: 7. rating score: 7. rating score: 5. <BRK>Summary: This paper demonstrates and analyzes the accuracy disparities induced by selective classification on subgroups. Overall, this paper is a useful addition to a growing literature on the disparate impact of various machine learning techniques on subgroups. Note: I did not evaluate or check all of the proofs. * The DRO section (7) feels like an afterthought, but it is actually quite important to the paper. The connection to DRO suggests a potential solution, at least in practice, for the problem identified in this paper.<BRK>The paper draws attention to a problem in the selective classification. 2.The proposed margin distribution for analysis is reasonable. Hopefully the authors can address them in the rebuttal period. Cons:1.Although exploiting the margin distribution can provide good reasoning on the monotonicity of accuracy coverage curves, analysis of comparison to group agnostic baseline seems not very clear to me: (1) In that section, the authors consider the case of a mixture of two groups. Page 4: there are extra "]" in both C(tau) and I(tau).<BRK>This paper discusses the impact of classifier abstention on the performance obtained for different groups of data. This is an interesting paper about an original research topic.<BRK>It would be natural to expect the analysis on observations can lead to a solution for the observed problem. The authors studied the margin distribution to get some understandings about the problem. If the disparity across different groups is important, performing a uniform standard classification over the unified data is not a good idea in the first place. Moreover, it is unclear whether all the phenomena of selective classification magnifying disparity can be captured in their concepts and analysis, since their findings are based on empirical observations over a few datasets.
Accept (Spotlight). rating score: 8. rating score: 7. rating score: 7. rating score: 6. <BRK>I advocate for the acceptance of the paper. Training deep learning models is becoming increasingly challenging due to a memory bottleneck that limits the size of the feature maps that can be stored. The novelty of MONET is that it jointly optimizes over: (a) global compute graph level techniques (such as checkpointing) and (b) local techniques (such as memory efficient implementations of individual operators).<BRK>Is that correct? If so, please (please) state this explicitly in the paper. Please include the version of PyTorch which was forked for the Monet  and Checkmate implementations in Section 5.<BRK>for both memory consumption and runtime calculations. This enables the authors to make both global and local decisions and to exploit the synergies in both.<BRK>In this paper, the authors propose MONet which tries to find the best checkpointing schedule that can jointly optimize both the above channels. The authors create an auxiliary graph to encapsulate operators and perform schedule optimization on the new graph rather than the usual graph in existing frameworks. Then, under a fixed memory budget M, we try to optimize the operators for computational efficiency.
Reject. rating score: 3. rating score: 4. rating score: 4. rating score: 6. <BRK><Conclusion>My initial decision toward this submission is rejection because the main contributions of this work are not fully convinced. (1) This work claims that “the current methods do not effectively maximize the MI objective because greedy SGD typically results in suboptimal local optima.” However, there is neither theoretical justification nor empirical results that show why the proposed approach does not suffer from this issue and why it is subsequently better than other works. Conceptually, they could be reasonable solutions to the issue, but there is no theoretic and empirical evidence that supports this argument. This work summarizes the two contributions in sec 1.2.<BRK>Extensive analysis and comparisons. The reviewers had two major concerns: (i) Theoretical or empirical justification/proof for the following claim (and the motivation) of the paper: “the current methods do not effectively maximize the MI objective because greedy SGD typically results in suboptimal local optima”. (ii) Lack of comparisons with newer methods from e.g.ECCV2020 etc.<BRK>Authors support their method with an experimental study. I have several concerns about the paper:a)	Authors state that some greedy optimization may end up lower MI optima. Since deep neural networks are not convex, is this a surprising point or is this expected one? b)	I think empirical comparison needs significant improvement. Authors mention the proposed method was outperforming the state of the art “at the time of writing”. However, currently I believe this is not the case. As far as I understand it is adding some extra blocks to already existing method and calculates mutual information between heads. Although I have some concerns about the paper, I would like to be extremely clear that I am open to change my view if more explanation and/or evidence supplied.<BRK>In this work the authors argue that IIC performs suboptimal maximization of MI, thus the obtained solutions focus mainly on low level representations and do not distill higher level semantic similarity of images. The proposed objective function is novel, however it involves a regularization constant \alpha to be set by the user. The experimental results indicate improved clustering performance. It in this work K 8 is set. 3) What happens for values of \alpha greater that 0.05? How sensitive is the method on the value of \alpha? 5) It is not clear whether IIC results are obtained using Sobel preprocssing or not. 6) The presentation of the paper could be improved, by first briefly describing the IIC approach and then presenting the proposed extension (DHOG).
Reject. rating score: 3. rating score: 4. rating score: 4. rating score: 5. <BRK>The texture sliding neural network (TSNN) is trained using the ground truth offset computed for each camera and pose. * Pros1) The proposed TSNN leads to the performance gain in the virtual clothing* Cons1) The current manuscript requires major revisions by addressing the following concerns. Especially, the following sentence just states the problem without how to address it. It is needed to mark both camera view and pose in Figure 4. Algorithm stating the overall procedure of the proposed method would improve the readability. 2) Comparative study seems to be insufficient. It seems that a more thorough performance analysis is needed.<BRK>The paper proposes a method to correct high frequencies details in the textures of animated clothes. Finally, the shown examples and experiments are limited to a few setups, without stressing the limits of the method. If this method can be applied also in real scenarios it would be really interesting. 2) Presentation:I had some problems following the train of thought in some passages. I think it can be clearer by:  Highlighting the contribution in the introduction, to clearly state the novelty of the work  Provide a more structured taxonomy of the previous works for the reader.<BRK>I would suggest the authors turn down the claims in the paper. The paper argues to learn the geometric details, thus a careful analysis of the reconstructed 3D geometry (especially emphasizing on the geometric details) would be more convincing. c).the quantitative analysis on the geometric details, how good the method can capture the details comparing with the baselines. 3.The scope of the paper is narrow. so I vote for a reject initially.<BRK>Negative: In the manuscript, it is mentioned that is necessary to train a separate network predicting texture perturbation for each camera. The approach is claimed to be applicable to a general setting in the manuscript ("We focus on the specific task of adding highfrequency wrinkles to virtual clothing, noting that the idea of learning a low frequency embedding may be generalized to other tasks”). Unless other applications are presented, I find the authors claim to be misleading. If so, this should be stated more clearly. Some math would have helped making the paper easier to read and more set contained. Post author response:After having carefully read the author s response and additional reviews, I confirm my original recommendation.
Accept (Poster). rating score: 7. rating score: 7. rating score: 7. rating score: 6. <BRK>The authors consider the problem of double descent in regularized linear regression estimators, and show that, with optimal regularization, such problems can be totally mitigated when the covariates are isotropic gaussian. Additionally, the authors show that there exist cases where ridge regression is provably non monotonic in its average risk. These theoretical results are complemented with some empirical results on ridge regression and small scale neural networks. The phenomenon of double descent has gathered substantial interest in the past couple of years, and this paper presents an interesting contribution towards cementing our understanding in the context of double descent in linear models. The paper is clearly written, and exposes the results in a clear and accessible fashion. I broadly agree with the points brought up by the other reviewers, and despite some weaknesses brought up by various reviewers, this paper is a good contribution to the community.<BRK>:The proof techniques seem to heavily depend on the specific choice of the loss function and the regularizer, that is, the mean squared loss and the ridge penalty. It is not clear if the techniques can generalize to other settings. To be more precise, the main takeaway from the paper is that optimal \ell_2 regularization can mitigate double descent, provably in certain linear ridge regression problems; and in practice, in certain deep learning problems. The paper is well written for the most part. I found the theoretical results insightful, and well supported by experiments.<BRK>Could the optimal regularization amount be so high that it starts to hurt performance? In my view, the paper takes an important question and analyzes it well from a theoretical angle and also provides empirical evidence to back up its main message in more complex models. The proofs are non trivial and I think the paper adds value in improving our understanding of the double descent phenomenon by providing a clear picture of the non asymptotic regime.<BRK>Pros:+ The motivation of studying the double descent phenomenon with optimal regularization is well explained in the introduction. Connections and comparisons with existing related works are discussed clearly. + The presented theoretical results on the linear regression model are non asymptotic, which is new and different from existing works. Cons:  My main concern is the generality of the results. The paper mainly focuses on a simplified linear regression model, where the response variable is linearly generated using some ground truth parameters \beta^*. The experiments need to be more extensive and better explained, especially for the CIFAR 100 experiments. It is important to discuss this difference clearly at the beginning.
Reject. rating score: 3. rating score: 5. rating score: 5. rating score: 7. <BRK>Overall, I found the main idea of this paper very interesting and the experimental results promising; however, there were several major and minor technical issues with the work that need to be resolved. In equations (4) and (5), the authors jump from a constraint on the true distribution to a constraint on the empirical distribution. 2.What score was used to select the best $\lambda$ on the validation set? In particular, it is never really made clear what the motivation for the work is. Extrapolating from the experiments, it appears that the goal is to derive an estimator with lower variance than existing estimators, but that is not stated in the intro. This constraint reduces variance because ABC. Additionally, lit review portion of the intro reads as a random list of methods with no clear connection between them or to the proposed method. It is incorrect that "the outcome of an alternative treatment has to be estimated". ": It is my understanding that TMLE is also semi parametrically efficient, so I believe this statement is incorrect as are similar statements in the "Comparison to other estimators" section.<BRK>The main idea is to use this orthogonality constraint during estimation of the model parameters as a regularizer. On the theoretical side, the authors provide sufficient conditions under which the regularization yields an asymptotically normal estimator for the average causal effect. It is not clarified in the paper that why the proposed regularization should in fact improve the estimation bias and variance. Page 4, the authors say "if we had access to the treatment effect \psi(X) f(X,1) f(X,0), we would also have access to the untreated outcome Y(0) even if we did not observe it. This does not seem to be true.<BRK>This paper proposes a novel regularization term for designing loss functions to estimate outcome and propensity score models, where the end goal is to estimate ATE. The method performs admirably: it is competitive on IHDP, and in a statistical tie with the best performing methods on Twins and Jobs. Unconfoundedness is an assumption that ensures that an estimator of the form of equation (3) is unbiased for a causal effect. However, the results are compelling. I have some technical questions that I believe the authors should address:*    Moving from equation (31) to (32) seems to require omitting any randomness in the denominator. Why is this justified? Are the authors assuming throughout that the treatment effect is deterministic given $X$?<BRK>The authors propose a regularization framework based on the orthogonality constraint and prove that a resulting estimator is doubly robust, asymptotically normal and with efficient variance. In summary, I am convinced that the this paper would be a valuable addition to this year s conference. Weak points:   The code for their simulations is not accessible (broken/incorrect url?). A discussion about the impact of the hyperparameters, especially the orthogonality regularization parameter $\lambda$, would give more insight into the importance of the contribution of the regularization term. Have the authors studied the behaviour of their method and its performance in (simulated) cases where unconfoundedness (4) does not hold but the orthogonality constraint (5) holds? Minor comments (that did not impact the score):   p. 2/3: equations (2) and (4) are the same.
Reject. rating score: 3. rating score: 4. rating score: 4. rating score: 7. rating score: 9. <BRK>## Summary: The authors develop a method to estimate the effect of a static treatment on outcomes over time under temporal confounding, with an emphasis on making the method transparent. This means that in order to use this model i.e., in order to obtain the ITE estimate for a new test patient, we need to first observe the outcome under t   1 or t   0. This makes the model not useful for making treatment decisions: it would only be useful to validated that some chosen treatment choice was good/bad. I do not follow the logical jump from "there is a large estimation error" to there is additional hidden confounding or the data generating model is not right. (2) In eq 5 m_{is} is not defined anywhere(3) The authors don t make a clear case (empirically or in the writing) as to why their chosen approach is transparent. This b vector can still be very high dimensional even with an imposed sparsity. (4) It is somewhat odd to have a causal graph where there is no arrow between the treatment and the outcome.<BRK>Estimating the causal effect with time series data is a practical and important problem. It is not clear why this matric is good a priori. It is possible that the network selects an arbitrary set of units to construct the control. A discussion on whether this assumption is required for synthetic twins to work would be useful. The experiment section could use more clarification. However, to show that the method uses fewer contributors, the paper only compared against synthetic control, even though robust synthetic control is a more natural comparator, as it explicitly isolates a small subset of contributors during its data pre processing. In the real world dataset, 1) while the train.<BRK>Summary:This paper provides an approach for treatment effect estimation when the observational data is longitudinal (with irregular time stamps) and consists of temporal confounding variables. The proposed method can be categorized under the matching methods, in which, in order to estimate the counterfactual outcomes, a subset of the subjects in the opposite treatment arm (i.e., contributors) is selected and weighted. The paper is easy to read and understand. For example, it is unclear why there is need for a new time aware representation $o_{is}$ while we already do have one, i.e., $h_{is}$? The authors claim in section 3.1, paragraph 1, lines  6 to  4 that their model does not over match however, $\mathcal{L}_r$ exactly does that.<BRK>The paper works with a particular model of the data generation and propose to do synthetic control in representation space which ensures that the considered data generation/model are not inflexible. While A.1 provides some justification in this regard, I would ve liked to see a paragraph about the identification guarantees for the proposed algorithm. It seems to me that the functional linearity assumption of the model is what allows the method to not require sequential overlap. I liked the idea of using the control outcome as a way to understand whether there is unobserved confounding as per the modelling assumption.<BRK>I m going to keep this review short because I thought this was a really well motivated and executed paper. The paper builds on the  synthetic control  approach that is popular in econometrics which uses a weighted sum of the outcomes of individuals in the control group as an estimate of the control potential outcome, which can then be compared to a given treatment outcome to estimate the individual treatment effect. My only substantive complaint is in the framing of the paper: the introduction describes this temporal setting as more challenging than the static setting, but given that the paper is not about dynamic treatments (which are indeed far harder to deal with), the fact that you see multiple observations for any given individual makes the problem easier not harder... To be clear   this is not bad thing, but rather than saying, "there are many methods for the static setting, but few are able to address temporal confounding" (which isn t really true   these approaches could easily be adapted to the single treatment temporal setting using the appropriate recurrent architectures)   instead emphasize the fact that the static approaches don t take advantage of multiple observations from the same individuals.
Reject. rating score: 4. rating score: 5. rating score: 5. rating score: 5. <BRK>Pros:This paper proposes a novel and interesting problem for cross modal transfer. The goal of the paper is to learn a mapping from image to state such that at test time the agent can directly use this mapping with a trained policy from the simulator to perform in the target domain. The experimental section needs clarity in baseline details. Cons:Are there any assumption about the target data?<BRK>This is a relevant problem to enable sim to real transfer. I have some concerns  which are listed below,1. The paper only shows transfer from the robot s state to images of the robot in the environment. that wasn t part of the recorded trajectories but was part of the full state policy. Making sense of vision and touch: Self supervised learning of multimodal representations for contact rich tasks.<BRK>My main concern is that it is currently hard to judge the thoroughness of the experimental evaluation since it remains unclear how the baseline is implemented and why it fails. **no investigation into why baseline does not work**: figure 1 provides one possible explanation (because of not taking dynamics into account), but later the paper mentions it could be because the necessary biases present in image to image translation are not present in image to state translation, finally it could be because the method was not tuned sufficiently. Therefore I cannot recommend acceptance at this point. I think the related work section could benefit from adding a discussion about this.<BRK>Overall, the paper is clearly written, and the topic is new and relevant to the field. ### Weakness  Insufficient experimental results. Specifically:   1. What s the performance of the policies trained in the original state space? Knowing these numbers gives us a better idea about how well the adaptation works. Unclear description of GAN baselines. Did I miss something?
Accept (Poster). rating score: 8. rating score: 7. rating score: 5. rating score: 5. <BRK>This paper presents an interesting conceptual advance connecting causality, disentangled representation learning, invariant representations and robust classification. The authors propose a Counterfactual Generative Network (CGN), which is basically "modular" generative adversarial network that can independently control the generation of independent factors of variations in the data corresponding to Independent Mechanism, i.e.independent factors in the structural causal model of the data. As the authors explain, this can be thought of as a generalization of "domain randomization".<BRK>  Summary  This paper proposes a new generative model that generate images from 3 seperate aspects: foreground masks (shapes), forground texture, and backgrounds. I encourage the authors to be upfront about the limitations of this method and write better descriptions of loss and hyperparameters tuning. Maybe authors can be honest about it. The loss descriptions in Appendix B should be moved to main text to help readers understand the method. 2.The name "pre masks" is confusing that originally I think it s a binary mask, but instead it s a colorful image. 3.More failure examples in Appendix E will better help readers understand its limitations. Evaluations  Overall I like this paper.<BRK>The proposal in the paper is to learn to generate samples where these correlations can be eliminated. To this end, the authors, distill trained conditional big gan into a transformation with explicit modules to capture the shape, texture of the foreground object, and the background. The proposed approach is motivated by the assumption of independent mechanisms where different modules of the causal data generating process are independent of each other. Once the decomposition of a training image into shape, texture, and background is obtained, any component can be swapped to generate counterfactual data. The proposed method appears to assume that the causal structure is known. in the invariant MNIST classification task it appears that the results are based on the assumption that the invariant feature   shape is known apriori. Some related work that seems to be missing [1][2][1] Kocaoglu, Murat, et al."Causalgan: Learning causal implicit generative models with adversarial training."<BRK>The main idea of the paper, i.e., using independent causal mechanisms to generate interventional images, has already been explored by Kocaoglu et al.in Causalgan: Learning causal implicit generative models with adversarial training, ICLR 18. Same as here, the authors there also "view image generation as a causal process" and "structure a generator network as a structural causal model (SCM)" and use a conditional gan to generate the image from the labels. I believe the experimental section should put more weight on this setting. Therefore, the authors should definitely cite this work. My general remark is that there is very little causality in the approach. The generation used here based on three variables, i.e., shape, texture and background seem to be a special case. we can intervene on a subset of them and generate counterfactual images " > What the authors call counterfactual images are actually interventional images from a causal point of view. Please consider changing "counterfactual" to "interventional" throughout the paper. "From a causal perspective, we maximize the average causal effect (ACE) of one IM on the classifier’s decision, while minimizing the ACE of all other IMs." The intuition on comparing with other methods is missing. I would like to thank the authors for their humility in the rebuttal and for clarifying the paper s contributions. Accordingly, I will increase my score. However, I still believe Section 3.1 s contribution, and the follow up of using this to improve classifier robustness, is useful only for a very specific type of data and it is hard to assess its value from a practical point of view.
Reject. rating score: 6. rating score: 6. rating score: 6. rating score: 7. <BRK>**Summary**The authors propose a generalization of Value Iteration Networks to unknown, potentially continuous state spaces. **General**Overall the approach is interesting, but the current results are somewhat limited and the model, in particular the executor, seems to depend critically on the availability of a suitable graph for pre training. The authors leverage a "CartPole inspired" graph for the majority of the tasks, and the tasks seem to have been chosen to be consistent with the statistics of this synthetic graph (from the text, Freeway s " up and down  structure aligns it somewhat to environments like CartPole"). While the authors illustrate a limited ability to generalize from graph/task misalignment on the maze task (8x8 to 16x16), it s still not clear that the model will generalize to more complex tasks where it s not straightforward to pre train on a representative graph. The main question here for me is whether the paper reaches the threshold on originality and significance. However, the limited results and dependence on the availability of a suitable graph makes the significance of these extensions unclear. Nonetheless, it could lay the framework for future work that pushes the VI framework into new domains.<BRK>Summary:The paper proposed a framework that combines TransE style world model learning and value iteration networks. Since the output of the algorithm is a policy, the model was trained using PPO. Claims1.The authors have made very strong arguments about the generality of the proposed method: "As a result, we are able to seamlessly run XLVINs with minimal configuration changes on environments from MDPs with known structure (such as grid worlds), through pixel based ones (such as Atari), all the way towards fully continuous state environments, consistently outperforming or matching baseline models which lack XLVIN’s inductive biases." The authors have made several arguments about non discrete and non deterministic environments in the paper, however, they haven t shown any results on challenging continuous and stochastic environments. How will the model perform in partially observable environments? If the proposed algorithm does not work for continuous action spaces (Page 4, footnote), the authors should make their claims weaker: finite and discrete action spaces + finite planning horizon makes the number of states visited also finite. Overall the experiments are relatively weak.<BRK>This paper proposed a novel policy prediction model that combines self supervised contrastive learning, graph representation learning and neural algorithm execution to generalize the Value Iteration Networks to MDPs. The method described in the paper is a combination of existing works in the literature but seems to work well in practice. My comments are based on the assumption that no existing work using GNN to do further representation learning on top of the individual encoded information. It would be better if the authors clearly state this novelty at the end of related works. Pros 1.The paper is very well written and provided sufficient background knowledge to let the reader follow the description. Cons 1.The novelty of the proposed model is a bit weak in terms of a lack of specialization for this particular task. 2.Figure 1 in the paper is not quite meaningful.<BRK>The paper tackles an open problem of the value iteration network paradigm. The proposed method (XLVIN) has a conceptual edge over traditional value iteration networks in that it can be applied to continuous problems and problems where the state space is either too big or not fully known in advance. The explanation provided in the paper makes sense but on the whole I get the impression that more experiments are necessary to asses whether XLVINs can truly replace VINs in discrete environments. There is also quite a bit of algorithmic complexity that results from using the graph neural network paradigm. This is justified by the fact that XLVINs are also applicable to a much wider range of problems. ### Pros* The method is applicable to continuous problems and large or not fully known state spaces, where VINs are not. It s understandable that some content needs to be left out due to space constraints, but at the same time, these questions would be at the heart of future work on this topic and the paper would benefit from addressing them.
Reject. rating score: 5. rating score: 5. rating score: 7. rating score: 8. <BRK>### StrengthsThis paper presents an intriguing analysis of the gradient distributions over the course of training for popular RL algorithms in common mujoco benchmarks. The authors also propose a new alternative (inspired by robust statistic) to the simple PPO clipping heuristic that does reasonably well even if it doesn t deliver any clear improvements over PPO. ### WeaknessesThe paper is well motivated, and has a collection of interesting observations but I am not sure if these empirical observations lead to something beyond that, in terms of a more general claim/conjecture. It seems a bit mysterious that this is only present in the on policy case and disappears when off policy. In particular why would the heavy tailedness in the importance ratios and the heavy tailedness from advantages be mutually exclusive?<BRK>I like the paper, but I do think the paper misses some comparison to prior work and alternate views of looking at the same phenomenon which I will discuss next. I am not sure if heavy tailedess of gradients is the best way of looking at it. Therefore, these are alternate ways of explaining the same issue that the paper points out. Perhaps some analysis in settings where optimization challenges exist and a comparison of the impact of heavy tailedness in scenarios where optimization challenges don t exist is a possible way to answer this question. Or for instance, centering the advantages which PPO uses can also help (see https://openreview.net/pdf?id SJaP_ xAb) and maybe even using self normalization on the importance weights which are optimized in PPO. How would these solutions compare to the proposed estimator?<BRK>Conditional on these concerns being addressed, I would recommend acceptance. **Major Comments:**While the likelihood clipping term in PPO is certainly heuristic, it serves a purpose that is not properly discussed in the paper: to prevent $\pi_\theta$ from deviating from $\pi_0$. This is a problem for the off policy analysis, since the PPO NoClip objective and its gradient do not relate to the original expected return objective, and so may not be meaningful quantities to measure or analyze. My initial assessment is that this highly reduces the value of the off policy experiments for me. This could be more clear.<BRK>Characterization of the heavy tail property permits further understanding of why our algorithms might fail, and how we might improve them. I would have liked to see error bars on figs 1 4. Are there more disaggregated plots of the kurtoses over the environments (in addition to the three in the appendix)? I m not totally sure what to make of averaging over environments whose dynamics could be quite different. For each environment, it also seems that only 10 seeds were run. **Summary**I m leaning towards a marginal accept for the paper: I do think the contribution of the paper is valuable, despite the concerns I have outlined above about statistical significance. I think this paper otherwise provides much value in characterizing an important property of PPO, opening the way for future RL algorithms that can better deal with heavy tailedness.
Reject. rating score: 4. rating score: 4. rating score: 4. rating score: 6. <BRK>Summary This paper presents an active learning approach to allocation of labels across tasks in meta learning. The approach is based on a contextual bandit setting, and the approach is tested on linear regression, sinusoidal regression, and a version of CIFAR. Comments The direction considered in this work is interesting, but the paper feels largely incomplete. Formulating the problem as a sequential decision making problem is logical. Indeed, given the sparsity of labeled data, it seems unlikely that a large amount of validation data should be withheld from training. I could not find a discussion of the size of this validation set.<BRK>The authors study the problem of finding the optimal allocation of labels across training tasks given a fixed budget. More specifically, the authors get the result on CIFAR FS that the optimum allocation is close to the maximal number of tasks, which is weird and not consistent with previous studies in the paper. Although the authors provide a possible interpretation that the model has enough capacity to perfectly cluster 5 classes with a very small number of examples for each class, such an interpretation is not convincing considering that meta learning models still have a big room to improve. Overall, the question proposed in the paper is interesting and important. I will suggest that the authors should further study the weird outcome on the real world datasets.<BRK>Meanwhile, for the cons of this paper, I can t really see the significance of the problem of data allocation, particularly the data allocation set up in this paper where each task has the same number of datapoints. This paper proposes a data allocation scheme for meta learning. Finally, the paper is not evaluated on some widely used meta learning benchmarks such as Omniglot, miniImagenet and etc.. Getting more empirical evidence on those benchmarks would be important. I also like the analysis of the data allocation scheme in the linear regression setting, which sheds some light on how we should balance the number of tasks and the number of data per task.<BRK>********Positives  The paper is well written and well organized. It can be generalized to multi task learning when we have several source tasks for a target task. ", However,  uniform data allocation  assumption does not align with the original question they wanted to answer! It seems to me that they answer the optimal number of tasks in meta learning approaches (MAML specifically) when each task has the same number of data. This problem would be interesting to see for other researcher in this area, and it could be generalized to other related research topics such multi task learning.
Reject. rating score: 4. rating score: 5. rating score: 5. rating score: 6. <BRK> Summary In this paper, the authors release a new dataset    Spoken CoQA which includes an ASR based version of the popular CoQA dataset. The dataset includes the corresponding TTS audio recordings. The cross attended representations are concatenated and used as input to any "CMRC" module. Models have been trained and cross tested using CoQA and Spoken CoQA. Strengths and Weaknesses The paper is interesting overall   while the model by itself is a trivial combination of existing multi model methods, they result in upto 2pt improvements. However, I d argue that  relying on the clean text kind of defeats the purpose of speech based conversational QA as motivated in this paper. Speech based conversation QA is an important problem and the authors make a good first attempt, but unfortunately the paper falls short of delivering a usable dataset for this task.<BRK>The authors also compile a new dataset for spoken conversational QA with the help of text to speech systems. However, I still think this work is not ready to be published at ICLR in its current form. This will give the reader an idea of the accuracy of the transcriptions fed as input to the student model. One of the other reviewers had raised an important point about the reliance of the proposed system on clean text which the authors should consider addressing in an updated version of this work.<BRK>The new dataset Spoken CoQA is derived from CoQA with additional features including audio data and ASR transcripts. It proposes a method utilizing data distillation to learn from speech and text jointly. The most important contribution of this paper, in my opinion, is the construction of the Spoken CoQA dataset. (2) Is it correct to say that the textual input is more useful than the audio input in this dataset? This might be an interesting question especially as the audio input is much larger and hence more difficult to process.<BRK>The task is to answer a question (in written text) given a question that is given in both audio form and text form. They create a dataset for this task by combining CoQA with some off the shelf text to speech and speech to text models. They then propose a new model, DDNet, which obtains improved performance on their dataset. I found much of the motivation for this new task to not be clear from reading the paper. It’s unclear why the Spoken CoQA dataset has to include text transcripts as well as the audio   to me, it makes more sense for that to be part of the model solving the dataset.
Reject. rating score: 4. rating score: 4. rating score: 5. rating score: 5. <BRK>A knowledge distillation framework is proposed for efficient object recognition. In this framework, the teacher network (TN) performs high accuracy prediction while two student networks (SN) mimic the prediction from TN. The design is made for online inference is that the BSN first recognizes the image, leaving the rare category objects to be recognized by the TN. Furthermore, an attention supervision scheme is proposed to enhance the CNN prediction by focusing on meaningful image content. The proposed method has been validated on CIFAR 100 and Tiny Imagenet. The major issue is on the motivation side of this framework design. However, there is no motivation for why distillation can indeed solve this problem. How class imbalance correlates to knowledge distillation is not clear. A common strategy is to adopt a focal loss based loss function to reduce contributions from easy samples. Suppose we want to use two CNNs for cascaded recognition, which is the core idea in this paper, the typical choice is to collect ordinary category objects for the first stage training and rare objects for the second stage training. A pure distillation from the TN does not ensure the differentiation of ordinary and rare categories. 2.The claim of online distillation is weird.<BRK>  The idea is interesting to focus on the frequent labels by distilling a special binary network. Overall, this paper does not reach the acceptance threshold. Detailed Comments:  Why is this method an  online  distillation considering that the teacher network is pretrained and fixed? "The OL detector uses the softmax output of the BSN..." How is the softmax output is used? It should be made clear. In the experimental part, only comparison to baseline methods are provided, but not to state of the art efficient object recognition methods in the literature. There should be comparison to these methods as well as baselines. Besides, the performance on Imagenet (not only tiny imagenet) is a common practice in the literature, but is missing in this paper.<BRK>The authors tackle the problem of efficient object recognition and outlier detection using online distillation. They also provide a large set of experiments to validate the proposed approach. I have two main concerns about the paper:  If my understanding is correct, the *hard positive* samples used to compute the attention triplet loss are selected from the most probable class assigned by the TN while the *hard negative* are using the second most probable class. Why is it desired for these first and second most probable categories to have different attention maps? And a few questions:  How does the SN perform when directly trained on all classes? Is there any reason for which the real valued network can t converge to the same level of performance as the one achieved by the BSN? Since table 5 shows that similar performances can be reached using a Resnet 18, why using a Densnet 201 for most of the experiments? It seems that it can improve the performance in terms of GiE but would just increase the overall inference latency of ENVISE.<BRK>The proposed work trains a teacher student network using an online distillation paradigm. The student is a binarized network (BSN) trained to be accurate on frequent classes. Strengths  The idea of using the attention triplet loss to improve the quality of attention maps of the BSN and thus increasing BSN accuracy is interesting  Experimental evaluation is very thoroughWeaknessesThe main weakness of this work lies in the presentation and organization of the paper. It is not clear why at inference time false outliers are then processed by the teacher network. Lemma 3.1 should be followed by a sketch proof, being one of the main contribution, with the full proof in the appendix (as it is already). The main two contributions are the faster convergence and the use of triplet attention loss. The lemma should have been better highlighted with a sketch proof or at least some intuition so that readers not willing to sift through the appendix could get a grasp of it.
Accept (Poster). rating score: 7. rating score: 7. rating score: 5. rating score: 5. rating score: 4. <BRK>The paper shows that this model can generate several experimentally observed properties of grid cells, and can be used in navigation of novel/mutable environments. Overall, the work should be of interest to any ICLR attendees who engage in research surrounding grid cells. Strong Points:Overall, the paper is very well organized and written. The presentation of the proposed model being able to unite previous normative and mechanistic models is elegant. Given that in the previous section, the authors propose grid cells as being weighted Fourier modes (eq 10), I worry that there might be explicit band cells  in the proposed model. Given that these neurons have been strongly questioned in the electrophysiology literature, this could be a huge impediment to the model, as with previous implementations of OIM. If instead, the fourier component is meant to be distributed among the input population, it the authors should make this clear<BRK>This paper shows how SR representation theory can be used in a model of grid cells to plan and navigate towards a target. A key insight of the paper is that velocity instructions modify the eigenvalues of the SR but not its eigenvectors, so the eigendecomposition does not need to be recomputed for all candidate velocity instructions, and the eigenbasis can be hard coded in the neural circuit (by the grid cells). Strong points:  important contribution to our understanding of how grid cells could support navigation towards a goal  rigorous mathematical derivation of the resultsWeak points:  the paper is written for a highly specialized audience, which limits its impact (see below for detailed comments). Main concerns:1) Too much prior knowledge is assumed from the reader. I have worked at the intersection of theoretical neuro and ML for 8 years and am familiar with some recent literature on grid cell modeling, but still couldn t make sense of many aspects of the paper because of the many assumptions of prior knowledge. One exception is a fairly good description of the SR. What did we learn from comparing these models with the present model? What would be the mechanistic implementation of this algorithm in a grid cell network?<BRK>This paper proposed a model of navigation based on grid cells and the successor representation (SR). 3) The paper claims that this model also unifies the OI models. Overall, I feel this is a good submission, but the quality did not quite reach the bar in its current form. Clarity: The writing requires some improvement. The key idea is not very clear. 5) It is stated that  “a computational role for the neural grid codes: generating a "sense of direction" (eq.9) even in new or bounded environments, via utilising a Fourier basis for a larger toroidal pseudo space”. Is there a way to falsify the hypothesis/model? the paper attempts to unify various kinds of grid cell models, which is interesting. the model is mathematically quite elegant and simple. The “sense of direction” added to the model is not clearly described. It would be useful if the novel contributions could be more clearly stated. The paper would benefit by highlighting the true innovations (assuming there are some). Clearly, the proposed model is related to many of the previous models, but to go one step further and say that it unifies these previous models, that would seem to be a over claim in my view. Are the predictions of the model consistent with neurophysiological data?<BRK>The authors propose an efficient method to predict directed transitions in spatial tasks by extending eigenbasis based prediction model. The authors show equivalence of the proposed method to classical models of path integration by grid cells   continuous attractor networks and oscillatory inferencce. Strengths:1) The proposed method is efficient and doesn t need a decomposition for future state occupancy and can be done directly by re weighing of eigenvector (by eigenvalues) of the transition matrix. 2) The datasets used for evaluation (and studying different properties of the proposed method) are oversimplistic. 3) Assumptions made about applicability of the proposed method to tasks without periodic boundary conditions are not substantiated with experimental evidence.<BRK>This paper extends the intuitive planning methodology with Fourier analysis to predict state occupancy of directed transitions on a graph structured state space. There are no baselines with which to make comparisons. I am leaning to reject this paper. On the other hand, the contribution feels limited, as it seems to apply almost exclusively to 2D gridworlds and velocity based actions. Additionally, many of the insights here about Fourier representations are simple, and the useful connections to reinforcement learning are well known from the proto value function work (Mahadevan, 2005). In the discussion, the paper claims “the resulting prediction framework is computationally efficient.” However, the paper provides no evidence for this claim. The current paper could use some organizational improvements, and the experiments could be more comprehensive and tuned to demonstrate sharper points. How was the data gathered and why does it appear to increase monotonically in discrete steps? I look forward to the author response so that I can refine both my understanding and assessment of this work. A simple editing pass will reveal many spelling errors and typos, e.g.“theorem to calculaet.”Overall the paper is too disorganized, unfocused, and limited to merit acceptance. The demonstrations in Figures 1, 2, and 3 are made in small gridworlds   50x50 is the largest. When it doesn’t, the suggested fix is to inflate the state space to at least twice its size. Currently, a reader would not be able to replicate the presented results. How were the grid cells simulated?
Reject. rating score: 5. rating score: 5. rating score: 5. rating score: 6. <BRK>This paper proposes a generative model for community detection and node representation in a unified framework. The paper also motivates the problem well. 1.The technical approach of the paper seems to be quite similar to the cited work vGraph (NeurIPS 2019). Should not it lead to a computational complexity of O(N^2)? 8.Can authors also report classification accuracy on Cora and Citeseer, as that is a standard adopted by different SOTA network representation algorithms?<BRK>This paper is potentially very interesting due to the large improvements it shows on some important tasks, but there are several non trivial issues that I discuss in the following, and due to which I feel the paper is not above the bar in its current form. 3.How is the notion of a community defined here? 4.Can the authors comment on the improvements they observe using their method? For example, have the authors tried to show that it achieves better than Cheeger s approximation guarantee?<BRK>The paper proposes a generative model, called VECODER, that aims to jointly learn overlapping communities and node representations. Strong points:  The paper addresses two important problems in network analysis, namely community detection and representation learning. In particular, the part of the proposed methodology is clearly presented and overall seems to be very interesting. Weak points:  The main concern about the paper is related to the experimental evaluation. In particular, some important baselines methods are missing. The authors mention M NMF in the related work but do not consider it in the evaluation mainly due to its scalability issues. I would propose the authors to mention the Laplacian matrix instead.<BRK>In particular, some part of the graph that does not performed well in all other models but correctly done by VECoDeR. (Summary)This paper aims to learn node representations of graph to jointly satisfy node embedding properties and community detection property. The authors claim that the proposed VECoDeR is capable of learning a single community aware node representation per node, which is jointly effective in both scenarios. Having some qualitative examples (than abstract explanation in high level about structure preservation) would be beneficial.
Reject. rating score: 5. rating score: 5. rating score: 5. rating score: 5. <BRK>This set specifies that the user prefers sample X_1 over X_2. This  penalty is simply given by the margin loss between the scores of X_1 and X_2. However, I still think there are unjustified claims about the method, especially those related to the strong correction scenario and how introducing fresh generated samples is an effective procedure (conceptually / theoretically). The authors then identify two major training situations which determine how easily the tradeoff can be achieved:  minor correction  and  major correction . The minor correction is when the  desired data  is very similar to the whole dataset. The major correction is the harder situation where the  desired  data are different from the whole dataset. The same is valid for FBGAN, a competing method, which is partially introduced in the method section. Weakness of the paper:	The main weakness is the soundness of the approach and the clarity of the paper.<BRK>The motivation of this study is to estimate the distribution of desired data from the entire data distribution. And the proposed solution extends existing GAN solutions by introducing an additional pairwise loss on the discriminator, e.g., its scores on the desired instances should be higher than the undesired ones. On a related note, as the proposed DiCGAN is very similar to RGAN, it is important to compare these two solutions to better understand how their design difference translates to empirical performance difference. I do not fully follow the discussion in Section 3.3, especially the part about the major correction aspect. The claim is when the desired data distribution is far away from the entire data distribution, the generator might have difficulty in satisfying the WGAN loss. And then the proposed solution is to gradually generate new instances to replace the old one. For example, in the first round, the quality of generator is bad, such that the pairs constructed from the generated instances do not reflect the preference direction. Hence the only pairs useful there are those from the initial training instances, which might still require major correction on the generator? **Acknowledgement of author responses**The authors  responses were helpful to clarify the settings and basic ideas of the proposed solution.<BRK>A corresponding loss function is added to the WGAN loss to ensure that the model learns samples that have high rank. Empirical results show that the method is able to reflect the ranking from the user. **Strengths**  The paper presents a decent solution to the problem where additional rank based supervision is provided. **Weaknesses and Questions**  Why would we consider pairwise preferences over explicitly labeling what the users consider to be good data or not? The CelebA image quality is very far from that trained with a decent GAN (like smaller BigGANs, which you can train with 1 GPU in about a day). If your goal is to make use of the larger dataset for better generation, then there are some other work, then there are some other importance weighted work to your interest: Choi et al.2020, Fair Generative Modeling via Weak Supervision.<BRK>The authors introduce DiCGAN, an algorithm to learn a generative model that comes up with samples whose likelihood is based on a real dataset but adjusted given user preferences. They train the critic to assign high values to samples with higher preference values and thus the generator tends to move its samples towards these points. The first problem is that the writing of the paper is awful. The authors also write too strong claims for a scientific paper: many times they write that DiCGAN learns the user desired data distribution (learning a high dimensional data distribution with finite data is not a possible not an interesting goal, the users should use "approximates" instead of "learns" and describe *how* it approximates it and what is lost), and things like "the superiority of DiCGAN is twofold". There s an entire section titled "Superiority of DiCGAN over FBGAN"! Finally, while the experiments are interesting, they re all on MNIST or aligned celeb A in 64x64, and the samples are terrible. It is hard to believe that this method could be scaled up as is, or at least there is little evidence to that regard.
Accept (Poster). rating score: 7. rating score: 7. rating score: 6. rating score: 6. <BRK>This paper poses set prediction as a conditional density estimation problem and subsequently develops an energy based model training/inference procedure. The motivation for this approach is twofold: existing approaches which impose structure via loss functions can induce model bias based on the metric chosen, and existing approaches also induce bias in the sense that they cannot learn multimodal distributions of outputs when, for example, we may want to observe and rank various candidate sets for a given input. These motivations clearly frame the development of the methods in this paper, and the experiments do a good job recalling these motivations as the focus for comparison to previous approaches. A variety of results for Langevin MCMC exist that can be exploited to provide this analysis. Finally, Table 1 was somewhat confusing.<BRK>They propose to use a noisy energy based model with langevin mcmc + noisy startup as their model. The can approximate the gradient of the likelihood function by computing the enery of ground truth pairs and energy of synthesized pairs where the target is sampled from the model distribution. They reason it is an effective way of covering multi modal scenarios. Pros: The paper is relatively well written. They introduce several novel ideas that can be significant in this line of research later on. cons: lack of computation cost, training time, inference time analysis.<BRK>The paper concerns a learning framework to predict set by formulating it as a conditional density estimation problem. The approach relies on deep energy based models and predicts multiple plausible sets using gradient guided sampling. The definition of set loss is unclear in this context, but the losses can be derived by modelling a set distribution (possibly multi modal) parametrically where the parameters of this distribution can be learned using deep neural network     2  diverse but simplified experiments and unclear evaluations:a) The experiments are diverse enough, but some of the setups are very simplified (eg generation of polygons & Digits experiments which considers a perfect input x and two numbers only).<BRK>The first phase learns an energy model of the probability of a given set Y given features x, modeled using deep networks. This allows to use the negative log likelihood as a loss rather than assignment based losses, which allows to model multiple plausible sets given a specific choice of features. ## ClarityOverall this paper was clear, but several clarifications could be added to improve readability, such as providing the mathematical form of the Hungarian and Chamfer losses, and avoiding the use of $Z$ for both the partition function $Z(x; \theta)$ and the random variable $\sim \mathcal N(0, \epsilon I)$. # SignificanceModeling distributions over sets with neural networks is, as the authors point out, a difficult problem due in part to  permutation invariance.
Reject. rating score: 4. rating score: 5. rating score: 6. rating score: 6. <BRK>The paper extends over hypergraph convolutional networks (HCN) by adding a temporal evolution module in order to solve prediction tasks in a dynamic environment. I would like to know why it is useful to model the task with hypergraphs. Experimental results are provided for dynamic prediction tasks over stock datasets. The proposed system seems to be a reasonable choice for solving prediction tasks on dynamic hypergraphs. But, in my opinion, some choices could have been justified and variations of the system could have been compared. Also for RSR and DHGCN. Thus I think that there is room for improvement. Therefore, in my opinion, the paper is not ready for publication. Please justify your choice.<BRK>This is important given that the newly constructed Tiingo and Stocktwits in the paper are proposed as benchmarks for comparison with baseline models. Novelty:(1) Similar problem statement addressed in https://dl.acm.org/doi/pdf/10.1145/3394486.3403389   Needs to be added as a baseline. Experiments: (1) The paper proposes a mechanism for encoding hypergraphs which evolve over time. (2) There are only two datasets both related to stock prediction. Also, it’ll be good to run the model+baselines on additional graphs from other domains (pointer can be seen at paper linked above)<BRK>This paper proposes a method called DyHCN for learning dynamic hypergraph convolutional networks where the hypergraph structure is allowed to evolve over time. The interactions within each hyper edge, that between nodes, as well as related are used to learn the hypergaph embedding. DyHCN gives better modelling accuracy as compared to some existing ones. Cons:  Only discrete time dynamic hypergraph is considered. The attention model and the modelling of the evolution of the centroid nodes are not particularly novel. The performance improvement as compared with the SOTA method is incremental. Only one particular prediction is adopted for the performance evaluation. Can the performance comparison be carried out based on the additional predication tasks?<BRK>This paper proposed a pipeline for dynamic hypergraph convolutional networks, with a two fold component that handles both the hypergraph convolution and the temporal evolution. I think the paper is a nice contribution to the less well studied area of time dependent networks, and to some extent, also that of hypergraph embedding. Regarding the performance metrics, the 3 ones currently used are less effective/appropriate when dealing with financial/stock price prediction data, where more relevant measures would be PnL and Sharpe Ratio, especially when weights are taken into account that relate to the liquidity of each of the instruments considered in the portfolio. The paper is for the most part fairly straightforward to follow, though it is full of typos: a few examples beingan financial, this work explore, an node, Spectral graph convolution transform features, outer at tention, to name a few. Some of the notation could be better explained.
Reject. rating score: 4. rating score: 6. rating score: 6. rating score: 7. <BRK>This paper deals with the out of distribution detection task in the multi label classification (MLC) setting. Overall, this paper is well written and organized.<BRK>Summary: In this work, SumEnergy is proposed for out of distribution detection for multi label classification. According to the results of the experiment, SumEnergy performs better than MaxEnergy in several datasets of multi label classification. +ves: 1.This paper is written well. The analysis and Qualitative case study are convincing. Questions during the rebuttal period: Some related work on energy based learning could be mentioned.<BRK>####################Pros:(1) The motivation of the paper, i.e., out of distribution detection in multi label classification is very important and deserves research further.<BRK>  original feedback  Review: This paper studies out of distribution (OOD) detection for multi label classification with energy based models. + the related work about EBM is not comprehensive. Generalizing it to multi label context is incremental.
Accept (Poster). rating score: 7. rating score: 6. rating score: 6. <BRK>The authors demonstrate strong improvements over quantization aware training that treats all nodes equally, achieving relatively small drops in accuracy for a large compression and speedup of GNN inference. * Percentile tracking is a component to the methods, but relies on a reference for full explanation. A more precise statement of this part of the method in the paper itself would help clarify for readers.<BRK>Finally, experiments show that the resulting training procedure works well for GNNs on a number of datasets, matching or slightly improving the baseline performances. In most cases, the proposed Degree Quant method also outperforms baseline QAT methods. Strong/Weak Points * (+) Empirical results show moderate gains over the baseline QAT methods for int8 quantization, and substantial gains for very coarse quantization to int4. * ( ) The paper is not self contained and hence not easily readable for people without background knowledge in quantization. 2.1, no technical details on quantization are provided in Sect.<BRK>The paper identifies the aggregation step to be where quantization introduces the most numerical error, and use stochastic masking and clipping the top/bottom values to mitigate the issue. There are places where the writing can be more careful. Removing the lines connecting the dots would make more sense. Claims like "it is not possible to deploy this technique on smartphones" (from intro paragraph 2) should be supported, since it s difficult for a readerto verify such a claim.
Reject. rating score: 5. rating score: 5. rating score: 6. rating score: 8. <BRK>Unfortunately after reading the response, I don t understand how it addresses some significant concerns I have about this paper and therefore I can t increase my score. (Original review below) Summary:The authors propose a threat model for adversarial robustness and a corresponding algorithm to defend against attacks in this model. Novelty:The method proposed is novel to my knowledge, but I disclaim that I m not familiar with adversarial defense literature. But this amounts to doing binary classification on the inputs, and the attacker can perform a black box attack on this binary classifier to generate adversarial examples which fool both $M$ and $M \circ P$. The paper would be much stronger if the authors could motivate this threat model by real world examples (or even hypothetical ones). I appreciate that the authors provide a clear description of their threat model (notwithstanding my concerns with that model).<BRK>In this paper, the authors present a novel approach to evade the transferability of adversarial examples between two models. The proposed approach can serve as a defense for both detect adversarial examples and defend adversarial examples. Another weakness is that, in the setting of black box paradigm, the authors consider both adversarial detection and defense, but they did not use recent detection or defense baselines. This makes their experimental results less convincing.<BRK>Summary:* This paper presents a novel method to prevent transferability of adversarial examples in black box settings. Weaknesses:* You evaluate robustness of using the luring effect as a defense with SPSA and ECO, but you do not consider an adaptive attacker that knows your defense and tries to attack it. It would be anyway interesting to know what happens in a white box setting, or it if has any effect at all (see also guidelines in [1]). * Real world scenarios in which this may be useful are not fully clear to me. In your threat model, you assume that the attacker has access to infinite queries. * As a minor final comment, I think it would be anyway useful to understand better if using a white box threat model would actually make the luring effect meaningless (because it is relying on a gradient free attack), and whether there are some gray box scenarios in which there are still some properties for the luring effect.<BRK>Summary :The paper proposes a new framework for addressing the problem of adversaries in black box settings in order to improve model robustness. Leveraging classical deception frameworks used in network security, the authors propose to fool the attacker by training what they call a `luring component’ that is augmented to an already trained model such that the new model does not later good samples and targets the adversaries to achieve the desired result. In order to achieve their objective, the authors introduce a new loss function called the luring loss. Extensive comparison against baselines is detailed for characterization of luring effect as well as for metric evaluation. Presentation:The paper is well motivated and fairly clear. It offers practical value given that it is data agnostic , applicable to black box settings, and can be applied to any pre trained model.
Reject. rating score: 4. rating score: 5. rating score: 6. rating score: 7. <BRK>However, I still think this paper has two significant deficiencies:1. Then, each token is embedded into two vectors — one using the surface form, and one using the entity ID it is matched to (if there is a match). These embeddings are then added and passed into a Transformer model to predict the next word given the context. Several neural net architectures exist (including large scale LMs) that do not use all inference paths but still count them in the total params. Given that the modeling contribution is simply adding an extra embedding layer, it would be good to have detailed analyses that provide more insights into the contribution itself. 2.The analysis of the proposed method (where it helps, where it fails, which hyperparams matter) is still lacking. The authors did mention one ablation in the supplementary that I missed, but I don’t think that is sufficient for a reader to understand how to build on this method in this future without re running all the experiments, doing an extensive hyperparam search, etc. work better vs those that don’t help as much.<BRK>  SummaryThis paper presents a knowledge aware language model pretraining method without changing model architecture. The authors propose a simple (i.e.without changing model architecture) method to give knowledge aware signal during pretraining. 3.Interesting to see that entities from fuzzy frequency based matching can make LMs significantly better. Need extra parameters to save entity embedding (~471M). However, as the authors mentioned, this can be viewed as an external memory module and embedding lookup doesn t need much computation. Even though KALM outperforms GPT 2, it is still unclear for me the advantages of KALM compared to Entities as Experts (or EaE, https://arxiv.org/abs/2004.07202). For example, EaE outperforms KALM on the LAMA probing tasks, and EaE also outperforms T5 11B when finetuned. Based on the experimental results, precise entity linking seems more effective to me. Although 4 out of the 11 KILT tasks are already included in the main paper, most of them are LAMA knowledge probing tasks or zero shot QA tasks. It is still unclear how much and how robustly KALM can transfer to other downstream tasks with fine tuning (e.g.Wizard of Wikipedia, FEVER, QA with fine tuning).<BRK>And the method improves the performance compared to the corresponding models without the knowledge. However, I have some comments and questions about the article. Since the main point of the paper is adding entity information to the models, the paper would be better to include more explanations about the entity information. Why did the authors use such knowledge rather than other knowledge like POS or so? How many classes are there for entity? How much can we improve the performance by adding entity information rather than additional data samples? The authors said that the models were trained with entity information from scratch. And how did the authors come up with the 8 dummy QAs?<BRK>Both reviewers read the paper in detail.) In addition to pre training a model with the traditional language modeling objective, this work proposes an entity prediction task as pre training. Furthermore, this work also incorporates entity tokens at the input level of the model by summing the word embedding and its corresponding entity embedding. In addition, it is also reported that such models perform better than a GPT2 model of the same size on TriviaQA, Natural Questions and Web Questions, in a zero shot setting; and achieves competitive results when compared to larger models. This work is timely because it shows that increasing the model size and pre training data size is not the only way to achieve strong performance on language related tasks (which is the current trend). Inductive biases like knowledge about entities can improve the performance of models to the point of achieving competitive results with bigger models. Other than that, I have the following questions:Interesting choice of the margin loss for entity prediction. At inference time, you are forced to put entity labels on the input text as a preprocessing step. Are the experiments on QA really zero shot?
Accept (Poster). rating score: 7. rating score: 6. rating score: 6. rating score: 6. rating score: 6. <BRK>The main novelty of this paper is to use a new optimization objective without relying on the Lagrangian method to formalize the objective. Could you elaborate more on this? The empirical results further demonstrate the advantage of their new architectures. The problem it studies is interesting and important, related to both game theory and deep learning, which is relevant and interesting to the deep learning community.<BRK>The good:1. It lies in the intersection between Game Theory/Mechanism Design and Machine Learning, it should be of interest to a lot of people on both sides. It is a good fit for ICLR community. and also inherits the drawback there: the trained mechanism is not strictly IC and uninterpretable. In this paper, all the simulations are based on the expected ex post regret. However, the authors need to clarify some statements in the paper.<BRK>This paper revisits a recent deep learning framework by Duetting et al (ICML’19) for learning Bayesian optimal auctions, which is a notoriously hard problem in theory. I’m not familiar with the details of Rahme et al., but based on the abstract it seems like both propose improvements to the same Duetting et al.paper in the same setting. The authors also propose to use this formula as a benchmark for comparing auctions with different revenue and regret guarantees. (If these were written by the same authors, I’m not sure why they wrote 2 different papers?)<BRK>This makes the paper somewhat unclear in terms of what it is offering   heuristic approaches that improve (empirically) on the previous work? Overall Rating:  I think this is an interesting paper. Other Feedback:  The paper is a bit confusing at times, but I think that is because the authors were forced to keep descriptions short in order to fit within page limits. I think going back and offering a bit more description for a longer version would be useful. Overall though the writing is fine.<BRK>This loss function is hyper parameter free unlike Duetting et al.(2019) which is one of the early works in this field, making it more robust and interpretable. The authors then propose a training method that resembles a two player game to optimize the new loss function. * The training method that resembles a two player game brings a new perspective to the problem (*)Cons/Questions:* The paper lacks adequate comparisons, both conceptual and experimental, to other existing works. * In Table 2. comparison with RegretNet will be useful. * What are the depth and the width of the neural networks used in the experiments?
Reject. rating score: 4. rating score: 4. rating score: 5. rating score: 8. <BRK>Strengths: The model setup is reasonable, and the paper writing is easy to follow. The work successfully includes the space information into the DNN based point process model with a KDE method. Weakness: The major concern is the contribution of the work is not significant enough for publishing on ICLR, because most of the work (in fact, almost all the work on the temporal part) is same as the work “Recurrent marked temporal point processes: Embedding event history to vector. KDD, 2016.” as cited by the submission, e.g.the conditional intensity in Eq.(5). This is not accurate as it is not conditioned on current time as mentioned above. why not use the L2 regularizer? What is the performance of other baselines? In experiments, what is the definition of joint RMSE?<BRK>The paper proposed a neural point process that learns to predict time and location of an event. The temporal component uses the parametrization of Du et al 2016 and the spatial component uses a kernel density function. Precisely, I don’t mean the model is not novel at all; it is indeed new. The key design is to factor $\lambda(s, t)$ as $f(s|t) \lambda(t)$, but this design is really not well motivated. How could they do both?<BRK>### SummaryThis paper introduces a mechanism to learn spatio temporal point processes using RNNs (for time estimation) and Kernel density estimation for spatial dependencies. They perform experiments on synthetic data generated by famous point processes such as Hawkes process. ### Strong/Weak points  The paper is well written (with a few exceptions)  It is very easy to read and understand  The model is very easy to understand, but very hard to reason about and to give guarantees  It is only tested on synthetic data and no real world example is tested uponI have a tendency to reject this paper, as the idea is straightforward: if one is asked to mix RNNs with spatial data, the way to combine these two is very clear. There is nothing new about the theory of point processes either, hence, no new understanding of these types of processes is provided.<BRK>It may however be similar to a spatiotemporal graph network (using a regular lattice graph connectivity structure). The authors should clarify this. The paper is well written however and sound otherwise. In spatial statistics these are marks. I suggest sticking to the terminology of the field? Figure 5: remove the names from the graphics at the top   put into captions. There are many capitals missing in the references and journal names that should be in full. Park (2019) has an et al in it?
Accept (Poster). rating score: 7. rating score: 7. rating score: 7. rating score: 6. <BRK>#### Summary:The paper focuses on soft constrained RL techniques and proposes a meta gradient approach for the same. **Method**: One of the biggest problems with the Lagrange Optimization based CMDP algorithms is that the optimization of the Lagrange multiplier is tricky (and most times very brittle). **Empirical results**: The authors have strong empirical results for their methods, and it seems that the meta gradient based approach is able to find the trade off for the more successfully when compared to the baselines. #### Weakness:   **Reproducibitlity**: There is no mention of code release. It ll be nice to have some guarantees associated with the solution quality in this case as the setting is motivated by safety and constraint violation.<BRK>The paper presents two soft constrained rl approaches built on top of D4PG. Specifically, they use meta gradients for the lagrange multiplier learning rate (MetaL), and use meta gradients for reward shaping (MeSH). I found the paper to very clearly written, the main algorithm MetaL is clearly presented, and the results are fairly conclusive: the meta gradient approach proposed in the paper works better than the tested baselines. As someone with a lot of deep RL experience, but not a lot of constrained RL experience, I found the authors did a very good job at explaining all the relevant background. I also really appreciated the detailed experimental analysis of the approach at the end of 6.1 – it highlights exactly why the method works well. One critique I do have, is that it would be great to have the intuition for the MeSH update in the main paper. It would be better to compare against the best baseline – instead of ~6 of them.<BRK>This paper proposes a simple approach to soft constrained deep RL optimization using the unconstrained Lagrangian, by meta learning the learning rate of the Lagrange multiplier. Pros:1.Proposes a solution to the important problem of soft constraint optimization in deep RL, which is a challenge to be addressed for real world deployment. The authors also include an analysis of how the and Lagrange multiplier and its learning rate evolve in a setting with high number of safety violations, to show the benefit of using the adaptable learning rate. With that said, this work convincingly shows that using this simple idea is effective for constrained optimization in the meta learning framework. 2.The paper introduces another variant of their main approach, but doesn t discuss it adequately in the main paper, and it s unclear why it does worse than the proposed approach. 3.The paper could be further strengthened by experiments on real world domains, where soft constrained optimization is a critical challenge.<BRK>This paper addresses the soft constraints problem in RL. The base solution to the Lagrangian optimization is D4PG. To adapt the learning rate of the Lagrangian multiplier and find a good trade off between reward and penalty, the paper customizes the usage of meta gradient method (Xu et al., 2018) to the problem in this paper. The problem being addressed is an important topic in RL, the method proposed seems to be novel (but the intuition behind it is not clear), while the empirical evaluation is not convincing to me. If so it is expected that the authors would show the empirical results on different $\bar{\lambda}$ values, and the proposed method works well in all of them. It would be good that the authors can have a more in depth discussion on this. What puzzles me is that the performances of the baselines vary dramatically across domains.
Accept (Poster). rating score: 8. rating score: 6. rating score: 6. rating score: 5. <BRK>Instead, the paper proposes to train a conditional autoencoder to generate perturbed versions of clean images from pairs of clean and perturbed images. The autoencoder can then be used to generate new perturbations that are similar to the training data. The paper provides theoretical arguments that perturbations generated in this way are close to the perturbations used for training. Using these learned adversarial perturbations, models can then be trained that are robust to them. ## Arguments for acceptance* The paper provides a method for generating adversarial perturbations that are similar to natural perturbations, such as lighting changes. * The paper provides a convincing motivation and thorough theoretical justification for the proposed method. * The method is evaluated only on CIFAR and the multi illumination dataset. This work will be of interest to many ICLR attendees, so I recommend acceptance.<BRK>This paper aims to learn generative models, CVAE in particular, such that a fixed region of the latent space in CVAE corresponds to a possible perturbation set in adversarial robustness. The CVAE is learned from collected perturbation pairs, and it is expected to contain close approximations of the perturbed data and assign sufficient likelihood to perturbed data. Pros:* This paper novelly proposes to learn perturbation sets with CVAE and addresses the problem of learning robust models without a predefined perturbation set. * The paper defines measures for evaluating the quality of learned perturbation sets and theoretically proves learning a CVAE matches the quality measures. * The paper demonstrates a robustness improvement on OOD corruptions and learned perturbations, and also some collected perturbed data, with CVAE data augmentation or adversarial training. It is also shown that the approximation error of the learned CVAE appears to be small. Cons:* In terms of robustness, the CVAE generator itself may not be adversarially robust and bring additional robustness concerns. For instance, is it possible to somehow attack the CVAE to find some small perturbations that can be missed by the learned CVAE? Post rebuttal update: Thanks to the authors for their clarifications. Thus I have revised my rating to 6.<BRK>I am leaning more towards accepting this paper. This work approaches adversarial training (robustness) from a different approach by learning the uncertainty set, as opposed to studying a prespecified one such as those using divergences. The proposed method *learns* an uncertainty set using a combination of neural networks and data and principally motivated. Theoretical results are then given and particularly shown how it can be solved using Conditional Variational Autoencoders (CVAE) and the properties are then empirically studied. I think this work makes a step in an important direction however I am somewhat reserved regarding future developments of such a method. In particular, I have two questions(1) What relation, if any, does this method have to distributional robustness? (2) while it is nice that uncertainty sets can be constructed with CVAEs, how can one satisfy Assumption (1)? While I have these concerns, they may be answered in a more theoretically focused paper. I believe the problem is important and the proposed method is novel and therefore I am in favour of accepting this paper.<BRK>** In this work, the author(s) have presented an approach to identify valid perturbation operations that can be applied to the model inputs, which can be exploited to boost model robustness via purposefully corrupting the inputs during training. Technically, a conditional variational auto encoder is trained to capture possible variations, where the perturbated inputs are used for reconstruction. The author(s) provided empirical evidence to support their claim, along with some theoretical justifications. ** While the presentation is okay, I have concerns wrt the technical novelty and significance of this work. ** While the idea of applying CVAE to learn valid perturbations is new, the technical contributions seem very incremental. But I encourage the author(s) to include additional comparisons wrt related techniques in order for the reviewers to better evaluate significance. For example, baselines such as AutoAug should be compared, as these methods also try to identify valid perturbations that do not affect the label. I would also love to see an adversarial variant of the proposed approach is compared as well. The author(s) seem to imply the KL terms should go away in order to bound the approximation error, this is definitely not the case for VI.
Accept (Poster). rating score: 7. rating score: 6. rating score: 5. rating score: 5. <BRK>Cons:  The proposed sharing experience among agents in the population is limited to off policy RL algorithm, as also noted by the authors;  There are a few lines of AutoRL and AutoML research are missing in references or in discussions. For example, the architecture search lines of work from Zoph et al., and for gradient based meta parameters optimisation like "A Self Tuning Actor Critic Algorithm";  For the main contribution in the paper, shared experience replay within the population, there s a recent work by Schmitt et al.called "Off Policy Actor Critic with Shared Experience Replay" which also demonstrates that sharing experience replay in off policy learning can improve sample efficiency dramatically. This innovation itself leads to 10x improvement on sample efficiency;+ Very clear description of the motivations, related work, the details of the training framework, and the experiments. In the framework, they optimise hyperparameters together with neural architectures.<BRK>To achieve this goal, they integrate three technologies, i.e., evolutionary RL for hyperparameter search, evolvable neural network for policy network design, and shared experience replay for improving data usage. The experiment is far from enough. From the perspective of robustness, the authors need to compare more off policy algorithms with and without the proposed method. Second, the paper claims there is no directly comparable approach for efficient AutoRL, which I do not agree with. The logic is clear, the solution is reasonable, but the details are ignored. In the training part, why should individual be trained for as many steps as frames have been generated in the evaluation phase, and why the training time could be reduced by using only a fraction j of the steps?<BRK>They evaluate the proposed method with TD3 in the MuJoCo benchmark suite. Overall, the proposed method is well motivated and well written, and they provide enough experiment/implementation details to reproduce the results. Compared to baselines, the proposed method is much more sample efficient. # Weakness  They only test on a single benchmark with one method, TD3. Actually, the visual world in MuJoCo is quite limited, so the encoder of the RL agent does not have to be huge. It could be more convincing if the authors can test on another benchmark, e.g.ProcGen.I think compared to computer vision tasks with huge neural networks, the search space for the architecture of RL models is much smaller, which can be observed in the ablation study. It seems that combining the tuning of architecture and hyperparameters is not that useful.<BRK>Summary: This paper propose a population based AutoRL framework for hyperparameter optimization of off policy RL algorithms. A shared experience replay buffer is used across the population, which as demonstrated in the experiments, substantially increase the sample efficiency compared to PBT and random search. By sharing the experiences across the population each experience sample gets re used more often during the training hence the increase in sample efficiency. Random search is not a very compelling baseline. How are they compared to the SOTA learning rate schedule or neural architecture?
Reject. rating score: 3. rating score: 4. rating score: 4. <BRK>##########################################################################Cons:  The proposed method is unclear. The method works by adding noise and bias terms to the learnedembeddings after each training session, and initializing the same model on thesame task with the resulting representations for further fine tuning. I am a bit confused about the role Maskers play in your model. (This is more of a rhetorical  question; I suggest clarifying the points above directly in the paper.)<BRK>The regularization method proposed in this paper, however, requires supervised labels on the training and validation set. This method is validated via CNN based text classification. This paper designed a novel approach to do embedding normalizations.<BRK>This paper proposes to improve word embedding by iteratively 1) adding noises to the trained word embeddings, and 2) retraining the model with the noised embeddings. Comment:1) In the introduction, the author claims the embeddings will often stuck in local minimums during training as one motivation for this work. This provide an unfair advantage for the proposed method, compared to the baselines which does not involves any task specific signals.
Reject. rating score: 2. rating score: 5. rating score: 6. rating score: 6. <BRK>The goal is to be able to handle the spatial aspect, causal effects of external covariates (e.g.the causal impact of rain, Christmas on supply/demand) and dependency between supply and demand. Strong points:+ highly relevant problem+ novel consideration of handling causality instead of simple covariate correlation in this contextWeak points:  very hard to read, many details missing and notation are not introduced  lack of relevant baselines (e.g.baselines using spatial information)  lack of relevant metrics to illustrate the benefit of the contribution  missing related work discussion for efficient attention computationI recommend a reject for this paper. While the problem presented by the paper is highly relevant for the community, the paper has several issues that makes it not ready for publication. But the most problematic bit is in its design: key aspects introduced in the paper are not asserted in the experiments.1) While the method claims ~5% error improvement on the ride hailing benchmark, only one baseline have access to spatial information while numerous methods have been proposed to handle spatio temporal forecast, only one baseline (TFT) is provided with spatial information (with no detail). Finally, I had issue with the Taylor expansion as its description was not clear (see detailed comments) but also I found description of related work missing in this aspect. 3.3 the beginning of the section mix model description and related work  3.3 Eq.(3) has two unintroduced notation, what is the \bar? 4.3: "CausalTrans outperforms all other competing methods primarily due to the use of the causal estimator DML and spatial information". One could imagine where it breaks.<BRK>Is there a smoother of this assumption where the demand is dependent on a moving window over the past and future supply? However, one result that will greatly help the conclusions are results that clearly show the interpretability of the predictions, given that the authors state this as one of the main differences of their proposed solution. The use of higher order Taylor terms to approximate the attention procedure is interesting but the time complexity reductions are obvious and therefore does not meet the novelty bar for an ICLR submission. Other suggestions   make the use of the word  collaborative  in the model more clear. I still have some concerns regarding the paper. The lack of baseline comparisons with spatio temporal data (as also observed by fellow reviewers). My other concern also remains   from the authors  response, it is not clear how once can clearly attribute explainability of the results from their analysis of the model.<BRK>The paper also provides good visualization regarding the casual attention model, facilitating to understand the proposed idea. + the proposed architecture is with merit: Fast S.F. I’d like the author to elaborate more on the reason. If there are hyperparameters for the spatial fusion, please do some ablation analysis in this regard. From table 1 the results show that, in case of Electricity, the proposed method can’t outperform the state of the art (TFT) due to lack of covariates and spatial information. I’d like to see the ablation analysis (like table 3) in case of Traffic, showing that in case of the iterative method, the performance can be gained by each submodule. For example, learning rate/strategy, batch size, optimizer, architecture settings...etc. If the author(s) plan not to release the code in the future, it’s better to list the experimental details in the appendix, for reproducing the performance. Overall I think that the proposed causal attention is valuable. to the transformer architecture, the proposed method is comparable to state of the art.<BRK>This paper proposes a new framework of casual spatial temporal prediction with high interpretability. Strength:+ The authors propose to reduce the complexity of the computation of the attention module. + The experiments on datasets from various domains are adequate, which can support the authors  claim. Weakness:   Both equations (1) and (2) are based on the authors  assumptions. In my opinion, some of the assumptions do not make sense, for example, one assumption is that $x_v(t + 1)$ is primarily affected by historical demands in $x_v(: t)$ and external covariates in $z$ without historical supply $y$. However, if the historical supply y is not enough, then the demand may raise because more and more demand accumulates. Does it have any limitations? For example, some of the coefficients are very large so that the higher order Taylor terms cannot be overlooked.
Reject. rating score: 5. rating score: 5. rating score: 5. rating score: 5. rating score: 5. <BRK>Pros:This paper designs class aware generators (increasing flexibility) for cGAN by RL based neural architecture search (NAS) algorithm. The architectures for different categories are still weight sharing, which is quite a common approach in NAS. For searching network for GANs, the main challenge lies in how to provide stable and efficient supervision as a reward. Searching for the architectures of G and choosing IS as a reward only helps the G compete against D. I wonder whether it is the optimal choice, so the authors need to consider the settings in [1], i.e., updating the architectures for both G and D.Many NAS algorithms for GAN models are relevant to this work, but none of them is evaluated against the proposed method in the experiments. I think the authors should add competing results from AdversarialNAS [1] and AutoGAN. Specifically, the Intra FIDs on CIFAR10, FID, and IS scores on CIFAR100 of the two methods should be reported. Neither time complexity nor space complexity is reported. Note that the resolution for most generation tasks is at least 256$\times$256 or 128$\times$128.<BRK>This paper proposes an interesting method that adopts NAS to search multiple class aware generator architectures for cGAN instead of class agnostic type. The search results also give some insights about constructing cGAN models. Strengths:  The perspective of adopting the NAS method to explore the class aware generator architectures for conditional GANs is relatively novel and interesting, although there are some related works about searching unconditional GANs. The proposed flexibility and safety search space is effective to address the categories grow issue. The developed mixed architecture optimization is a clever way to improve the efficiency of the search and re training process. The authors conduct extensive experiments and give some interesting insight/discussion about the results. Weaknesses:  There are no innovative approaches toward neural architecture search are proposed, and this work only focuses on how to bring existing RL based NAS methods to cGANs while overcoming some issues. The motivation for using distinct architecture to generate images for each class instead of using one architecture is unclear. The idea is similar to the dynamic routing/inference networks such as Blockdrop [1], and it needs a related discussion about the difference. The experiments are only conducted on low resolution datasets.<BRK>The authors design a Class Modulated Convolution to allow for the weight sharing among different searched architectures. The proposed NAS caGAN outperforms the model that employs searched class agnostic architecture on CIFAR 10 and achieves better results compared with cproj (Miyato & Koyama, 2018) on CIFAR 100. My major concerns are about the baseline choice, computational costs, and the evidence to support the method’s utility (see cons below). Hopefully the authors can address my concern in the rebuttal period. 2.Overall, the paper is well written and technically sound. (2) It is interesting to inject CMConv operation at the first layer of BigGAN, which achieves a better result than other operator settings. Why not use BigGAN as the baseline? 3.Could the authors provide the details about computational and **time** costs of the proposed method? 4.In the Sec.4.1, the paper claims that this method could benefit other works. Considering the lack of convincing experiments to support this argument, it is doubtful about its correctness.<BRK>Summary:  This paper proposes to use neural architecture search (NAS) to automatically discover useful conditional generative adversarial network (cGAN) architectures. Architectures learned by the NAS reveal insights about how to use existing building blocks, such as where best to place feature modulation layers in the network. Majority of improvement comes from fine tuning on each class individually. It is unclear how much of this improvement is simply due to additional capacity. Complexity of the model appears to be disproportionate to the improvement in performance (lots of implementation effort for a somewhat small gain in performance). Recommendation and Justification:  While I think this paper is well written, after reading it I am not convinced of the usefulness of the core idea, which is that generator architectures should be class aware. For this reason I think that this paper is currently marginally below the acceptance threshold, but look forward to the author s explanation. Is this procedure the same for NAS cGAN and NAS caGAN?<BRK>This paper proposes an interesting idea that adopts NAS to find a distinct architecture for each class based on cGAN framework. The paper is well written with sufficient figures and plots. The proposed idea is straightforward and convincing. However, I still have some concerns:  From my point of view, the proposed CMconv has exactly the same architecture as the one in Karras et al., instead of using class embedding as input. Please clarify the difference and clearly point out in the paper. The proposed method is not compatible with the SOTA. Although the paper claims that the proposed idea can be applied to the existing methods and performs better, there’s no evidence showing that. It’d be better to also list infra FIDs of one existing unconditioned GAN method in the Table 1, for the lower bound / baseline of the experiments. For fair comparison, it’s better to have a table with quantitative results of IS/FID on CIFAR 10, listing the existing methods with conditioned/unconditioned, such as AutoGAN, style GAN etc. Overall, the proposed method is interesting, NAS by MDP with class aware functionality, which can ideally outperform the class agnostic based method. The experiments are comprehensive, with strong ablation study and analysis. However, it still requires some clarification and convincing experiments to demonstrate the performance.
Accept (Spotlight). rating score: 8. rating score: 7. rating score: 7. rating score: 6. <BRK>The paper describes a novel implementation of RED, regularization by denoising, which better leverages multicore architectures to achieve a significant speedup. That being said, I think the results are interesting enough and the description of the method compelling enough that I recommend this work be published as part of the proceedings.<BRK>The main contribution is a combination of asynchronous processing and stochastic activation of blocks in a distributed computing environment. * Theorem 2 shows that the "convergence" is not variance reduced.<BRK>Overall the paper is well written and has the potential to be a nice contribution to the community. Although the experiments in this work do not demonstrate such deficit, the reviewer suspects that, for some images, the Async RED may not do well on the pixels near the edges of the blocks.<BRK>Numerical experiments on the CT image reconstruction have justified the proposed efficiency and significant improvement in terms of running time. The importance and contribution of this work in compressive sensing algorithms stand.
Reject. rating score: 4. rating score: 4. rating score: 6. rating score: 6. <BRK>Prefix vs infix: see [5] for previous related work on this and more. My overall feeling is that the paper shows a lot of experimental data,but it does not bring sufficiently interesting new insights. My score will stand, but I would like to encourage the authors to dig deeper and follow the suggestions given in this and other reviews.<BRK>This paper studies the efficacy of transformers on a polynomial simplification tasks. One of the contributions of the paper is the creation of dataset of polynomial simplifications. This is a hard question for sure. But is this question as hard as in Lample and Sarton? But is it as hard as the above? I can t tell, but this paper does not even specify what is a baseline. This paper fails to specify what is an interesting message and fails to specify that message.<BRK>Questions to the authors:  The task is pretty simple compared to other (mathematical) reasoning tasks studied in the literature. They might employ more advanced search ideas, but I think it would be good to state why your paper does not want to go in this direction. So how could this help for real problems? This sounds very much like an artifact that could stem from the lack of training data or so.<BRK>Maybe this could help the Transformer learn and be more resilient to coefficient size? So while the current version of the paper is ok, there are a few areas for improvement which prevent it from being a clear accept.
Accept (Oral). rating score: 8. rating score: 7. rating score: 6. <BRK>The authors apply iterated learning   a procedure originating in CogSci analyses of how human languages might develop   to the training of neural module networks. The goal is for iterated learning to encourage these networks to develop compositional structures that support systematic generalization without requiring explicit pressures for compositional structures (in past work, such explicit pressures have generally been necessary). While SHAPES and CLEVR may be not as toy ish as datasets used in the past, they still are pretty toy ish, so I’m not sure if this paper can reasonably claim that one of its contributions is to expand iterated learning to realistic domains. 2.Though the paper in general was very clear, I found Section 3.2 to be a bit hard to follow, and that section is important as it is the part that describes the structure of the iterated learning framing. I think this section would benefit from starting each subpart with a more high level, intuitive description of what that stage accomplished, before diving into the details. In general, for the bibliography, check to see if a paper has been published at a conference or journal; if so, cite that version instead of the arXiv version.<BRK>This paper proposes to combine iterated learning (the process of repeated language transmission from a ‘parent’ agent to a ‘child’ agent) with neural module networks (NMNs), in order to emerge NMN layouts that perform better at systematic generalization. The paper evaluates on their new variant of the SHAPES dataset that tests systematic generalization, and on CLEVR / CLOSURE, showing improved systematic generalization performance while requiring a small amount of ground truth layout supervision. Pros:  I think the idea of combining IL with NMNs is really clever (heh). Treating the program generator and execution engine as two agents that need to coordinate through a shared language (NMN layouts) is really interesting. While CLEVR is more complex than SHAPES, it is still a fairly artificial dataset targeted ‘compositional’ in nature, compared to general VQA. 3) The method still requires (a small amount of) ground truth layout supervision, which is not obtainable in general VQA or most other tasks. Overall:I think the ideas in this paper are interesting enough, and the execution good enough, to warrant acceptance.<BRK>Review:The authors address methods to encourage the emergence of the layout expression structures on the frameworks of neural module networks (NMN) for the visual QA problems. In particular, the main idea of problem formation, layout expressions in NMN as emergent languages is very fresh and interesting. Specifically, the authors do not provide enough information of language structure such as the superiority compared to other methods and the structure similarity of recovery levels. I recommend  ok, but not good enough – reject’ for this paper. Pros:  The authors propose novel interesting problem and their solutions. They find and report good performance for out of distribution accuracies for visual QA datasets. Also, it is not enough how program generators and execution engines are specified, even though some explanations are in appendix. I think that it would be better understandable to show usability and superiority with the experimental results on realistic visual QA such as VQA and GQA.
Accept (Spotlight). rating score: 7. rating score: 7. rating score: 6. rating score: 5. <BRK>It is claimed that the former property helps avoiding sharper minima which generally improves generalization. **Quality** The paper is well written. However, to the knowledge of the reviewer, this work combines the ideas of the two directions in a coherent and original way. **Overall** In the reviewer’s view, the paper has clear merits in bridging between the theory of gradient noise and label smoothing for learning under label noise, both empirically and theoretically. However, it can benefit from more clarity and additional informative experiments to better understand the effect of the proposed noise. 3. the paper has two clear claims regarding the sharpness of the found local minimum and on overconfident predictions. 4. the performance of some of the baselines are low.<BRK>Summary:mitigate inherent label noiseThis paper studies the effect of applying SGD noise on mitigating the inherent label bias which is common in real world datasets. It introduces stochastic label noise (SLN), a variant of SGD noise induced by controllable label noise. With such propositions, it shows that SLN can help the model to avoid sharp minima and prevent overconfidence (Claim 1 2). How sensitive the results would be by choosing different sigma?<BRK>This paper studies the noisy stochastic gradient descent algorithm in noisy label learning. By comparing different noisy SGD algorithms, the authors demonstrate that the proposed SLN algorithm not only help the model to escape from sharp local minima, but also help it to be over confidential. The experimental results in Figure 3 is very promising. For example, on CIFAR 10, Asymmetric noisy with 40% noise, the accuracy of DivideMix is 92~93.4% and SLN MO LC is 87.85%. Overall, I think this paper brings an interesting idea to the community, but the experiments are not enough for me.<BRK>This paper studies learning robust models with noisy labels. The authors argue that a specific SGD noise induced by stochastic label noise (SLN) can mitigate the effect of label noise. Pros: The paper provides an interesting view of SGD noise in the lens of noisy labels. Some convergence analysis under reasonable assumptions may be helpful. It is claimed in the paper that training without SGD noise under label noise can converge to sharp minima, and SLN helps escape from the sharp minima.
Reject. rating score: 3. rating score: 4. rating score: 6. rating score: 6. rating score: 6. <BRK>Empirical results show that the proposed method provides rapid learning with prior demonstration data and online experience. I have a major concern about the novelty of this paper. The major component of the proposed AWAC method, updating the model parameter using supervised learning, is exactly the same as AWR (Peng et.al., 2019). This paper provides an analysis on challenges of combining offline RL with online improvement, which motivates this paper.<BRK>The main difference that I can see is AWR doesn t have a fine tuning step but this paper does. Fine Tuning with random policy data is not convincing either. I d suggest running some experiments with D4RL [5] dataset like * medium expert, * medium replay, and * random to build a better case for your proposed method. ( I disagree with the authors that CRR is a concurrent work with this paper)   This paper claims that AWAC can utilize various types of prior data without any changes to their method and it is agnostic to the type of behavioral policy (e.g.random vs. expert).<BRK>The empirical study shows that AWAC leads to a faster convergence rate compared to the competitors. However, the proposed algorithm demonstrates some practical benefits in simulated scenarios and is analyzes from different perspectives. Hence, I vote for accepting, given the following concerns are addressed in the rebuttal period. In addition, offline pre training and online fine tuning is not quite accurate as the algorithm 1 is using the mix of data in one phase which does not involve pre training, or is it actually in two phases? The paper seems to be an incremental improvement/combination of several recent works mentioned in the paper. The experiments are all on simulated environments.<BRK>The paper describes an approach to use offline data to accelerate theonline learning process of reinforcement learning. It is not clear how close the static dataset has to be to a "good"policy and how it affects the learning process. The idea is to use a static dataset of experience tuples forpre training and use some online interactions to learn the optimalpolicy for the current task. This constrain is incorporated in the optimization and solve using theLagrangian.<BRK>Summary:In this paper, the authors intend to accelerate on line reinforcement learning with off line datasets. The proposed method is easy to implement. This strategy is especially advantageous when the dimensionality is high. Besides, since this algorithm does not need the parametric model any more, it is more friendly for users who are not familiar with the off line dataset. 3.The details of experimental settings are provided, and thus there should be no issues with the repeatability of the experiments. There is no judgment or proof that online training also suffers from the bootstrapping problem. But I would like to say that this is not a big problem, as many influential reinforcement learning algorithms are straightforward but have very significant improvement, e.g., DDQN and PPO.
Accept (Poster). rating score: 7. rating score: 7. rating score: 6. rating score: 6. <BRK>This paper inherits some of the mysteries of self supervised techniques: what function does the projector provide? + The results are convincing and significant. Cons:  While experimentally the paper is on solid ground, there isn t much intuition presented as to preccisely why this combination of losses is the right thing.<BRK>The proposed approach is reasonable, the motivation is strong, and the solution is (quite) simple and effective. This makes the comparison not too fair to other methods. + I think it is a nice application of the recent unsupervised learning methods to settings that are more realistic (it is not practical to assume that there is absolutely no chance of getting a single label, but instead some label with noise). "Webly supervised learning of convolutional networks."<BRK>This paper proposes a weakly supervised learning method based on the self supervised contrastive loss to combine supervised learning and unsupervised contrastive learning. A label correction mechanism is proposed by utilizing distance in class prototypes. Proper discussions of related papers are encouraged. The performance on Webvision dataset is nice  Writing it high quality and easy to read.<BRK>To train a model with a noisy weakly supervised training set, this paper proposed a momentum prototypes method for label noise correction and OOD sample removal. Overall, I think this paper is well presented and the results are solid, thus updated my rating to reflect this.
Accept (Poster). rating score: 7. rating score: 7. rating score: 5. rating score: 5. <BRK>This paper looks at loss functions for semantic segmentation. Instead, a proxy/surrogate loss function is learned jointly with the network in a two level optimization. Code will allow others to verify and build on these. al., "Optimizing Non Decomposable Loss Functions in Structured Prediction"This kind of approach is an omission from related work that might be worth correcting. The networks trained on the surrogate losses do well on the original metrics.<BRK>In this paper, the authors suggest using differentiable surrogate parameterized loss functions that more closely approximate some of the frequently used metrics for segmentation, including variations of accuracy, IoU, and F score for the whole area and the boundaries, and use reinforcement learning to tune the parameters instantiating the surrogate loss functions. Moderate improvement is shown through experimental evaluations compared to cross entropy and other used loss functions.<BRK>3.)The writing of the abstract needs to be improved. In comparison with traditional loss function such as Cross Entropy, WCE, DPCE, and SSIM, the proposed method achieves competitive performance. 2.)The comparison experiments with some recent and important methods are missing. As the Auto Seg Loss is particularly designed for Auto ML based semantic segmentation, the comparison with the related methods is required.<BRK>The metrics typically contain one hot labels and logical operations. Then the logical operations applied on the one hot label are extended by a continuous parameter function which is Monotonical and has the same output as the logical operation with 0/1 input. The experiments have been performed on Pascal VOC 2012 and Cityscapes datasets, showing the searched loss outperformed the traditional ones such as cross entropy. Cons:1.My main concern is that this paper is poorly written and difficult to understand. The abstract and the introduction do not provide necessary information about the high level design of the proposed algorithm. Is there any other option? Is the algorithm sensitive to a different number of Bezier segments?
Reject. rating score: 4. rating score: 5. rating score: 5. rating score: 8. <BRK>Summary: The authors proposed to randomly wire the GNN layers. I don’t understand how this can help us design or improve the proposed architecture. I do not see any theoretical reasoning that we should use ER graph instead of the other choice of random graph generator. The authors mention that small world and scale free networks have been studied and use in [2], but I don’t see any detailed comparison why the ER graph is a more preferred choice. The other weakness is that the “theoretical analysis” mentioned in this paper neither leads to any reasoning in model design nor guarantee in performance. However, the reported results are significantly different and much worse than those reported in [1]. [2] “Exploring randomly wired neural networks for image recognition.,” Xie et al., In Proceedings of the IEEE International Conference on Computer Vision, pp. 1284–1293, 2019.<BRK>Summary:This paper extends the technique of randomly wired neural nets from [1] to Graph Neural Networks and show that they perform better than tradtional GNN architectures. They demonstrate the improved capacity of this architecture via a number of experiments on the benchmark in [2] and ablation studies. Diminishing model performance in GNNs with increased number of layers is an important problem and it looks like randomly wired GNNs provide monotonically increasing performance for up to  L   32    The models generated using random wiring outperform the baseline model across a large number of settings such as the graph convolution operator used and the number of layers    Augmenting the randomly wired networks with a sequential path is interesting. Ablation experiments are extensive and convincing. Cons:     The novelty is only incremental, building on the core idea from [1], but the results are strong so this is not a big issue. Overall:  Extension of an existing method to GNNs which produces strong results. I vote to accept this paper. Exploring randomly wired neuralnetworks for image recognition. 1284–1293, 2019.<BRK>This paper utilizes Randomly Wired architectures to boost deep GNNs. I do not agree with the authors  response that APPNP is not intended to address the over smoothing problem. Experimental results on three non popular datasets demonstrate the strength of the proposed model. Overall, the idea is interesting. Theoretical analyses verify that randomly wired architectures behave like path ensemble and it enables adaptive receptive field. Hence, I still believe this paper is below the acceptance line. Graph neural networks exponentially lose expressive power for node classification. #############post rebuttal############I have carefully checked all other reviewers  comments, the authors  response, and the revised version. Thank the authors for their detailed feedback. As already pointed out by R3, [3] has set up a nice notion of framework on explaining how over smoothing happens and why deep GCN fails. Actually, a more in depth discussion of over smoothing on general GCNs (including ResGCN, APPNP) has also provided in an arXiv preprint paper [4]. It does show that the residual networks are capable of slowing down the convergence speed to the subspace and thus alleviating over smoothing. Since the idea of random wiring is initially proposed in CNNs, the contribution of this paper that we expect is to answer how this idea can be utilized to solve the specific weakness in the graph domain.<BRK>Summary:The paper proposes a new method for building graph convolutional neural networks. The proposed method is an interesting direction in the field of graph convolutional neural networks. Moreover authors analyze proposed randomly wired architectures and show that they are generalizations of ResNets. They show that by using randomly wired network, together with trainable weights on the architecture edges and sequential path, the network could tune the desired size of the receptive fields to be merged to achieve an optimal configuration for the problem. Cons:1.Figure 1 is not clear to me. 2.In the final version of the paper, the ablation study should be reported on all datasets (however the authors remark that, they do not report results on this version of paper due to space constraints). 3.I would like to see the more extensive analysis of DropPath, e.g what are the scores for different levels of the drop probability. Why the authors use different types of GCNs during the ablation study? Hopefully the authors can address my concern in the rebuttal period.
Accept (Poster). rating score: 8. rating score: 7. rating score: 6. rating score: 5. <BRK>The innovative part of the work consists in the trick (8), which allows for a kernel based generalisation of Gaussian mixture models viewed under a discriminative perspective. The experiments are performed on standard benchmarks, and comparisons are provided against some natural competitors of the method. In essence, the authors replace the penultimate softmax layer of a deep net classifier with their model and train end to end. An aspect that is missing is how they initialised the models. In summary, a slightly novel paper with some interesting insights and some pretty standard nowadays, yet marginally impressive experimental results.<BRK>The paper proposes an interesting extension to both discriminative GMMs and CNNs with a softmax outuput. This novelty and the clear experiments merit this work to be published at ICLR 21. 1.The language could use another review. 2.Appendices were missing in my version; the paper makes an explicit reference to at least appendix A. 5.Most of the Bayesian approaches used in the comparison are dated. A deep dive is warranted to better understand their relative performances. 8.It would be better if Section 4.2.1 also include large datasets or more challenging domains.<BRK>The paper proposes a sparse classifier via  discriminative GMM. This model is trained based on sparse Bayesian learning. The paper is well written and easy to follow. However, in this paper, experiments are performed on the small datasets, so was the proposed method evaluated on ImageNet as well? 3  There are many versions of softmax classifiers such as “large margin softmax”, “angular softmax”, and “additive margin softmax” , and  [1 ,2 ] that address conventional softmax classifiers’ issues. 4   Finally, I could not find any guarantee for the convergence of the learning algorithm in the paper? I think training with softmax would be much faster, easier and also provides promising classification results in practice.<BRK>* qualityThe paper presents an interesting idea that uses sparse Gaussian mixtures, but it lacks theoretical guarantees. Although the method is Bayesian, can we also give frequentist non asymptotic bounds? * clarityThe paper is well written. * significanceIf the paper had more theoretical guarantees, its results would be more significant. The current version is a bit weak.
Reject. rating score: 5. rating score: 6. rating score: 7. <BRK>The technical contribution seems weak as the paper mostly explores known methods and well known  trade tricks  (goal conditioning or loss on goal) towards a grasping centric problem which is also heavily explored as part of various RL tasks in literature. The main weakness of the work however is the lack of clear motivation for why such a complicated procedure is necessary compared to the expert planner already being used   the experiments aren t designed to address this question. The problems studied could be addressed by planar grasping as well. Complex setups with clutter, etc would better motivate if the presented approach is able to scale to scenarios where 6D grasping is necessary. I appreciate the additional experiments in the real world and comparisons with the open loop policy.<BRK>The paper employs a combination of imitation learning, reinforcement learning, and auxiliary losses for training this policy. The paper also does systematic experiments in simulation to judge the importance of the different components. Shortcomings: While the problem is interesting and important, and the proposed approach is sound, the experiments have entirely been done in simulation. In particular, the paper focuses on the design of closed loop policies. Closed loop policies are more relevant when there is noise in the motion of the robot, or there are hard to predict dynamics arising from the interaction of the gripper with the object. Thus, it is not clear to me as to what aspects of this paper will be applicable to the study of this problem in the real world. Update: I thank the authors for providing clarifications and additional experiments, in particular the comparison to open loop grasping (SOTA grasp detection method from Mousavian et al.).<BRK>__Supporting Arguments__Overall the approach seems sound and the success rates of around 90% seem good for closed loop 6D grasping of unknown objects. The ablations that disentangle the contributions of the different design choices are valuable and I was delighted to also find a comparison to the more common and arguably more intuitive approach of using the goal predictions as input to the networks. As the approach and the results are presently sufficiently clear, the paper provides a valuable contribution and should thus be accepted. The experiments don t seem to show the benefit of closed loop grasping by only considering an uncluttered environment and not performing comparisons with open loop approaches (e.g., [1] or [2]). Some design choices do not seem to fit well to closed loop grasping, which is mainly important for dynamic scenes, e.g.for re grasping slipping objects. The approach is not evaluated on a real robot. I hope that the authors consider publishing the code after acceptance. The paper should be more specific here.
Accept (Poster). rating score: 8. rating score: 7. rating score: 6. rating score: 6. <BRK>I still believe that this paper is a novel contribution with clear impressive results. The authors address this by mapping the noisy image into a latent space in which. This is a clever idea, it essentially is MAP gaussian denoising with a variational auto encoder as the likelihood model. Did the authors explore any hyper parameter sweeps (of the Unet architectures specifically) in this space to see if performance was further improved or impoverished by different choices? Strength:Can be utilized with other existing gaussian denoisers to improve their performance. Weaknesses:The visual improvement for NN+DnCNN is debatable though the other results show significant improvement. It is noted that there is an expectation term in (4) that can be estimated by the Monte Carlo method and there is no need for a large amount of samples in practice. The first term, which is  1Eε||Fθ1(Gθ2(y)+ε)−y||2  does prevent a zero mapping, but does not on it s own prevent the identity map (it is in fact encouraging it). Please update this sentence to be more correct.<BRK>The proposed methods are reported to have improved performance compared to original Gaussian denoisers combined in their methods. The idea of this paper is interesting and the presentation of the major idea is relatively clear. There should be a constant. Further, seem that the dimension of an image, i.e., $N$, is also used as an iteration number in Algorithm 1. Please avoid this confusion, unless there is a reason to do so. I have this concern because in my mind, when updating $x$, it should not go too far away from $y$, and it gradually converges to the final results. 5.I recommend that the authors show the latent image and denoised image in $k_{\rm th}$ iteration step for different $k$ in Algorithm 1. Further, please clarify why the results for the proposed method in Hou et al.(2019) are not compared. After rebuttal:I think the authors well addressed my comments. I would suggest that the authors include some results in their response in the paper or supplementary results if they have not done it.<BRK>ADMM is used to update the weights by minimizing a cost function where one of the terms fits the noise image (using an additional UNet decoder trained jointly) and the other term is associated to the denoiser. The Bayesian motivation for the method is too detached from the algorithm that is finally applied. How do the results change with different choices of encoder/decoder? It would be very helpful to add some ablation studies. How should it be estimated if you want to train on a single image, where the noise is not Gaussian? *Significance*:It seems to me that the authors have designed a promising method, but unfortunately they do not provide enough insight and analysis to show why it may work, or what the limitations may be. This will limit the possible impact and usefulness of the paper significantly. *Pros*:The method is interesting and novel to the best of my knowledge. Updated review: I appreciate the authors  response and have updated my rating. However, I still believe that the clarity of the exposition could be improved.<BRK>The paper shows good numerical results. 1890 1898This paper introduces a denoising method which works on a single noisy image and is agnostic to the noise distribution. It looks like the experiments show that just the NN based Gaussianization of latent space is not effective when the noise distribution is significantly different from Gaussian. The authors had to  introduce an additional VST in the latent space to make it work. I think this is a limitation of the work   the method just by itself is not capable of handle of noise type significantly different from Gaussian. In my opinion this should be acknowledged in the paper.
Reject. rating score: 4. rating score: 5. rating score: 7. <BRK>Then the goal is to find a distribution q such that its density is supported in a k dimensional affine space and it minimized a certain loss D(p_emp,q), where D is a measure of distance between two distributions. The paper then presents 4 examples of problems that can be formulated in this framework: 1) maximum mean discrepancy; 2) distance based on the higher moments; 3) Wasserstein distance; and 4) sufficient dimension reduction. I think the paper is not well motivated, and it is not clear what are the novelties of the paper. The experiments are also very inconclusive. For example what happens for k 10?<BRK>Comments:Much of the paper consists of highly technical mathematical derivations, and unfortunately I do not have the background to assess this content. An introductory section or appendix defining the basic terms and facts is standard in such cases, and would go a long way in making the paper accessible and self contained. The experiments do not show a clear advantage of the proposed methods; in fact, when compared with two other baselines, each of the three methods gets the best result on exactly one third of the experiments in Table 1, and the differences between them are generally small and doubtedly significant. Is there a further message in these results that point to some advantages of the proposed method? I could not find a discussion of the running time / scalability of the proposed method compared to PCA and the other baselines. Conclusion: The mathematical content of the manuscript is largely opaque to me. The experimental results are not outstanding, though perhaps the new approach presented here has some conceptual merit that could justify acceptance (this is currently difficult for me to judge). Given the diverse spectrum of ICLR target audiences, I would advise revising the presentation to be friendlier to the general ML community. Post rebuttal update: I thank the authors for their response.<BRK>The paper considers the unsupervised dimension reduction problem, in which one is given a finite number of points in R^n drawn from some distribution and wants to find a low dimensional distribution that approximates the underlying unknown distribution. It is also not clear when to use the primal and when to use the dual form of Algorithm 1. It would be better to compare the running time with the existing algorithms, too. The paper then conducts experiments by applying this general theory to specific distance functions and kernels.
Reject. rating score: 4. rating score: 5. rating score: 5. <BRK>This paper solves the constrained clustering from a probabilistic perspective in a deep learning framework. I will illustrate my concerns point by point. 1.The authors mention that none of the existing work in the deep (constrained) clustering models the data generative process. First, this is not true. Second, the authors should illustrate the benefits of data generative process for constrained clustering. 2.If I understand correctly, Eq.(9) and (10) are the core techniques for the proposed algorithm. This point is not a drawback. Is there some normalization to make alpha within a small range? If I were the project manager in charge of annotation, I will directly label their categories, rather than providing the pairwise constraints.<BRK>1.The different ideas used in this paper, such as adopting a mixture of Gaussians as a prior over the VAE latent space for clustering, or the specification of the conditional prior over the cluster labels to integrate pairwise constraints, are not new by themselves. 2.The paper is well written, and the proposed method is clearly motivated and described. 4.The authors claim efficiency, but complexity analysis and training time comparisons are missing. It would be useful to investigate the impact of this assumption on datasets exhibiting very unbalanced cluster sizes. One possibility is to preprocess some of the considered datasets to create such case.<BRK>The empirical results shows that in comparison to unconstrained clustering the small amount of pairwise constraints significantly improves clustering performance. **Quality**The paper is well written albeit with numerous typographical error (some of which are listed at the end of this review). Thus the conditional ELBO loss objective is thus a simple extension of VaDE objective. In summary, the work carries very little novelty. However, the subtle difference CVaDE brings to the table is how to incorporate them into prior probabilities. In eq(2), shouldn t it be $\mu_{z_i}$ instead of $\mu_{x_i}$. Is function $f(z_i; \theta)$ not deterministic ? 3.Under experiments, please make clear what are we solving for   $z$ and $c$ ?
Reject. rating score: 5. rating score: 5. rating score: 5. rating score: 6. <BRK>The authors propose to cluster a data set into k groups using k VAE. The paper is straightforward and goes directly to the point: VAEs improve AEs. :"Our k(DVAE improves upon the current state of the art clustering methods in several facets: (1) A novel model ...(2) A novel, "Novelty is not per se an improvement. Besides, the third point is not novel since it is what does k DAE. Section 3.4 typo: "in VaDE and the [R]econstruction quality"To some extent, an AE is always a generative model. Section 4.4: k DVAE is also prone to error:  lines for 3, 4 and 5 contain a 9"This obviously affects the clustering accuracy of VaDe/DGG given in Section 4.3." "In contrast, each decoder network of the k DVAE was able to successfully reconstruct its corresponding digit by only using y random normal noise as an input." This is a bit far stretched: the decoder of VaDE is also able to reconstruct a number from Gaussian noise. Typo: "by only using [y] random normal" The fact that the paper is straightforward is a quality. For example, at first sight having k AE to  train seems a bulky situation. However the authors claim that the SoA performances can be reached with smaller architecture. How does this evolves with k? I other the words, what are the limit of VaDE and k DVAE.<BRK>The paper is rather difficult to read because of many typos and confusing statements. The authors should spend more time in refining the clarity of the presentation. Here I struggle to understand which are the respective methods and which are these methods. “However, this objective seems to miss the clustering target, since the reconstruction term of is not related to the clustering and actual clustering is only associated with the regularization term optimization.”  Besides some typos, this seems a rather strong statement as VaDE and GMMVAE indeed are quite good at clustering and the fact that the clustering objective is enforced by the prior distribution in the latent space does not result in “missing the clustering target”. A reference or a more systematic description could help. Hence, the proposed approach does not differ much from the k DAE approach and I believe it cannot be described as a “novel Variational Bayesian framework". 3) Latent space. The proposed method, on the other hand, chooses a unit normal distribution instead. I would find the paper more persuasive if it stated what the authors do over and above.<BRK>The paper proposes a deep clustering method based on variational autoencoders. The proposed method was shown to achieve better clustering accuracy over several other recent deep clustering methods on four real world data sets. The paper is good that it provides relatively detailed explanation of the proposed method. The proposed method has been compared with a reasonable collection of baseline methods and data sets. Besides, the empirical results are not very interesting. It would be better if more results could be demonstrated. For example, could any difference between the autoencoders for the different clusters be identified in data sets other than the MNIST digit data set? The paper does not seem to provide the source code. So there is concern on the reproducibility of the results and whether the proposed method can be easily used by practitioners. Overall, the proposed method has been shown have state of the art performance. However, the paper is not very exciting as the proposed method appears to have limited novelty and the improvement in clustering accuracy is relatively small.<BRK>The paper proposed to cluster the data using k different VAEs’. The method is different from the existing VAE based deep clustering method (VaDE), which uses only one VAE but employs a Gaussian mixture prior to achieve the clustering goal. The difficulties of the proposed model lie at how to train the model efficiently. To this end, some approximations are made to the ELBO by using the MAP value to replace the expectation as well as dropping some KL term. The approximations are the key to the training, but not justified well. Experiments are conducted on several image and text datasets, and show superior performance comparing to existing deep clustering methods. The idea of using K different VAE’s to perform clustering is interesting, and is a good complementary to existing deep clustering methods. 2.The experimental results demonstrate the superiority of the proposed method over existing ones. But here, what you need is not the expectation value, but the gradient of the expectation w.r.t.model parameters \lambda. So, when you replace the MAP value with the expectation in (5), the gradient computed from the MAP expression will be much different from the exact gradient.
Reject. rating score: 5. rating score: 5. rating score: 6. rating score: 7. <BRK>MACHINE READING COMPREHENSION WITH ENHANCED LINGUISTIC VERIFIERSThe authors propose two linguistic verifiers for improving extractive question answering performance when the question is answerable. The first replaces interrogatives in the question (who etc.) with candidate answers and evaluates this both in isolation and in combination with the answer containing sentence to do answer verification. The second verifier jointly encodes individual sentences and spans with questions in a hierarchical manner to improve answer prediction performance. Limitations:  The paper is understandable but the presentation could be significantly improved. Overall I feel that the paper could use some additional polishing. The paper has lower novelty wrt ML elements. The component architectures/models that make up their system are well established. When there is more than one interrogative, the authors back off to simply appending the answer to the question... perhaps this can be done all the time without compromising the performance gains? Overall Assessment:A solid applications paper on extractive question answering. Furthermore, the manuscript is in need of significant revision before it can be considered for acceptance at ICLR. However, I still feel that the paper still needs significant polishing before final publication (figures, grammar, presentation), and that the paper is better suited for an NLP focused conference, and so I have not updated my final score.<BRK>This paper proposes two types of linguistic verifiers for machine reading comprehension task in span extraction form. One is a rewritten question oriented verifier that checks the linguistic correctness of the extracted answers, and the other is based on a hierarchical attention network for answerability classification and boundary determination. The two verifiers are trained independently and then combined together via interpolation. Overall, the paper is well organized and easy to follow. The rewritten question oriented verifier could improve the linguistic correctness of the extracted answers. Some important baselines are not included in the experimental analysis, such as GPT 3 Few Shot [1] on TriviaQA which achieves 71.2, and RAG [2] on TriviaQA which achieves 68.0. 2.Some important details are missing in the experimental analysis. For example, in Table 2, it is not clear what "DA Verifier" means, and which verifier is used in the method "ALBERT + verifier". For a new method that has marginal performance gain, the extra computational cost should be considered. 4.The illustration of HAN based verifier in Fig.1(b) is not complete, which should have included the part for answer prediction and verification loss, etc. Language models are few shot learners.<BRK>The former uses a question rewritten method to replace the interrogatives in the question with the answer span for further verification. The latter adopts a hierarchical multi granularity (sentence, segment, and paragraph) cross attention to extract and utilize the context information of long paragraphs. Compared with the strong baselines, both the verifiers and their combination achieved relatively significant accuracy improvement on three mainstream span extraction MRC tasks: SQuAD2.0, NewsQA, and TriviaQA. The idea of bringing the answer back to the question for further validation is sound and it is reasonable for humans to do this process to verify the candidate answer in real world practice. 2.The question rewritten strategy is simple and effective, which brings improvements. HAN also handles the problem of long sequence well. 3.The overall method achieves state of the art results. How about their contributions to the final performance? 2.There is no test result reported for SQuAD2.0, though it is possible to obtain the results without making it public. Therefore, the clarity, “Due to anonymous issues, we have not submitted our results in an anonymous way to obtain results on the hidden test set.”, is not quite convincing. 3.The improvement of accuracy is mainly reflected in the questions of HasAns, which has no obvious contribution to the recognition accuracy of NoAns, which is one of the main challenges of the current MRC tasks. (Section 1 page 2 line 26) The paragraphs are divided into segments, with fixed length (e.g.,512 tokens with strides such as 128 or 256) and then divides the segment into sentences. So when dividing the paragraph, what if the dividing point is in the middle of a sentence? Would the incomplete sentence be discard？If not, how to further divide the segment to sentence level? Further clarification of the process would be beneficial. It obviously damages the sentence structure. 3.(Section 2.2 page 4 line 17) Multiple losses are employed, but the paper did not distinguish the practical effectiveness of each loss. My concern is whether each of the objectives is necessary since the experiment results in Table 3 has verified that  $l’_{3}$ does not significantly improve the accuracy of the model. Would the authors further verify the contribution of other losses to model performance (except for apparently indispensable l1 and l2)? Besides, may the authors list the number of parameters of each model (QRV, HAN, and Combination)?<BRK>In this paper, two linguistic verifiers are proposed to improve the model performance on machine reading comprehension datasets, such as SQuAD v2, NewsQA and TriviaQA. The first verifier rewrites the question by replacing its interrogatives with the predicted answer phrases. The second verifier leverages a hierarchical attention network, so that the long context can be split in to shorter segments, which are then recurrently connected to conduct answerability classification and boundary determination. The Empirical results of this proposed method is very strong. Finally, the proposed model also exceeds the BERT model on TriviaQA. Overall, it is a good paper. However, I have some comments:1. In table 2, what does “Regular Track” mean? It would be much easier to draw a fair comparison. 4.You used Albert xxlarge, but some methods in the tables used smaller pretrained models. 5.The font of figure 1 looks weird and a bit ugly. I suggest the authors make it more reader friendly. 6.Will you submit your model to the SQuAD v2 leaderboard? ***********************************Post rebuttal: The author has addressed most of my questions, and the SQuAD v2 test result is on par with the state of the art, partially indicating the proposed method is effective. So I am happy to increase my rating and champion for the acceptance.
Reject. rating score: 6. rating score: 6. rating score: 6. <BRK>The paper demonstrates the superiority of non autoregressive generator in the context of text GANs through various experiments including unconditional text generation, latent space manipulation and unsupervised decipherment. While these components are well studied in previous works, I think this work presents a neat combination of them in order to solve a well known problem. The paper provides rich discussions in training text GAN and comprehensive experiments and ablations to demonstrate the usefulness of an implicit text generator in different contexts. **Concerns & Questions to Answer during rebuttal**  The original text GAN papers were mainly motivated to address the *exposure bias* problem in maximum likelihood estimation for autoregressive generators. In other words, when we use an autoregressive generator to sequentially generate tokens one by one, there is a distribution mismatch between training and test phase. In your case, now that you already have a non autoregressive text generator, is there any *theoretical motivation/insights* for using the adversarial training framework? The non autoregressive (NA) text generator have been well studied recently so the novelty of this work is more on the integration of NA generator with adversarial training. Thus the main challenge here is how to solve the non differentiability problem. The paper directly leverages a traditional workaround, the straight through estimator, which is a biased gradient approximation. Is the independence between $z_1, \ldots, z_L$ going to be a problem? When we use transformer to do neural machine translation, the attention mechanism will capture the dependence in the input sentence ($z_1, \ldots, z_L$ in this context) and then produce the output correspondingly. The paper also propose to use the Max Gradient Penalty from image GAN domain. However this work uses the vanilla GAN objective (eq 12 and eq 13), which is not the WGAN or LGAN framework. Experiments: In table 2, all the results for NAGAN use the dropout with a positive ratio. How does NAGAN perform without dropout? Also I wonder if the comparison in the table and figures are fair, since most previous methods.baselines such as MLE or SeqGAN only use a vanilla RNN/LSTM, while NAGAN has a more complicated structure with transformers, and additional regularization such as gradient penalty and dropout. Perhaps we should control at least the number of parameters to be in the same level.<BRK>The paper creatively extends text GAN by introducing non autoregressive generator, which is a well known notion in translation and VAE like generation but not often applied in a GAN setting. The paper argues that a non autoregressive generator brings more effective gradient based optimization and also good latent representation learning capability. 1, Given very strong text generation capability of MLE learning and pre training, NAGAN makes little contribution to push the generation SOTA. Audiences of this approach are also limited. In this paper, given a very old baseline of MLE and a bunch of text GANs, the overall performance of NAGAN is still not much leading. Let alone compare it to other strong pre trained generators. 2.When claiming good latent representation learning capability, there should be a big gap between NAGAN and text VAEs in this aspect. If the author adds more control and manipulation experiments in text VAE, NAGAN will be not as shining as now. 3.Non autoregressive generator has difficulties in generalizing to long text generation and conditional generation. How does the author consider such settings, instead of simple unconditional generation in toy datasets like COCO?<BRK>This paper introduces the non autoregressive generator in the GAN based text generation, making textGAN can be trained without pre training and better utilize latent variables to control the style of generated text. Introducing non autoregressive architectures into GAN based text generator is a natural idea, and the modelling ability of non autoregressive generator has been verified at BERT. However, this paper should have more analysis of how non autoregressive architectures work in the GAN based text generation. Can you provide some analysis or examples about it? (e.g.change the value of some latent variable continually (from 0 to 1?) 2.Can you give some analysis about the generation process of the non autoregressive generator, (e.g.attention map), which makes the generator more interpretable. 4.Is dropout necessary for the non autoregressive generator? What if the dropout rate is 0, how the performance of generator changes? 5.Can you give more details about experiments, such as model parameters, training time, inference time and GPU you used?
Reject. rating score: 6. rating score: 6. rating score: 6. rating score: 7. <BRK>I have been playing with Nystrom approximations myself and I know the results are a bit disappointing, but it is grounded on strong theory. The reason for why my judgement is below acceptance is that I believe both theory and results altogether are not strong enough so they live up to what is promised in the abstract. Typically when new methods are proposed with the promise of bridging a gap and solving a relevant problem, I hope they will have a thorough theoretical justification, or the results will be compelling. None of this convincing enough here. 1)On the theoretical side, I missed a convergence analysis, as the one in the Nystrom method. The theory side focuses on some derivative calculations, but I would love to see how the interplay between sparsity, Nystrom method and LSH lead to better convergence. I acknowledge this can be hard to do, though. Without having the theory it is hard to understand whether any reported experimental result is a consequence of choosing particularly good example for their method. 2)I found the experimental/methodological side was a bit disconnected from the rest; the paper contains  several vignettes about some applications/improvements in the practical side, but when reading that felt like belonging to other paper. I recommend authors work in creating a more coherent story2a) I found the discussion on Multi Head OT unjustified and even a bit misleading. But none of these papers have any OT at all. The analogy of  "softmax for rows" is not convincing as this is simply a softmax applied many times. There is a world of difference between that and the result of sinkhorn algorithm, but the narrative seems to downplay the actual difference between them. I recommend the authors elaborate more on this connection, because otherwise it is hard to follow (and the subsequent results). Why would your method outperform Sinkhorn if it is only approximation? since an explanation of this phenomenon is missing I am led to believing. Authors should improve the exposition of the baseline "original". Why does full Sinkhorn does better than original?. In summary, I think authors should improve the discussion about the validity/significance of their empirical results, highlighting the regimes when they are supposed to express and when they are not. 2c)The main figure is Fig 2. I recommend authors build on this and expand those results so it is clear when their method is better and when it is not<BRK>This paper studies how to approximate Sinkhorn computation using more efficient kernel matrix representations (low rank approach + LSH based sparse approach). Neither of both ideas is completely new, the authors studies a combination of them that hasn t been explored in the literature, and use the proposed tech in three applications: ranking, embedding alignment, graph distance regression. Con:  I think the presentation is a bit out of focus. Some sections can be left for appendix (e.g., Backpropagation in Sec 4. and Sec 5.) since some are either very standard in the literature or not really solidly experimented. Since I consider the main contribution to be the empirical evidences, the space should left for more details on those empirical experiments (for reproducibility purpose). the paper s idea is not particularly innovative. (yet it is not the sole reason for my scoring). Some relevant works on low rank ideas have not been compared/cited. A lot citations/references are not particularly relevant to what this paper is about and make some parts not enough self explained. The authors addressed my concerns and I raised my evaluation ratings.<BRK>Proposal:  First log linear time algorithms for entropy regularized OT that work for complex   real world tasks using high dimensional spaces with little to no loss in accuracy  ... many claims in one statement  Locally Corrected Nyström  ... this would deserve a single paper   just to proof everything is still fine and valid  in particular to alternatives  In the way this paper is written I am positive it gets accepted   (because it fits the writing style of the more recent papers in the field)  and contains sufficient novelty  ... but I always wonder if good (published) science originates from clarity (or confusion)comments:  sorry but the text is very hard to follow ! Optimal transport is concerned with the problem  (on p.3)   I think some introductive  work may not harm in the first page  the reader is thrown up by terms and references   in my view more confusing than enlighting   > it may not harm to add some brief explainations of terms (from Cuturi:)   A transport plan is a flow on that graph satisfying source (a i flowing out of each node i)    and sink (b j flowing into each node j 0 ) ...   which is simple an optimal flow  in a graph ... I am not sure why we not simple can call it like this but need to come up with  new terms  The paper is written (following the very strange title   ... although Cuturi did the same it would be nice if we can stop having marketing titles  but focus a bit on science again ... in particular in 10 years many things proposed nowadays  are not lightspeed or warpspeed anymore) like providing an all issues solving theory    > this does not improve the readability of the paper  For example Eq.2 what is the  meaning  of (s) and (t)   I have an idea but it is not written there  it is hard to proofread and verify a paper if it is written with the objective to confuse the  reviewer ; )  widely incremental work by combing some known ideas (Nystreom, LSH, ...)   with a lot of addon  theory which is probably correct but not very clear in the presentation   Since the Nyström method is a low rank approximation it only accounts for theglobal structure of the kernel matrix K and not for the local structure around each point x.     this is actually wrong (!) reconstruction	 > there is a lot of work on the approximation bound of Nystroem (and related methods)   see  e.g.work by Dhillon  where is the definition of  sparse approximation K^sp  used in Eq 3? how precisely does \bar{P} (after Eq 3) link to the part around Eq 2? is the Kernel K_{ij} used  here as well and / or where is the actually input data Kernel matrix   in Eq 1 what should be a cost function here and how do you obtain C_{ij}? and although I understand that you like your method most it would still be good to provide some  oldfashion baseline (and not   not from sinkhorn)   We propose the graph transport network (GTN) to evaluate approximate Sinkhorn and enhanced optimal transport and advance the state of the art on this task.<BRK>Numerical experiments in several settings are performed to compare  the proposed approach with existing ones and demonstrate its scalability. Evaluation:I believe the proposed framework is a valuable contribution in terms of practical performance and wide list of applications where OT could not be used before because of the high computational cost. So, I would recommend accepting this paper. Pros:1.High scalability of the proposed approach and linear up to log factors in dimension complexity. 2.Flexibility of the framework due to a combination of sparse and low rank approximations, which are complementary to each other. Cons:1.Some parts of the paper seem to be not clear for a general audience. 2.What is "set of embeddings"? c. Last but one paragraph on p.2. $d$ is not defined. d. In (1) $F$ stands for the Frobenius product, does it? 5.What is "OT with multiple heads"? g. What is meant as embedding? If this is an $L_2$ distance, the convolutions can be used to accelerate the standard Sinkhorn and it would be nice to see the comparisons with convolutional Sinkhorn, which is also log linear. $B,r,b$ are not defined when they are first used. k. In (19), (20), how were the first equalities obtained? m. Appendix G. What is "similarity matrix"? Maybe it is too strong to state in the abstract that this is the first log linear time algorithm given that when the Sinkhorn kernel corresponds to a convolution, the Sinkhorn s algorithm is log linear by using the FFT. (Altschuler et al., 2017) did not show $1/\varepsilon^2$ bound for the Greenkhorn. The bound for Sinkhorn was improved to $1/\varepsilon^2$ in http://proceedings.mlr.press/v80/dvurechensky18a.html and the bound for Greenkhorn was improved to $1/\varepsilon^2$ in http://proceedings.mlr.press/v97/lin19a.html. 4.Appendix A. I believe that in this framework a general value of the regularization parameter $\lambda$ is used. If it is the case, then the number of Sinkhorn iterations to find an $\varepsilon$ solution to the regularized problem is $1/(\varepsilon \lambda)$. This follows from http://proceedings.mlr.press/v80/dvurechensky18a.html Theorem 1 and an estimate for $R$ in Lemma 1. The bound $1/\varepsilon^2$ corresponds to finding an $\varepsilon$ solution for the non regularized problem. In this case one has to set $\lambda \varepsilon/(4 \ln n)$, which may be too small.
Accept (Poster). rating score: 7. rating score: 7. rating score: 7. rating score: 6. <BRK>Keeping the performance of deep neural networks against data perturbations is an important and open problem. The authors propose an optimal control based approach by taking dynamical systems perspective. Authors supply theoretical analysis and (small) experimental evaluation. Overall, I believe paper is a good. However, I would like to get some points clarified:a)	Authors used manifold assumption (which is a reasonable assumption for many problems) to define running loss (eq 3). Would you please comment on the form of the loss and its impact to the method? b)	Let’s assume dynamical systems perspective is a right perspective for analysing deep neural networks (to be honest I don’t have any criticism about this). To use control theoretic tools, one needs to comment on controllability and observability of the controlled system. I suspect these mentioned properties are a function of the neural network architecture or do authors think the proposed method (as shown in Figure 1) makes each and every deep neural network architecture controllable and/or observable? c)	As I mentioned before, the empirical study is quite small, and I didn’t see any baseline (do I miss something here). I would like to emphasize one more time that, I am positive about the paper.<BRK>### SummaryThis study develops a closed loop control strategy to improve robustness of neural networks to adversarial attacks. The study is technically sound and the empirical results on classification tasks are convincing. ### QualityThe paper is technically sound and the claims are appropriately backed by empirical evaluation. However, I would recommend the authors to discuss a bit more the additional computational cost of running the closed loop method. ### ClarityThe manuscript is clearly written and provides enough information for an expert reader to understand all the steps to reproduce the results. ### Significance of the workThe results suggest that the developed approach is a solid step towards developing robust neural networks. ### Some typos: instead of "cause different data distribution deviating", "cause data distributions to deviate"; instead of "The resulting control policy [...] make it", "The resulting control policy [...] makes it"; instead of "the embedding are effective", "the embeddings are effective"; instead of "the perturbed states in Fig.2 [...] has", "the perturbed states in Fig.2 [...] have"; instead of "to obtain all the intermediate hidden states [...] and accumulates", "to obtain all the intermediate hidden states [...] and to accumulate"; issue with reference "E. 2017".<BRK>This paper first introduced layer wised projection from the poisoned data to the clean data. 2.The results show improvement of the robustness over the baseline on different types of attacks. The statement of the closed loop control is a little bit ambitious. The overall methods are the layer wise projection from the poisoned data to the clean data manifold. So closed loop should be at least multi steps within one layer. For different layers, the closed loop control will have different control signals, since the dimension/ distribution between layers is much different. So this method is only a one step layer wise projection. Also, this method is still a feed forward network, not a "loop" control. I do think this method is interested, just a little ambitious. This could be a useful extension of the resnet based network, since the control $u$ can be viewed as a complicated version of residuals. 2.The experiment is weak for only comparing with one baseline. Also, can the author provide which baseline model that the author is comparing with? I would appreciate it. This in principle learns the data manifold and provides the projection to this manifold. 3.Table 3 should be more self explainable. It s a little confusing in the current form. post rebuttal  The authors addressed most of my concerns and the revision is better than before.<BRK>Pros:+ Active controller based projection of intermediate features is an interesting idea and the utility of Pontryagin s maximum principle to address the challenge of high dimensionality of the state (features) is a good observation. While comparison with pixeldefense is useful, it would be good to launch an attack similar to the reference above and then observe the effectiveness of the approach. The state of the art for the used dataset in the paper is significantly better than the effectiveness of the presented approach. But firstly, the use of manifold based projection as a cost function is itself a non robust defense against adversarial examples. Second, the experimental evaluation in the paper is significantly lacking and does not meet the standards of a venue such as ICLR. The reviewer will strongly recommend reviewing the advices in https://arxiv.org/abs/1902.06705 on this topic. At this point, the paper is interesting but it needs significant development and is not yet ready for publication. What prevents one from using projection to a lower dimensional embedding space followed by a state space control method? 2.Is it realistic to assume the "input perturbation to be a random vector" in Section 5 for theoretical analysis when we are considering adversarial attacks such as PGD, CW? After author s response:* The response of authors identifies the problem of using running loss in projected space. While one can try to get around it by projecting the loss function as well but that would be a convoluted way to solve the problem, and in any case, not a strong criticism of the presented approach. * The updated document has generalized the derivation to take general perturbations into account. With these improvements, the reviewer is happy to recommend acceptance of the paper.
Accept (Oral). rating score: 8. rating score: 8. rating score: 7. rating score: 7. <BRK>This paper proposes a fully end to end TTS system with adversarial training. Overall, I vote for accepting the paper. Solid evaluation is performed, and experimental results and speech samples are convincing. + The main contributions are clearly stated and supported by the experiments. Section 2.5 on DTW is rather lengthy. In Section 2.1, T is used to denote the total number of output times steps of the aligner, while T in Section 2.4 denotes the number of mel frequency frames.<BRK>## SummaryThe authors propose EATS, a method for TTS from unaligned audio and text data, directly to the waveform. The point of the authors is that their methods is simpler because the training is in one stage. Overall I think this is a really good paper, that is likely to prove quite useful for the development of end to end speech synthesis solution. Previous work either use aligned phonetic features, or output spectrograms that are later converted to a waveform by a deep vocoder model.<BRK>This paper presents an approach for end to end speech synthesis, where every step is learned jointly with the others. Specifically,  the proposed model takes a character sequence as input and outputs an audio signal directly. The model is trained using an combination of losses, including an adversarial loss. The experiments are insightful, showing the impact of each part of the system. Detailed comments:  The reason behind using an adversarial loss is not really explained in the paper. A few lines before section 2.1 would help clarify that.<BRK>There are two major strengths in the paper. First, several modules in the proposed system are novel and smart, including the aligner and the dynamic programming loss, and it is, to my knowledge, the first feedforward text to speech system that does not rely on the intermediate representation. However, it is worthwhile to expand the discussion a bit by showing further experiments that demonstrate the potential benefit of end to end training. The training loss is like the superposition of common loss terms in the speech synthesis community, making the method look a bit heuristic.
Reject. rating score: 4. rating score: 5. rating score: 6. <BRK>The main goal of CM is to enable long term memorization as opposed to memory networks that suffer from gradual forgetting. They evaluate their model both on synthetic data as well as a few downstream benchmarks. Overall assessment:I really struggled with this one and I think there are some interesting ideas in there. However, it was very hard for me to understand the main motivation and story behind the proposed model and its design choices. I will provide detailed feedback below. It would be very helpful for the reader to understand early on what the target of the paper is. If not, it would be helpful to formally define them. If the task is defined the authors can use examples to make these concepts more clear. For example paragraph 2 in the introduction. If the authors are willing to submit a modified version during the author response period, I will re read and re evaluate my score.<BRK>This paper proposes the continual memory machine, which learns to compress an input stream with a continually learned memory module for reasoning after (long term) memorization. Their target scenarios are long term (text) question answering and lifelong sequence recommendation. The paper also conducts experiments on its syntectic dataset. The major weakness of this paper is the experiment design. Can you at least show some examples of your synthetic dataset? The mentioned related works can also save their memories after training and then use them for reasoning and inference only, what are the main disadvantage of doing that? Can you show the differences in math and in experiments? I thought R_q is the number of all queries, no?<BRK>Pros:  "Reasoning after memorization" is an interesting problem and the proposed solution generally makes sense under this setting  The proposed solution combines several techniques, some of which seem novel and useful  The experiments are diverse with good results and SOTA for recommendation taskCons:  The writing sometimes is misleading and vague  There is no major novel contribution  The synthetic task is poorly described   The experiments lack details of baselines and hyper parametersDetailed comments and questions:  In the introduction, "Castatrophic forgetting" [1] is about continual learning over multiple tasks and thus, different from the problem the paper is addressing. As in Eq.3, the memory slots seem to be updated independently. In Advances in neural information processing systems, pp. Please elaborate more on this point or correct me if I misunderstand. Please consider stronger baselines that can reason and remember [4,5]. Also, the related work is incomprehensive without these methods. Also, the authors should consider a comparison between baselines  number of hyper parameters  The authors claim that segment level memorization is better.
Accept (Poster). rating score: 7. rating score: 7. rating score: 6. rating score: 5. <BRK>The authors further investigate the meta learning problem on a hierarchical Bayesian model, discuss the lower bound and upper bound with maximum a posterior estimator. Overall I feel this paper study an important problem of the meta learning and the derivation all looks correct. The main drawback is that the presented minimax lower bound is quite pessimistic that may not fully exploit the potential task similarity in practice, but I feel it is still acceptable to first study the minimax lower bound, and leave the case with more structure as future work.<BRK>The paper studies the information theoretic lower bounds in the minimax setting of meta learning. The paper also discusses upper and lower bounds in the hierarchical Bayesian framework of meta linear regression. It would be great to have a discussion on this and the assumptions that one needs to take to improve upon this gap. In the hierarchical regression setting, the assumptions for theorem 3 and theorem 4 are different. **After Rebuttal**I have read the reviews of other reviewers and the responses of the authors to the questions posed by the reviewers and Ahmad Beirami. However, I believe the paper is an important step in this direction.<BRK>The authors study the performance of a meta learner theoretically in two settings. In the first one the overall number of possible tasks is limited and tasks are close in KL divergence. The second setting is MAP estimation for a family of linear regression tasks. Lower and upper bounds are provided on minimax parameter estimation error.<BRK>For instance, the asymptotic analysis provided in paper gives some insight on the performance of meta learning algorithms with the number shots, ratio of observation noise to the sampling noise and the number of tasks. For instance, it is assumed that the distributions are $2\delta$ separated while close in the KL divergence sense. This is a minor issue and up to the authors to change it or not but it may help with the readability of the paper.
Reject. rating score: 4. rating score: 4. rating score: 5. rating score: 6. <BRK>Weaknesses: While SVAE is useful in the context of the paper as it uses explicit PoEs, the experiments don t show conclusive differences in performances with VAEVAE and VAEVAE* variants, reducing the potential impact of the novel elements of the paper. They also propose a novel model (SVAE) built on the PoE approach that, compared to previous models, aims to better handle missing modalities. They can t support the proposed model, nor the other insights of the paper. In particular, they introduce additional networks that estimate the marginal distributions of the latent representations of the missing modalities given the observed ones.<BRK># UpdateI thank the authors for their comments and answers. This is a fundamental aspect as modalities can be coherent but very far from the true data distribution or still not exactly close to the reconstruction, which is just a mode of the whole distribution. I believe they could be addressed in new major revision of the paper and I encourage authors to do so. As another contribution, the authors introduce the SVAE as a variation of a sota PoE VAE, the VAEVAE, for bi modal learning. # PresentationThe paper is generally understandable and well written. # ContributionsOn the one hand, the discussion of the pros and cons of PoE VAEs and MoE VAEs boils down to one conjecture ( AND  versus  OR  modality fusion) that is shallowly tested in the experimental section. However, one ordering over modalities for conditioning (according to Appendix B) shall be chosen, and it is not clear how this can influence learning and inference (in a similar fashion variable ordering influences autoregressive models). One additional downside of the experiments is that only coherence inter modalities is measured as a metric.<BRK>Pros:This paper proposed SVAE as a more general form for the previous VAEVAE model. 2).From all the experiments listed in the paper, we can see that VAEVAE performs best. From the subsection "SVAE vs VAEVAE" we know that SVAE is a general form of VAEVAE, but it becomes less important since the general form performs worse than a special case in practice. In other words, the importance of the works is not that convinicing. 3) In equation (10), will weighted sum be a better choice?<BRK>The paper clearly frames the contribution within the relevant literature, the introduction is well written and the paper is well structured. Claims are supported by experimental results, that use MNIST, SVHN, CUB Captions. A final discussion summarises the claimed contribution and advantages of the proposed approach. How well do you expect your approach to work on modalities presenting considerably different dimensionality (also in comparison with other approaches)? Quantitative results show comparable scorse between SVAE and VAEVAE variants. Where/why would SVAE be most beneficial/advantageous? Which applications would see MoE approaches outperform PoE? I think this point is important for completing the analysis and comparison of the two approaches.
Accept (Poster). rating score: 8. rating score: 7. rating score: 6. rating score: 6. <BRK>The authors capture uncertainty in deep networks by employing stochastic activation functions but deterministic network weights. 2.The choice of the GP kernel seems to have a large effect on the learned activations (Figure 8). Overall, I  enjoyed reading this paper and only have minor quibbles. 1.As with any GP based approach computational scalability is a concern.<BRK>In this paper, the authors contribute to the recent trend of replacing uncertainty in the weight space in favor of uncertainty in the function space. The mathematical reasoning is sound and the Figures are of excellent quality: in particular, Figure 2 is a very good illustration of the paper s core concepts.<BRK>The paper proposes a variant of BNN with stochasticity added to the activation layer (auNN). This part is more like different designs of the kernel. Here is the reason: in auNN, f1 is only dependent on a1 and f2 is only dependent on a2.<BRK>The idea of moving the uncertainty from the weight to the activation function is not new. Given that this is built on GP, a complexity analysis on running time (both for training and inference) is provided. This work is heavily built on the DGP work with help from the reparameterization trick. You are definitely correct in asserting that uncertainty in weights manifests as uncertainty in activation functions.
Reject. rating score: 5. rating score: 6. rating score: 6. rating score: 7. <BRK>This paper focused on developing a new regularization technique for autoencoders, which shapes the latent representation to follow a manifold that is consistent with the training images and that drives the manifold to be smooth and locally convex. So the architecture of the proposed model includes a standard autoencoder, a discriminator and the loss mentioned above. The authors tested the proposed AEAI method on one synthetic plot dataset and COIL 100 dataset and compared the results with three other models. The effectiveness of the proposed technique is tested by both visual inspection and comparing the reconstruction error against the available ground truth images. I would also suggest visualizations of the latent space using the PHATE technique from the same paper. I also feel that the authors should try a more irregular/noisy dataset to further show the effectiveness of the model.<BRK>It is experimentally tested by measuring interpolation error on two datasets: a new custom pole dataset introduced by the paper and the COIL 100 dataset. I don’t see why it should. 3.Ablation study helps comprehend the contribution of each loss term. This could use clarification. For example, there’s no measure of the effects of the better conditioned latent space on downstream tasks such as a classification on SVHN or CIFAR10 like some other works in the domain checked. 14.The loss terms are called $L_{a,c,s}$ but the weighting hyper parameters are called $\lambda_{1,2,3}$. Actually it should probably be presented before AMR. 3.“Researchers have demonstrated the ability to interpolate between data points by decoding a convex sum of latent vectors (Shu et al., 2018) ...”  I believe there’s prior art before that, for example, just to name one: https://arxiv.org/abs/1611.03383 4. I seem to only see results for COIL 100 and the pole dataset. Most of my queries were clarified and I raised my rating accordingly.<BRK>This paper introduces several autoencoder (AE) regularization terms that aim at reproducing continuous realistic deformation by interpolating latent codes of images. The authors assume there is a continuous process generating the data and introduce three novel loss terms (in addition to the standard AE reconstruction loss). The second term is called cycle consistency and enforce injectivity of the decoder. The method is tested on a synthetic "pole shadow" example, and COIL 100. The results are convincing and seem to provide a good arrangement of the latent space. In terms of contributions: the incorporation of a discriminator loss in training of AE to provide more realistic interpolations was done before, as the authors acknowledge (e.g., Beckham et al.2019).This diminish some of this paper s contribution. The last question is interesting both for in distribution test examples, as well as out of distribution test examples.<BRK>## SummaryThe paper presents a method of regularising the latent space of an Autoencoder in a way that pressures the data manifold to be convex. This allows interpolation within the latent space which does not leave the data manifold and results in realistic reconstructions as one moves from one point to another. This is done through the introduction of adversarial and cycle consistency losses, over and above the usual reconstruction loss and a smoothness loss. It is structured logically and the authors  arguments are easily followed. The results are compelling and the proposed AEAI technique clearly outperforms other methods in the qualitative experiments, with quantitative results to substantiate it. The approach is compared with modern competing techniques and the work is well positioned among recent literature in the field.
Reject. rating score: 3. rating score: 4. rating score: 4. rating score: 8. <BRK>Based on this motivation, the paper proposes a method to do the knowledge transfer in the model space instead. The idea is to apply K NN method in the model space to pick up a group of strong classifiers trained on the head classes with sufficient training samples available that are closest to a weak classifier in the model space and then a linear combination of the group of strong classifiers and the weak classifier to form a stronger classifier to the tail classes where only few samples are available for training; the linear combination weights are learned from a simple neural network, called Alpha Net.<BRK>The experimental results on two benchmark datasets outperform some existing methods. # Strengths  While the idea of constructing classifiers by a linear combination of other classifiers has been proposed for several different problems (e.g., zero shot learning), its application to long tailed classification seems to be novel. I think these new materials can greatly strengthen the paper. The proposed method improves the performance of tail classes. 2) It s nice that the authors compare the shared and non shared alpha net.<BRK>* It seems there is a tradeoff here, where to learn better tail classifiers you hurt head class performance. The paper presents an interesting idea, transferring of knowledge between head and tail classes at the classifier level, ie create stronger classifiers for the tail classes by linear combinations of  a tail classes  nearest neighbour classifiers with the current "weak" one.<BRK>Technical Quality:The technical content of the paper appears to be correct. Presentation/Clarity :The paper is generally well written and structured clearly. While this method is a clear winner on Few classes, it is not performing as well in Medium classes, as shown in Table 1. An explanation about this issue could strengthen the paper.
Reject. rating score: 3. rating score: 5. rating score: 5. rating score: 7. <BRK>I previously reviewed a version of this paper and unfortunately the primary issues with it have not been addressed sufficiently. While some parts have changed,  I will draw on relevant portions of my previous review where appropriate. This paper sets out to investigate the respective "inductive biases" of LSTM and Transformer neural networks, two dominant model familiesthat are frequently employed in applied NLP tasks. They also seek to compare the "inductive biases" of CNNs and MLPs. The air quotes are placed here because all generalization and thus any claim concerning the generalization performance of a modelnecessarily concern (whether explicitly or implicitly) inductive biases. In many settings we canformally characterize the bias of a hypothesis, e.g.through learning theoretic complexity measures. However, here the term is used excessively with fuzzy claims made about some models having "stronger" or "weaker" inductive biaseswithout invoking any concrete measure of the expressivity of a hypothesis class. Second, I do not believe that the current exposition is suitable for publication. Throughout the authors confuse what has actually been showing in prior works for what has been speculatively claimed in prior works. The authors show plots that simply depict performance but describe them as characterizing the bias variance tradeoff (absent any discussion of variance). The authors have a lengthy discussion of calibration that does not make much senseand parrots incorrect claims from previous papers such as the bizarre claim that label smoothingcalibrates classifiers (it s rather easy to see how label smoothing could lower ECE for an otherwise overfit classifier but how in general it does not calibrate and can even decalibrate classifier.The idea that calibration magically falls out of knowledge distillation[[[previously this review had the clause:"or that any of these models is "perfectly calibrated" (a claim they actually make)"however, as the authors rightly point out, this was a mistake in my review,the context here was defining a marginally calibrated classifier,not in claiming that the KD models achieved perfect calibration.]]] is bizarre and unacceptable in a proper publication. The knowlegde distillation experiments are interesting but the speculative interpretationsgo far beyond what the actual experiments show (that distilling from a better performing teacher modelgives a better performing student model, regardless of the student s architecture). In short, this paper is not suitable for publication and must be substantially rewritten. The authors need both a more compelling result and a more forceful editor.<BRK>This paper shows that knowledge distillation from a (teacher) model A with an appropriate inductive bias to a (student) model B lacking it can lead to B generalizing better than if B was trained without knowledge distillation (but not as well as A), including out of distribution. The authors also show that the resulting learned representations inside B, as well as the shape of the training trajectories, are more like those of A (than those of B without knowledge distillation). This is not very surprising but is still interesting from the point of view of the understanding of the nature of inductive biases. We already knew that inductive biases (like translation invariance) can be transferred through examples (e.g.by generating data transformations such as translated images), so this paper extends that kind of idea to knowledge distillation to provide the targets for such examples. One concern I have is  so what? .Have the authors thought of possible way (in say, future work) to take advantage of that observation? It is not obvious, because if you already have a teacher model A with the right inductive biases for the given task, why would you care about training a student B which is going to be worse than A anyways? In addition, unlike for the original motivation of knowledge distillation, we normally expect that B would have MORE capacity than A (because it needs to  learn  the inductive biases, so one would expect it would not work to choose B much smaller than A, in the sense that the gain would be much smaller, and certainly not as good a model as using A). We already knew that examples could transfer inductive biases, now we know that knowledge distillation can do it, but why would that be useful? Also, the outcome of such experiments would not be apriori obvious (we would expect a gain vs the regular B, but would it be sufficiently interesting to be worth it?). The paper already shows the unsurprising  result that distilling into B from A helps somewhat OOD. It would also be interesting to explore whether taking inputs outside of the training distribution of A as distillation examples when training B would increase the robustness of B OOD. Minor comments:  fig 4: caption is insufficient to understand the figure  the sec 3.2 sentence with  almost closing the gap  is too strong and needs to be weakened (there is still a significant gap, with almost twice the error with B compared with A)  the conclusion sentence with  demonstrate having the right inductive bias can be crucial  should be reformulated, since this is not a new demonstration (and reading it without reading the rest of the paper may give that false impression)<BRK>In this manuscript, the authors investigate the power of KD to enable benefiting from the advantages of different models at the same time. It first talks about inductive bias can be crucial in some tasks and scenarios, and further show that when a model has the right inductive bias, we can transfer its knowledge to a model that lacks the needed inductive bias and indicate that solutions that the student model learns are not only quantitatively but also qualitatively reflecting the inductive biases of the teacher model. The paper is well written, but not easy to follow. The efforts of this study may help better learn and find suitable models for some AI issues. It seems that these experiments are well planned but without more detail, it was challenging to thoroughly evaluate the proposed work. 2.How the large data sets will be integrated and analyzed, and what might come from the collective analyses was not described in detail. Some concern was expressed regarding whether the large amount of data generated would address the defined goals of the manuscript. More details regarding the integration and analysis of the experimental findings would help clarify this. 3.I also wonder that how the batch effects among different data and data types will be taken care of, as the integration of multiple data types and analysis is the key to this model.<BRK>The paper investigates the oft overlooked aspect of knowledge distillation (KD)   why it works. Compared to prior work showing that better teacher performance lead to better student performance, this paper also shows that the student s performance on different aspects becomes more similar to the teacher s   (1) if the teacher is strong on metric A and weak on metric B compared to a student on its own, the student can become stronger on A and weaker on B when distilled using the teacher; (2) if the teacher can generalize well to a separate, previously unseen dataset but the student generalizes poorly on its own, after distillation the student can generalize much better than it can possibly learn to on its own. Some (not all) experiments shed light on how the hypothesis seems to be true. (see above)  Comes up with ways to measure transferred inductive bias, by highlighting different aspects of generalization for a student and comparing with and without distillation. 1.The abstract is especially not telling the readers much about what is in the paper. I personally would be confused and skip reading this paper because I thought the paper discusses "can we distill knowledge using knowledge distillation". Inductive bias come in many forms and is not often discussed, and it helps to use examples to tell the story directly, e.g.by mentioning the specific differences between inherent priors in CNNs/MLPs or LSTMs/Transformers in the abstract *and* the first paragraphs of the introduction. Although after extensive thinking I believe the paper is distinct from previous KD analyses, the paper does not itself distinguish its findings enough from what is known in the literature. 1.Granted it is hard to distinguish the inductive bias transfer aspect of KD versus other aspects of KD, it is hard to experimentally prove it because the field does not quite know what are the aspects of KD that makes it work. People expect soft labels to help not because they make models better calibrated, but because they boost performance, and it s not clear if people think better calibration leads to better main performance. 1.The CNN/MLP experiment only has tasks that CNNs outperform MLPs. It would make it more interesting to see a task where the MLP outperforms CNN, e.g.a made up task whose ground truth is the xor of a few pixel positions, which could be hard for CNNs while easy for MLPs. Just two datasets and two sets of networks is not very convincing to claim these findings generalize to other architectural changes. And some experiments are not convincing or irrelevant. It would make the point clearer if a worse CNN is used such that the MNIST vanilla performance is the same as the MLP, and show improved generalization results on MNIST C.    1. Figure 1 does not tell readers much, because latent representation can be both inductive bias and regular representation power, and we already know that KD can improve the student s representation power. UpdateWhile I agree in principle with Reviewer 1 that this paper has jarring flaws in writing and the rebuttal version does not adequately address it, I disagree that the writing warrants such a low score. I would also disagree with R1 that there is no interesting result in this paper, because there is no prior work I know that even considers how distilled models generalize like their teacher. If I were to grade this paper based on different aspects, the originality and significance would be both 9 s, quality a 6 due to experiment issues and careless generalization, and clarity a 3 4 due to unclear motivation in the abstract/early intro and poor differentiation from prior work in terms of experiment design and analysis. That said, the rebuttal did not change my mind that the writing probably will not be improved enough post rebuttal, I would thus not be able to consider this a top paper despite the interesting observations.
Accept (Poster). rating score: 6. rating score: 6. rating score: 6. rating score: 6. <BRK>### summaryThis paper introduces transformer network to visual navigation, specifically, object goal navigation. It also develops several new feature descriptors as the input of the transformer encoder and decoder. To properly train the whole model, a supervised pre training stage is used to warm up the transformer model. 5.Why not also adding global features into the transformer encoder? This paper provides a good example. Most importantly, this paper focus on the representation learning part of the whole pipeline, which isn t that straightforward of how to use a transformer. The best results of VTNet in Tab.1 used TPN. 2.The writing is mostly clear with clear motivation and background discussion. 3.The performance boost, especially SPL, is relatively significant compared to previous SOTA, and the ablation studies have verified most of the design choices. Is the semantic label the class index (1,2,...,C)? What s the specific reasons of using AI2 Thor over habitat? Please address my questions. I m looking forward to discussing with the authors and the peer reviewers. Why DETR doesn t suffer from it? The penalties are the same for different model in RL,  why this transformer based representation learner suffers from "early stopping"?<BRK>**Paper summary**The paper addresses the problem of navigation towards objects (ObjectNav) in a virtual environment. The idea of the paper is to incorporate spatial information of objects using a transformer based framework called Visual Transformer Network. The paper compares the results with a number of state of the art ObjectNav models and provides an ablation study. The results have been reported on the AI2 THOR framework. **Paper strengths**  The idea of incorporating object information using transformers for a navigation agent is new. The ablation studies show that the introduced components are effective. For example, the introduction discusses details such the difference between DETR and Faster RCNN or difficulty of training the transformers. There are several sentences with grammar issues. It is a bit strange that nothing is learned without the imitation pre training. I recommend running the method on some other frameworks which include slightly larger scenes to see if the method generalizes to those as well. Writing is the main issue of this paper. The rebuttal addresses my concerns to some extent (writing has improved in the revised version, but it still has some issues).<BRK>The paper uses DETR for object detection and learn an association between local descriptors (from the object detector) with global descriptors (ResNet18) using the proposed VT model. They show that using VT improves performance on the object navigation task in AI2 THOR simulator compared to existing methods. Strengths   The paper proposed a novel transformer architecture that learns an association between local object descriptors with global image region features so that actions can be grounded to visual regions in the image. Additionally, I didn t fully follow how authors obtain the appearance features from Faster RCNN based method. The navigation task is also made simpler by discretizing into a grid. Single room environments and discrete grids simplify a lot of navigation related challenges and the authors don t discuss how the proposed architecture will generalize to more complex object navigation tasks. Other questions:   Instead of pre training without employing the navigation policy, did the authors try using shortest path based demonstrations to help learn the navigation policy as well? In the first stage, the navigation policy learns using imitation learning and then finetuned with A3C? What is the step size of the agent for the forward step? Is it superior visual representations (Faster RCNN vs DETR) or the fact ORG only chooses objects with the highest confidence while VT uses all the detected objects?<BRK>This paper demonstrates a model that uses the Transformer to encode the visual features that appeared in the visual input image during navigation. The model is firstly pre trained under imitation learning objective with self generated shortest path trajectories. The empirical results show that the model used in the paper outperforms previous methods on AI2 THOR environment. The authors also show some studies on the contributions of each component in the model. Paper strengths:+ The proposed method further show that the Transformer is a powerful model for feature extraction+ The authors demonstrate one method to make the training of Transformer work, i.e.pre training transformers using shortest path trajectories+ Empirical result support the authors  claims. Cons:  The paper adopts the Transformer and adapted it into the navigation problem. No new architecture/model is proposed.
Reject. rating score: 4. rating score: 6. rating score: 7. rating score: 7. <BRK>Summary:The goal of the paper is to perform node classification for graphs. However, from table A.2 in the appendix   it is clear to see that MLP s outperform GNN s with or without the attention sorting in the dis assortative graphs   and the performance of the MLP s and proposed augmented NLMLP are well within one standard deviation from each other. This questions the need to employ a proxy graph construction on top of MLP s for these datasets as it appears like the data can be treated as i.i.d (and not relational).<BRK>This paper targets on addressing the node embedding problem in disassortative graphs. The sorting order depends on the attention scores computed with the local embedding vector of a node. The presented simple approach is an interesting idea to “push” the distant but informative nodes together. However, it is unclear how the “attention guided sorting” is aware of the “distant” nodes. The local node embedding vectors z can be obtained either by the node content, or by GNN. The discussion was very helpful.<BRK>+ the experimental evaluation methodology is sound, and comparisons with several previous works are madeWeaknesses:  the approach is difficult to interpret: it s difficult to convince someone working on GCNs why it would work. This algorithm has the advantage of being asymptotically faster than other non local aggregation schemes, and the paper demonstrates that empirically it can do at least as well as some of the other methods. From the discussions, it seems that there are quite a bit of concerns raised about the experimentation process.<BRK>The paper is well written and easy to follow. This can be treated as a variants of the proposed NLMLP to show that sorting the nodes is more efficient and more effective. 3.Experiments well support the claim of the paper.
Reject. rating score: 3. rating score: 5. rating score: 7. <BRK>Major comments:(1) The authors made little attempt to actually compare their model with experimental findings, or at least make concrete testable predictions. (2) Critical questions about how real gradients are computed and transmitted to inferior olive is not answered. The authors completely circumvent this problem by saying it is “outside of the scope of the current paper”. Here are a few that I noticed. It is an approximation that works well empirically in the cases studied in that paper.<BRK>The authors consider the architecture of the cerebellum as the predictive component of a decoupled neural interface (DNI). Using this framework, they perform experiments training networks with BPTT on several temporal tasks. The paper is exceptionally clear and the experimental investigations are well done. The logical induction that the authors make "predicting that the cerebellum is particularly important for more temporally challenging tasks" does not require the DNI to be established. Therefore, I cannot offer strong support for acceptance.<BRK>Reading Jaderberg et al.(and the SM), it s clear that the synthesiser is instead continually trained on bootstrapped estimates. The model seems to offer a compelling and fairly novel explanation of cerebellar deficits (including non motor) with a broad significance across the neuroscience and deep learning communities. **General**The paper presents a very intriguing hypothesis, and I believe that its publication will benefit the community and stimulate fruitful discussion. In Section 2, the cortical network uses the backward synthesiser to avoid needing to wait for the loss signal   however, this seems to merely shift the problem since now the synthesiser will need to wait for the loss signal to train.
Reject. rating score: 3. rating score: 5. rating score: 6. rating score: 6. <BRK>I think I understood most parts of the paper. It would also be nice to test the SIN trained model from Geirhos et al 2018a, since it has a high shape bias. General comments:Page 2 “In contrast, there was also evidence that classifiers trained on one type of adversarial example often do not generalize well to other image types [Geirhos et al, 2018b, Nguyen et al., 2015]”. Both references do not provide any support for the claim made in this sentence. Suggestion: It would make the work stronger if the authors considered more adversarially robust models, e.g.[10], [11]. They do not show results for adversarial examples. [2] show that adversarial robustness does not transfer easily between attack classes. This paper has been presented at ICLR 2019 and this should be reflected in the references.<BRK>They also show that adversarially robust networks do not outperform non robust networks on corrupted data. Finally, they perform some analysis to determine whether intermediate features are more related to shape or texture, finding that these representations to intertwine both types of information. **Quality:** The paper is well written and the experiments are very interesting. However, the contributions may be somewhat incremental (see below). However,  given that the shape bias of adversarially robust networks was already known previously (as pointed out by the authors) and that there is no methodological contribution, I think that the contributions are somewhat incremental.<BRK>This paper takes a step further to understand the relationships between the adversarial trained CNNs (R CNNs) and shape based representation, and delve deeper into the R CNNs via studying the hidden units. Many understanding is complementary to the findings in the literature, such as [1] that adversarially trained models are shape biased, [2] that there re nor significant correlations between shape biased and robustness against common corruptions, and [3] that low frequency help the generalization. The edge map is easy to drive from the silhouette and may be more suitable to be tested here. The value of the edge map and silhouette may be set to binary or greyscale [0, 255]. The reviewer wonders if any quantitative criterion can be designed and be reported thereon. The reviewer wonders if the author can test on Gaussian additive noise distorted images and report the results to further justify the claim. Since the authors reply near the discussion phase end, I cannot ask follow up questions. For the second point, how would you formulate a regularize?<BRK>Summary:The submission concerns an experimental study of the behavior of networks trained with and without adversarial robustness criteria (using Madry et al., 2017). Given a set of such trained networks, a detailed look at the behavior and properties of adversarially robust networks and their non robust counterparts is taken. Furthermore, visualizations of filter banks are analyzed and compared, as well as an analysis on the neuron level is carried out, using the NetDissect framework by Bau et al.Insights include that adversarially trained networks are more shape biased (and reliant) than their counterparts which are known to be texture biased. What I am missing a bit, however, is a discussion on what the mentioned findings may mean for future developments of adversarial training, adversarial attack design, or other mitigations of adversarial attacks. The discussion of possible consequences is limited to the last paragraph, which is meant to discuss future work. For example, is it necessary to define “S networks” and “R networks”, where R networks happen to be *s*hape biased and S networks *t*exture biased? My preference would have been to simply spell out “adversarially trained” (“ adv”) vs. “not adversarially trained”. This is just a personal opinion and does not affect the rating.
Reject. rating score: 3. rating score: 4. rating score: 5. rating score: 7. <BRK>The paper sets out to address a class of pure exploration problems. Of particular interest to the authors seem to be settings where the sample complexity requirements are very stringent. Even though the problem domain is very interesting, the paper is just not fit for publication in this form for the following reasons:1  The write up is just horrific:  a  The problem setting doesn t mention anything about intermediate rewards, which was very confusing because the reader is given the impression that learning happens based on just the total reward at the end of the trajectory. 3  I find the experiments very unconvincing: as mentioned above, a really good motivating use case is hyperparameter tuning for a very complex models or at least a diverse set of examples. Please get one of your colleagues to read the paper before resubmitting it.<BRK>Summary:In this paper, the authors present an adaptive model that can learn a good policy by adversarial training. They focus on the setting when the query budget is small and conduct some experiments to verify the proposed method. Using adversarial training is interesting. I feel like the query budget in the experiment is very small (T 20). The current experimental results are not very convincing for me. Uncertainty or SGBS seems to be a good choice in some cases as well.<BRK>The authors should choose a motivating example  (ideally, it should be a real world domain, but it could also a be a simplified, synthetic one)  on which to explain all the basics of the proposed approach: which are the equivalence classes, how is the adversarial training leveraged, and how/when will one suffer the catastrophic loss referred to at the very bottom of page 1. The empirical validation will greatly benefit by tightening the various arguments. For 20 Questions:  the scalability issue should not be confined to APPENDIX I: what would have taken to train on the entire dataset? They should also provide in an APPENDIX several 1:1 graphs that compare the proposed policy against each of the other main contenders.<BRK>The framework is based on the idea of making use of a measure of problem convexity (each problem is parametrized by a parameter theta) to solve solving a min max objective over policies. The rationale behind this objective is that the resulting policy of solving this min max objective should be robust to the problem complexity. As it is written the method does not seem to scale beyond very small dimensional problem instances, since otherwise the value N would have to be exponential in the dimension, and therefore intractably large. The paper falls within the differentiable "meta learning" for bandits literature, and it does a good job of placing itself within that literature. It also has a convincing experimental section. I also find particularly interesting the use of the model complexity balls that can be defined using other existing results in the literature such as in the case of transductive linear bandits.
Accept (Poster). rating score: 6. rating score: 5. rating score: 4. <BRK>This manuscript provides an intriguing discussion on the different roles that the width and parameter size could play in a neural network. While these two aspects are traditionally treated as  if not identical  correlated, the authors managed to develop a couple of configurations to decouple and analyze both separately. I m only concerned about two aspects: i) The section 3 could have been perhaps easier to follow if the work by Jacot et al.2018 had been briefly recapped. ii) The experiments are all conducted on image dataset. One might wonder whether we could draw the same conclusion for datasets of more general nature.<BRK>In this paper, the authors analyze the enhancements brought by widening networks with the number of parameters fixed. From the theoretical side, the authors relate the training dynamics of neural networks to kernel based learning, in  the infinite width limit. Pros.1.The idea of widening the neural networks with fixed number of parameters is novel and the authors conducts abundant experiments to support their assertions. From my opinion, since the authors have asserted that they try to avoid big adjustments on structure of the networks, the bottleneck like methods are inherently excluded from the list. I think further explorations can be made.<BRK>2.For the experiments, I wonder whether you have done enough fine tuning for the hyperparameters of each model and used the best test accuracy. Pros:1.Understanding the reason behind the generalization performance of neural networks is a very important problem, and the idea of decoupling the effects of network width and the number of parameters on the generalization performance is interesting. I think the goal is to analyze the effect of width but not to get a significant improvement on the test accuracy. 2.This paper is clearly written, well organized, and generally easy to understand. The relationship to prior works has been clearly discussed. 3.The experimental methodologies and theoretical computations appear to be correct. 2.The definition of "number of parameters" is a bit unclear in this paper, making the problem that the authors want to analyze a bit vague. These terms are not formally defined or explained in detail, making the points made by the authors a little vague.
Reject. rating score: 4. rating score: 4. rating score: 6. rating score: 6. <BRK>This is particularly important since standardized comparisons are missing and so simply presenting some results gathered from other papers is not enough in my opinion. Also, what about the other experiments? And even when the approach does not outperform existing methods in all aspects, at least I would like to be able to judge what are the scenarios where the method performs well. STR shares a lot of similarities with this work in the sense that STR also introduces a per layer threshold for pruning that can be efficiently optimized using a differentiable soft threshold operator. There is only one comparison point in Table 2, which however seems to be based on a different implementation thus resulting in a different baseline accuracy. However, the major concern of mine is the experiment section. * The experimental hyperparameters are not fully listed and the ones that are listed are scattered throughout the paper. In particular:    1. The contributions list in the introduction mentions the number of pruning and fine tuning epochs for some experiments but the experiment section doesn t provide a full overview of pruning+fine tuning epochs. As such the method is well described and the benefits are clear. Table 3 provides some of these numbers but not all details. 3.What about training parameters? Are those the same as in the original paper? It seems scattered, the presentation is confusing, and most of all it makes it extremely difficult to assess the performance gains of the presented method in comparison to existing methods. There is no single figure that allows me to assess the prune accuracy trade off across a large range of prune ratios and for various comparison methods. Also the results are presented inconsistently. ## Other Minor Feedback* I find the introduction and related work interesting and it serves as an appropriate motivation for the work. 3.More comparison methods: Unstructured pruning has seen quite a few advancements over the last couple of years and as such I believe it is crucial to compare to many more pruning methods.<BRK>This paper proposes soft pruning and soft l0 regularization. That seems to me ineffective compared to L1 or similar methods. That needs some clarification. As the paper then focuses on L0 regularization, I missed comparisons to related work on that regard. This is not the first paper aiming at using L0 and proposing a differentiable approach. How this compares to others? I am confused with one of the contributions is to "provides a trace of checkpoints with varying pruning ratios andaccuracies. Because of this, the user can choose any desired checkpoint based on the sparsityand performance requirements for the desired application"Why is this different from any other approach? As soon as the code saves the checkpoint (which most do) then, the user has access to the same flexibility, right? Each architecture is using a different hyperparameter, how a user could set these? Text suggests Kusupati uses a better baseline, would it be possible to show results of LTP on that baseline? if not, why not? For Renda, seems like the key difference is the training process. Would be good to see the benefits of a longer training process for LTP. It is not clear to me that LTP can get better results even training for longer. Results on more compact networks compared to the self implementation of Global pruning seem promising for larger compression rates. The paper also claims the benefit of learning the threshold per layer, however, provides no result on the distribution of those parameters. Would be interesting to see how these values are distributed in each architecture to reinforce the value of not using a single value for all layers and architectures.<BRK>The paper proposed a new method to prune a neural network. The method is interesting, innovative and effective. It makes it possible to learn tunning parameter via back propagation, hence learn together with network s weights. The paper is well structured, the writing is clear and easy to follow. The conducted experiments are thorough and clearly show the efficiency of the proposed method. The work would be beneficial for others if the code is published open. While I am still positive about the approach/methodology, I am not confident about the technical details of the experiments, without which, it s very hard to justify the effectiveness of the method. I share other reviewers  views regarding inconsistencies, e.g.Tables 2 5, that have not been fixed in the updated paper.<BRK>I think that using $\approx$ would be more standard, and easier to understand? I feel that it could be useful to highlight the top results in each column in each scenario in bold? For example you could put the different rates as different columns, and use eg top 5 accuracy throughout the table. (or put top 1 and top 5 accuracies as tuples perhaps?) ie, at least some points need to be in the transitional region. Appreciate the observations on how to set $\lambda$, and $\eta_{\tau_l}$. [post discussion edit]After discussion, I lowered my  score to  marginally above acceptance threshold :  the theory section of the paper looks very interesting to me  I find it hard to see clearly from the experimental section the extent to which the method beats existing methods  I feel that the experiments could be made more rigorous to clearly show the benefit compared to other techniques  concretely, I feel taht the tables could be structured in such a way that one can glance at each single table, and see clearly in what way LTP is better than the baselines. in the text it says that Renda et al needs more training     why not put the amount of training required as an additional column in the table? The text mentions training epochs are fewer for LTP, but the table doesn t show this benefit (there is no column with number of training epochs)  table 3: this table is a little apples and oranges I feel. Preference to state at equation 11 that $\lambda$ is a hyper parameter, and $\eta_{\tau_l}$ is the learning rate for the threshold of layer l. Hmmm, does this mean that each layer has its own learning rate to tune for its threshold? Concretely, for the results tables:  table 2: LTP gives worse accuracy than Renda, and worse compression. In addition, I feel it is important to include the accuracy in the table. It normally means  is distributed as , but that meaning doesn t seem to make sense here? Currently, the LTP pruning is on a worse  parent  model, and performs worse than the other baselines in terms of accuracy. Table 4.If torchvision gives worse results than caffenet, then why not use caffenet instead, or re implement the caffenet version of alexnet in torch?
Reject. rating score: 4. rating score: 4. rating score: 6. rating score: 7. <BRK>This paper proposes a new transfer learning approach to leverage the previously learned knowledge stored in a pre trained teacher policy to facilitate the learning in a new task. The proposed method combines an established technique termed kickstarted training with a simple experience filtering method. The experience filtering simply performs thresholding to filter out transitions generated by teacher policy which have reward below some threshold, where the reward for thresholding is taken from the target task. I lean slightly towards rejection because the novelty of the method is somewhat too limited and the empirical evaluation does not give much new insights to the problem of policy transfer. The only difference between the proposed method and kickstarted training is that the method uses more data, which are the transitions generated by the teacher policy and could receive high reward at the student task domain. The authors only use an alternative weight on the control reward to create a distinct task. But essentially these can hardly be considered as alternative tasks for policy transfer scenario. I wonder how the weights for policy distillation loss are specified for the baseline method kickstarting.<BRK>The proposed method relies on kickstarting, thus using policy distillation as an auxiliary loss for transferring from a source task to a target task, as a starting point. In addition, the authors add  instance transfer , i.e., selecting some prioritized data from the source task to be used to train the target task. The combination of these two features produces fairly strong performance on various transfer experiments in four simulation environments. The motivation is clear and related approaches are described. The instance transfer method, which is rather heuristic, requires tuning a threshold which may be a limitation. The improvements are minor for all the experiments when compared with the baseline or kickstarting. Pros  Clearly described method  Well designed experimentsCons  Lack of novelty beyond the combination of 2 known methods  Results don t show that the method is substantially better than kickstarting alone<BRK>This paper suggests an approach that builds on two approaches: kickstarting and instance transfer. Kickstarting can be viewed as distillation of a policy with a dynamically tuned coefficient that control how much to weigh the distillation loss against the actor critic loss. Applying this as is to tasks with very different reward functions would lead to a degradation in final performance on the new task. The second approach considered here is instance transfer. * One suggestion if for greater discussion into why this works. * The paper mentions automatic identification of task similarity as future work   do the authors have ideas on how best to do this? I think the results are promising although given the size of the error bars on some of the plots it s not clear it is completely worth the effort to use this method in all cases.<BRK>Overall I vote to accept the paper for now, but my final decision will depend on the authors  clarifications to my questions below. Strengths:   The algorithm is a simple combination of ideas each of which seems important according to the results. As I describe below though, more can be done in terms of experimental rigor and clarification is needed for some of the presented results. This indicates it does not have to do with the transfer of the teacher policy where ostensibly the drop corresponds to critic training. I think it is important to have a good explanation for this. Figure 2b) indicates that using the top 20% metric for advantages works as well (if not better) than the one used in REPAINT. For instance, it would be interesting to see how the proposed method compares to say DAGGER. Apart from these, there are technical details which could help improve the work that did not affect my decision but list below:  The main text mentions that the clipped loss L_clip is used even for instance transfer in Algorithm 1.
Reject. rating score: 3. rating score: 4. rating score: 4. rating score: 6. <BRK>The paper investigates the ease of using an algorithm by doing a fair hyperparameter search for different algorithms. Deep Reinforcement Learning that Matters. As an additional point, it is useful that you ve identified MAPPO as a good multi agent algorithm. This is particularly problematic for a paper where the main focus is on experiments to provide insight. Further, it would be good to clearly state in the paper that the results now include reward normalization. This also makes the algorithms different from their original intended use. 2.There are several inconsistencies or questionable choices in the paper: a.<BRK>## SummaryThe paper provides a useful benchmark (a suite of cooperative multi agent RL tasks) and a nice comparison of common as well as uncommon but successful algorithms on these tasks. ## Quality & ClarityThe paper is presented according to a high standard of quality. They document and explain their methods clearly, including hyperparameters. ## SignificanceFrom the conclusion:> “Given access to equally sized compute budgets for hyperparameter optimization, MAPPO is the most consistently successful algorithm across the studied environments, performing well at SMAC, as well as any algorithm in the MPEs and the best in hide and seek and Hanabi small. However, the work provides little insight into why that is the case. The next lines of the conclusion confirm that:> “Of course, these results are likely dependent on the choice of hyperparameters and architectural choices. Thus it may be the case that the initial point around which we perform our grid search for MA DDPG/TD3/SAC does not contain any good solutions and that performance of a set of hyperparameters on the MPEs is not predictive of performance in other domains.<BRK>The author s writing and organization are very good, so that I can clearly understand the content of the paper, and also has certain highlights, but I think it has not reached the ICLR criteria. Unfortunately, I found that the author simply I enumerate the performance differences of different algorithms in different environments, and there is also a lack of analysis of what methods are suitable for what tasks. I suggest that the author conduct further analysis and experiments, and also pay attention to the advantages and disadvantages of different algorithms. How to compare different algorithms fairly is of great concern, but the author s content is still relatively weak, and the author is recommended to improve this part of the discussion.<BRK>Also, this paper found that under constrained hyperparameter search budgets, the multi agent PPO algorithm has more consistent performance over the other algorithms across different tested multi agent environments. The code base is open sourced for public use, which benefits the MARL community. Researchers in MARL often find it difficult to find a useful benchmark for multiagent learning algorithms and multiagent environments. Thus this paper makes a good contribution to the community, but, on the other hand, since this paper is not really intended to present any major technical contributions, that could be considered a weak point for an ICLR submission. As the first work that attempts to fill this gap, this paper presents a comprehensive implementation of popular MARL algorithms tested on a representative list of MA environments, which is the major reason for acceptance suggestion. But there are some limitations of this paper. For example, there is little discussion about the results and the authors did not attempt to do more technical investigation for understanding the their findings about the algorithms. In particular, it would be helpful if  the authors have any insight on why the MAPPO works more consistently than the other algorithms.
Reject. rating score: 4. rating score: 4. rating score: 4. rating score: 4. <BRK>The paper is not difficult to follow. It would make the paper more convincing if the authors can report quantitative results on more datasets. The authors claim that the proposed method decouples reconstruction and disentanglement. Therefore, it is not clear whether the proposed method outperforms the baselines in terms of reconstruction. More experimental results might be necessary. Compared to the existing ControlVAE, the contributions of this paper looks incremental.<BRK>#### Summary The paper proposes DynamicVAE, a variant of ControlVAE that is claimed to decouple the optimization reconstruction error and disentanglement, hence mitigating the inherent trade off between the two. They empirically show that their proposed method can in fact decouple the two optimization objectives. My main concern with the paper is that it lacks extensive empirical evaluation. 3.What values of beta and gamma did you try for the baseline methods? #### Comments:  It is known that the pixel wise reconstruction score does not give a clear picture of the sample quality [3]. My current rating is primarily based on the fact that the paper requires a lot more evaluation with competitive baselines on more relevant datasets.<BRK>It can be viewed as a refinement of the ControlVAE approach of Shao et al 2020, varying only in the specifics of the strategy used. Though the approach is sensible and empirical results of the work are reasonable, I believe the contribution of the work is too small and too specific for publication at ICLR. *Strengths*  The paper is reasonably well written and easy to follow. The general approach is sensible and the control theory approaches used to control beta are well principled. The experimental results are reasonable, albeit not particularly impressive.<BRK>The authors do not put their work into the context of the closely related information bottleneck principle  2. 3.ExperimentsFig.2 a) depicts the reconstruction error of the proposed method and different baselines for up to 1250K iterations. Are the reported error values standard deviation? This needs to be explained in the caption of the table. 3. c) How does the proposed PI controller based approach compare to a simpler approach of beta VAE with a fixed beta schedule: After a pre defined number of iterations beta goes down to a lower value. 3. d) Related to 3 c): Is the schedule for the beta parameter that is generated by the PI controller significantly different for different datasets? Conclusion: Dynamically controlling the beta parameter is an interesting idea but this paper has several shortcomings and is not ready for publication.
Reject. rating score: 4. rating score: 5. rating score: 6. rating score: 6. <BRK>The graph sparcification approach is based on resistance. I also appreciate the experiments as the authors tend to demonstrate their method in a comprehensive way. However, I think that this work is only with limited novelty given some previous works and therefore cannot give a recommendation. However, there two techniques seem to be contradictory in some sense: Obviously, the graph sparsification method in this work is independent of the GAT building block. One concern arises during the discussion, where the sparsification technique here only works for GAT instead of GCN or other GNN smoothing models, which brings me some concerns about the technique. Second, the fundamental difference between this method and GDC [1] is still not clear.<BRK>In particular. My main concern is about the novelty (a direct application of sparsification) without too much discussion about the broader sparsification family and the whether this technique is universal enough on other GNN frameworks. ## ProsThe paper tackles the problem of fast GAT training/inference, which is an important problem when training large scale networks. The proposed method shows good empirical results with a proper theoretical guarantee. 2.Applicable to other GNN networks. 3.The comparison of FastGCN [1] w.r.t performance may not be that fair. Overall, I think the authors tried hard to prove the concept of “edge” sparsification helps for speedup of “attention” GNNs which I also think they explanations/rebuttals succeeded in doing this, though the established theory seems quite standard and is not the same as in the experiments. However, pertaining the results I still do not see a claimed comparison of GCN, “FastGAT” sparsified GCN in the revised work based on the replies to R3 (only FastGCN is reported.Note this is not a direct adaption of the authors’ method, but from the previous node sampling literature). As is claimed by the authors, performing sparsification on GCN does not provide seminal speed boost. For the current status, I would still lean towards a rejection.<BRK>The sparsification is based on the spectral properties of the input graphs, and seems to be backed by strong theorems that dictate the upper bound on the distance between the computed features and full graph features. It looks like an interesting and novel paper that could enable large scale applications of attentional GNNs. I recommend acceptance, and would invite the authors to consider the following:  The authors mention that GraphSAGE is not amenable to attentional models, but in principle, isn t GraphSAGE style update (node batching + subsampling neighbours with replacement) what GaANs already usefully applied to Reddit? After discussing with other reviewers:  I agree that the sparsification method proposed here is also in principle applicable to GCN like models;  The authors should have provided results for "FastGAT" style sparsification on GCN, rather than countering the reviewers using passages like "Hence, it is mainly the question of necessity rather than applicability which guided our choice of studying the GAT model in depth." In light of these discussions, I am decreasing my score to a weak accept, and I hope the authors will take this advice for the next iteration of their work (which otherwise, in my opinion, deserves being published in a strong venue).<BRK>Section 4   The novelty of the theoretical results is perhaps somewhat oversold, as they follow rather immediately from the definition of spectral sparsification. Overall: The approach generally makes sense and the experiments show benefit, so pending some clarifications about the algorithm requested above, I think the paper can be accepted. I can see why the practical algorithm would work even if it cannot be explained formally via spectral sparsification, and including the slow unimplemented algorithm just for the sake of its analysis feels a bit forced. What does it add to our understanding, and was it worth making the paper that much more confusing? Nevertheless, in the end the authors were straightforward about all this in the discussion. As I said originally, I like the overall approach, so as long as the clarifications about the gap between the analysis and the implementation are included, and pending other reviewers  concerns about novelty and experimental validation (I am less versed in the empirical literature on GNNs so prefer to defer to them on those points), I think the paper could still be accepted.
Accept (Poster). rating score: 8. rating score: 7. rating score: 7. rating score: 6. rating score: 4. <BRK>Summary:This work presents a goal conditioned RL, which estimates probability density using a classifier. + The paper is well written and clear. Weaknesses:  Although the reported evaluation results are competitive to baselines, it would have been even stronger if the performance is substantially improved. Do you have any insight on how better results can be achieved?<BRK>This paper explores learning a classifier to predict if a given state/observation will be reached in the future from the current state and action pair. Using this classifier, the paper is able to create a probability density function that could be conditioned on reaching a goal state. I have some concerns which are listed below. The probability of reaching a future state from the current state + action is very low and the experiments show only training on a human demonstrated dataset.<BRK>This paper studies a problem of predicting future state distribution in an MDP. Apparently at this core the idea is not novel. The connection of UOM paper is interesting but not discussed unfortunately. In the UOM paper, the authors there appear to focus on reward less MDPs, where you can generate/compute the value function given a reward function on the fly. R5 s main concerns are the clarity and the motivation of Bayessian classifier and off policy learning.<BRK>The authors propose a new algorithm, called C learning, which tackles goal conditioned reinforcement learning problems. Specifically, the algorithm converts the future density estimation problem, which goal conditioned Q learning is inherently performing, to a classification problem. The experiments showed that the modification allows a more precise density estimation than Q learning, and in turn, a good final policy. Especially, the idea is valuable in that it allows a better understanding of prior Q learning based approaches in choosing a sensitive hyperparameter. After rebuttalI ve read the authors  feedbacks and other reviewers  comments.<BRK>[summary]This paper studies to predict future state density function by using an indirectly method via classification. al.2019)) is more relevant to value based method, where we can set the reward function as the indicator function and estimate the average reward of the policy $\pi$. I think in Definition 1 the right hand side you are defining a new condition density function, not a new future state. Is that stabilized the learning process?
Reject. rating score: 5. rating score: 5. rating score: 6. rating score: 6. <BRK>The paper proposes a simple protocol to allow for both robust mean estimation and secure aggregation by sharding users, applying secure aggregation between shards and then doing robust mean estimation on the means returned by each shard. Experiments suggest the proposed method is more robust to various attacks than competitors. Any Theorems which are quoted should be fully attributed, and any which are novel should be accompanied by formal proofs. It is quite unclear what value the discussion of creating IID shards brings. Non IID data between clients is a concern in federated learning, and it can cause issues such as diverging model parameters when each client takes multiple steps of GD locally. The authors should identify what value having these IID shards brings. Additionally it is unclear that the Lindeberg CLT is required here, as the shards draw directly from the mixture distribution induced by the clients $D_i$ distributions.<BRK>##########################################################################Paper summary:The paper considers robustness to poisoning and backdoor attacks in the context of federated learning. It proposes a defence based  on splitting the clients into shards, averaging their updates via secure aggregation and then using a robust mean estimation on top to ensure robustness. The authors point out that controlling the number of shards is a way to trade off privacy vs robustness, thus potentially dealing with both malicious clients and an honest, but curious server. ##########################################################################Pros:  The paper studies an important problem. In particular, all attacks apart from the backdoor one are tailored against some of the baselines. Overall, the paper is well written and it tries to justify the proposed method both theoretically and empirically. The analysis is Section 4.1 and 4.2 assumes that the data of the good clients is i.i.d.. I was also unable to find proofs in the supplementary material. Theorem 3 states that for small enough number of Byzantine workers, a dimension independent error can be obtained.<BRK>**Paper summary**The paper claims to be the first paper that simultaneously handles Byzantine threats while ensuring privacy in a federated learning setup. **Suggestions**I think there are some papers that use the dimension independent robust mean estimation techniques for non FL learning. The algorithm first divides all the machines into shards. Within each shard there is secure aggregation. One of their main claims is that this is the first algorithm that provides dimension independent robustness guarantees against byzantine threats (I have some concerns regarding this claim). For example (Yin et al., 2019) (this is different from the one cited in your paper). **Post Author Feedback Comments**The authors have tried to address my main concern by adding an assumption on the distributions of gradients. For these reasons, I think the paper needs some more justification for its theoretical claims. This will make comparisons between related works easier. Thus, the distribution of the means of the shards is not identical and hence not iid.<BRK>Summary: The authors consider federated learning setting and how to defend the overall learning task against malicious clients and a semi honest centralized server. Though there are known ways to prevent attacks, they suffer from a large error in the estimator and also do not preserve privacy of updates since the server sees them in the clear in order to adjust for error. Overall it is a very nicely written and presented paper. Also I was not clear if one needs to make assumptions on the knowledge of the proportion of malicious clients in order to carry out the algorithm (Alg 2). Would that be known or there would be a known upper bound? Please state if there is an assumption on non collusion between the server and the clients.
Reject. rating score: 4. rating score: 4. rating score: 4. rating score: 4. <BRK>It would be better for the authors to provide such experiments to further validate the claims. Therefore, I am leaning towards rejection. **Post Rebuttal**After reading the rebuttal and the other reviewers  comments, my concerns persist:  The technical contribution is limited. The authors are encouraged to include ImageNet results as well as transfer learning evaluation.<BRK> **Update after rebuttal**I want to thank the authors for a long and highly detailed rebuttal. They clarified a lot of my questions and hopefully in the process they were able to improve the paper. I will therefore retain my rejection rating. The module p`erforms an affine or homography transformation $\phi$ on the input $x_1$.<BRK>I am missing some experiments motivating the choice of architecture. The performance improvement shown does not seem enough to merit publication, given the benchmark used. I think the combination of these two factors (the numerical improvement and the benchmarks used) make the paper weak. Have the authors considered using using a concatenation of the two vectors x1, x1  instead of its difference to predict the homography?<BRK>In experiments on CIFAR10, CIFAR100 and SVHN, improvement of a up to a few % points is observed. Parts of the paper state well known facts (description of H and A, there properties), some are irrelevant   for instance par 2. in the intro about transfer learning. The technique is close to augmentation and experimenting with A and H is not novel. I see very limited benefit to the reader.
Reject. rating score: 3. rating score: 4. rating score: 4. rating score: 4. <BRK>This paper studies the data leakage issue in the federated learning. In vertical federated learning, the gradients of part 1 of the network does not need to be exchanged with the server, as there is no average  operation needed, even the parameter itself does not need to be transferred to the server for the same reason, will your method work under this setting? It can recover the input data via gradient matching, and the authors claim that their method performs well even with large training batch sizes, e.g.over 40.Finally, the author also studies the possibility of attacking during learning, where they suggest that multiple updates of fake data helps. The following are some questions:What does index alignment mean?<BRK>This work introduces CAFE, a novel training algorithm to leak training data in a federated learning setup. However, DLG does not work when the mini batch size increases due to a messy gradient representation. In this work, the authors propose to keep track of the batch index. It is clear from the obtained results that this method works, and that images are recovered. Pros: + In the given conditions, CAFE clearly outperforms the other approach to leak training data from gradients during FL. Cons:  The conditions necessary to the success of the proposed methods seem to be quite strong and not really connected to a realistic FL framework. Small ideas can lead to drastic changes in the field, but the core idea of the paper is to solely store batch indices. "Aggreaged", "upload fake gradient" only once .<BRK>The description of the attack setting and the attack algorithm is provided at a high level and detailed description is missing, making it hard to understand the novelty of the contribution. However, the overall presentation of the paper could be improved to be ready for a publication. The authors should make this distinction clear if it is indeed the case. Federated learning usually has many more participants. Are there any assumptions on same data being used in each iteration. [Presentation]“As a motivating example, Figure 1 compares”: it would be best to motivate the algorithm key insight and not its improved performance over previous work that was already mentioned.<BRK>Two types of federated learning systems are considered. This method suffers difficulty when the number of samples in one round is large. The authors conduct experiments to show that the proposed algorithm outperforms previous works. However, I have the following concerns about the paper. However, this is no well justified for the following reasons:(1) The assumption that the server knows the indices of the samples that are selected in each round is not valid in general for the HFL setting since each agent can sample a batch locally. (2) In HFL settings, it is generally assumed that the number of agents is large and each agent only participates in a few rounds, which is not considered in the experiments in the submission. It would be better if the authors can also include the training error on each epoch in the same plot.
Accept (Poster). rating score: 7. rating score: 6. rating score: 5. rating score: 4. <BRK>This paper proposes some techniques to improve the accuracy of binary networks without adding much computational overhead. To improve model capacity, the author proposes mixture of experts convolution with a winner takes all gating mechanisms. To deal with the limited representation power of binary activations, the paper proposes utilizing group convolutions. The performance is further improved by careful selection of hyperparameters and improved training techniques. Clarity: This paper is pretty clear. The methodology is well motivated and the algorithms / experiments are described clearly. Originality: To the best of my knowledge, the utilization of mixture of experts and group convolutions for binary neural networks is novel. Significance: The propose techniques, despite being simple, achieves good performance. They can be a new baseline for future research. Overall I think this is a good paper and should be accepted.<BRK>The authors improved the performance of BNN by adopting group convolution and data driven expert binary networks which choose a weight group that takes part in inference. In addition, by searching for the network architecture under the condition of the same number of operations, they achieved SOTA accuracy on ImageNet dataset. The results in the paper are strong. The adopted data driven expert network which increases the number of parameters, but maintains that of parameters participating in inference is also interesting. Below is my remaining concerns and questions about the experiments. 1.Let us assume the condition that the proposed BNN which has four experts processes many images (i.e.multiple batches). In worst case, expert weights which a few images (extremely four) select by gating function can be all different, which gives rise for processors to have to fetch 4 times more parameters (i.e.all trained parameters) compared to single image inference. So, I concern if under the condition of using multiple batches there is the performance degradation in terms of inference speed or energy in hardware compared to other BNN models. 2.Unlikely other previous works, mixup is used even on ImageNet. According to [1], it is stated that using mixup on ImageNet slightly degrades the accuracy. If an accuracy of your model trained without mixup (and if possible, accuracy of other models like Bi Real Net or Real to Bin trained with mixup) is provided, the proposed model performance will be more clearly shown under the same training condition. I could not clearly understand the message or meaning of Fig.1b.Detailed explanation of this figure seems to be needed. 2.I think it will be more helpful for readers to understand Fig.2 if the authors provide clearer information about it: the detailed network information for each constellation in Fig.2a and for type of lines in Fig.2b.3.I have a question if the searched network architecture is just optimized to BNN or not. 4.As mentioned earlier, the experimental result is impressive, so it seems that other researchers or related people might want to use your model, but the stated training process seems to be little bit complicated. Are you going to make your code public? Reference[1] Brais Martinez, et al.Training binary neural networks with real to binary convolutions. ICLR, 2020.<BRK>Summary:The paper addresses the problem of filling the gap between the performance of binary and real valued networks. The authors propose a series of procedures to improve the model and representation capacity of binary neural networks. Different binary network architectures are obtained through a new network growing approach and compared. It is a good idea to start translating some of the main tools from the standard NN literature to the binary setup. Weaknesses:The majority of the tools proposed for boosting the performance of binary networks are not new and have been already used in standard NNs. The inclusion of real valued experts seems to make the final network not completely binary and it is not clear whether the advantages of BNN (e.g.the gain on computational costs) are preserved. It is not well explained how the architecture space is searched and how to interpret the results in Figure 2. Questions:  is the cost of Conditional computing included in the total cost when the main results are claimed (e.g.in "Without increasing the computational budget of previous works, our method improves upon the state of the art by 6%")? More generally, when does the fixed number of BOPs include any training step? does the expert selection of the proposed method work better in the binary case than in real valued networks? Is it fair to compare the obtained hybrid model with real to bin? has Grouped Convolution with a similar scaling factor been already used somewhere? The authors  reply mainly answers my questions, especially regarding the difference between applying the proposed techniques to the real and binary setups. I agree with all authors comments but would tend to confirm my overall score for two reasons:the architecture search method is not simply a block rearranging but looks more like a heuristic approach than a clear methodological contributionthe proposed mixing of real and binary weights may preserve the advantages of fully binary networks but, again, makes less clear the net contribution of the paper from a more theoretical perspectiveHowever, as I recognize that the paper contains significant experimental results, I would be happy to support acceptance if all other reviewers agree on that.<BRK>1) Training a super network (ensemble of BNNs) and dynamically selecting one BNNs to execute conditioned on input. 3)  Designing the architecture using EfficientNet considering the width, depth, groups and layer arrangement configurations simultaneously. The effectiveness of the paper has been justified on ImageNet classification but can be further strengthened. + Using conditional dynamic routing to improve the BNNs capacity is interesting. Specifically, the paper proposes to learn a supernetwork (i.e., ensemble of several experts) during training and dynamically select the path during testing, which enhances the capacity while preserving the inference efficiency to some extent. Weaknesses : 1: This paper ensembles some existing compression/NAS approaches to improve the performance of BNNs, which is not significant enough. For example, the proposed dynamic formulation in this paper has been used in several studies [2, 3]. + Varying width and depth has been extensively explored in the quantization literature, especially in AutoML based approaches [Shen et al.2019, Bulat et al.2020], to design high capacity quantized networks. + The effectiveness of the group convolution in BNNs was initially studied in [1]. Later works also incorporate the group convolution into the search space in NAS+BNNs methods [e.g., Bulat et al.2020a] to reduce the complexity. However, for deeper networks, such as ResNet 101, it will include ~100 full precision layers, which can be very expensive especially in BNNs. As a result, it deteriorates the benefits and practicability of the dynamic routing mechanism. Even though the full precision operations only account for a small amount of computations in statistics, it can have a big influence on the efficiency on platforms like FPGA. Then the problem comes. This paper can formulate the <width, depth, groups and layer arrangement> as configuration vectors and optimize them using policy gradients and so on, with the binary gates learning unified in a gradient based framework. So what is the advantage of the "semi automated" method of EfficientNet over the gradient based optimization? In addition, how about learning a policy agent via RL to predict the gates? I encourage the authors can add comparsions and discussions with these alternatives. 5: More experiments on deeper networks (e.g., ResNet 50) and other network structures (e.g., MobileNet) are needed to further strengthen the paper.
Reject. rating score: 4. rating score: 5. rating score: 5. rating score: 6. <BRK>It proposes two metrics: robustness score and overlapping score, and uses them to specify desirable characteristics of a robustness benchmark: coverage and balance. Those reasons (laid out in the "Concerns" above) are why I don t think this paper is worthy of acceptance. Positives:* I think the authors are correct that some corruptions in ImageNet C are correlated and therefore, using a simple mean over corruptions is going to bias towards models that improve on the correlated corruptions. Maybe it s ok for this particular case of analyzing the benchmark itself, but that should be called out very clearly. * There is an assumption made throughout the paper that using a corruption as data augmentation during training imparts robustness to that corruption on the trained model. I have seen empirically that it isn t always the case. * I worry that Algorithm 1 is itself not robust to changes in architecture or training methodology. The results you show are purely a result of the initial set of candidate corruptions you chose, and you provided no methodology for how to perform that step. Similarly, I could introduce a new corruption that isn t in your candidate set and suddenly ImageNet NOC would look like it has low coverage. * On the whole, I think this paper would be a lot stronger if the proposal was just a set of weights for each ImageNet C corruption, such that the top line robustness metric used would be a weighted average that is less biased towards correlated corruptions. It would make more sense at the beginning of section 4. A good solution needs to be totally agnostic to architecture and training algorithm, which this method is not.<BRK>##################################################################Summary:This paper proposes an algorithm to build a benchmark of Non Overlapping Corruptions. The proposed ImageNet NOC dataset is balanced and also covers a wider range of corruptions. ##################################################################Pros:(1) This paper first provides detailed analysis of dataset coverage and balance. If these two results are similar, then we can conclude that ImageNet NOC covers ImageNet C. (2) The experiments in Sec 5.4 show that SIN+IN is more robust than ANT on ImageNet NOC. ##################################################################[Edit]After reading the authors  response and other reviewers  comments, I have lowered the scored to 5. I agree with Reviewer#1 that the comparisons between ImageNet C and ImageNet NOC are not solid. And that "the paper should have done an equivalent evaluation, comparing ImageNet C as a whole vs. ImageNet NOC as a whole." Since the core idea of this paper is to propose a better robustness benchmark, solid comparisons between ImageNet C and ImageNet NOC are critical. I also agree with Reviewer #3 and Reviewer #4 that, the improvements provided by ImageNet NOC does not make a significant difference. The reviewer appreciates the analysis of the coverage and balance of the robustness benchmarks, which in my opinion, is valuable.<BRK>This paper proposes a new dataset for estimating robustness to distribution shift, in particular corruption robustness. They accomplish this by proposing an alternative to ImageNet C, ImageNet NOC, which uses different corruptions. They consider corruptions not in ImageNet C, and they argue that their dataset is superior because they have more "balance and coverage." The paper is missing key qualifiers. "Then, the higher (1) is, the more the robustnesses to c1 and c2 are correlated, i.e.the more c1 and c2 overlap." ... provided the models are robustified through exactly training on c1 or c2. If these papers have very different ratings, then there s a problem with this review process. For example, we see "(Evgenia Rusak, 2020)" instead of "(Rusak et al., 2020)". "weight decay set to 10e 4."<BRK>## Paper summaryThe paper considers the problem of measuring the robustness of image classification models to common image perturbations. The present paper proposes a systematic approach to select types of perturbations in a way that spans a large variety of perturbations and assigns similar importance to each perturbation. The present paper takes an important step in the direction of making evaluation of robustness to non adversarial corruptions more principled. 3.The overlapping score is based on a practically relevant quantity, namely the performance of a network on the corrupted data. 5.A new alternative to ImageNet C is created with the proposed algorithm. The new dataset has improved coverage and balance properties. The computational cost of the method is high, and not stated or discussed. As far as I can tell, at least one neural network needs to be trained for each candidate dataset. This should be addressed further. Table 3 starts to address this question, but it would be useful to compare ImageNet C and ImageNet NOC mCE across a wider range of models (e.g.pretrained models available online). What is the rank correlation between ImageNet C and ImageNet NOC mCE? ## Conclusion and suggestionsThis is a borderline submission. 11.Discuss the optimality of Algorithm 1.
Reject. rating score: 2. rating score: 4. rating score: 5. rating score: 6. <BRK>1.The main problem I have with this paper is that this paper idolizes the paper [distributed EF SGD by Zheng et al.NeuRIPS 2019]. as one of the instances. To do proper experiments by using compression techniques, the authors can check a very elaborative work and codebase by [Hang Xu et. In that case, I would encourage the authors to plot relative data volume vs. test accuracy similar to Figures 6 and 7 therein. 12.Why did not you compare with sign sgd algorithm? You may did it in the Appendix and I did not check the Appendix. The authors may argue that "The classical theoretical analysis of SGD assumes that the stochastic gradients are uniformly bounded".<BRK>This paper proposes a gradient compression approach to remove the communication bottleneck in distributed stochastic gradient descent. + Specifically, they send and encode the difference between the current sign vector and the previously transmitted one. This reduces the entropy of the  difference vector  so it can be compressed more. I believe the main contributions of the paper are:  The introduction and evaluation of lossless compression on top of sign based gradient compression  A theoretical improvement of the constants in the rates from (Zheng et al.2019)  A proof that SignSGD with delta coding and a bias towards changing signs can still be a $\delta$ compressor, as long as the bias is extremely small (not covering the experiments presented in the paper)I find the ideas presented in this paper interesting and novel and the experiments well executed. I do, however, have two concerns:  The method is said to exploit temporal correlation in the gradients by using delta coding. Given that p < 0.5, this is actually anti correlation rather than correlation. This lack of simplicity could be compensated by convincing experimental results, but it seems that many gradient compression schemes achieve similar results to the proposed scheme at similar compression rates (see Xu et al.https://repository.kaust.edu.sa/bitstream/handle/10754/662495/gradient compression survey.pdf) for an overview).<BRK>The authors present a new scheme for compressing gradients for use in distributed training. The idea is an interesting (even if a fairly simple one) and leads to a greater than 50% savings. This leads me to believe that the compression benefit they are seeing comes from higher values of $\alpha$, i.e., by throwing away information. That would appear to be an interesting baseline to see how much benefit comes from the temporal aspect v/s the lossiness induced by $\alpha$. These two appear to be currently confounded. ### After RebuttalIncreasing rating based on the authors  clarifications on the source of the gains. Open to further changes based on further review and discussions with other reviewers<BRK>This paper proposed an extension of blockwise scaled sign compressor in Zheng et al.(2019).The proposed method exploits the temporal correlation between two consecutive gradients. The authors show that one can have a higher compression rate by inserting distortion to the compressed gradient. The experiments show that the proposed compressor can achieve additional 40% 50% reduction on communication compared to the scaled sign. Overall, the reviewer thinks the idea is interesting. In this way, the sign is always correct for the elements that have opposite direction from the last gradient. I wonder will the results change if we consider flipping the sign of the elements that have opposite direction? 2.Since alpha has a very small upper bound, it is hard to see any theoretical improvement over scaled sign. 4.For the distributed training with high speed network, the extra overhead incurred by compression is not trivial and cannot be overlooked. As there is no results against CPU wall clock time, it is not clear if the proposed method is really faster than the scaled sign in terms of elapsed time. 5.Can you show the final test accuracies on ImageNet achieved by each algorithm?
Accept (Poster). rating score: 7. rating score: 6. rating score: 6. <BRK>This paper builds on the work of Xu and colleagues (2020) on auto encoders (AE) with relational regularization. In order to do so, a quadratic optimal transport problem is used (the fused Gromov Wasserstein  FGW ), which has a super cubical complexity. This work proposes to replace this uniform distribution over the sphere by a von Mises Fisher distribution on the sphere, that is alike a ‘Gaussian distribution’ on the sphere, and also a mixture of those distributions. The paper is clearly written and interesting. The experimental results are very good and clearly show the benefits of the method. in general, directions are drawn randomly for every batches of samples. it seems that the sliced FGW is computed on mini batches of samples. In the end, I think that using the von Mises Fisher distribution is an interesting and original idea, which might have a broader impact than the sliced FGW. I do not agree with the complexity of FGW being solved in $n^4$ which should be more related to $n^3$ for the type of distance considered in the paper (see analysis in [1]), but yet the point is still sensible for considering the minibatch version.<BRK>The paper proposes a new pseudo distance called the spherical slices fused Gromov Wasserstein distance (SSFGW). This is a solid paper. The paper tackles the problem of solving for the best sampling directions of slicing. Some comments and questions:  Where does von Mises Fisher distribution come from? Swap SFG and DRAE in the title of 2.1 to consist with the content of 2.1. The paper does not mention an mps DRAE. Please explain that. A Wasserstein paper or a generative modeling paper? The main contribution of the paper, which tackles the problem of generalizing sliced Wasserstein distance and max sliced Wasserstein distance, is on the new pseudo metric but the paper only argues the contribution from the perspective of its power in generative modeling.<BRK>The proposed method is based on the new relational discrepancy which is called the spherical sliced fused Gromov Wasserstein (SSFG). ##########################################################################Reasons for score: Overall, I have a positive impression about the paper. I think that the proposed relational discrepancy using the von Mises Fisher distribution is a reasonable extension of the SFG using the uniform distribution. Hopefully the authors can address my concern in the rebuttal period. ##########################################################################Pros: (1) Applying the fact the von Mises Fisher distribution is an extension of the uniform distribution and the Dirac distribution, the authors successfully presented an extended relational discrepancy of the SFG and its max version. I reckon that the MSSFG, which adopts a mixture of the von Mises Fisher distributions, is a reasonable extension of the SSFG. If not, I wonder whether the power SSFG and power spherical DRAE have sufficient flexibility compared with the SSFG and spherical DRAE. It is good to find that the Power Spherical distribution also includes the Dirac distribution as a limiting case.
Reject. rating score: 4. rating score: 5. rating score: 6. <BRK>The use of $\ell_1$ norm constraint is analogous to the use of LASSO in sparse linear regression. They emphasize that they study the case where the returned network is regularized using an $\ell_1$ penalty (as in eqn (3)) instead of the case where the minimization is explicitly constrained to an $\ell_1$ ball. As the authors say, regularization is more convenient to implement than constrained optimization in practice. They claim that this makes the problem they analyze quite different from analyzing the explicitly constrained version, because the minimization is technically over an unbounded class of functions. This paper makes some other overstated claims about the novelty of their result and analysis. The authors claim that their analysis is based off of some new techniques from high dimensional statistics which have not appeared in the neural net generalization literature. For example, the "generalized noise" (8) which they are concerned with is basically just the empirical Rademacher complexity of a single neuron of the last hidden layer (consider the case where the noise u is Rademacher). Overall, I think this work does not provide much fresh insight into generalization bounds for neural networks, and I would tend towards rejection. The idea of analyzing generalization error based on $\ell_1$ norm (which associates with sparsity) can be traced back at least to the paper  The Sample Complexity of Pattern Classification with Neural Networks: The Size of the Weights is More Important than the Size of the Network  by Bartlett  98.<BRK>The paper is well written and organized. # WeaknessesHowever, my main concerns with this paper are as follows:    The study of the in sample prediction error and not the generalization error. A crucial problem in deep learning is to discover why neural networks that are over parametrized have low generalization error. The related work, specifically Neyshabur2015, study the generalization error or out of sample error so the comparison with their bound does not seem to be direct. In general, it is not clarified how this work supersedes previous work. In particular while the dependence on the input vector is shown to be better, a concrete example of when such an improvement would manifest itself would be very helpful. There seems to be other work that seems relevant but of this I am not certain for eg. Also, a large amount of literature on bounds on generalization error in the case of classification i.e.Neyshabur 2017a (arXiv:1707.09564), Arora et.<BRK>The paper introduces techniques well known in high dimensional linear regression for deriving guarantees in deep learning. 2.The paper introduces methods that are more elegant and simpler than previous methods. 4.The comparisons with related work are well addressed. While the paper provides interesting theoretical results, it  would be interesting if the authors can show empirical consistency for some of their claims, for example, their conclusion that connection sparsity is suitable to handle wide networks, but nodesparsity is suitable only when complemented by connection sparsity or other strategies. Similarly, while the paper cannot provide ways to select regularization parameters, the dependence on m,n,p should be compared empirically. The authors should also clarify the conditions under which their guarantees do not hold.
Accept (Poster). rating score: 7. rating score: 7. rating score: 6. rating score: 10. <BRK>The paper addressed the deterministic inference issue and enabled the conditional generation in multimodal spaces. This paper is very dense. Ablation studies are also performed to show ``` momentum  as a supplementary aid” is helpful. At this moment, I don’t have major concerns regarding this submission. I incline to accept this paper, and am willing to further change my rating. Below are some minor comments:1. Based on the description in section 2 to 4, and also the strong experimental results, I am convinced that the proposed approach converges faster than cGAN, and can learn diverse representations and enables multimodal generation. However, it is a bit surprising/interesting that proposed method worked consistently better than other prior methods in almost all the downstream tasks explored. Can you elaborate more on why learned representations worked consistently well in almost all downstream tasks that have tried in the paper. It would be good to have a short section clearly compare the model size/capacity of your models and the counterpart models. 3.The authors can consider comparing and contrasting with the normalizing flow related work.<BRK>After that, with an extensive set of experiments, this paper experiment on several such tasks with different datasets. Originality and Significance：Due to the ill posed nature of conditional generation in multimodal domains, this paper proposed a novel generative framework with a simple and generic architecture instead of the adversarial loss. And it may be worthwhile to be accepted if more latest experimental comparison can be shown, and still have the outperformance. They have also reformatted the paper so the paper becomes clearer for readers to read. Consider the above all, I think it’s a good paper and is worthwhile to be accepted. So I improving my rating to “7: Good paper, accept”<BRK>Summary: In this paper, the authors proposed a general purpose framework for conditional generation in multimodal space. This general method can be applied to a lot of down stream tasks, improving inference performance without the need to carefully design a network structure. Pros:1.The proposed method is very general. It is quite impressive how comprehensive the evaluation is. There are quite a few typos and presentation flaws, which makes it much harder to read. For example, the numbering of the figures is not consistent. 2.I understand that the authors want to show as many results as possible, but the presentation becomes very crowded, especially on page 8. I would recommend the authors to leave the critical experimental results here and move some to the supplementary materials. For example, what is the relationship between this model with a VAE model? The similarity is of course the continuous latent space.<BRK>This paper proposes a family of cost functions and a framework for modeling a continuous multimodal (CMM) space. The proposed model converges more stably and faster than conventional methods and shows high quality results in several tasks. It was partially possible with many GAN methods, but there is a significant improvement in diversity. Various and extensive experiments\The experimental settings and results support the effectiveness of the proposed method. The toy example in Figure 3 clearly shows that the proposed method can model CMM space properly. There are no major weaknesses in the overall content. It s good to discuss this. It would be great if there is a follow up study to see if it could be extended to generate an image from random variables, like Conditional GAN. And some naming is required to represent the method.
Accept (Oral). rating score: 8. rating score: 8. rating score: 7. <BRK>This paper proposes an iterative method that jointly estimates viewpoints, light directions, depth, and albedo from single images, by projecting intermediate renderings to the nautral image manifold. Intuitively, the method works by generating, with pre trained GANs, multiple views of the same object under different lightings, and then inferring 3D shapes from those variants. The key idea is to use pre trained 2D GANs to make such data generation photorealistic. (2) How robust is the algorithm to the shape initialization (ellipsoid shape)? If they are in R6, then results on zooming in/out should be included. Otherwise, there seems to be no point in defining them in R6.<BRK>It proposed a novel method to learn 3D shape reconstruction using pre trained 2D image generative adversarial networks. #### Comments ####Overall, this is a very interesting paper with good presentations, promising experimental results, and solid quantitative comparisons with the previous work. Reviewer would like to point out the potential weakness of the paper as follows. W1: Though impressed by the results (especially the proposed method works for horse and building), reviewer suspects the paper only works in a very simplified setting: (1) the GAN was previously trained on a large amount of 2D images of a single category with many variations in identity, viewpoint, and lighting; (2) the initialization (or step 1 in Section 3.1) step seems very critical to the overall performance; and (3) viewpoint and lightning randomization seems have to be hand tuned. Reviewer suspects the method in the current form cannot handle them well. W2: Some important experimental settings are neither presented nor clarified.<BRK>Pros:1.This is the first work that attempts to reconstruct 3D shape from 2D image in an unsupervised way using GANs. On 3D shape reconstruction, performances are reported on two datasets and demonstrated it outperforms SoTA method by a large margin. The authors claim in the introduction section that this proposed method has advantage over previous method as it doesn t assume symmetry of the instance. But in this proposed method, a symmetrical ellipsoid is used as the shape prior. Is there any experiment to explore how the shape prior affects the models  training results? But in the experiments and comparison with previous works, all data used are symmetrical: human face, animal face, cars, etc. Visualizations on buildings are shown in appendix but there is no quantitative analysis or comparing with current SoTA method. As the generator is always fixed during training, I assume the generator is using some pre trained network?
Reject. rating score: 4. rating score: 4. rating score: 4. rating score: 5. <BRK>This paper proposes STRATA, a simple adversarial attack against the code2seq model. Have the authors evaluated the transferability of the model trained with STRATA adversarial examples? I noticed that the F1 scores of models without perturbations are different for STRATA and the baseline attack. This attack decreases the F1 scores more compared to the baseline attack from prior work.<BRK>The paper is clear and well written. The proposed strategies for generating replacement tokens are trivial and easily detectable (e.g., concatenating the same token five times). The more subtle strategies are significantly less effective. As such, the paper does not seem to prove that a robust model can or has been trained using STRATA samples. The notion of adversarial examples against code in the sense used in the paper seems vague.<BRK>The paper proposed an Adversarial attack strategy for the code2seq model. The paper said a black box attack (even in the title), but they reply on white box information showing that L2 distance of high frequent tokens are more. I like the simple approach to launch the adversarial attack, which shows a promising result. Only for one model (Code2Seq) the authors have tested their scheme.<BRK>The paper proposes a gradient free method to craft adversarial examples for Code2Seq model. Overall I think authors proposed an interesting idea of an attack, however evaluation should be improved. Thus I recommend to reject paper at this point, but encourage authors to improve the paper and resubmit.
Accept (Poster). rating score: 6. rating score: 6. rating score: 6. rating score: 6. <BRK>The paper studies how the initialization and regularization of the factorized layers $W U\Pi_i M_i V^T$. It focused on the two known techniques, spectral initialization (SI) and Frobenius decay (FD) (i.e., regularize the weight $W$ rather than the factorized terms $U$, $V$, and $M_i$ s), to initialize and regularize such layers for training. 2.Figure 1 shows the bound in equation (2) is tight for the compression. Weaknesses:The paper can be seen as merely applying known methods to different areas and observing its performance.<BRK>This paper discusses about applying low rank matrix and tensor factorization of weight and applying weight decay on them. This type of low rank regularization is an important mechanism in deep learning models and already many researchers have shown interest in this topic, hence this paper would interest many researchers in the community. The experimental results are very encouraging since good improvements are shown with popular datasets. Also, the paper covers compression using ideas related to the state of the art tensor factorization methods.<BRK>This paper studies initialization and regularization in factorized neural networks (reparameterize a weight matrix by the product of several weight matrices). In the experiments in section 5 (knowledge distillation), default initialization is used instead of spectral initialization. The authors also proposed Frobenius decay that is to regularize the Frobenius norm of the product of the factorized weight matrices. I also have some questions as below:1.<BRK>This paper studies how to initialize via spectral initialization and regularize DNNs via Frobenius decay. The behaviors of these two components in the experiments are expected. That is, it replaces the weight decay $\|W\|_F^2$ in deep nets by plugging in the low rank formulation of $W$.
Reject. rating score: 3. rating score: 4. rating score: 5. rating score: 5. <BRK>The topic is not significant. Although the paper proves that the model can be optimized to obtain similar parameters as HMM, it doesn t include any comparison or experiment for the usefulness of the model.<BRK>This is important to distinguish, and I also think the title is incorrect by stating: "HMMs are RNNs." In section 3.3., N is not still not defined, which is confusing. While this application is undoubtedlyimportant, given that the premise is to show that HMRNNs can do thingsbetter than HMMs, a much better benchmark would be for a sufficientlylong time series, such as speech data. The authors would probablygain more traction considering speech data, as:1.)<BRK>### Weakness* The motivation for the connection between RNN and HMM is not very clear, and it hinders the significance of the work. ### Strength* It is an interesting new connection between RNN and HMM that was found by authors. * I found the Alzheimer s application is very inspiring.<BRK>Note that the authors argued and stated in Section 3 that HMRNN mimics the operations of the standard HMM and produces statistically similar solutions to the B W algorithm. The problem setting over Alzheimer’s disease case study is not clear. It is unclear for the reason of post processing to compare with the standard HMM.
Reject. rating score: 4. rating score: 5. rating score: 6. <BRK>Pros:1.The problem of learning a generic representation from limited training data is important. , which might not be accurate. As current vision language BERT models are trying to learn a generic representation for both image and text, where downstream tasks are ways to evaluate the representation. 3.The paper mentioned multiple times that the proposed approach is specific for medical images (Last paragraph of Related work). The author might also need to re state the novelty contribution of this paper.<BRK>In this work, the authors propose a new model, named ConVIRT to learn the medical visual representation from paired image and textual data in an unsupervised strategy. But, it might miss some baselines or other state of the art methods. The results of the proposed method should outperform this kind of baseline. I might not agree with the text transformation function used in this work as the following reasons.<BRK>However, it is an interesting idea of applying contrastive learning to medical image and text. Overall, although the paper doesn t provide insights around what the representations have learned and how they differ from representations learned/used by existing methods, they have provided substantial evidence to suggest that pretraining helps in a lot of downstream tasks. #####################   Questions   #################### I have some questions for the authors:(1) What the medical visual representations have learned? (3) The proposed ConVIRT is well motivated.
Reject. rating score: 5. rating score: 5. rating score: 6. rating score: 7. <BRK>The paper proposes combining multiple intrinsic rewards and proposes a meta gradient based method to learning the fusion of these intrinsic rewards. The idea of learning to use multiple intrinsic rewards and using different combinations of them at different times seems to be helpful in the experimentsWeaknesses/Comments/Questions1. My main concern is that the contribution of the paper might not be sufficient. 3.It would be good if the paper evaluates on some domains that are naturally sparse reward setting instead of converting a dense reward setting to a sparse reward setting by accumulating and delaying rewards. Comparison with those would be good.<BRK>In this paper, the authors present a generalization of the model prediction error based intrinsic reward method by fusing predictions from multiple models. The authors considered the sparse reward scenario in reinforcement learning. References should point to the published work rather than the arxiv entries, when the former is available (e..g. Exploration by random network distillation, ICLR’19). I would like to understand the computational costs involved in using such a fusion approach., both in terms of individual methods and alpha optimization.<BRK>In this paper, the authors explore a model based intrinsic reward generation mechanism, in environment settings where the reward assignment is sparse. Pros:(1) A clearly written paper and easy to read and understand. (2) The approach demonstrated consistent top performance in all benchmarks. Cons:(1) Although the author provides a few ablation studies. In all experiments, the authors used a fixed interval of 40, at which extrinsic rewards are computed. This will also help make the author s statements more convincing.<BRK>This paper proposes an intrinsic reward formulation to address the challenge of sparse reward in reinforcement learning. The parameter alpha can be tuned automatically throughout the training process using meta gradient methods. The method is evaluated on 6 OpenAI continuous control benchmarks (with delayed reward), and demonstrates better performance than several state of the art prior works on intrinsic reward. Overall, the paper is tackling an important challenge of reinforcement learning. The paper is also well written and the results are promising. This prior work is very relevant.
Reject. rating score: 3. rating score: 5. rating score: 5. rating score: 7. <BRK>While the idea of generalization among many policies is an interesting topic in RL, there are many theoretical and experimental issues that prevent acceptance. These works introduce value functions which generalize to many policies. Indeed, this is the case for the papers [2,3,4] cited by the authors in the comment above. I think the most interesting contribution of this paper is the proposed policy representation. Finite?Or that $\Theta$ is only a subset of the space of value functions? The authors should provide strong evidence that RPR is not enough and the policy representation is needed. How is this addressed in the experiments and how does nonstationarity affect the theoretical claims? This is not true.<BRK>Summary The paper proposes to learn a value function that takes as input both a state and a policy embedding (PeVFA), which is used to design a new version of a generalized policy iteration (GPI) algorithm, named PPO PeVFA. Considering the value of multiple policies is an interesting and neglected line of work, which could prove useful in many ways for RL, so I commend the authors for taking a step in this direction. I also particularly liked the theoretical analysis, as well as the experiments supporting the claim for local and global generalization. However, I believe there are a number of issues with the experiments and clarity of the paper, which I would like to see addressed. WeaknessesMy main concern about this paper is the significance of the results and the empirical evaluation. Have you looked into the sensitivity of PeVFA’s generalization with respect to the set (and number) of policies it is trained on?<BRK>This work proposes an interesting idea of using the policy as an input to the value function. Based on this, the key algorithmic contribution is to propose generalized policy improvement (GPI) which can guarantee policy improvements among many different policies following the generalized value functions. The authors claim to build up from PPO, but the experiments are not well justified for the generalization or transfer learning aspect? Empirically, it is not clear what the paper is trying to propose. What are the authors trying to demonstrate in this case? Would that be a doable experiment that might add value to the contributions? Overall, I think it is an interesting idea; but there are major concerns I have about the work which seem to be unaddressable? The paper discusses on the representation learning aspect of the policy. However, the claims are not made clear and difficult to understand.<BRK>The paper conditions the value function on a representation of the policy. As mentioned in the paper, conditioning the value function on the policy was considered by some people before. This paper makes it work and demonstrates clear benefits. The Appendix D.5 is missing the review of the related works. Demonstrated benefits in experiments. String "together with PeVFA end to end" should be "together with PeVFA is end to end". String "policies along the policy improvement path naturally provides" should be "policies along the policy improvement path naturally provide".
Accept (Oral). rating score: 7. rating score: 7. rating score: 7. rating score: 7. <BRK>The paper is extremely well written, it was effortless to follow the arguments, and I enjoyed reading it. The authors motivate this work as a new formulation of individual fairness that decouples accuracy and fairness with a regularization term, enabling the practitioner to tune the parameter.<BRK>Quality  This paper is largely well written with clearly stated theoretical results, and a range of experiments on three datasets. This is a significant enough contribution to the study of individual fairness, that makes it more applicable in practice.<BRK>The paper provides theoretical analysis of the proposed algorithm. They then provide an algorithm that enforces their DIF definition of individual fairness. The hyperparameter selection procedure is well documented in Appendix B.<BRK>This paper proposes a variant of individual fairness, develops an algorithm to enforce this definition, and evaluates it. The paper is well written and thorough.
Reject. rating score: 5. rating score: 5. rating score: 6. <BRK>Summary: This paper proposes a framework for generating conditional data using multivariate polynomials, which treats both discrete and continuous conditional variables in a unified way. The author compares the proposed method with multiple methods on the class conditional generation and image2image translation tasks, but none of the comparison methods are state of the art. The choice of dataset and task is not very appropriate. It is unfair to compare specific methods for inappropriate tasks.<BRK>This paper proposes a conditional generation framework (cGAN) that bridges the gap between discrete and continuous variable used in the generation. They do so by proposing a new network architecture that implements higher order multi variate polynomials (MVP). They show that MVP generalizes well to different types of conditional variables and has good expressivity even in the absence of activation functions. The method is well explained and illustrated. It is my opinion that more justification and evidence is needed on why MVP is needed under common settings.<BRK>However, I think the paper is on the borderline, and the rebuttal and revised paper could be much stronger. The proposed network, MVP, is applied to various conditional GANs, including discrete, continuous, and mixed condition scenarios. Novelty over $\Pi$ Net is not significant  $\Pi$ Net has shown that deep polynomials can be useful for an unconditional generation. While the paper claims that "unifying discrete and continuous conditions" is a key property of MVP, standard conditional GANs can also handle those cases and are already discussed in the literature. While the paper claims that "network without activation function" is one of the main contributions, it was originally investigated and heavily discussed in $\Pi$ Net.
Reject. rating score: 5. rating score: 5. rating score: 7. rating score: 7. <BRK>This paper shows that the widely used dot product self attention is not Lipschitz with respect to any p norm with $p \ge 1$, and formulates L2 self attention, which is Lipschitz. However, my main concern is the “significance” of Lipschitz property. (3)	Are there any references claiming some lower bounds of the performance in terms of the Lipschitz constant?<BRK>This paper studies the Lipschitz continuity properties of self attention. A novel L2 self attention is proposed and proven to be Lipschitz continuous. Experiments show that the upper bound of Lipschitz constant for L2 self attention is asymptotically tight. The paper is equipped with solid mathematical analysis and experimental results. under which self attention models are Lipschitz.<BRK>Summary: This paper theoretically analyzes the Lipschitz constant of the self attention module. Cons:   From the experiments, the expressivity of the proposed L2 self attention is a big concern. + Overall, the paper is clearly written and the derivations that I have checked are correct. It would be great to include some discussions (if it is correct).<BRK>The presented manuscript studies the Lipschitz constants of self attention networks. It proofs that the widely used dot product self attention is not globally Lipschitz. The authors propose a so called L2 self attention and derive its Lipschitz constant. The experimental section evaluates the proposed approach from three quite distinct directions.
Accept (Poster). rating score: 9. rating score: 8. rating score: 5. rating score: 4. <BRK>The authors introduce DreamerV2, a modification of the influential Dreamer RL agent (hereafter refered to as DreamerV1). The proposed improvements over DreamerV1 (which has seen fruitful applications in other work) might be simple, but DreamerV1 did not work well on this baseline, and this does. This is impressive work. The modification over DreamerV1 is simple (simple enough that they can describe important optimization details within the main body of the paper   great!) and the results are a convincing demonstration of its utility. The methods are detailed and well described. Further, the benchmarking discussion is very useful for the community. Edit after reading other conversations. Two points on this:(1) If I am understanding the paper and conversations, SimPLE significantly underperforms in the metric (Atari "end performance" under certain normalizations) that the authors care about (and is a fairly established metric). (2) Pixel based future predictions generally perform poorly, and this is I think fairly widely thought to be a strong contributor to the failure of model based approaches. Again, it would be nice to see that happen here (and it might be insightful to see the quality of the frame predictions) but I think there is a reasonable expectation that this would work poorly.<BRK>Pros: That being said, it is an important setup from the perspective of model based RL. What are the current limitations of DreamerV2 in terms of Atari? Contribution of the experiments:   Pros: Experiments provide valuable proof of concept, showing that model based RL can outperform top model free algorithms on Atari benchmark despite years of research and engineering effort. Novelty of the conceptual idea for the solution:   Pros: The model is elegant. After Rebuttal  After reading other reviews and the rebuttal, I stay at my current score. Given that the paper does not contain much analysis about "why", the paper is about an empirical discovery of a mode that is critical in getting good performance. I think this kind of discovery paper is also important to share with the community.<BRK>This paper proposes DreamerV2, a set of modifications to the existingmodel based RL system Dreamer, and shows via an ablation study thatthese modifications improve performance on the standard Ataribenchmark over Dreamer. It is shown that DreamerV2 performs wellcompared to model free RL methods as well, especially when using a newreporting scheme proposed by the authors that normalizes against theworld record on each game rather than the more standard performance ofa strong human player. (2) Figure 1 greatly surprises me: although it appears on the firstpage, the results do not seem commensurately compelling. For instance, are these games perhapsmore amenable to model learning for some reason? (The authors do talkabout why Video Pinball doesn t do well, but I m curious why do some of theother domains do very well?) Some other questions:1. Some clarification would be usefulsurrounding this. 2.Since you re only focusing on Atari experiments, where we actuallydo have a low dimensional ground truth model in the form of the ALERAM state, I am curious if the authors have tried ablating away theworld model learning phase of their approach, and instead just usethe ground truth ALE transition model?<BRK>The authors build on the Dreamer architecture, that is able to learn models of an environment, to build DreamerV2, which learns a model of an environment in latent space. As with previous papers on model based learning for Atari (i.e.Kaiser et. al (2019)), the goal of learning a model has been to reduce the number of environment steps. However, the authors use the same number of environment steps with the only difference being the model is trained in latent space. Training the model in latent space can speed up learning. Is this the main contribution of the paper? There is no analysis as to why using a world model for training might lead to better results than training in the real world if the same number of environment steps are used. What is the authors  perspective on this? Did DreamerV2 use more steps in the world model environment than in the real world environment? Given that the latent space is trained based on some reconstruction error (instead of only being useful to a value function as with value prediction networks) it is not immediately obvious that the latent space will be a better place to learn a policy. After rebuttal The authors have partially addressed my concerns, however, I am still not quite sure why their method would be better than SimPLE. I am assuming that the architecture and hyperparameters that the authors use are different than SimPLE.
Accept (Spotlight). rating score: 8. rating score: 7. rating score: 6. rating score: 6. <BRK># SummaryIn this paper, the authors proposed a unified seq2seq model for structured prediction tasks in NLP. Different NLP tasks, including relation classification, entity relation extraction, NER, etc. can be converted into this seq2seq problem by adding special tokens. The experiments show that the proposed model does better than the previous state of the art, albeit with the help of multi task and multi dataset learning, on some of the tasks/datasets. A unified framework that allows for multi task and multi dataset learning. Their experiments also show that their model could benefit from multi task, multi dataset learning. The experiments on few shot relation extraction show that their model could transfer knowledge from high resource tasks to low resource tasks. 2.In this paper, the DP alignment method is a post hoc method.<BRK>This paper proposed TANL, a novel approach by using generative models to solve structured prediction tasks in NLP. The author also shows that the proposed approach is data efficient and has an advantage in low resource settings. Weakness:  The experiments show that the proposed approach does not perform very well on two tasks: CoNLL 2012 for coreference resolution and MultiWOZ 2.1 for Dialog State Tracking. The key idea is that we can formulate this as a translation from natural language input to the augmented natural language with the structure of the input, and we can leverage the label semantics of the label in the augmented natural language output.<BRK>This paper follows this line of research ideas by reducing a structured prediction problem to a translation problem. The general idea is novel and very interesting. Pros:   A novel and interesting idea for formulating structured prediction tasks to translation problems. This idea is well motivated in low resource scenarios and multi task learning settings. The general framework is easy to implement (only requiring some scripts). Some words are not precise. I am willing to increase my score if some of the questions are well clarified by the authors.<BRK>This paper presents a text to text translation approach to a variety of structured prediction problems. The resulting model gives better results than existing models in the tasks of joint entity relation extraction, relation classification, and semantics role labeling. The experimental results in the multi dataset and multi task settings are also interesting (although not much analysis is given in the paper). p.5: don’t  > do not? p.5: previous state of the art  > previous state of the art?
Accept (Poster). rating score: 6. rating score: 6. rating score: 6. <BRK>SummaryThe paper presents a framework to reduce internal redundancy in the video recognition model. The authors show that the framework achieves favorable results on several benchmarks. Strengths+ The paper is well written. + Solid ablation studies and analysis. Rating  Aiming at acquiring an efficient model in a data driven manner is indeed important for video models.<BRK>This paper proposes a novel framework called $\text{VA RED}^2$ to reduce spatial and temporal features to be computed for video understanding, which can reduce FLOPs when inferencing the video but remains the performance. The authors have done extensive experiments on video action recognition tasks and spatio temporal action localization task in the area of video understanding. Results show that this framework is promising, which reduces the computation but main the performance.<BRK>Pros:1.The motivation of the submission is clear and the writing is easy to follow. The performance is promising on most video networks. 3.The efficiency loss is widely used in searching efficient image models, like in Proxylessnas. We know that Gflops is not a good indicator for speed comparison.
Accept (Poster). rating score: 8. rating score: 8. rating score: 7. rating score: 5. <BRK>Summary: This paper addresses when to use a single network model vs an ensemble of convolutional neural network models based on resource budgets. The authors challenge the notion that ensemble methods should only be used when resources are a non issue. The authors consider more than just accuracy but also at inference cost and memory usage which are important parameters for deployable code. The authors explore how performance changes across number of models in the appendix   a question I thought of while reading the paper and did not expect to get answered. Questions:The authors assumption that the number of parameters is directly proportional to the resources used seems reasonable but I did wonder if there was a citation or related work to back this point up?<BRK>Summary The paper evaluates an under explored space of neural models: ensemble of smaller networks (shallower or narrower). Extensive experiments show that when growing the total capacity (number of parameters) beyond a threshold, these ensembles get better performance than a single network, and train faster. Pros   Sound methodology to quantify the "conventional wisdom" around ensembles of networks, and explore a larger design space  Good experimental design and exploration, especially with limited resources (1 GPU year)  Really interesting main result, how ensembles typically get better at exploiting additional capacity than single networks, once the capacity is large enoughCons   Code is not released  Only small to medium datasets are used (nothing like the scale of ImageNet 21k for instance), so some of the observations may not hold for larger datasets or models (although the main conclusions are likely to)Recommendation I recommend **acceptance** of this paper, as it provides new insight on when to use ensembles of smaller models, and justifies it by exhaustive experiments. As another example, figure 4 shows that for larger models, ensembles tend to reach the accuracy of a single *comparable* model, and do so faster, but that would not mean much if the performance of that single model was bad for some reason (overfitting, for instance).<BRK>The paper provides a comprehensive evaluation of building ensemble networks compared to a single network while keeping the number of parameters equal. The experiments are conducted on four datasets. The comparison is conducted for test error, training time per epoch, time to optimization, inference time, and memory usage. However, I do understand that this is due to the limited space. The authors may try to move similar results to the appendix and bring the other important results back in the paper.<BRK>This paper establish a robust and holistic framework to compare scaling up an ensemble with scaling up a single networks, where test accuracy, number of paramaters, inference time, memory consumption and training time to converge are considered. Through extensive experiments on SVHN, CIFAR 10, CIFAR 100, and Tiny ImageNet with VGGNets, ResNets, DenseNets and WideResNets, the authors discovered an surprising and consistently emerging phenomenon named The Ensemble Switchover Threshold: When the amount of resources (measured by number of parameters, training cost) is beyond this threshold, ensembles methods provide better performance and computation trade off. This is the first paper to  conduct an extensive and robust comparison between scaling up a single model and scaling up an ensemble. Post discussion Update Thanks to the authors for addressing my concerns. However, after viewing the other responses (especially the comments from Ekaterina Lobacheva) as well as the author s explanation, I think this submission missed out some quite important references.
Accept (Poster). rating score: 7. rating score: 6. rating score: 6. rating score: 6. <BRK>The paper proposes a new Information Bottleneck objective, which compresses the latent by learning to drop features similar to DropOut. The experiments show that the works well. While the approach is limited to dropping input features, which does not make it a general IB objective, it seems to work very well in the presented RL experiments as well as show robustness that is better than DVIB’s. The RL experiments on VizDoom and DMLab are convincing as are the ones on ImageNet. The additional experiment on the Occluded CIFAR 10 dataset in the appendix is also well thought out and shows the advantage of this straightforward method over DVIB. DB cannot provide the same generality as other IB objectives: the input (latent) has to be sufficiently disentangled already as the objective itself does not encourage further disentanglement by itself. ### RebuttalI thank the authors for their reply. I m more confident this is a good paper now.<BRK>Summary:This paper proposes an information bottleneck method, Drop Bottleneck, that allows the input to be compressed by dropping each input feature with probability p_i. The model then learns the drop probability vector p   [p_1, ... , p_n], where dropping "redundant" features will reduce the "compression penalty" term I(XZ). I m not an RL expert so I won t comment on the strength of the RL results, other than that their methods were clear and they seem to have been careful and fair in choosing baselines. My biggest criticism is that the robustness experiments on ImageNet compare to VIB (Alemi et al 2017) as a baseline, but they should really compare to the more recent adversarial results on Conditional Entropy Bottleneck (another information bottleneck approach that outperforms VIB) given in Fischer and Alemi 2020 (https://arxiv.org/pdf/2002.05380.pdf).<BRK>**Summary**This paper proposes the Drop Bottleneck (DB) method that performs feature selection during the training with the mutual information. It is actually fairly close to the core idea of the feature selection because it finds the compression by dropping the original feature or not unlike the other IB compression methods. Here are a few questions to authors:* What if we just drop the feature space only using the mutual information between X and Y and drop them to achieve a similar number of features that was resulted by DB   it is basically the classic mutual information feature selection. Would that perform as good as DB? Can you make a comparison? Looking forward to seeing more discussion with the classic feature selection method and some evaluation on tasks outside of RL (if possible). I would be happy to revisit my score **Post rebuttal comment**I thank the authors for the rebuttal. Authors have addressed my concerns and clarified some of the confusing points that I had. I would like to recommend this paper to be accepted.<BRK>Summary:The paper contributes a novel method, Drop Bottleneck (DB), for discretely dropping input features that are irrelevant for predicting the target variable. Key idea is to instantiate the compression term of the information bottleneck framework with learned term that sets irrelevant feature dimensions to 0. Experiments show that DB works better than VIB in VizDoom and DMLab when a noisy TV noise is added to the input images. The method won t work if the location of the noise changes. In general, limitations of the work are not discussed. The experiments on ImageNet are more interesting. It only discusses connections to prior bottleneck methods. The paper does not perform experiments on datasets with meaningful features where a feature selection makes more sense than for specific pixels in images.
Accept (Poster). rating score: 7. rating score: 7. rating score: 7. rating score: 6. <BRK>The paper proposes a novel approach for uncertainty estimation with RNNs. Apart of these issues, the paper is relatively well written and the considered problem is important to various applications. This, in turn, can also be used to estimate the uncertainty of the model. One important detail that the paper does not discuss but would be important to understand is how S_t is trained/updated?<BRK>This work proposes a novel method to estimate uncertainties in recurrent neural networks. The proposed model explicitly computes a probability distribution over a set of discrete hidden states given the current hidden state in an RNN. Leveraging the Gumbel softmax trick, the proposed method performs MC gradient estimation. Experiments are conducted in a variety of sequential prediction problems, including a reinforcement learning task, demonstrating the effectiveness of the proposed uncertainty estimation method. Cons:While the proposed method demonstrates good performance on both modeling stochastic processes and estimating out of distribution data, it is unclear whether the method itself can separate epistemic uncertainty from aleatoric  uncertainty if both exists; meanwhile, most of the selected baseline methods focuses exclusively on estimating the epistemic uncertainty; if possible, it is desired to see a comparison of the proposed method with baseline methods that are designed to exclusively model aleatoric uncertainties for RNNs;It is mentioned that a large number of states improves performance in the experiments for predicting OOD data; a plot for the relationship between performance and the number of states used would be useful to understand how sensitive the performance is to the number of states used;If possible, the authors should also discuss the proposed work’s relationship with the sampling free method of Hwang et al.[1] and how the choice of using discrete state distribution would outperform a parametric distribution.<BRK>Summary This paper presents an approach to uncertainty modeling in recurrent neural networks through a discrete hidden state. The training of this discrete model is done using a reparameterizable approximation (in particular, using the Gumbel Softmax trick). The authors show the utility of this method on a variety of problems, including showing effective out of distribution detection and improved calibration in classification tasks. The authors must clarify the presentation of their method, and have this presentation be distinct from discussion of previous work. Overall, the experimental results seem compelling and interesting. The discussion of epistemic versus aleatoric uncertainty in the appendix is also interesting.<BRK>Summary:This paper proposes a method to quantify the uncertainty for RNN. Different from  the traditional Bayesian RNN, the proposed method is more efficient. The hyper parameter tau of the Gumbel function  is learnt from data to better capture the inherent uncertainty in the data. More importantly, is unclear why the Gumbel softmax function, even with the learnt tau parameter, can capture the data uncertainty and better theoretical justification  is needed. The authors should also compare their methods to SOTA methods for each task.
Reject. rating score: 2. rating score: 3. rating score: 5. rating score: 6. <BRK>To demonstrate overinterpretation on CIFAR 10 and ImageNet, the authors use Batched Gradient SIS to select a small subset of pixels for each image and trained CNNs on the modified images. The main reason is that this paper lacks novelty. The paper is well organized. Both the phenomenon of "overinterpretation" and the proposed solutions have been thoroughly studied in previous works. While the authors show that CNNs could rely on irrelevant features, they did not investigate the cause of their behavior. Specifically, the paper does not investigate whether such pathological behavior is caused by the properties of the datasets or it s originated from the model architecture. Predictions generated by neural networks are uncalibrated.<BRK>The main problem with the work is the discrepancy between claims and results. This is against the assumption that there generally there exist sparse features that correlate with the class label. One important modification that seems necessary is to create the training set using one architecture and then test the hypothesis using a different one. I personally cannot be sure that the observed results are not simply indicative of the correlation between class labels and the shape of the SIS masks. One simple way of answering this question is how good a model trained on the sparse features is on clean images. Note that the reverse experiment (high accuracy of models trained on clean data when predicting sparse images) is not enough as in this experiment, the model itself is used to generate the sparse subset of features (which again means that it s a model specific mask). While that might be true, this paper s observations as mentioned above, do not provide enough evidence. This is a very interesting observation. * The interpretation being 5 10% of the images is not necessarily an indicator of poor behavior.<BRK>################################################Reasons for score:The paper is overall well written and presents some interesting findings on the so called “overinterpretation” of DNNs classifiers. This experiment seems important since otherwise it is not convincing that the model only uses those remaining pixels to make the decision. I am thus not fully convinced by the statement “We show misclassifications often rely on smaller and more spurious feature subsets suggesting overinterpretation is a serious practical issue”. Another minor concern is about the usage of mean SIS size for measuring semantic meaning. It might be more interesting to also show the performance on spatially transformed images (those should be heavily influenced when only sparse original pixels are used). The statement "full images are highly out of distribution for a model trained on images with only 5% unmasked pixel subsets and hence such a model cannot properly generalize to fully unmasked images" makes sense. However, I still feel that the authors need an experiment of this flavor to support their claim of “We show misclassifications often rely on smaller and more spurious feature subsets suggesting overinterpretation is a serious practical issue” as mentioned in my initial review as well as pointed out by R1. the stated experiment trains and tests on the same model so it does not address the concern that the observed phenomenon is model dependent.<BRK>Most importantly they show that training NNs on these SIS from a previously trained network achieves similar results. Pros:1.The paper targets a very important subject. My concerns on the description of the SIS methods and results on the proposed mitigation are not addressed. I am not convinced as R1 and R4 that training on the SIS and testing on the full image is the correct way of testing if SIS is sufficient for the model s predictions. I am confident humans trained on the tiny SIS can learn to classify the examples with much greater accuracy than 20%. As pointed out by other reviewers we can see some patterns in the SIS. To me the most important is that a given model architecture can be trained on the SIS of another trained model (with different random initializations) and still be able to learn and generalize. However, the presentation of the paper should be improved, for instance there lacks explanation of SIS and Batched SIS in the main paper to help the reader follow. I am fairly confident a human could be trained on the sparse versions and classify well the test example afterwards.
Reject. rating score: 4. rating score: 6. rating score: 7. rating score: 7. rating score: 7. <BRK>## Summary:The paper proposes a new program representation to find semantic code embeddings (ProGraML). The authors use these embeddings to perform a number of traditional dataflow analysis tasks that you can find in modern day compilers. * Impressive scale of the dataset collected that require considerable engineering work. Important related work is missing. If DDF is far slower, then its usability may be limited inside a traditional compiler, especially if traditional dataflow analysis gives more precise results. * What is the largest graph in the dataset and how long does a typical DDF task take on that graph for T 30? If yes, what are the results? Do the embeddings generalize with minimal finetuning? ### Correctness Guarantees* I did not see any discussion about guarantees of correctness of DDF.<BRK>Summary A methodology for learning representations of programs using graph neural networks applied to graphs extracted from a compiler intermediate representation is presented. The graph representation captures both control flow as well as data dependencies and also represents calls to/returns from functions. * ( ) I m surprised by the lack of "real" tasks in the main text of the paper, as it means that the main experiments are not on well studied tasks. Recommendation I think this is nice work, but that the paper could still be significantly improved by reporting more details and potential new experiments. Did you analyse the model results by language? If the latter, how does it compare with results of the former?<BRK>I am revising the score to 7 from 5 based on the reply and the revisions to the paper. The representation used is not particularly novel, since it can be straightforwardly constructed from LLVM IR. These empirical results do not assure us that the method will perform well on other, more interesting tasks, either. I see that there are downstream tasks evaluated in the appendix. It would also be exciting to see how the paper s methods can be used to other tasks relevant for compilers, such as code optimization related tasks which would be amenable to a reinforcement learning approach.<BRK>Thus I recommend that it be accepted. However, I have lingering reservations about their approach that I hope the authors will consider. I am unable to find in the paper the number of rounds used for the algorithm classification experiment. I think the paper could be strengthened by putting more emphasis on downstream tasks and by addressing the null hypothesis that the network is not doing anything like dataflow analysis for them. Needless to say, just because GNNs can recapitulate standard dataflow analyses does not necessarily mean that they should need to recapitulate these analyses, particularly since (a) they are not 100% accurate and (b) they are wasteful and difficult to scale (as discussed in  DDF: Scalability Challenges ).<BRK>* Models are sensitive to the number of iterations, indicating they are perhaps not learning the same functions as in the exact algorithmComments:Overall I enjoyed reading this paper and I would be in favor of acceptance. The benchmark dataset is appreciated and can be useful for future studies. I also think it would be nice if the authors can provide more information about the limitations of the representation, and describe when it could conceivably introduce errors/approximations. Questions:* One problem with the existing approach is that it is not stable to the number of iterations. Do you foresee this as a sensitive parameter in downstream tasks? (also, does the operand order on the switch really matter?) Could it be confounding to the analysis to assume a call to an empty function?
Reject. rating score: 4. rating score: 4. rating score: 4. rating score: 8. <BRK>This paper studies the problem of attacking graph neural networks for spatio temporal prediction problems (e.g., traffic speed prediction). The modelling of the problem and the algorithms are relatively straightforward. This makes the attack more realistic as only one vertex needs to be attacked rather than the full graph.<BRK>The paper proposes a new one vertex adversarial attack to evaluate the robustness on deep spatialtemporal graph neural network.<BRK>This paper proposes a single vertex based white box attack on spatiotemporal GNNs. Is it really important to implement the one vertex attack?<BRK>This paper investigates the attack of a spatiotemporal GNN and also proposing a method to find the weakest vertex for this attack. The results are convincing.
Reject. rating score: 4. rating score: 4. rating score: 4. rating score: 5. <BRK>Summary:The paper proposes a neural network based solver for PDEs based on the Picard Iteration. In the numerical experiments section the paper applies the method to solve 1d or 2d PDEs. Therefore, an evaluation of the contribution and significance is not really possible for me. **Post Rebuttal Comments**: The authors improved the paper during the rebuttal but the clarity is not sufficient and the results are still puzzling. Is that correct?<BRK>These are crucial questions for deep learning results, and I always check this when I peer review. Instead of a classical convolution, which combines the neighbors with fixed weights, they learn a "functional convolution" that combines the neighbors with weights that are themselves functions of the neighbors. This takes advantage of "translational symmetry" in discretized differential operators. It is key that the PDE systems are sparse (dependent on a limited number of neighbors). The input & output is not always clear. What did you use to choose hyperparameters? ~~~~~Update:I think that the revised paper is an improvement,  but it s not ready. I think there is still missing information to make it clear what you did (as the other reviewers have commented as well.)<BRK>Technical commentso The functional C is undefined and unexplained, what does it mean? It seems to be a “form” instead of differential. The paper proposes input dependent convolutions for PDE learning under Picard solvers. The experiments show that the method perfectly learns example systems with practically 0 test error.<BRK>Summary: the work proposes to use neural networks to learn a kernel C(x,p) for PDEs. I agree with other reviewers that this paper is not ready to publish. Another concern is about the experiments. It can help me better evaluate the performance of the method. >> I feel the biggest problem is that the authors didn t clearly state the problem settings. If I understand correctly, in their framework the equation is fixed but unknown. The training data are several points in the domain (with parameters input) and testing data are other points.
Reject. rating score: 5. rating score: 5. rating score: 5. rating score: 5. rating score: 6. <BRK>For example, forward predictors with better pixel accuracy do not necessarily lead to better physical reasoning performance. This paper provides a thorough evaluation of the forward prediction models by considering different state representation and model class, resulting in some interesting observations about the relationship between the modeling choices and the physical reasoning performance. WeaknessesMy primary concern of this paper is the novelty is a bit limited. This paper does not propose any new method, but mainly focus on comparing several existing forward prediction approaches by assessing their ability to perform physical reasoning on the PHYRE benchmark. Although the best model achieves a new state of the art, I would not consider it to be particularly novel. It is hard to know whether this paper s observations are still valid in more diversified and complicated environments. For example, this paper suggests that "pixel based models are more helpful in physical reasoning" than object based models, which may not be true if we apply the methods in three dimensional environments where pixel based models could suffer from occlusions and a poorer estimation of the 3d location and geometry of an object. Given that the results are very sensitive to small variations in the initialization, the task solution model and the search strategy proposed in this paper may require extensive samples to find a suitable solution to circle around bad local minimums. The object based model may have a better performance if the graph is built dynamically. How did the authors specify the sampling space? In Figure 5, are there any intuitive explanations of why the red curve first decreases and then increases? Many of the reviewers share similar concerns:(1) The novelty is a bit limited as the paper did not introduce any novel technique approaches. While not every paper needs to propose a new method, a more in depth analysis of the benchmarked approaches may be needed to provide insights into how existing methods fail and how we can improve them. (3) The conclusion that the pixel based model does better than the object based counterparts may be a bit controversial.<BRK>Rebuttal Update #####I thanks the authors for their responses to my questions. They were very helpful, and I think the work, when explored further, would be a great submission to a future conference. However I do share sentiments with other reviewers about following set of issues. (1) The novelty is a bit limited as the paper did not introduce any novel technique approaches. While not every paper needs to propose a new method, a more in depth analysis of the benchmarked approaches may be needed to provide insights into how existing methods fail and how we can improve them. (2) The scope of this paper is a bit narrow where the authors only evaluated the methods on PHYRE that is fully observable and only contains open loop tasks with rigid objects of simple shapes in 2D space. ######Strengths:The paper is pretty well written. I enjoyed the analysis in the paper and thought the experiments were relatively thorough. WeaknessesThe overall approach seems to be rather incremental,  with many past papers on MPC control on some type of learned dynamics model, some with reward functions and others with value functions. For example see [1]. Why is it that better pixel prediction accuracy in object models actually lead to lower AUCCESS? Could authors provide more intuition on the templates in which an agent does poorly? I think it would beneficial for the community if source code for the submission was provided, as it still seems there are many free parameters that seem difficult to describe in the paper. Minor:[2] might a somewhat related reference that might be good to add. ICML 2019might also be worth citing about learning physical reasoning.<BRK>  Update  After reading the rebuttal I have left my score unchanged. I appreciate the clarifications, but am very concerned about the result that the pixel based models perform worse than the identitiy function in the FPA metric. In its current form, the paper does not provide that. Original Review  The paper evaluates different methods for forward prediction on the PHYRE benchmark for physicalreasoning. The pixelwise accuracy of forward prediction was not found to becorrelated with task performance of the resulting agent. Strenghts: * The paper is one of the first to offer quantitivate results on the PHYRE benchmark, and the first   (to my knowledge) to do so for forward prediction methods. These shortcomings are not discussed or analyzed in depth, even though they potentially explain   much of the empirical results. * It is not clear to me why joint training is described as a unique capability of the   deconvolutional model, e.g.by stating that it is its "key advantage". There is no conceptual   reason why joint training should not be possible for the other models, and Figure 10 in the   appendix seems to indicate that it is even helpful. While I think that not every paper needs to introduce new techniques, and that papers providinganalysis or negative results can be valuable contributions, in this case, I find it difficult toderive actionable insights from the presented study. As a results, I view it as slightly below the threshold in its current state. Questions: * How does the forward prediction accuracy (FPA) (Fig.5) compare to naive baselines, e.g.the identity   function? Given that not all scenes even exhibit movement, it is hard to judge how strong the   reported values really are. If it is no better than the no forward model,   then that would disprove my concern regarding model capacity mentioned above.<BRK>This paper discusses the importance of forward prediction in physical reasoning, and particularly in the PHYRE benchmark: a dataset of physical tasks where the agent is asked to place a ball of a chosen radius in a 2d environment. `Given the classifier, one can solve PHYRE tasks by sampling actions uniformly at random, and using actions that achieve good scores under the classifier. The introduced method delivers a new SOTA and some of the reported results are interesting. I find the paper interesting, but I have mixed feelings about it for I find some of its aspects deeply unsatisfying: 1. Specifically, I am concerned about the fact that in this paper s experiments the pixel based model does better than the object based one. This is surprising, as authors correctly note, but I am concerned about the justification: "it is easier to determine whether a task is solved in a pixel based representation than in an object based one" as stated in the 2nd paragraph of the intro. This would not be the case in any partially observed environment, or even fully observed 3D where some parts of the objects are not visible due to self occlusion. I consider this a severe oversight. This is made even worse by the fact that the best model of the paper is the one trained end to end, which is impossible with the considered object based models, but would be possible with the models I mention above. The best example of the lack of clarity is the following. All models considered in the paper supposedly can be described in terms of encoders, dynamics models, state decoders and task solution models as shown in Figure 2. The paper, however, fails to explicitly mention what encoders and decoders are used for the considered dynamics models. Why is the image represented like that? Another example is that of inputs to dynamics models and the task solution models: they all seem to have access to the whole history of inputs and/or latent states. 4.Solving PHYRE tasks is done by sampling actions uniformly at random and evaluating their success probability under the model, and the authors choose to try K 1000 such actions. As it stands, I think that this paper requires a little bit more work before it can be accepted. Other remarks:  in the 2nd sentence of the intro there is an object change from "humans" to "we" that is a bit confusing. in the abstract you also say that "[...] these improvements are contingent on the training tasks being similar to the test tasks", which is fair, but is hardly surprising and is an issue with deep learning at large: models generally fail to generalize out of distribution. If so, why report in in the abstract? also in the abstract you say that "Surprisingly, we observe that forward predictors with better pixel accuracy do not necessarily lead to better physical reasoning performance". I think this finding was reported in a number of papers now and is not "surprising" anymore.<BRK>The authors propose two variants of forward prediction model, i.e., object based and pixel based. The authors also design a classification model, taking predicted results as inputs, to evaluate the efficiency of the prediction model. An interesting conclusion, indicated by the authors, is that an accurate predictor does not necessary help the success of physical reasoning. Pros:+ Quality: This paper is well written. The method part is clearly presented with sufficient details, including the model architecture and input/output description. The whole paper is overall easy to understand. + Significance: The conclusions claimed by the authors are all supported by convincing evidence. The performance boosting on physical reasoning of complex scene proves the usefulness of the proposed method. Meanwhile, authors also point out that the generalization issue, i.e., generalizing to other template, is still challenging. Cons:  Originality: My major concern lies in the novelty. In my point of view, this work is more like an analysis project. This most significant part is the design of the full pipeline connecting the prediction model and the downstream task, i.e., physical reasoning. The detailed architecture of prediction model and training scheme generally follows the basic configuration of prediction task. However, considering that the main focus of this paper is not pursuing a better performance of prediction model, I think this part is not so important.
Reject. rating score: 4. rating score: 6. rating score: 6. rating score: 7. <BRK>The Unsupervised Learning of Transformation Equivariant 2D Representations by Autoencoding Variational Transformations is used for 3D shape descriptor learning, which the authors claimed as "self supervised" learning. [1] Zhang et al., AET vs. AED: Unsupervised Representation Learning by Auto Encoding Transformations rather than Data, in CVPR 2019. (AVT)Besides, there s no proof to verify transformation equivariant representations learning. The current presentation of the paper is based on the 2D project image representation for 3D objects. May some visualization results can better convince the audience. The proposed approach had experimentally verified its effectiveness in 3D recognition. Unfortunately, it did not address my concerns about the novel technical contributions in the proposed paper. Obviously, the authors applied the "Transformation Equivariant Representations by Autoencoding Variational Transformations" directly to 2D projections of a 3D object and then fused the deep representation by a shared weight NN (shown in Figure 1). I am not sure why in the rebuttal, the authors claimed, "Their proposed method distinguishes from AET significantly in two aspects". (AVT)Another concern was critical but not yet addressed neither: The authors could propose a method that can be developed based on the "3D Transformation Equivariant" to 3D objects directly instead of its 2D projections. I am not sure why the authors answered that "3D objects are unavailable at the testing stage." The proof of Autoencoding Variational Transformations for 3D data directly should not depend on the availability of 3D data.<BRK>Summary:This paper proposes a self supervised learning framework for 3D object classification and retrieval based on multi view representation, where a sub task of transformation estimation is adopted as a regularizer. By adding the proposed MV TER loss, the STOA approaches can gain notable improvement in performance. Strengths: The paper is well written, and the idea of transformation equivariant representation of 3D objects is easy to understand. The proposed method further improved the stoa methods on 3D object classification, which is already quite high. I assumed that the Euler angle representation is used in this paper, but I think it is not proper to use the MSE loss in (5) or take the average in (8) for Euler angles. Thirdly, any quantitative results of the transformation estimation?<BRK>The authors propose a self supervised learning technique for multi view learning based on a simple intuition that the transforms of the 2D views of a 3D object will be in an equivariant manner as the 3D object transforms. They show its effectiveness by clearly improvements under two frameworks MVCNN and GVCNN. + clear presentation+ simple idea+ convincing experiments+ detailed ablation studyOverall, most parts of this paper are satisfactory. While my main complaint is that the idea of this work is very simple and even can be called as common sense, and I have encountered this idea in many other papers, though in different fields, such as human pose estimation and 6d object pose estimation. I recommend the authors adding a full discussion of this simple idea based on transformation invariance.<BRK>Summary of the Submission:This submission proposes a self supervised learning scheme for 3D object recognition. The basic idea is to predict a 3D transformation from 2D views. The features that facilitate 3D transformation prediction then generalize to other 3D object recognition tasks such as object classification and retrieval. Update after Rebuttal:An additional experiment on real data was added which I find a valuable addition to the submission. AR2 points out a similarity with AET which I did not notice in my initial review. As the rebuttal points out there are differences between the proposed method and AET but the core idea is very similar. In my opinion there is enough difference to still recommend acceptance. It would be great to see how the learned features generalize to real data such as e.g.pascal.The paper title and the writing is in general talking about 3D transformations but it seems experiments have only been conducted with 3D rotations. The main weakness of the submission is that it was only evaluated on synthetic data.
Reject. rating score: 6. rating score: 6. rating score: 6. <BRK>The authors perform a variety of experiments attempting to show that non texture features are being The authors make a very strong claim in this work: that by deforming the output of convolution, they force the model to rely on other cues, such as shape. However, their analysis showing this to be the case is circumstantial. For example, in the case of the random shuffling experiment, there might be a dependency on shape that is being obstructed, but there could also be a dependence on spatial relations. Wrt the adversarial robustness experiments, it is not clear that their method yields more robust results or simply obfuscates the gradient attack so that a stronger attack is required. I would expect to see a plot showing their model s inference accuracy against attack strength, as measured by the number of PGD iterations. In ICML, 2018. I thank the authors for their thorough comments and experimental details.<BRK>The authors argue that by doing so, the CNN is encouraged to learn less from object texture and more on features such as shape. But I found it more important to understand why such a simple method would achieve this effect rather than using it to defend against adversarial attacks. I hope the authors could provide more motivation and experiments to understand the effect that defective convolution layers have on the CNNs. This work could serve as the initial step for answering this question. The literature review is sufficient and well organized. This paper needs better motivation. What would be a proper mathematical definition of texture? For instance, if all defective neurons are on the edge of the filter, we essentially reduce a large filter to a smaller one.<BRK>Summary: The paper proposes a method to improve adversarial robustness of the current convolutional networks. The method is based on dropping outputs of a fraction of neurons. The approach is novel as it tackles the issue of adversarial examples from a different angle than the usual denoising appraches, however, there is some room for improvement. Possible improvements and questions:  EGC FL (Yuan&He 20) performs equally well in the experiments, the authors argue that the main advantage of their method is the lower runtime but do not give any quantitative information on runtime. This information needs to be presented/commented upon in more detail. Without explanation this seems like cherry picking.
Reject. rating score: 4. rating score: 5. rating score: 6. <BRK>#### Paper summary:In this work, an imitation learning (AL) approach is proposed to imitate multiple active learning algorithms, in order to take their advantages to learn a better active learning algorithm. #### Disadvantages:  The idea of using DAGGER to combine multiple active learners was explored in Liu et. The experiments are not sufficient for the following perspectives:    The performance gain is somehow not very significant, the datasets are restricted to images with the same number of classes. al., which also utilizes DAGGER to imitate active learners. Overall evaluation:I think the paper proposes a reasonable solution, while the contributions and novelty are somehow below the standard of ICLR.<BRK>[1] Online choice of active learning algorithms. The experiments with ablations by excluding various experts are informative and insightful. Weak points:  The novelty of the proposed method is not very strong: The main idea is to select the best behaviour among several heuristics, that was explored before (Hsu&Lin, 2015, 1, 2, 3), the learning mechanism relies on imitation learning, that was also explored in previous works for AL (Liu et al, 2018), and the implementation in terms of state parametrization seem to be similar to previous works too (Contardo et al, 2017, Konyushkova et al., 2017, Liu et al, 2018). This paper puts an emphasis on the batch nature of the proposed method (for example, when contrasting against the related work). The transfer to more complex datasets such as CIFAR is shown in the appendix, but the gains by the proposed method seem to be marginal. However, the authors did not provide any explanation of this phenomena.<BRK>Furthermore, a batch mode active learning strategy is desirable compared to the previous work (Liu et al.2018) that uses a single sample. However, the way that batch mode is achieved in this paper seems trivial by computing and using the top k instances according to certain metrics. 2.It is also unclear how the pool of experts used in the paper generalizes beyond the setting in (Liu et al.2018), and why this might be desirable. I am not sure if such empirical validation is sufficient. How about other datasets that contain digits such as SVHN? How about NLP tasks that are reported in (Liu et al.2018)?On the other hand, I think the ablation studies provided by the authors do help to demonstrate how different components of the framework will play and influence the eventual outcome.
Accept (Poster). rating score: 7. rating score: 7. rating score: 7. <BRK>The proposed framework combines differentiable physical simulations and differentiable rendering to map physical parameters into images differentiably. This paradigm is then used to recover physical parameters from image sequences by means of gradient based optimisation.<BRK>This work presents a fully differentiable physics simulation coupled with neural rendering such that input video can be used to estimate object properties or find control policies to move those objects by trying to generate the same video at the output. The paper is well motivated by presenting a natural progression of ideas from this literature and it does a thorough job discussing related work.<BRK>  SummaryThis paper presents a framework for performing both differentiable physics simulations and differentiable rendering.
Reject. rating score: 4. rating score: 5. rating score: 5. rating score: 6. <BRK>This paper presents a reptile based meta learning algorithm called Eigen Reptile for few shot learning with sampling and label nosing. The writing is confusing and not clear enough. The idea of Eigen Reptile to alleviate gradient noise by eigenvector of parameters related matrix is interesting and authors prove the effectiveness of the idea in theory. By idea of self paced learning with prior model to solve noisy few shot problem is reasonable and ingenious with theoretical proof.<BRK>For instance, in the main paper it is not very clear how the algorithm works and implemented, this important information is provided in Appendix (Algorithm 1 and 2). I want to thank the authors for the revision and the rebuttal. The assumption of Theorem 1 is strong that the gradient contains Gaussian noise from the noisy labels. How is it possible to have a lot of noisy labels when only a few data is provided? I acknowledge the author s response by increasing the score but I think the paper needs to be further revised. This paper only provides the mini ImageNet dataset. Furthermore, some theoretical analysis about the limit to the noise can be verified for the future direction. 3.There is no comparison with the existing methods for combating noisy labels.<BRK>This paper is concerned about the update of meta parameters in gradient based meta learning approaches. However, the theoretical proof for Theorem 2 in the Appendix needs to be made clear. In short, this work is well motivated and the overall idea is sound. pp.586–591.Thank the authors for the detailed response. Different from Reptile, this work proposes to update the meta parameters by the "main direction" of task specific parameters. After reading the response and the comments of peer reviewers, it is still felt that this work needs to better clarify some key issues and strengthen both theoretical and experimental study. How is this option compared with the main direction? 2.The theoretical analysis in Section 4.1 is not novel.<BRK>This paper proposes a novel and effective meta learning algorithm using the so called Eigen Reptile to update the meta parameters and address the gradient noises. The authors provide the solution to address the high cost of computing eigenvalue and eigenvector in the ER process, and also offer the theorem to demonstrate that the eigenvectors will not be affected by gradient noises. The experiments are solid to demonstrate the usefulness of the Eigen Reptile algorithm. Missing comparisons to more recent meta learning and few shot learning approaches. ER is built upon a CONV 4 backbone.
Reject. rating score: 4. rating score: 6. rating score: 6. rating score: 6. <BRK>The paper studies the approach to  correct the bias  of momentum in supervised learning and reinforcement learning (especially, in the temporal difference algorithm). The approach averages the TD update over different steps via a modified momentum update.<BRK>Reason for score: The idea of using momentum for TD learning seems quite interesting. 1) The bias correction via Taylor approximation of the gradients seems new to me. The authors claim that the bias doesn t play a significant role in supervised learning tasks. The authors claim it is for speeding up learning. In this case, maybe the authors can have the MSE plot for just Adam updates for a better comparison?<BRK>This paper proposes a modification to momentum. The effect of this modification is studied on supervised learning and TD learning and on different representations. Computing the extra terms exactly is computationally expensive but it is possible that better approximations will be introduced later on.<BRK>#### IdeaThe paper explains and studies an interesting issue with momentum in TD learning, which is its staleness while doing TD updates which is in contrary to the supervised learning. However, the paper could benefit from more investigations. Is it considered as a geometric distribution and thus this formula? As mentioned by the authors: "Since the architecture required to train ...", the scalability of this approach is under question.
Accept (Poster). rating score: 6. rating score: 6. rating score: 5. rating score: 5. <BRK>3.The change of hyperparameters allows the model to accomplish the change of sample selection strategy and transform the training strategy of the model from fully supervised to self supervised. 3.A brief conclusion of the article and a summary of this paper s contributions need to be provided. Thus, the selection of the hyperparameters in the paper needs further explanation. The authors use both loss function and prediction invariance for sample selection.<BRK>This paper proposes a curriculum learning method to handle noisily labeled data. I had the comment that the title of the paper was misleading. After the discussions and the interactions with the authors, it came to my attention that one of my comments was wrong   I looked at a different paper that led me to have the conclusion that the authors cited a wrong performance number from a competitor in the literature; it turns out that I was wrong and I apologize to the authors. The authors disagreed with me but I was still not convinced by their argument.<BRK>So far I am unfortunately not conviced, that the progress is due to the proposed method alone. The proposed usage of the exponential moving average is reasonably motivated by the oscillating patterns of the instantaneous loss values. This and the fact, that a ablation study is missing makes it hard to judge the methods contribution. # Detailed commentsEq.2 is referenced before it is stated, consider rearranging. Minor comment, the text in Figure 1 could be largerMinor typo "Simply removing noisy data from training discards important information about data distribution."<BRK>Some of the insights provided in this paper, seem rather interesting, like by transitioning from supervised learning, to self supervised learning of noisy data, can better benefit the learning process. ICLR 2017. If I understand correctly, one of the key contributions is the interplay between the regular loss and the consistency loss, but the scheduling part is not super principled and seems to involve a lot of ad hoc tuning of the balancing parameter $\lambda$ and the temperatures. The paper seems to have combined a lot of existing techniques. I think this paper has the potential of providing some great insights, but the current set of results are rather noisy. Currently it seems like the authors run out of space and rushed through the experimental results.
Accept (Poster). rating score: 7. rating score: 6. rating score: 6. rating score: 6. <BRK>When training data is unlabeled, it performs significantly better than existing OOD detection algorithms based on AE, VAE, PixelCNN++, Deep SVDD, and Rotation loss and it performs only slightly worse than the labeled case. The authors show additional gain is possible when some OOD images are available during training, i.e., one or five images per class. This paper is very well written and I think the novelty and contributions are significant enough to merit its acceptance. Would it make sense to have a higher weight for OOD data since it has fewer samples than in distribution data? Some ablation studies that remove one or more of these would be interesting.<BRK>The authors tackle out of distribution detection without requiring additional detailed labels by leveraging the recent advancements in self supervised methods combined with Mahalanobis distance metrics in feature space. They find that their approach matches or slightly outperforms supervised baselines, and widely outperforms other unsupervised approaches. They also consider a few shot version of the task and extend their method to take advantage of supervision when available. There is clear motivation for the problem. I would prefer to see the related work discussed before the authors proposed approach, so that their method can be better placed into context with the existing literature by the reader.<BRK>Second, I think it is important to include some popular baselines here. Third, why is the cluster based outlier scoring method used? ", is a classic and well studied problem in the anomaly/outlier detection community. There have been many studies over this problem. ", to find some of these studies. ; methods that use a few labeled outlier data such as Deep SAD, DevNet, REPEN, etc. Please see table 1 in that survey paper for more details. They may be able to work without parameter tuning on each dataset, but this is not parameter free. More advanced approaches (see some of the methods mentioned above) can unify the anomaly scoring and the representation learning together and the labeled outlier data is used to optimized the entire anomaly detection pipeline, rather than the representation learning stage only.<BRK>However, the method and analysis provide interesting insights. The proposed framework is sounded and can well handle a variety of cases when class labels or OOD data are available. 2.The way it utilizes a small amount of OOD data is novel (eq.3). This assumption is not valid in general cases. The paper should explicitly point out this as a substantial limitation and provide some experiments (ex: use SVHN+CIFAR10 for the labeled OOD data) to elaborate it. 2.The evaluations are mainly on toy datasets. There are two types of labels: class label and in/out of distribution label. Please consider polishing the use of these terms in sections 1 and 2.
Reject. rating score: 4. rating score: 5. rating score: 5. rating score: 5. rating score: 6. <BRK>The three additions are a large multilingual language model, the use of language adapters, and smoothing on the token probabilities. In the context of this paper, when the paper talks about the long tail problem, what distribution are we talking about? Please state it clearly if this is only a speculation. > The gains ... of the head languages, although tail languages ...Again, what are head and tail languages? It should be the the KL of distributions, not labels. Later on, for example in equation (3), y is used as labels.<BRK>The paper sets out two goals 1) "to improve the overall performance of multilingual ASR tasks" and 2) (implicitly) to flatten the distribution across languages. One way to mitigate this is to pose the problem not as solving universal, multilingual speech recognition, but rather improving performance specifically on tail languages through training on higher resource languages. First, there are mismatched subscripts for \pi_y and c_i. Equation 3 would result in \pi   [1/1   1/(2*1), 1/1, 1/1]   [1/2, 1, 1] which is not a valid distribution. Minor comment: Figure 7 is mentioned in Section 2.3 but is only included in the Appendix. It would be clearer to either describe Figure 7 where it is first mentioned, or present this information in Section 2.3 as forward referring to Appendix material.<BRK>This paper proposes an Adapt and Adjust framework to address the long tail problem in multilingual ASR, which assembles three techniques: 1) leveraged a pre trained model mBERT to initialize the decoder, 2)  language specific and language agnostic adaptors, 3) class imbalance adjustments. 3) The effectiveness of a component (mBERT) need to depend on other components, otherwise it does not work. Overall this paper is clearly written and easy to follow. This would be more effectiveness than simply using mBERT.<BRK>The 3 include logit re balancing based on class priors, fusion of a BERT based language model, and the use of a common and langauge specific adapter layer in parallel. To my knowledge, the logit adjustment has not been applied to the long tail problem in speech recognition. It s quite unclear what the long tail refers to in this paper. It would be more convincing if the author discussed this a little more, including why it improves quality. It s unclear how x_{CTC} is defined in fig 1. Is it the output of the encoder?<BRK>This paper studies multilingual ASR with a focus on the long tail problem. A new method using dual adapters is proposed. Why did you choose to use distill mBERT over other alternatives (mBERT, XLM etc.)? Con:1.The framework combines many techniques together and it is hard to tell if any one of those is the  silver bullet .
Accept (Poster). rating score: 8. rating score: 7. rating score: 7. rating score: 4. <BRK>This paper proposes a method for binarization of neural networks of 3d point clouds. Two modules of entropy maximum aggregation and layer wise scale recovery are proposed to conquer the problems of discrimination loss induced by feature homogenization and scale imbalance, which are caused by model binarization. The authors provide theoretical analysis about the proposed method.<BRK>This paper proposes a method for learning binary neural networks on point cloud inputs. Then they analyze the scale of binary activations and propose a learnable scaling to reduce the effects of scale distortion.<BRK>The paper proposes a binarization approach for efficient deep learning on point clouds, called BiPointNet. The authors propose Entropy Maximizing Aggregation(EMA) and Layer wise Scale Recovery(LSR) to reduce the side effects of binarization.<BRK>From what the authors show a vanilla BNN (XNOR Net) applied to point clouds does not give very good results and for this reason the authors identify solutions that boil down to applying a shift and a scaling.
Reject. rating score: 4. rating score: 5. rating score: 6. rating score: 6. <BRK>Moreover, an exploration strategy for distributional reinforcement learning is also presented. The former is a trivial extension of previous work, and the latter is not properly explained. Pros:1.The experiment results look good. 3.Theorem 1 could be problematic. The quantile projection defined in this paper doesn’t seem to be contractive.<BRK>The paper then also proposes to estimate the exploration bonus for each state using random network distillation. ###  Weak points 	The experimental comparison/conclusion with IQN in the present form is unfair. To be fair, I think the paper should include to IQN any orthogonal improvements that NDQFN has. It is unclear from the experimental results how helpful are each non decreasing quantile function and DPE individually. ### My final recommendation The authors have attempted to address some of my points but these points require more time to fully address as they require to run more experiments. For the current form, I remain my inital score and recommend rejection for this time.<BRK>Paper Summary: This paper mainly contributes in two parts: 1). A non decreasing structure for quantile values in quantile based distributional RL. However, some experiment results are still missing and the paper still needs some editing before published, especially the experiment section. The paper stops at experiment results. The authors partially resolves it by using a fixed set of support, but if the support is changeable the issue remains. As the support p* is fixed, I do not believe that they should be part of the input.<BRK>This paper studies distributional RL and proposed two extensions. How do you convince us enforcing a non decreasing ordering of the learned quantile functions is helpful? I understand your arguments, but there is no evidence in the paper showing that doing so is helpful. The paper argues that DLTV is not applicable to continuous quantiles. However, it would be to include this comparison especially they have results on Atari games as well. The empirical results are not very strong, with 13 and 14 ties and losses with/to IQN.
Reject. rating score: 3. rating score: 5. rating score: 6. rating score: 6. <BRK>The paper also provides basic experimental results, both in the form of a naive toy example, and of results on two machine learning datasets. ### Questions / CommentsThe paper is described (and titled) as a HP tuning paper. The fact that it can be combined with any supervised learning method and can incorporate hyperparameter (HP) tuning is interesting, but the end results is still a unsupervised prediction model. Can the authors shed more light on this aspect? I would expect much simpler baselines to perform much better, for example: 1) just using the default HPs of the given algorithm 2) running on a single arbitrary source task the whole budget as a simple BO task, and use the best found HPs of this source task. In Experimental Procedure, What does (1) do? How is the ML model tuned if not by MSU HPO, which seems to be done later. It is not clear to me from the text. Additionally, the experiments would benefit from clearer descriptions and stronger baselines.<BRK>Results show the method does what it promises to. Weaknesses:  While it was easy to follow the paper s rationale, I found it difficult to motivate. This point I think should be easy to address given a small rework of the intro or perhaps a running example to periodically come back to. Again, this is a question of clarification and can easily be addressed. The fact that the Naive method beats the other two baselines and performs comparably to the unbiased proposed method makes me wonder whether (a) these are challenging enough tasks, or (b) those are competitive baselines. For example, in both experiments the true labels were known and the authors can measure the error in the divergence estimate.<BRK>2.Based on the proposed estimator, a transfer hyperparameter optimization algorithm to solve interesting problems is introduced. 2.I can imagine that someone may ask for more experiments of a large scale or of the type exemplified in the intro. On the other hand, the experiments can be regarded as designed concisely to demonstrate the authors  main points. This combination of transfer HPO and importance sampling estimator seem novel, interesting, and well demonstrated, with which many interesting applications can be imagined. **Questions**  On the line right below eq.(2), the loss function L is assumed to be bounded. Is this condition is necessary in proofs of any theoretical ones? It seems that, in all experiments, all losses are unbounded. While reading the paper, the questions arose were mostly answered after a few lines. The reading was pleasant for me and the paper is well structured.<BRK>This is of interest in several practical applications (e.g., advertising, as the authors discuss). The exploration of a new problem together with the introduction of principled estimators make both the paper s goal and methodology significant. ** The paper was a pleasure to read. Very clear and well structured. **Easy experiments. This is secondary considering that the theoretical analysis is solid, but tuning a wider range of ML algorithms (such as neural networks/NAS) would have made the case even stronger by showing that transfer learning is possible across a diverse class of models. Applying the method to more challenging scenarios would further demonstrate the benefits of the proposed approach. While many transfer learning baselines are inapplicable as most assume target labels to be available (as discussed in the related work), this is not the case for all of them. As this does not look at the labels of the target task, it could be compared against.
Reject. rating score: 4. rating score: 4. rating score: 5. <BRK>This paper aims to provide a systematic study on the problem of uncertainty quantification for spatial temporal forecasting. It is a bit vague in Sec.3.3 and therefore not clear what is the proposed frequentist UQ method specifically. The main conclusion here is that Bayesian methods perform better in terms of mean predictions and frequentist methods perform better in terms of confidence intervals, which is no surprise I guess. It is interesting to see MC dropout actually harms the prediction accuracy and the only one method with significant improvement margin is the SG MCMC. To me, even the results in the appendix (as a figure) are not useful enough. It is difficult to evaluate DeepGLEAM’s improvement. Although some results are interesting. it is unclear whether this paper is a pure study or there are actually proposed new methods. The setting of the COVID forecasting task is also a bit strange and the authors’ response does not fully address my concern. I would like to keep my ratings unchanged.<BRK>## SummaryThe paper considers the problem of uncertainty quantification in spatio temporal forecasting with deep neural networks. The work is not clearly motivated with respect to the state of the art. There are multiple issues in the experiments. ## Strengths  Overall, the paper is easy to read. Uncertainty quantification in Deep forecasting models is an important problem for optimal decision making. The authors never really define uncertainty quantiﬁcation, which has different meanings depending on the context. Which one are you trying to capture? There is a clear lack of rigor in the terminology used. 1.The description of the Mean Interval Score is misleading. The MIS tries to find the interval with the right coverage. I find this statement a bit too strong. We often contrast point prediction and probabilistic prediction (or forecasts). Accurate predictions vs better generalization  Confidence intervals are not prediction intervals  "Frequentist methods generally outperform Bayesian methods in conﬁdence interval." Why an ensemble model would solve the quantile crossing problem? Given that this is the main topic of the paper, this section should be extended with more references. In Table 1, the comparison between forecasting methods is unfair. In fact, optimizing MIS and evaluating on MIS will tend to win. This might give you more insights into the differences between the methods. I find it too ambitious. Is COVID 19, the right dataset to use? what are these assumptions? "It is difficult for DNN as they often do not generate explicit likelihood outputs." > I suggest you cite some normalizing flows papers.<BRK>The paper define first a metric that is used in statistics and econometrics for interval forecasts, mean interval score Then it reviews the main DL and UQ methods for temporal and spatio temporal methods. I found the paper clear and interesting. This paper is almost a review paper, yet I think it has a great value to the community. The comparisons are showing interesting findings, the state of the art is curated and the use of the  new  metric is also explained. While no new method or even metric is proposed, I think this paper is worth publishing. Questions/remarks:  In the second experiment (covid), what exactly is the point in using the encoder decoder sequence? isn t the  standard  propagation, by  road  and in between neighboring states, neglected? From Figure 3, it looks like the confidence bounds often do not contain the ground truth. Do you think it is because of lack of data or because the model is here too simplistic (some key parameters are not used as input)? Table 2  > Table 1 (change ref.in the text)
Accept (Oral). rating score: 9. rating score: 8. rating score: 7. rating score: 6. <BRK>This paper introduces PARROT, a novel approach for pretraining a reinforcement learning agent on near optimal trajectories by learning a behavioral prior. This paper presents an interesting novel approach and presents strong support for its value. In addition, the evaluation results are impressive. The paper indicates that the training data should be “near optimal” multiple times but never explicitly defines what is meant by “near optimal”. However, this paper still ticks all the boxes for me in terms of novelty and value, and so I would argue for its acceptance. Some discussion about how to avoid this/whether it is a concern would be helpful.<BRK>### SummaryThis paper proposes PARROT, a method for learning a policy prior from a dataset of expert state action pairs that have been derived from multiple similar tasks. Is there a point at which RL starts to outperform PARROT in the tasks considered in this work? What steps would need to be taken to try PARROT in an environment with discrete actions? The whole method is elegant. The experimental results are strong (though there are many opportunities for additional experiments)  The baselines are well chosen  The pseudocode in appendix A is clean and clear. All of the notation throughout the paper is too. Figure 2 is very helpful and clear  In the problem setup, there is discussion of fixed state and action space dimensionalities, but it is not stated that the spaces are vector spaces.<BRK>This work proposes a method, PARROT, to learn data driven priors for deep reinforcement learning agents. Motivated by the idea of pre training with existing data of similar tasks, the authors propose to learn state conditional behavioral priors from a set of similar tasks for reinforcement learning agents, such that a learning agent explores its environment in a meaningful way.<BRK>The paper is clearly written and well structured. 2) I was particularly disappointed by the experimental analysis. However, I have two main issues with the paper:1) This paper is directly related to the research domain of transfer learning or knowledge transfer, but this is completely ignored in the related work section.
Reject. rating score: 3. rating score: 3. rating score: 4. rating score: 4. <BRK>## Detailed Comments  "There are problems with supervised learning and machine learning in general." "machine learning requires huge amounts of data"   unclear what this means. "severe labeling issues were found"   what issues? "we provide the an overview"  "(AL),"   missing space. this happens at several places in the text  "continue iterative"   iteratively  "other stopping criteria"  > "criterion". "If a learner does not choose his strategy"  > their strategy  eq at bottom of page 2: what is "u"?<BRK>This paper aims to evaluate the performance of seven automated labeling algorithms in terms of accuracy. The authors conducted a set of experiments on six datasets from different domains under two typical settings where 10% and 50%of labels in the datasets are available. Many papers of the empirical study investigated the performance under more complicated settings. They are only strategies for the selection of unlabeled instances.<BRK>In this paper, the authors present an empirical analysis of seven machine learning algorithms based on six benchmark datasets: Four graph based semi supervised learning algorithms and three active learning algorithms have been evaluated on image data sets (CIFAR10, Digits), texts (Fake and true news, 20news), and other data types (Iris and Wine). Based on the empirical performances of these algorithms, the authors provided a ranked list of the studied algorithms. How does this setting apply for active learning algorithms? How is 80/20% decomposition applied to semi supervised learning algorithms? Also, in general, semi supervised learning and active learning are different problems.<BRK>The paper presents an empirical comparison of different approaches for datalabeling. The authors describe their experimental setup and findings, makingrecommendations for when to use what approach in practice. The authors reference their own anonymous work throughout the paper asjustification for the presented investigation and its parameters. The authors evaluate their approaches on only six datasets. Some details of the experimental setup are unclear. It appears that only ranks were used for this comparison and not theactual performance numbers. Finally, all methods evaluated by the authors have hyperparameters that need tobe set. It is unclear how the authors chose the particular values they used inthe experiments, and tuning them for best performance may have a major impact ontheir performance and the rankings.
Reject. rating score: 4. rating score: 4. rating score: 5. rating score: 5. <BRK>This paper formulates meta learning using the information bottleneck. The method develops a meta learning algorithm based on Gaussian Processes, which they also interpret as a memory based algorithm. The paper seems to lack focus. It consists of two orthogonal components: (1) applying the information bottleneck to meta learning and (2) a GP based meta learning algorithm. Each of these components is a separate meta learning algorithm: (1) alone can extend MAML to "stochastic MAML" ($\beta > 0$) and (2) alone is "GP" with hyperparameter $\beta 0$. The fact that information bottleneck with $\beta 0$ reduces to standard learning is well known. The experimental results are mixed, and the paper doesn t give much of an interpretation of these results. The experiments in the appendix are similarly mixed. The experiments about the efficiency regarding the amount of data (Figure 2 right) are promising, but this claim would be more significant if evaluated on multiple tasks.<BRK>The paper presents a method for Bayesian meta learning. However, someone skimming the paper might not realize the existence of this NN at all, which would make the results very surprising. The information bottleneck is used to motivate a choice of approximate posterior. Things I liked:  This paper was interesting and on an important topic. The VI approach in that paper doesn t distinguish train/support and validation/query sets, and doesn t motivate the weight beta which is placed on the prior, while the information bottleneck does. It would be nice to have this spelled out, as well as how this gradient was computed (whether it is simple or required some non trivial tricks). The method proposed in the paper adds a lot of complexity on top of MAML and variants, so I think the paper needs to build a strong case and clarify exactly which settings this method is preferable for. A fair comparison should also compare to MAML(+variants) without data aug, and/or GP methods with data aug6. This is an interesting plot which I would like to see more of in few shot learning papers.<BRK>Summary:The paper proposed variational approximations to the information bottleneck objective functions for meta learning. The authors then provided three different settings using their variational loss functions, namely SMAML, GP, and GP + MAML. The authors  motivations for these three settings were to study the effect of stochastic gradient based method, non parametric method, and the combination of gradient based method with non parametric method. Reason for the score:I find the paper interesting for establishing a variational information theoretic objective function for meta learning. It would be great if the authors couldclarify them. The paper is easy to follow and I did not find typos. Yet, in Figure 4, it seems that pure GP is basically on par with GP+MAML and outperforms GP+MAML when more inner steps are introduced for MAML. On the classification tasks, it seems that GP models perform better than the standard MAML mostly when $K$ is large. This is also somewhat true for the regression tasks because we know GP is smooth and interpolates better.<BRK>## SummaryThe paper derives a meta learning framework based on the information bottleneck principle. However, it is an interesting observation that this extension subsumes gradient based methods (by using parametric encodings) and memory based ones (by using nonparametric encodings). While the setting is slightly different, it would make sense for the authors to address how the two papers are related, especially since the GP method proposed by the authors can be used in the transductive settings as well. Regarding the new methods, I think that the use of a GP encoder with a deep kernel is interesting but again not particularly challenging from the technical perspective. The authors use MAML as the unique baseline ignoring more up to date meta learning methods which undermines the message of the paper.
Accept (Poster). rating score: 6. rating score: 6. rating score: 6. <BRK>The paper introduces a framework to statistically test whether a given model is individually fair or not. I think the paper tackles an interesting problem and has nice results, both theoretically and experimentally. I think there should be a formal definition for the fairness notion you have in mind. How is the gradient flow attack related to the dual problem in Eq.2.3 and 2.4? I will raise my score if the issues raised above are addressed.<BRK>This paper proposes a test to determine whether an ML model violates individual fairness. Conceptually, this paper rests on the "gradient flow attack," which produces a mapping that, given an example, produces another example which violates the inviddual fairness constraint. The ratio of these quantities is what the authors use for their hypothesis testing problem: is it below a the limit of tolerance or not?<BRK>The main contributions are:1. 2.After finding the adversarial examples, a hypothesis testing framework is proposed to test the model for individual unfairness. Many of my concerns were addressed, and I am increasing my score as a result. A follow up thought: It would be nice to add some discussion on the runtime of the proposed framework. On a high level, the idea of testing for individual unfairness is an interesting one, specially given that there aren t many metrics of it individual unfairness out there. It is not clear how to reconcile these two problems. Can such user specified metrics be handled by the methods in the paper?
Reject. rating score: 4. rating score: 4. rating score: 5. rating score: 6. <BRK>It investigates 3 semi supervised (SS) losses for this purpose: 1 is the loss proposed in (Ren 2018), and 2 novel losses, the “Silhouette loss” and the “DB index” loss, inspired by the corresponding internal clustering metrics. I think there is a lot of improvement here, before the paper can display improvement over the current state of the art. + The work explores 3 different losses for inducing clustering in the embedding space, 2 of which seem novel. The 2 new losses are quite intuitive as they are inspired by well established metrics. Where is this comment based on? The review is limited both from the scope of unsupervised clustering, as well as from the point of view of semi supervised learning (both in general and specifically to time series). This is a result of limited literature review I guess (weak point 1). It would be interesting to have 2 sets of unlabelled data, one for training and one for evaluation.<BRK>The paper presents an autoencoder based approach for semi supervised time series clustering. To advance the state of the art, the method needs to show improvement in the entire benchmark and not just on three datasets, which we are not sure how exactly they were picked and why. Pros  Well written paper that tackles an important problem  Study of validity clustering indexes and their integration to current CAE architectures  Results on three datasets showing the potential of this methodologyCons  Missing related work  No comparison against unsupervised time series clustering methods  No comparison against semi supervised time series clustering methods  Only three datasets used despite 100+ availableDetails:  The paper is missing a decade of progress in the area of unsupervised time series clustering.<BRK>This paper benchmarked three different existing losses, DB/Propotype/Sihouette, on time series clustering. As there is a labeled dataset, the reason to solve this problem as two steps is not clear to me: 1. learning the semi supervised representation and 2. do the clustering. Why not just benchmarking by semi supervised learning metric, like accuracy? 2.The effect of the clustering method looks unclear. It will be more interesting if the author can propose a method, which can consistently win other methods. Minor concerns: 1. the bar plot should have space between each category. 2. it will be more convincing if baselines from other papers are considered.<BRK>The model combines the regular reconstruction loss usually employed in AEs with two new losses based on the intrinsic clustering evaluation metrics, Silhouette and DBIndex. Positive points:Overall, the presentation of the paper is good, it is well organized and easy to understand. The experiments are plenty and well thought to evaluate the proposed model in three different datasets. Otherwise, what p_j means? Is the number of clusters defined by the number of classes? Points to improve:In “Both LSTM and convolutional autoencoders have been show to be successful at learning latent representations of time series data.” Please, provide at least one reference for each type of model. Therefore, I would suggest adding other metrics such as Normalized Mutual Information (NMI), Purity, and Clustering Accuracy. Although I’m not aware of other methods for semi supervised clustering of time series data. There several options to compare with in the unsupervised clustering case.
Reject. rating score: 4. rating score: 5. rating score: 5. rating score: 5. <BRK>The paper presents experiments on CIFAR 10 and LSUN that were synthetically long tailed or imbalanced. The results show that the proposed term generates samples that follow a more uniform distribution over classes. Getting a classifier to work on long tailed datasets is not an easy task and people are still investigating the development of techniques to learn from imbalanced datasets (see for example I). According to the text (below Eq.3), this loss follows the training distribution which in the context of the paper is long tailed. However, the proposed regularizer penalizes the GAN to generate samples following a long tailed distribution. Unfortunately, the CIFAR 10 and LSUN datasets have a small set of classes in them.<BRK>They induced the class distribution information using a pre trained classifier, and the regularize utilizes the class distribution to penalize excessive generation of samples from the majority classes, thus enforcing the GAN to generate samples from minority classes. + Simple/directly applicable approach that seems to work experimentally, butCons:  The method part is not easy to follow. I am wondering about the performance on large scale imbalanced datasets like iNaturalist. Updated:Thanks to the authors for the provided extra experiments and clarifications. Some of my concerns (e.g., how the batch size affected the performance) have been diminished, but I do agree with other reviewers that more baselines should be included.<BRK>After all, the method is quite interesting, since in contrast to a conventional conditional GAN, the discriminator is not provided with the class identity of the generated image directly. The method and evaluation heavily relies on the quality of the pre trained classifier. The paper has also experimented only on artificially created imbalanced datasets, which contain small number of classes with the model being trained with the batch size higher than the number of classes. Post rebuttal feedback:Thanks to the authors for the provided extra experiments and clarifications.<BRK>Since the method is very intuitive, more experiments on datasets with larger pixel number and more categories are more convincing. This paper is well written and structured. The authors applied the methods in long tail classification to GAN. The results are better than the ones without the proposed constraints.
Accept (Spotlight). rating score: 7. rating score: 7. rating score: 7. rating score: 7. <BRK>This paper extends the idea of language model style self supervised learning approach to training logical reasoning models from unlabeled mathematical expressions. The model is required to predict the masked subtree at the decoder end. Furthermore, the model also exhibits good conjecturing ability in generating quite reasonable amount of new theorems that are provable and useful, which is quite encouraging and impressive. This seems to be a waste of self supervision signals. This is interesting, but I’m wondering if the proposed method be used in various other downstream reasoning tasks? This would show how generalizable the skip tree pretrained model is on various other downstream reasoning tasks, which would greatly enhance the strength of the work.<BRK>The authors show that self supervised language models (the Transformer architecture to be exact) trained on the proposed skip tree training task for mathematic theorem proving enable mathematical reasoning capabilities. They compare the mathematical reasoning abilities of the skip tree training task with skip sequence and show an impressive performance improvement. I strongly disagree that this is a good way of measuring mathematical reasoning abilities. I do have some comments about presentation clarity (look below), that I hope the authors can address during the rebuttal. I will clarify this as well in the cons section below. Pros: (1) I like the idea of skip tree training for structured problems. I recommend the authors clearly define this in the paper.<BRK>QualityThe paper is quite well written and the experiments seem well thought out. In addition, the free form conjecturing evaluation can be more clear. However, this approach is a new take on learning mathematical reasoning with self supervised learning, rather than supervised learning like in other previous work. The proposed skip tree technique seems to make a lot of difference for training.<BRK>The authors propose a self supervised learning task to enhance the reasoning capabilities of machine learning models on mathematical formulas and to perform conjecturing in higher order logic. Overall, the paper is clearly written and the bibliography complete. Also, the experimental analysis is undoubtedly valuable for the machine learning community. Consider the following examples for conjecturing on assumptions and free form conjecturing. The authors have addressed and clarified some doubts about the originality of their idea and the reproducibility of their experiments in the discussion phase. If so, I would suggest to rephrase by saying that the main skip tree task is quite flexible at masking statements. Is the proposed training unsupervised?
Reject. rating score: 2. rating score: 4. rating score: 4. rating score: 8. <BRK>Update: The author response has not changed my opinion that there is insufficient new material in this paper vs the ISIT paper, and the presentation of the material from the ISIT paper does not note that this material was previously presented there. I think the only new material is the experimental results on the CLF algorithms. The authors should note that GSA BUR has already been published as the SURI technique (referenced as Liu et al 2018), even considering this paper is a version of the ISIT 2020 paper.<BRK>The results indicate that the method improves the baseline. The experimental section is acceptable. **Weakness:**   This paper has substantial overlap with the literature [1 3], especially [1]. It would be beneficial to mention how this work is different from these papers. In my opinion, there is not enough novel material in this paper for this conference. **Questions:**1) What is the difference between the method presented in this paper and [1]2) Provide a comparison with the literature [1 3]. IEEE, 2020. [2] Liu, Shiyu, and Mehul Motani. IEEE, 2018.<BRK>4.A technical flaw: I disagree that author mentions that "the UR is the same to the unique information in the partial information decomposition (PID) frameowork". 1.It seems to me, in terms of methodology, that the difference of this work to [Liu, 2018] and [Liu, 2020] is that this work has two ways to estimate UR, one is based on the KSG estimator, another is based on a classifier. There is also a very early work that explicitly implement this idea [1]. However, the perspectives are not new.<BRK>In this paper, the authors recognized the function of unique relevance (UR) of features for optimal feature selection and augmented the existing mutual information based feature selection (MIBFS) methods by boosting unique relevance (BUR). However, I think the paper could be improved from the following two aspects. However, some of the points are not made clear. For example, at the end of Sec.3.1, they stated that "We note that the optimal feature subset S* may also contain features with OR and no UR at certain situations. For example.....",   which seems contradictory to the Proposition 1. The current version is not easy to follow.
Reject. rating score: 4. rating score: 4. rating score: 5. rating score: 5. <BRK>The main motivation of this paper is based on the theoretical results of meta learning. To ensure the assumptions of the theories, the authors propose a novel regularizer, which improves the generalization ability of the model. Some results on few shot learning benchmarks show the proposed method improves w.r.t.those baselines. Here are the main concerns of this paper:1. However, the theoretical setting here is not fully consistent with the few shot learning setting. For example, there is no validation set in Eq.1.The authors should make more discussions here to show will these differences influence the final results. 2.One main theoretical assumption in meta learning theory is the task distribution. Could the authors make this notion clear? Should we do empirical results on those tasks with different kinds of task distributions?<BRK>##########################################################################Summary: The paper reviews common assumptions made by recent theoretical analysis of meta learning and applies them to meta learning methods as regularization. Results show that these regularization terms improve over vanilla meta learning. The main idea of applying theory to practice is reasonable, but the regularization methods proposed are mainly known. Assumption 1 in Du et al.states that the ground truth weight should cover all directions evenly. It cannot be ensured when the tasks are fixed. The proposed regularization penalizes the condition number of the weight matrix during training, which is more similar to the spectral normalization proposed in [1]. As to regularizing the Frobenius norm, there exist a line of literature showing weight decay works for general settings apart from meta learning. Thus, I think the regularization proposed in this paper is known. However, as shown in [2], even by with some simple tricks, meta learning can be more stable and achieves better results. This casts doubt on the value of the proposed method.<BRK>Summary:In this paper, the authors aim at bridging the gap between the practice and theory in meta learning approaches. Specifically, they propose two regularization terms to 1) capture the diversity of the tasks and 2) control the norm of the prediction layer, thereby satisfying the assumptions in meta learning theory. + The paper is well organized and clearly written. + The experimental setting is designed in a good manner and the results are promising. So what is its difference to a simple l2 weight decay? Is this means the proposed regularizes would become trivial while applied on top of a more complicated model, e.g., LEO[1]? It would be better to add some comparisons with recent methods. The details to calculate the subgradients of the singular values, which is quite complicated, are missing. Especially seeing that there is no guarantee that an auto differentiation tool will do that correct. ICLR 2019Above all, since the contribution and the technical details to calculate the subgradients are not clear to me, I have to currently recommend a weak reject.<BRK>To improve the practical performance of meta learning algorithms, this paper proposes two regularization terms that are motivated by two common assumptions in some recent theoretical work on meta learning, namely (1) the optimal (linear) predictors cover the embedding space evenly, and (2) the norms of the optimal predictors remain bounded as the number of tasks grow. Numerical experiments show that the proposed regularization terms help achieve better performance of meta learning in some tasks. This work serves as a nice attempt to instruct the practice of meta learning with theoretical insights. Below are some of my concerns. In some experimental results, the improvement due to the proposed regularization seems to be at the same level of the standard deviation, as well as the difference between the reproduced results of existing meta learning algorithms and those reported in earlier papers. The authors often talk about "enforcing/ensuring the assumptions". However, from my understanding, whether the assumptions (on the optimal linear predictors, or "ground truth" predictors) hold or not depends on the learning problem itself, NOT on the algorithms. Therefore, there is no way we can enforce/ensure these assumptions.
Accept (Poster). rating score: 7. rating score: 7. rating score: 7. rating score: 7. <BRK>Based on this finding, authors apply GNN to fully connected graphs with memory/attention, i.e.the use transformers. The paper is well written, and methods and analysis approach used are clear. The selected approach and the authors findings are meaningful in a neuroscientific context, as the transformers approach better resembles the function of a human brain. The contribution is original and relevant for the community. The introduction does not clearly explain why the assumption that restricting the model and encoding morphological information may be beneficial. A weakness of the manuscript is the assessment of the results.<BRK>This work considers continuous control environments in which each agent limb (actuator) is associated with one action and a set of observation factors. The results would be stronger if hyperparameters had been systematically tuned. Suggestions  The paper mentions in passing that this work involves agents “with each non torso node having an action output”. This limitation probably deserves to be highlighted more prominently. In other places, the paper contrasts transformers with GNN based methods (“substantially outperforms GNN based methods”), as if transformers were not GNNs. The following phrases are important but unclear for readers who are not very familiar with GNNs or transformers:  “Having an implicit structure that is state dependent is one of the benefits of AMORPHEUS.”  and  “the implicit state dependent message passing schema learnt by AMORPHEUS can be better”  The paper says “We use entity to denote both vertices and edges.” But the term “entity” appears nowhere else in the paper.<BRK>The paper focuses on the problem of multi task control with a shared policy in the continuous action setting. The paper includes ablation experiments that clearly show that current works that use the body morphology structure to constrain the graph structure of graph neural network based approaches do not actually improve the performance. The paper instead forgoes trying to input the body structure and uses a transformer based architecture that is capable of learning the appropriate (even dynamic) graph structure actually useful for control. If there is some sort of aggregation going on, it needs to be clarified as to specifically how.<BRK>This paper instead proposes to use Transformers as a simpler mechanism to be able to train and discover the helpful morphological distinctions between agents in order to better solve multitask reinforcement learning problems. It is stated in the paper that amorphous does better for state of the art incompatible continuous control?
Accept (Poster). rating score: 8. rating score: 6. rating score: 6. <BRK>The paper proposes a new discrepancy between probability distributions called the sliced Kernelized Stein Discrepancy. Thus the main selling point is that the sliced version has a much better behavior as the dimension increases. The authors then propose to use the new divergence for two applications: goodness of fit testing and for models learning. Strength:	  The proposed framework is neat and the experiments are thorough and convincing with many additional discussions and results in the appendix. However the main idea is clearly explained  and the advantage is clear both in terms of theory and throughout the experiments. Questions:	  In 4.1.1, the distributions p and q although high dimensional, they often have independent components. This might be very advantageous for the sliced version of the algorithm, especially when using a set of orthogonal projections for the projections $r$.<BRK>## Strong and weak points of the paper### Strong points  Provided a novel slicing technique for KSD and provided its statistical estimator based on U static. Experimental results supports that the proposed sliced KSD is very promising approach for high dimensional models. At least, the main algorithms should be presented in the main paper. ## Rating  Clarity: For me, the main paper is not enough to understand the paper and the proposed method clearly. Novelty: The idea seems very interesting and important in the community.<BRK>Experiments on goodness of fit test (synthetic high dim Gaussian & RBM) and model learning (ICA on synthetic data & amortized SVGD on MNIST) are reported in the main body of the paper. This paper is well motivated and the writing is good. To the best of my knowledge, the idea of  slicing  is novel to solve the high dimensional problem of KSD and SVGD. Pros:1.The  slicing  idea is novel to KSD and SVGD;2. 2.The notation $f$ is used for denoting both $R^D\to R^D$ and $R^D \to R$ mapping, which is misleading. Besides, it is a little hard to understand how Eq.(5) can be derived based on Eq.(2).
Reject. rating score: 5. rating score: 5. rating score: 6. rating score: 6. <BRK>Problem Setup: The paper proposes a mutual information objective to learn a latent representation whichcan be used for planning. The paper note that most of the existing model based RL methods learn a model of the world via reconstruction objective, which requires to predict each and every detail of the visual input, and hence can be detrimental in case of noisy inputs or in the presence of distractors. Proposed idea: In order to tackle this problem, the paper proposes a mutual information objective to maximize the mutual information between the latent codes at distinct time steps. The paper proposesto use a mutual information objective between the observation and the encoding of the observation (as in Dreamer), as well as consistency objective in the latent space (already used before). The authors also evaluate the robustness of the proposed method by evaluating the capability of the proposed method in dealing with complicated backgrounds (given the scenario, when the entity of interest occupies a small region in the input). Shaping belief paper [1] also uses a CPC style objective for learning a model of the environment. [2] also learns a model of the environment by predicting only the relevant information by constructing a temporal information bottleneck but still within the framework of maximum likelihood prediction. Since the underlying idea has been tried in some other context and in this work the contribution is to make it work for deep RL problems, it becomes important to evaluate on more challenging problems and tasks.<BRK>  Summary  The paper proposes a method for visual model based reinforcement learning that relies on contrastive learning to learn the predictive model. Building on Hafner’20, the paper replaces the image reconstruction objective with a noise contrastive estimation (NCE) objective for the latent dynamics model, an NCE objective between the images and representations, and an additional maximum likelihood objective for the latent dynamics. Decision  The paper tackles a relevant and important problem of building predictive models that do not rely on image reconstruction, and proposes a promising method to this end. However, there are several technical weaknesses of the presented approach, and further important baselines and ablations are omitted. Due to lacking experimental evaluation, I lean toward rejecting the paper, but would be happy to update my score if the experimental evaluation were improved. The experiments show the effectiveness of the proposed method in simple continuous control scenarios, further supported by some theoretical analysis. The paper presents only intuitive justification for the three objectives. ###   Update  The authors  response does not satisfactorily address my concerns. While in the revision one baseline was added to the appendix, the main experiment still only contains a comparison to Dreamer and ablations. Further, it appears that the proposed method fails completely when the non markovian part is removed.<BRK>Summary:This paper proposes an information theoretic framework for learning a world model that encodes task relevant information of the world. It shows that the learned encoder and dynamics model can be used to train the policy and fitting the value function to agent to perform comparibly well to Dreamer on standard tasks and outperform them when there are distractions in the scene. I believe this theoretical analysis is novel. The experiments provide an insightful ablation and show that the proposed contrastive model is less agnostic to the background distraction. This paper misses a few recent relevant work [1, 2, 3]. In the orignal paper dreamer also proposes a contrastive loss for training the latent model although it is not temporal. Also, how important is having a recurrent dynamics model as supposed to a feedforward dynamics model? It would be helpful if the ablation can show the importance of other components such as Non Markovian consistency and dynamics smoothing. It proposes a simple framework with experimental and theoretical support. The main downsight lies in its novelty as a few other works that aim to tackle this issue in a similar way.<BRK>Summary:The paper introduces a new method for model based RL that learns a dynamical latent representation from pixel data (images) using a maximum mutual information criterion together with a predictability loss. Relevance:The paper addresses the very relevant problem of learning representations usable in a control task. Scientific quality:  The proposed approach is in general well motivated. However.I am not convinced by the emphasis that the authors put in the proposition that the maximum mutual information loss helps to learn task relevant features. For example, the video backgrounds in the experiments are completely task irrelevant and at the same time highly predictable. In general, the encoder cannot trulely promote task relevant features without having access to the reward structure. However, the authors should include more baselines, possibly including other model based methods such as [1] and model free methods such as some variant of DQNs. References:[1] Hafner, Danijar, et al."Learning latent dynamics for planning from pixels." International Conference on Machine Learning.
Accept (Spotlight). rating score: 8. rating score: 8. rating score: 7. rating score: 7. <BRK>**summary** the paper puts forward an idea that deep enough VAE should perform at least as well as autoregressive models. Authors explore this in the context of image generation, and construct VAE model that is a generalisation of typical autoregressive architectures. They use several tricks to ensure stable training of very deep VAEs and show that final performance exceeds all autoregressive models. The tricks used to stabilise training are pretty ad hoc, but their effectiveness, showed experimentally, is important in advancing the field. **cons*** The main criticism I have is around ablation studies that justify the proposed architecture choices and training stabilisation tricks, as well as comparison to other tricks in the literature (e.g.Vahdat & Kautz (2020)). **comments*** Further investigating the relation between using NN interpolation in upsampling and having active latents in all layers would be very useful.<BRK>**GENERAL**The paper claims that high quality of generated samples and SOTA bpds are achievable by VAEs if the model is deep enough (deep in terms of the number of stochastic layers). Interestingly, they are able to learn VAEs with up to 78 stochastic layers, and achieve SOTA bpds on CIFAR 10, ImageNet 32, ImageNet 64, FFHQ 256 (5 bit), and setting a great result on FFHQ 1024 (8bit). **Deficiencies:**D1: The prior is not explained in the paper! It would be interesting to compare at the conceptual level both heuristics. R2: It seems that the authors do not use BatchNorm.<BRK>Although the code is provided, the experimental protocol is not well described in the paper (learning rate, number of epochs, hidden size, ...)3. Furthermore, the model can be trained without using freebits or KL annealing   although additional tricks are required (gradient skipping and prior warmup). The empirical results strongly support that "very deep VAEs can outperform autoregressive models on images" and the authors introduced a minimal architecture allowing to do so. They demonstrate that likelihood performance is correlated with depth and report state of the art performances on multiple image datasets.<BRK>Here, the authors show that a simple hierarchical VAE architecture inspired by previously proposed ones can outperform autoregressive models if it s made sufficiently "deep". The authors report impressive results on multiple datasets generally using less parameters than competing models. This point is a bit weak and not well demonstrated in the paper. I cannot find any other references to masking. An ablation study of the proposed modifications to the architecture and training tricks would be useful, e.g.what s the most single important modification that makes the model work ?
Reject. rating score: 3. rating score: 3. rating score: 4. rating score: 6. <BRK>This paper focuses on developing an RL learning algorithm that is simple and can significantly improve performance over existing algorithms. The paper presents the algorithm AWR, which is an extension of the algorithm reward weighted regression. The development of simple, effective off policy algorithms for reinforcement learning is still an open problem, and this paper tries to take a step towards addressing it. The paper gives a detailed derivation of the proposed algorithm, and the ablation studies provide some information as to what components in the algorithm make it useful. However, I do not believe this paper is ready for publication because the extensions are minor, and experiments lack scientific rigor, making it unclear what is to be learned from the paper. statistical power analysis in deep reinforcement learning experiments. The paper says there is a theoretical analysis of the algorithm, but I only see the algorithm s derivation. Experiments:The primary motivation and claim of this paper is the design of a more effective algorithm. The main issues are with how hyperparameters are selected and the lack of statistical analysis of the results.<BRK>This paper presents a reinforcement learning algorithm that applies advantage weighted regression. The idea is very similar to the work published in “Neumann, Gerhard and Peters, Jan R, Fitted Q iteration by advantage weighted regression, Advances in neural information processing systems, 2009”, starting from reward weighted regression and further developing to advantage weighted regression. The difference in this paper is to add a constraint on the policy search, requiring the policy to be similar to the sampling policy. However, this constraint has also been studied in the paper "Christian Wirth and Johannes Furnkranz and Gerhard Neumann, Model Free Preference based Reinforcement Learning, AAAI 2017" (It seems not in reference). Overall, it may enhance this paper if it has more technical novelty when developing a new algorithm.<BRK>This paper presents Advantage weighted regression, which relies on two sub routines : (a) train a value function baseline using regression, and, (b) policy learning, which is advantage weighted. Some questions:[1] Contributions: it appears the objective is a minor modification off prior work, where, the returns are replaced with advantages. While this makes intuitive sense, and it does indicate improved results, the actual contribution is rather limited. While the paper does present results in off policy settings, these derivations etc. [2] The authors mention they present a theoretical analysis (e.g.in page 1) of AWR. While I see a derivation of the AWR update, this isn’t a theory analysis of any of the proposed algorithm s behavior. In particular, can the authors make a rigorous claim as to why this is a better algorithm than current approaches? Alternatively, one can view the exponentiation of the advantage as reward transformations which tend to impact the convergence, if one utilizes a policy gradient method for obtaining the new policy (see for e.g.Ghosh et al.2020 (An operator view of policy gradient methods)). [3] Experiments: Looking at table 1: I see that AWR appears to be significantly worse than SAC in several tasks   it is again unclear as to why this is considered “competitive in terms of final performance compared to prior works” as written in table 1’s captions. Furthermore, in terms of performance of TRPO, I find that the results appear to be very much lower compared to what I believe they obtain (for e.g.see Rajeswaran, Lowrey, Todorov and Kakade (Neurips 2017))   could you mention how these experiments were conducted and why there are these discrepancies in terms of final performance?<BRK>This study presents a deep reinforcement learning method, Advantage Weighted Regression (AWR). The policy update of AWR is constrained as in a similar manner as REPS (Peters et al., 2010). Although the benefit of AWR is not clear in the reinforcement learning tasks, AWR exhibits its advantages in the context of imitation learning and off policy learning with static datasets. The paper is well organized and easy to follow. However, there are some unclear points. I would like to ask the authors to clarify the following points. In my understanding, both MPO and AWR solve the same optimization problem to satisfy the KL divergence constraint, and both MPO and AWR update the policy by maximizing the weighted log likelihood. The difference I’m aware of is 1)	MPO uses the Q function, while AWR uses the advantage function2)	MPO uses Retrace, while AWR uses TD(\lambda)The use of Retrace may not be important because Retrace can also be used for AWR. If we remove the baseline from AWR and uses Retrace instead of TD(\lambda), is it equivalent to MPO? Please correct me if I misunderstand anything. Other questions:  I do not clearly understand why AWR is suitable for off policy learning with static datasets. Please provide the rationale. Minor points  Although the “simplicity” of AWR is claimed several times, I’m not sure whether AWR is really simple because the difference between AWR and MPO is not big.
Reject. rating score: 4. rating score: 5. rating score: 5. rating score: 7. <BRK>This paper proposes an algorithm to improve the robustness of reinforcement learning. The experimental results show the proposed algorithm indeed outperform the respective baselines here (single adversary training and domain randomization) in its ability to generalize the other test domains. In that case, the failure is due to the learner policy is not even solving maximin problem. I think, it s probably because of this misunderstanding, the authors motivate the issue of the single adversary training as the agent would overfit to the single adversary. Because it uses multiple randomly initialized adversarial policies, it may have a higher chance to overcome the non convexity issue in the min part of maxmin and  therefore has a higher chance to find the maximin solution. In fact, when given the learner policy, I believe by further taking the min among the multiple adversaries here and uses that for the learner update (i.e.the leaner would not use trajectories from all but the worst one), the robustness of the algorithm might further improve. * Experiments Given this work is lacking theoretical insights, I expect more experiments to be done to verify the proposed algorithm. In figure 2, I think a fairer comparison should let agents of both sides face the same adversary. Nonetheless, I agree that the performance plots later on are sufficient to show that RAP is better. However, I m wondering if the bad performance is due to that the learner s policy is not expressive enough.<BRK>Summary: This paper proposes to improve robustness in reinforcement learning via a population of diverse adversaries, where previous works mainly focus on the use a single adversary to mitigate the problem that the trained policy could be highly exploitable by the adversary. Specifically, at each iteration, it randomly selects an adversary from the population for rollouts, and it is trained by PPO. Experiments are conducted on 3 MuJoCo environments in comparison with vanilla PPO, domain randomization. Strong points: Using a population of adversaries to improve robustness in RL is interesting. The idea is simple, and the writing is clear. A side effect using a population is that RAP needs to update n adversaries at each training iteration compared with using a single adversary, and will incur more computation overhead. c. Could authors compare with a naive extension of the single adversary case in which the single adversary sample n actions? A number of works using population based methods are built upon off policy algorithms as agents in the population can share the samples and could be beneficial. e. For Figure 3, the performance gain over using a single adversary is not significant on HalfCheetah and Ant, and the results is not convincing enough to support the claim.<BRK>This paper extends the existing work on robust adversarial RL by training multiple adversarial agents from a population. Solid experimental results are presented to show that the proposed method improves the single adversary setting and domain randomization. The experimental results in this paper seem solid to me. My biggest concern for this paper is that the conceptual novelty seems quite incremental. One can augment all the adversary networks as a big network and then the problem formulation is the same as before. From this perspective, there is not too much conceptual novelty, and the main contribution of this paper is doing some more detailed study showing how to combine augmentation and adversarial RL.<BRK>#### SummaryThe authors present a scheme that can be used to train agents to be robust against a population of adversarial policies, in which adversaries can perturb actions via an additive perturbation. Motivated by the observation that agents trained against a single policy may overfit to that policy and hence will lack robustness to new/unseen policies, the authors seek to show that their method generalizes well to unseen policies at test time. While the motivating example WRT different forces acting on an agent may not be a scenario that robustness against adaptive perturbations can handle, the general claim seems to hold water. The formulation for RAP is presented very clearly. The latter case would lead to a lack of observability for both agent and adversary. It seems that the claims of the paper could be strengthened if more environments/tasks were considered. There are a few weaknesses, as I enumerated above, but even still I think that this is a valuable contribution.
Reject. rating score: 5. rating score: 5. rating score: 6. rating score: 6. <BRK>However, there exist some issues that need to be solved. +ves: + The exploration of long range interactions for point clouds is interesting. + The paper is well written. In the introduction part, the authors describe many tasks that rely on point cloud presentation. 3.Point cloud is indeed important for many tasks as described in the introduction part, but the authors just explore the effects of the proposed algorithm in a "synthetic" experiment. The experimental results are not convincing for readers, the authors should conduct more real world tasks to verify the effectiveness of the proposed method.<BRK>Pros:   Presents a long range convolution layer, with an efficient implementation so that a neural network model can benefit from both short and long range interactions among data points. There are no experiments with real life point cloud data. The authors state that the proposed method could be a useful tool in a wide range of Machine Learning tasks. However, in the paper, only estimation of potential energy for Coulomb particle configurations is demonstrated as an application. The paper presents a way to incorporate long range convolutions in neural networks, which could be beneficial.<BRK>Edit: I was unaware that papers could be submitted to arXiv simultaneously, I am sorry for that. An efficient method for fitting long range style interactions in point clouds is presented. This 2 steps strategy consists in training the short range part of the kernels well with a lot of (supposedly inexpensive) short range data, and fitting the long range kernels with less data in a second time. Overall, the paper is clearly written and clearly exposes the methods used.<BRK>The paper proposes an efficient long range convolution method for point clouds by using the non uniform Fourier transform. Overall, the paper is clearly written with high quality. The originality of the paper seems to be not very strong since it directly adapts the NUFFT to this work. Besides that, there are several concerns about this paper that need to be addressed:1.
Reject. rating score: 4. rating score: 5. rating score: 7. <BRK>This paper proposes a novel controllable Pareto multi task learning framework, which aims to learn the whole Pareto optimal front for all tasks with a single model. The motivation is straight forward and the proposed method is inspiring. 3.Some notations are not clear, e.g., does the loss used in the paper denotes empirical loss?<BRK>This paper proposes a controllable Pareto multi task learning model by generating the Pareto stationary solutions. Even though the idea to generate a Pareto stationary solution seems interesting, the proposed method is overstated. It is well known that the MGDA can find Pareto stationary solutions, which however are NOT Pareto optimal solutions. Authors claim that their method can generate Pareto optimal solutions but actually they can generate only Pareto stationary solutions at most. Authors confuse the Pareto optimum with Pareto stationary solutions and this is misleading.<BRK>The paper proposes a method to controllably generate models on the trade off front of multi task learning problems. The experiments are a bit disappointing. The hypernetwork can be trained along with the MTL in an end to end manner. + The idea of using a hypernetwork in the context of MTL is new.
Reject. rating score: 5. rating score: 5. rating score: 6. rating score: 7. <BRK>This paper proposed ALANS, a semi neuro symbolic learner specifically designed for improved systematic generalization on RAVEN dataset. The operators are shown to have improved systematic generalization performance. I have doubts that the proposed method can be readily adapted to PGM, not to mention other types of reasoning tasks. Clarity:In general the paper is written in a clear and understandable manner. The authors only show the marginalization for the attribute  number . Summary:Overall I think this paper propose an interesting approach that improves systematic generalization on RAVEN datasets.<BRK>Could you discuss this issue with the result in the paper? Pros: 	The authors provide a specialized solution for RPM well with CNNs and linear models. Due to the lack of the information, the readers can not directly compare them with respect to the previous viewpoint.<BRK>It would be nice to discuss this in the Related Work section. The general idea of the paper is easy to follow but that does not mean the proposed method is trivial, far from it. Experiments are clearly described and the authors give particular attention to split their dataset to systematically test for generalization. This would give an estimate on the advantage of using CNNs as neural modules.<BRK>Experimental results show that they exceed by a marginthe previous models used for this task. The task is interesting, in fact some research has been done on trying tolearn intelligence tests in various ways and a number of the relevant onesare properly cited in the paper.
Reject. rating score: 4. rating score: 4. rating score: 5. rating score: 7. <BRK>** ProsThis may be a mechanism for capturing the fact that generalisation performance will depend on the structure of the data and not just on the complexity of the function space of the learning machine. In the motivation (paragraph 3 of section 2) we are invited to imagine a random function with a very good fit to the training data and a low complexity measure. A linear separable function will with high probability have a much lower generalisation error for a perceptron than learning a completely unstructured function, but the complexity provides guarantees on the generalisation gap not on the generalisation performance. I am deeply puzzled by your comments that traditional bounds are optimistic (e.g.Remark 1). What is the learning scenario when the label generating function is not fixed. I think there is potentially a lot to be gained from this approach. The generalisation performance does depend on the problem being learned and not just on the architecture of the learning machine (or complexity of function space F). With a clearer explanation of what these theorems are telling us I believe this couldbe an important contribution to the field, but in it present form I think the paper adds more confusion than light.<BRK>Evidence (i) is clearly too trivial to be detailed, for example. This new measure acts like a joint entropy and leads to tighter bounds on the generalization error in this setting. The main idea is to extend the classical complexity measure of Barlett & Mendelson (2003) by introducing a new function space: the generator space is defined as the function space of all possible LGFs satisfying ad hoc constraints. Evidence of these results is reported in the supplementary material. 15.Many superfluous parentheses in the demonstration of Theorem 2 do the reading of the latter tedious. –As it stands, this work does not seem to me to be ready for publication. In particular, I am quite surprised that Sections 2, 3 and 4 are not subsections of the introduction (Section 1). 2.LGF is not properly defined, whereas it is a central notion. 10.As noted above, Appendix A suffers from a lack of context. Besides, there is no mention of this appendix in the text s body, other than in the outline. I think the authors meant to refer to Section 5.<BRK>This paper studied a novel perspective on generalization error bounds, by introducing the "label generating function "(LGF). The properties of the measures and generalization error bound with respect to these complexity measures are studied. However, there may be uncountably infinitely many function \tau s satisfying g(\tau(z))   g(z). In this case, how is I_G(z) in (5) defined? And the definition R_m^I(F, G) and R_m^D(F, G) rely on z _i in I_G(z_i) or z _i not in I_G(z_i), but what is the distribution of z _i over I_G(z_i) (for finite I_G(z_i), countably infinite I_G(z_i) and uncountably infinite I_G(z_i) respectively)? What is the relationship of this corollary and Theorem 2 4?<BRK>##########################################################################Summary: The paper provides an interesting perspective to view generalization error for the machine learning model. In particular, it proposes to investigate the constraint on the label generating function space. They propose a concept of co complexity analogous to the entropy ish concept which measures complexities between two function spaces. ##########################################################################Pros:  1. For me, the problem itself is of significance and interest. The experiment can be better designed to verify the theorems. Probably the proportion of data generated by invariant transforms can be set with different levels so that it can be verified that the second term in co complexity reflects the change of R_d. (3) Remark 4 is not coherent to Theorem 5 very well.
Accept (Spotlight). rating score: 8. rating score: 8. rating score: 6. rating score: 2. <BRK>This fact has been fairly well known, but apparently not widespread enough. Having an ICLR paper published on this issue will help spreading the fact, which is significant on its own. Proposed DASALC framework is quite simple and uses mostly standard techniques, and this is an advantage as a reference point. Still, DASALC significantly outperforms previous neural LTR approaches. Also, although the idea of applying these standard techniques on LTR seems straightforward, but I argue that s only due to the benefit of hindsight; neural LTR has been a fairly active area of research, yet these techniques haven t yet been widely used in LTR literature. Clarity: The paper is quite easy to follow.<BRK>I both enjoyed this paper and think it s a strong contribution to the ranking literature. It was well written, clear, and nicely organized. The appendices are full of useful experiments. Sec 3.2   Hyperparameters for neural networks   learning rate and batch size are usually crucial for neural networks, but are not tuned for any of of the baselines (as far as I could tell). LightGBM is very fast.<BRK>SummaryIn this paper, the authors study the problem of neural LTR models. Pros:This paper discusses potential reasons why neural LTR models are worse than gradient boosted decision tree based LTR models, and uses empirical results to show the effectiveness of the proposed solutions. It seems that some feature engineering work can help improve performance. Comparing to traditional gradient boosted tree based LTR models, is it really worth putting efforts into studying neural LTR models?<BRK>It first conducts a set of experiments to show that GBDT outperforms some neural rankers. The resulting neural models perform on par with the state of the art GBDT models. I think this paper establishes an unfair comparison between GBDT and neural based models. As known,  neural models are good at learning great representations from the raw inputs, such as audio, images, and texts, while GBDT models are good at dealing with sparse features. A more fair comparison could be having the neural models to learn feature representations, which will be concatenated with the normalized sparse features. Finally, the technical contribution of the paper is also quite limited.
Accept (Poster). rating score: 7. rating score: 7. rating score: 6. rating score: 6. <BRK>This paper studies a graph coarsening strategy where a new way of assigning weights to a coarse graph is proposed. Based on the observation that better informed weights enable us to obtain better Laplace operators for coarse graph, a GNN based weight adjustment method is proposed. Overall, the paper is well written, and the contributions are concrete. The Laplace operator is considered to be one of the most important operators because it can explain much about the graph structure. It would be great if the authors can explain how the proposed method can be utilized in downstream tasks.<BRK>The experimental results demonstrate the effectiveness of the proposed method. Recommendation: I think contributions of this paper on graph coarsening are new and technically solid. The authors propose a new way (GNN model) to do graph coarsening. (2) A new framework is proposed to learn better edge weights of the coarse graphs by using a GNN model in an unsupervised manner.<BRK>The paper studied the problem of graph coarsening in the context of data driven deep generative models. The paper is well motivated by providing extensive theoretical analysis to support the rationale of the proposed model. The proposed model is developed based on GAN, which automatically learns a mapping function from the fine grained graph to the coarse grained graph. In general, I believe this paper is well written, and the results are strong. In particular, the authors claimed that " we are the first to propose and develop a framework to learn coarse graphs with GNN in an unsupervised manner".<BRK>Comment before review: This submission seems to use a different margin. As far as I know, the proposed method is novel. The description of the proposed method does not appear until page 5 (aside from the introduction). I recommend the authors to condense the discussion in section 3.1 through section 3.3 and focus more on their own contributionStrength of the submission:  the proposed method is novel  the proposed method demonstrates empirical improvement Weakness of the submission:  My main concern with this submission is that it lacks an understanding of the proposed method. This paper points out that learning based method can further reduce the eigenerror by assigning better weights to the coarsened graph, which I am convinced. The author should consider using linear model or simpler models like MLP to learn the edge weights. Given the superior performance of GNN over MLP, I am more convinced that the usage of GNNs in this application is justified.
Reject. rating score: 5. rating score: 5. rating score: 6. rating score: 7. <BRK>This paper analyzes the convergence of decentralized SGD with asynchronous updates, quantization and local updates, which is novel and challenging. 3.The authors have done extensive analysis of the convergence under different settings with detailed proofs. 4.The authors have done some large scale experiments and show their algorithm performs great in practice. 2.The algorithm is very general, several existing algorithms can be its special cases by different choice of parameters. Step size requires the knowledge of the number of total steps. Number of total steps needs to be larger than $n^4$. For example, if we optimize the convergence rate in Theorem 4.1 over $H$, the best choice is $H   \Big(\frac{\lambda_2^2}{r^2} \cdot \frac{f(\mu_0)   f^*}{L^2 M^2} \Big)^{1/3}$. That is, the optimal $H$ is smaller when $r$ is larger. If set $H \to \infty$, then this bound should reduce to the single machine SGD. Does it work if $T$ is greater? 5.Definition of $T$ is confusing. 6.Arguments for acceleration is not convincing. The algorithm only have one pair of nodes communicate, it s not clear how to replace $T$ with $nT$. As of the current version, the proofs need to be improved. However, I believe the authors can improve in the next version. I think there will be one term that depends on $\rho^2$. It may be better to remove some small terms to make rate more clearer. For example,	   For Theorem 4.1, use $1 \leq \frac{r^2}{\lambda_2^2}$ can get rid of the constant $1$. 3.The 1st equation in Section E has an extra    . However, if the analysis can not explain why more local updates can reduce communications, I would not recommend to accept.<BRK>These techniques include asynchronous, decentralized, or quantized communication. The authors prove that this combined algorithm converges to a local optimal point. The authors claim that this is the first work to consider decentralization, local updates, asynchrony, and quantization in conjunction. Overall, the contribution of this paper is relatively marginal. (1) The first two paragraphs of the introduction look wordy. How these techniques are applied and combined is also unclear. (3) In Theorem 4.1, the assumption that T> n^4 (n^4 can be very large) is the disadvantage of this algorithm because the same convergence rate O(1/sqrt(T)) has been achieved without such assumption in some distributed settings, including plain distributed SGD, federated average, etc. (5) In the theoretical part, I did not see in which measure does this new algorithm excel the existing ones. The authors should clarify this. In the experiments, the objective function value of interest is not compared. (6) On page 2, the authors said “SwarmSGD has a Θ(n) speedup in the non convex case, matching results from previous work which considered decentralized dynamics but which synchronize upon every SGD step.” What is the measure, is it the number of communications, local SGD iterations or gradient evaluations? “Matching results” can be interpreted as equal to the previous rate, which seems to contradict with Θ(n) speedup. (8) The authors use multiple variables to denote the number of nodes, including n, P and m. Please use only one.<BRK>These techniques include asynchronization, local updates, and quantized communication. Combining these techniques into decentralized SGD is new to the best of the reviewer s knowledge. It requires an r regular graph. That is, the number of edges connected to one node is the same for all nodes. This condition is very difficult to satisfy in applications. Therefore, the application would be limited too. The quantization part is limited comparing to the other two parts. What does the effect of quantization on the convergence rate and the communication cost? Therefore, there is still synchronization in Alg. 1.Whether is it possible to update one node based on the results from multiple connected nodes (i.e., one node is activated)? Algorithm 2 is unclear. avg  is computed but not used. What are j  and  i ? ## UpdateThe authors  response addresses some concerns, and I would like to keep the initial scores.<BRK>Three extensions of the algorithm are also proposed to relax different constraints, while maintaining the convergence guarantees:1. synchronous updates and decentralized data: if the number of local gradient updates $H_i$ before an edge update is constant, convergence guarantees hold for decentralized data, as long as partitions are i.i.d.from the original distribution;1. asynchronous updates: the number of local gradient updates $H_i$ can vary between nodes and between every edge update;3. reduced communication: model exchanges can be quantized to reduce communication complexity. Experiments in the distributed setting are carried out for image classification and speech recognition, showing that the algorithm is generally able to achieve performance comparable to a model trained in the centralized setting at increased execution time, but faster than state of the art distributed SGD methods. They consider several settings, which are all theoretically founded. However, the paper is generally hard to follow, also because it has many contributions that are cited in the main text but deferred to the appendix. ### Remarks on theoretical analysisTheorem 4.1 shows that the average second moment of the loss gradient evaluated at the average model $\mu_t$ is bounded and decreases with $T$, proving that the model updates converge to a local minimum. This bound however stands for the average of all models obtained at each global step $t$, meaning that it is not necessarily a tight bound for the second moment of the last obtained model, which is the bound we are ultimately interested in. It would be also interesting to report communication complexities, with and without quantization, and compare them to state of the art methods. ### UPDATEI thank the authors for addressing my concerns and confirm my initial rating.
Reject. rating score: 5. rating score: 6. rating score: 6. rating score: 6. <BRK>This paper presents a methodology for incorporating factor graphs into model based and model free RL methods. The work starts by assuming access to a correct and factor graph showing the relationship between individual state factors, actions, and rewards. The authors propose to make use of this factor graph by using a Factored Neural Network   which is similar to the standard feed forward MLP networks that would typically be used to parameterize a policy or Q function   except that it masks out connections between input and output nodes that are not connected in the factor graph. The authors demonstrate how these factored NNs can be incorporated with model based MCTS as well as model free DQN and PPO. In short   the algorithm remains unchanged and the only substition seems to be the Factored NN rather than a fully connected NN. The authors provide the manually defined factor graphs used for each of these environments in the Appendix. It s also nice to see that this approach   because it only modifies the underlying NN   is straightforward to integrate with many of the existing model based and model free algorithms. In my opinion, the main drawback of this work is that many of the domains examined are either poor representatives of RL problems would ever be faced by an RL agent or simple enough to not require neural network function approximators. In summary   the approach is reasonable, the results are impressive on toy domains but less impressive on realisitic domains. is not defined anywhere. Does it refer to the parents of a node? I have read the authors response and updated paper and appreciate the discussion of applications that admit factor graphs.<BRK>This paper looks at how to build deep RL agents that can perform more efficient learning by directly leveraging the factored structure of problems when that information is given. The authors detail a version of MCTS that can leverage independent components of the state, a version of DQN that can leverage independent components of the reward, and a version of PPO that generates actions based on independent state components. I very much like the idea of making connections with the factored MDP literature. However, as the authors point out in the conclusion, an approach that actually discovers the causal factors of the problem would be much more interesting and generally applicable. That said, it was not clear to me how common this setting is where the agent can directly leverage known structure about independent aspects of the state and reward like this. The experiments felt a bit contrived to me, which I think at least partially reflects this inherent difficulty of  finding an environment that naturally fits the setting the authors explore. I am very much on the fence about this paper as I like the overall direction as a stepping stone towards a model that discovers factored state representation as well. However, I think the paper could really benefit from more evidence either theoretically or empirically that the factored neural network structure is always likely to lead to improvements when this structure is present. For example, could you say anything theoretically about the way that this leads to sample efficiency improvements during learning? I also agree with some of the points they made about the complexity of domains that they considered in their experiments.<BRK>The graph factorization of state, action and reward components is realized via an adjacency matrix that adds masking across input and output components. They evaluate factored RL approaches for both model based and model free RL algorithms:  MCTS, DQN, PPO. In the case of model based approach (MCTS) the state transition function is factorized. For model free approachs, DQN & PPO, decomposed Q functions and policy functions are used. Strengths & Weaknesses:The authors detail the scope of the problem domain and embed their approach well in the existing literature. For me the greatest weakness of the paper is that there was not a great deal of insight given with respect to how scalable this approach might be in terms of other RL domains such as for instance in navigation or planning and the impact that a factored approach might have on representation learning where experience across diverse parts of the environment may be important to learn how to encode information over a diverse set of experiences. Furthermore, the factorization algorithm needs to be  defined by hand for each task which may incur additional complexity and pose problems when the aim is to optimize for generalization in RL agents. However the authors do state this explicitly and note that a future direction for this work could be in online discovery of factored graphs. Other Points:Figure 4a only contains one curve (Presumably this should be FactoredDQN rather than DQN?)<BRK>D4RL: Datasets for Deep Data Driven Reinforcement Learningreview:summarization:In this paper, the authors consider using factored neural network (NN),instead of directly using forward NN in model based or model free reinforcement learning. 2.Extensions on PPO, MCTS, DQN is studied and evaluated,making the paper quite comprehensive and the conclusion proposed more robust and convincing. 3.The paper is well written and in general easy to understand. The figures in this paper are very helpful as well. 4.The algorithm is also applicable to high dimensional problems such as humanoid. Cons:1.The algorithms were not combined with existing state of the art algorithms,which includes algorithms such as SAC, TD3, MBPO. It would be interesting to see if the proposed method can lead to state of the art performance. 2.Does it mean that the structure has to be manually specified for each environment? 3.The paper reminds me a lot of NerveNet [1],which also considers factored or graph NN in reinforcement learning. In International Conference on Learning Representations.
Accept (Poster). rating score: 7. rating score: 7. rating score: 6. rating score: 6. <BRK>Summary: This work builds on the vulnerability of VAEs to adversarial attacks to propose investigate how training with alternative losses may alleviate this problem, with a specific focus on disentanglement. I wish the paper would make a stronger connection between disentanglement and robustness. The paper is well written and the underlying reasoning is easy to follow. 2.The experiments are sound and well documented (results are reported across latent space dimensions, and adversarial attack parameters)Questions:1.<BRK>The paper considers the problem of training VAEs which are robust to adversarial attacks. It shows that learning disentangled representations improves the robustness of VAE. The paper then shows that using hierarchical VAEs can ensure robustness without sacrificing reconstruction. The problem considered is interesting and relevant. 2.The paper uncovers some interesting phenomenon about VAEs such as links between disentangled representations and robustness, and tradeoffs between disentanglement and reconstruction accuracy.<BRK> Summary:They proposed a robust method for the adversarial attack on VAE using a hierarchical version of $\beta$ TCVAE and conduct analysis on the relationship between disentanglement and robustness to support their choice of approach. The paper is well organized. 3.They demonstrate that the proposed method is more robust to other VAE baselines for the attacks.<BRK>Because it has been known that disentanglement behavior is related to the dimension of latent space. 2.In section 3.2, the paper empirically demonstrates the connection between disentanglement and adversarial robustness. The paper demonstrates their results in the benchmark datasets considered in the disentanglement and computer vision literature. Secondly, regarding "noiseness", it is not explained what extra would disentangled version of VAEs (e.g., TCVAE) provide compared to the standard setup.
Accept (Poster). rating score: 8. rating score: 7. rating score: 4. <BRK>### **Evaluation**  The paper is tackling the topic of explainable Deep One Class Classification. This fact facilitates the interpretability. The approach is well motivated and compares well to the state of the art AD methods. The paper provides sophisticated theoretical as well as empirical insights. However, in figure 18. the output for "toothbrush" is shown a huge anomaly blob for all samples, while the ground truth shows just minimal areas. Why is that the case? The paper provides a good and detailed description of the theoretical concepts. **   the conclusion is quite sparse, and even missing an own section.<BRK>This paper presents a one class classification method using a fully convolutional model and directly using the output map as an explanation map. The method is dubbed FCDD for fully convolutional data descriptor. * a qualitative study showing that the explanation provided by FCDD is more useful compared to other methods. CONS: * The novelty is incremental: Combining fully convolutional CNN with a hypersphere classifier. *  The paper also describes a fixed kernel (Gaussian) upsampling method using strided transposed convolutions, which I find unnecessary to be added to the paper.<BRK>########################################################################## Summary:This paper presents a modified approach, on top of an existing method (DSVDD), for anomaly detection with a state of the art achievement in the unsupervised setting. Lastly, as the authors also called out in the last session, it s not robust enough to overcome spurious features, which can be big problem for real applications.
Accept (Poster). rating score: 7. rating score: 6. rating score: 6. rating score: 5. <BRK>This paper addresses the active learning paradigm in which the learner queries an oracle to obtain the class label of some inputs. Depending on the querying strategy, the learner can improve its classification model more or less efficiently. To achieve long run convergence of MOCU to zero (and thus get obtain the optimal classifier), the authors propose a so called weighted strategy that solve this issue. The results indicates that the proposed strategy seems to provide good results in a wider range of situations as compared to SOTA. The major drawback of the paper is that the scope of the proofs is very limited. The proof (unless I am mistaken) also do not account for approximation errors incurred by replacing expectation with empirical versions. A few comments are provided in the appendices concerning multi class problems. Should this be understood as the myopic strategies standpoint ?<BRK>Summary:This paper provides an interesting algorithm to address the previous Bayesian active learning query strategy in (binary) classification. In experiments, the proposed algorithm can achieve the advantages of ELR and BALD simultaneously. Reasons for score:Overall, I vote for the marginally above the acceptance threshold. However, there are some lacks in addressing the problem of ELR in a qualitative manner. The assumptions of theorems look strong. Pros:1.This paper was well motivated by the drawbacks of ELR and insightful comparison of BALD and ELR. In Bayesian approaches, these issues can be interesting and valuable. The stuck in the convergence of ELR can be due to the lack of considering the long term effects.<BRK>This paper studies the label solicitation strategy in active learning. What does that mean? The readability is improved. The paper’s finding on the existing ELR method is interesting and novel. 2.The theoretical analysis of the convergence of the proposed method seems to be sound. So I encourage the authors to further demonstrate the nice properties of the proposed algorithms in more realistic settings in the future. A similar question is also, for certain data there is a larger gap between the proposed method and ELR (in appendix). 5.The visual presentation can be improved for the paper, as well as the explanations.<BRK>The authors of this paper introduced a new acquisition function of active learning for optimal Bayesian classifier. The new query strategy is based on mean objective cost of uncertainty, defined as the expected difference between losses of the optimal Bayesian classifier and the optimal classifier. I think this paper can benefit from revisions to improve clarity. The lack of clarity makes it hard to appreciate the interestingness of the proposed approach. Perhaps section 3.1 needs to be segmented into more subsections. Then, the authors can have more space explaining the intuitions behind the proofs and the newly designed weighted MOCU. It is nice to see the convergence analysis, but it also makes me wonder how useful it really is. 6, While it is unclear how useful the theoretical guarantees are, it is also unclear if the empirical results should enough evidence.
Reject. rating score: 5. rating score: 5. rating score: 7. <BRK>After discussions:I read the author s response and other reviews. The authors made a considerable effort to address my concerns. As a result, the paper has improved and I increased my score. My main concern is still the accessibility/readability of the results in this paper, which I think can be further improved for the benefit of both the community and the authors (more accessible paper  > more imapct). Summary:The paper studies the approximation power of equivariant neural networks in a very general setup: the input space is assumed to be a function space (compared to a finite dimensional space in standard setups) and the group is assumed to be any locally compact group. This is then used to derive universal approximation theorems from the well known results on fully connected networks. Strong points:Important problem   invariant/equivariant models provide a very helpful inductive bias for many tasks on symmetric inputs. The first time you see the contributions are in a short list at the end of the section. It is very hard to understand them this way. I think that the basic idea of this subsection can be written in a much simpler way. Discussing the invariant case might also help. I would also suggest rewriting the paper in the following way: start with S,T finite and present all the results. General comment: *Please add examples*   this will help the reader follow the paper. Perhaps choosing a particular case of S,T an G and exemplifying any result in the paper on this setup can be useful. Relation to prior work is not discussed. What is the relation of theorem 13 to the results obtained by Maron, Keriven and Ravanbakhsh? Specifically, these works show that in some cases high order tensors are needed for universal approximation by invariant neural networks   can this result be recovered from your results? I recommend a significant revision of the paper to make it more readable.<BRK>Second, infinite dimensional fully connected networks (FNNs) and general (equivariant) convolution neural networks (CNNs) are described. The main result of the paper is the Conversion Theorem (Theorem 11), and its consequences. The theorem specify the conditions under which an FNN can be approximated by a CNN. I think the general CNN formulation and the Conversion theorem are of merit but i think the paper should undergo a rather serious revision before ready for publication. The main issues (that are detailed below) are: the paper does not sufficiently relate the discussed model or the universality results to previous or concrete models (set and graph NN, equivariant group NN, other unused but potentially useful variations), it does not provide sufficient explanation and justification to the different conditions in Theorem 11, there are some details in the proof and the description of Theorem 11 which are missing/unclear, Theorem 16 has some unclarity. I think attending these will provide a much more accessible and useful paper for the community. DETAILS.> Relations to previous work. I think exploring the relations to existing generalization to convolution neural networks and equivariant networks is in order. Does theorem 1 implies that Deepsets (as equivariant model) is universal (as was proved in several previous works)? This example could help the reader grasp this extension and also relate to previous works on equivariant learning. What can be said about the equivariant tensor models and graph neural networks using the universality result in this paper? First, looking at the proof, I feel there is a condition of the FNN $\phi$ that is missing from the theorem s formulation. How is it guaranteed that $\phi$ can be extended if internal states are defined on non base domains? Can you provide the proof for a simple example of equivariant networks such as Deepsets? I think some examples and explanation should be provided. I could not really figure out, despite the following paragraphs, in what situations can we verify this condition and in what sense it limits the scope of the theorem. > In Theorem 16: How  do we make sure the first layer can be seen as a generator? This also relates to the question I asked above about the condition in Theorem 11. > The authors mention that the invariant case cannot be handled. However, invariant functions can be made equivariant by considering the trivial representation, e.g., in the discrete case let $f(x)$ be invariant to $S_n$, then $f_i(x)  f(x)$ for all $i\in [n]$ is equivariant I believe. Can we approximate this equivariant function?<BRK>The paper proves a universal approximation theorem for equivariant maps by group convolutional networks in an extremely general setting. The proof applies to discrete and continuous settings, including infinite dimensional ones. First, it is shown that an equivariant map is determined by its generator, i.e.its values on the orbits (so far, not surprising). This means that one only has to prove universality for the generator. Universality is then proved for the generator by a fully connected network, and separately an approximation theorem of FCNs by CNNs is proved. Proving a result of this generality is technically very challenging. For instance, the orbit space can be topologically very complicated. Having spent about a day with this paper, I think that at least the general idea of the proof is sound, and the authors have sidestepped several technical problems I could think of using well chosen technical assumptions. Universal approximation theorems are considered to be an important kind of result, and this paper proves a very general one for equivariant maps. Update:I do agree with the other reviewers that the paper may be difficult to read, especially for non experts.
Reject. rating score: 4. rating score: 5. rating score: 6. rating score: 6. <BRK>This paper explores the effective dimensionality of the Hessian as a possible explanation for why neural networks generalize well despite being massively overparametrized. While I concur with the intuition, I think in the current state of the paper, some points could be improved and clarified. ### Relationship between Hessian and posterior covarianceWhile you mainly reason in the Bayesian framework about the posterior, it seems that networks in fig 1, 6, 7 are trained using ML. So why would the Hessian of the ML estimator relate to the covariance of the posterior? ### Theorem 4.1Theorem 4.1 shows that even with $k \gg n$ parameters, there are only $n$ directions in which the posterior covariance changes from the prior. But the rest of the discussion shows that the effective dimension actually decreases, which is not captured by your theorem. I however think that relationship between the hessian of ML estimator and the covariance of the posterior, as well as the empirical study, should be improved before this is published.<BRK>summary: This paper provide a unified view of the generalization ability in the Bayesian deep learning framework through the effective dimensionality. The authors claim that some phenomenon in the deep learning such as generalization in #parameter >> #data settings, and double descent can be explained by the effective dimensionality. For example, in Figure 2 abd 7, the effective dimensionality shows a very different behavior from test loss and test error when the width is small. In other words, effective dimensionality does not seem to account for the first descent in Figure 2 and Figure 7 (although it follows the second descent well).<BRK>**Summary**: In this article, the authors revisited the idea of *effective dimensionality* as a complexity measure for large scale machine learning systems, and in particular, modern deep neural networks. **Strong points**: The authors revisited the idea of *effective dimensionality* as a complexity measure for large scale machine learning systems, and in particular, modern deep neural networks. Insightful discussions were made on the connection between the proposed effective dimensionality and the double descent phenomenon, width depth trade off, function space homogeneity, and other norm  or flatness based generalization measures. The paper is in general well written. * are the (Hessian) eigenvalues assumed to be all **positive** in the definition of effective dimensionality? This may not be the case for neural networks. * "Therefore, effective dimensionality explains the number of parameters that have been determined by the data": at this point (of the article), it is not yet clear to me how the effective dimensionality defined above is connected to the data. It seems to me that the proposed metric is more "accurate" for (nearly) interpolation models (i.e., models with zero or low training loss): this is also seen at the bottom of the left plot of Figure 2 where the effective dimensionality (with high training loss) is low, while it is not the case for test loss.<BRK>Effective dimensionality is the number of parameters determined by the data (derived from the curvature of the posterior at the MAP estimate), and shown to be more informative than simple parameter counting. The authors argue that double descent is an artifact that can be understood by studying the effective dimensionality of the model. They take a detailed look at width depth trade offs using numerical experiments. Effective dimensionality helps us understand width depth trade offs in deep learning. Effective dimensionality is a metric for generalization solely based on the training data. In both proofs: Why not argue using the SVD of the feature matrix? Validity for neural nets is only hypothesized and demonstrated empirically. Is this correct? * Page 6, Equation 3: It would be helpful to explain all symbols (i.e.$p(\theta_{MP}|\mathcal M)$ is the prior evaluated at the MAP estimate...) Central results (Figs.1 and 2) are already shown early in the paper and later explained in more detail... Why do you restrict the effective dimensionality to a maximum of 100 in most of your experiments? An instance where your ambiguous use of "effective dimensionality" leads to confusion can be found on page 5: "For Bayesian linear models, the effective dimensionality of the parameter covariance is the inverse of the Hessian"   it s not clear to me what you mean by that... Page 2: "we expect models with lower effective dimensionality to generalize better"   why?
Reject. rating score: 5. rating score: 5. rating score: 7. rating score: 7. <BRK>This paper proposes a method, called Causal Screening, to improve the interpretability of GNN. Basically, the proposed method adds each edge in GNN in a greedy way by evaluating its causal effect on the prediction. The experimental results show the improved performance of the proposed method compared to others. Pros:Overall, the presentation is clear and the idea is interesting. To mitigate this issue, besides the forward phrase, the authors may consider adding a backward phase to remove spurious edges. It is different from the normal formula of the potential outcome estimation. I do not understand why the authors estimate the mutual information between a constant (a particular graph) and the prediction. Post rebuttal:Thanks for the feedback. By optimizing Eq.(3), the learned graph is not guaranteed causal, so I would like to suggest the authors not emphasizing "causal".<BRK>This work proposes to explain graph neural networks from a causal effect view. Strengths:+ Explaining GNNs is very important and interesting. This work focuses on the causal attribution view, which is a good direction. Weaknesses:  The first concern is the motivation of this work. The motivation is coming from the running example in the introduction. It claims that other method leads to confounding association that (shorts, on, man), and (man, has, hand) are correlated with the prediction, rather than causing it. For interpretations, the method needs to explain the model. How do we know (shorts, on, man), and (man, has, hand) are not causing the prediction? The technical contribution may not be enough for ICLR publication. The computational cost of the proposed method is very high. Update after rebuttal I have read the authors  rebuttal. I am increasing my score to 5 as some of my concerns are addressed. 2.I still believe the proposed method is very straightforward and the novelty is limited.<BRK>How does this choice affect the goal of finding causal edges? 5.What is an interpretation of a z representation of a node? Also, it is unclear if the main results are obtained using the cluster based approach or not. **Minor technical comments**+ the qualitative figures are interesting in that it shows the potential ways that the proposed approach can better find the key connections compared to the baselines. **Originality**While similar methods have been used in the image domain, to the best of the reviewer’s knowledge, the work is original in the context of graph networks. It’s better to start the “Task Description” section by something similar to “we define a graph of interest …”. The paper is significant in that the approach is fundamentally different from the baselines and that the results are qualitatively and quantitatively different or better. Although the results on accuracy are not surprising, they are still useful for applications that are interested in finding substructures responsible for a certain prediction. 3.While the approach can generally be expensive, the time complexity of the cluster based approach seems to be comparable to GNNexplainer and CXplain. This is in fact quite commonly useful in many applications dealing with graph networks such chemistry, biology, image understanding, social networks, etc. However, the mere improvement over the baselines is not surprising since the proposed explanation method directly optimizes an objective to mimic the full graph’s function. That being said, the results are quite convincing such that it empirically validates the approach. What clustering algorithm has been used for the method? I believe it is improved in the revised version, so I increase my rating. If so, what are the results of the cluster based method for w.r.t.contrastivity and sanity check? What number of clusters is used for table 1, if it is cluster based. The distinction between “irrelevant edges” and “redundant edges” should be important here if the goal is to find the “causal” subgraph.<BRK>To be precise, it starts from an empty set as the explanatory subgraph, and incrementally adds the edges, testing them for the individual causal effect. Strengths: The paper is well written. I also like the idea to do cluster by cluster screening where edges across two clusters serve as a super edge. I also agree that causal attributions of edges of edges are more reliable than information from gradients, often used to explain models. I also liked the running example which clarifies the contributions. Weaknesses: It is interesting to see what is the value of eq.4 in practical applications. I see that doing clustering can leverage this effect. One edge which is quite strong can impact the behaviour of the whole cluster. I guess that it is interesting to study individual impact of edges on the clusters.
Reject. rating score: 6. rating score: 6. rating score: 8. rating score: 8. <BRK>In this paper, they provide the risk bounds of  kernel ridge less regression (the regularization $\lambda \rightarrow 0$) based on the CV_{loo} stability. They show  that the interpolating solution with minimum norm is the minimal bound of CV_{loo} stability, and can be controlled by the condition number of the empirical kernel matrix, which establishes an  elegant link between numerical and statistical stability. Pros:  1:  The stability of Tikhonov regularization has been well studied, but the study for the unregularized regression probelms is lacking. This paper fills the gap for unregularized regression. 2:  They provide an upper bound on the stability of interpolating solutions, and show that among all interpolating solutions, the minimum norm solution has the best test error. They further provide emprical experiments to verify their theoretical findings. So, we think the upper bound proposed in this paper may be too loose.<BRK>The paper establish the average leave one out stability bound for the interpolation solutions, and show the above bound depends on condition number and spectral norm of kernel matrices. The authors establishes a nice connection between numerical and statistical stability. A nice property is that among all interpolation solutions, the upper bound on stability achieves the minimum at the solution with the minimal norm. The paper is clearly and well written. In Theorem 7, the authors show that the stability can be bounded by the spectral norm of $K,K^\dag$, the condition number of $K$ and the norm of $y$. This means that the upper bound is vacuous and may not explain the true generalization behavior of the interpolation solutions. If the upper bound is loose, then even if the interpolation solution with the minimal norm achieves the minimal upper bound, this may not convincingly show that it outperforms other interpolation solutions. I would suggest the authors to take a close look at it. 3.Lemma 5 is standard in the literature.<BRK>UPDATE:As the authors were already aware of the zero loss case and analyzed this previously, I am confident that the authors can address this to the point in an updated version. This paper shows that the minimum norm interpolating solution is optimal (among all interpolating solutions) with respect to a derived bound on the expected leave one out stability, and thus also optimal in the same sense with respect to the excess risk. ####################################################Pros:The paper is well written, to the point, and technically (mostly!see cons) sound. That doesn t mean that any of the theory is wrong, but that creates two problems in my opinion:1. The story about leave one out stability does not make sense anymore. In fact the expected leave one out stability is just the expected risk for interpolating solutions. 2.I would imagine that most of the results can be simplified because of that. ####################################################Additional feedback:  When I first read the title I thought that you wanted to show minimal norm solutions are NOT stable, as it has  minimal  stability.<BRK>This paper investigates kernel ridge less regression from a stability viewpoint by deriving its risk bounds. However, related studies on kernel ridge less regression are still sparse. The present study fills this gap, which, in my opinion, is also one of the main contributions of the present study. 2.The study presented here brings some novel insights into the relationship between minimizing the norm of the ERM solution and minimizing a bound on stability and also reveals the role that the condition number of the kernel matrix plays in kernel ridge less regression, see formula (6). 3.The paper is well presented and well polished. The analysis conducted in this paper seems to be sound. I understand that this condition is common in learning theory but are expecting more comments.
Accept (Poster). rating score: 7. rating score: 7. rating score: 6. rating score: 6. <BRK>This paper revisits the catastrophic forgetting problem that is common in multi task learning. It concludes that the information encoded in the lower layers of the neural networks is more task independent than higher layers, thus freezing the lower layer can reduce the risk of catastrophic forgetting. Another interesting finding is the semantic similarity between tasks can control the degree of forgetting. It also investigates several methods to mitigate this issue, with quantitive and analytical results to test their effectiveness. Various methods are proposed, and the idea of "SubSpace Similarity" is very interesting, which studies whether the model predicts the two tasks with different subspaces. However, the novelty of this paper is limited. It s already well known that the lower layer of the neural networks captures more general knowledge and it s already good practice to only fine tune the higher layers for transfer learning. The following subspace analysis is similar to section 6.4 of Kornblith et al 2019 but with CKA added. Section 5 is an analysis of existing mitigation methods and no new method is proposed. Some conclusions lack evidence. The results of this paper are based on CIFAR data with a few different settings, with conclusions supported by limited diversity of experiments. The findings here would be more convincing with a broader range of tasks / data. The norm of equation (2) is undefined (I guess it s l2 norm because of Cauchy Schwarz inequality).<BRK>Lastly, using analytic models, the paper investigates the connection between forgetting and task semantics and shows that the intermediate similarity in sequential tasks leads to maximal forgetting. The layer wise analysis of catastrophic forgetting and investigation of different mitigating forgetting methods are interesting, and the work is certainly very relevant to this venue. 4.The results can potentially help to suggest new approaches for developing and measuring mitigation methods. The paper misses important discussions that I describe in the section below. 3.The experiments are only done on image classification tasks with CIFAR10 and CIFAR100##########################################################################Questions:1. The conclusion that higher layers contribute to catastrophic forgetting is not evident from the paper s experiments in section 4.1. It would be helpful to have this discussion in the paper.<BRK>In this paper, the authors provide an empirical investigation that the outer layers of a neural network are more responsible for the catastrophic forgetting effect than inner layers. The paper reveals that continual learning techniques (EWC, SI, a replay method) mitigates the change in outer layer image representations, as per these metrics. The paper reveals that some methods cause outer layer features to live in orthogonal subspaces to those of previous tasks, and other methods cause reuse of existing outer layer features from previous tasks. They reveal experiments that show that in a mixup dataset, intermediate levels of task similarity lead to maximal forgetting. That said, it is not clear how general these observations are. For example in the EWC Paper (Kirkpatrick et al.2017) it was observed that permutation tasks resulted in significant changes to inner layer representations (as measured by Fisher information) relative to outer layers. At the very least this paper should reconcile or comment on this difference in observation with the presented results.<BRK>5  It seems that the paper is written hastily. This analysis suggests that the deeper layers become more dissimilar when training is performed on a new task. There are a few typos. The paper then studies the effect of different mitigation strategies for catastrophic forgetting and shows that for experience replay type approaches the performance is retained by keeping the subspaces of different tasks from overlapping from each other, whereas, in the regularization type approaches the performance is retained by keeping the subspaces similar and resuing the previous feature mappings. Finally, the paper studies the effect of task similarity on catastrophic forgetting and shows that the intermediately similar tasks are the ones that are most prone to forgetting. **Positives**1  The experiments are done thoroughly and different hypotheses are explored in detail. 2  I quite like the task semantics part of the paper(Section 6). I thank the authors for their detailed rebuttal. Instead of looking at which layers contribute to more forgetting, it should be which layers forgetting effect the most. In the former case (continual learning case), the solution of the previous task should inform on the next task and I expect the earlier layer weights to have small changes. In most deep networks, on image classification tasks this norm decreases as the gradient backpropagates to earlier layers. Am I understanding that correctly?
Accept (Poster). rating score: 7. rating score: 6. rating score: 6. rating score: 6. <BRK>This paper describes a system for separating "on screen" sounds from "off screen" sounds in an audio visual task, meaning sounds that are associated with objects that are visible in a video versus not. The results, while leaving certain aspects of the optimal training program underspecified, demonstrate that the system is useful. It would be possible to evaluate on completely synthetic mixtures, although this is perhaps outside of the main contribution of the paper, the audio visual combination. The paper presents an interesting approach to solving the on screen vs off screen sound problem in audio visual source separation.<BRK>Summary of the paper:This paper proposes a multi modal sound source separation framework in which they aim to separate on screen sound. The proposed method extends the recent unsupervised source separation framework MixIT by conditioning video input. Strength: This paper extends the existing source separation approaches to enable multi modal source separation. If the notation appears for the first time in the paper the authors must explain what it is. “The concatenated visual embedding is resampled, fed through a dense layer, and concatenated together with all convolutional blocks.”  > resampled how? 2.Why does Global video embedding have to be an input for On screen classifier?<BRK>This paper proposed an unsupervised method for open domain, audio visual separation system. However, there are several concerns with this submission that need to be addressed first. My main concern is the contribution of this paper. The authors presented a fairly complicated system comprised of several modules. do we need it in both audio and video? A question for the authors, since you treated this task at an unsupervised task, did you try to run some subjective evaluations?<BRK>But from two cents, these are related to the model. *Experiments*Currently the authors only evaluate the model on their own dataset. The authors introduced a new, large scale, open domain dataset for on screen audio visual separation. How does the model work on existing datasets? The dataset will definitely be very useful for the community as it is way more diverse than before. Otherwise its very difficult for people to do an apple to apple comparison of this work and prior work.
Reject. rating score: 4. rating score: 5. rating score: 5. rating score: 6. <BRK>In this paper, the authors introduce a NAS technique with an adversarial component. This is quite a nice idea. The connection between Algorithm 1 and Algorithms 2 seems fairly tenuous to me (although I could be wrong). On a formatting issue, the text in the paper doesn’t look right compared to other ICLR submissions (it’s too pale). It would be worth looking in to this. The paper is otherwise fairly well written. I would recommend crediting the authors and not the institution. The details regarding training and the generator architecture are relegated to the appendix. These are very elaborate, which makes it very difficult to tell what is exactly contributing to the algorithm working. Table 7 seems to indicate that the discriminator itself can be removed for quite a small change in mean accuracy (94.2ish to 94.1ish). Table 9 in the appendix (minor note — it’s not really an appendix when it’s in a separate file!) This is unreliable, as when deploying these algorithms in the wild we are far more interested in how they do in expectation (particularly if they fail completely some of the time). Although the idea of using an adversarial framework to tell apart architectures in a search space is nice, the implementation has many moving parts, and doesn’t appear noticeably different to the standard REINFORCE NAS approach (which just has a generator as an RNN). The presence of a discriminator has a very minimal effect, which is a shame. Some evaluation choices are very questionable.<BRK>This paper proposes a Neural Architecture Search algorithm (GA NAS) based on adversarial learning. The generator constructs architectures auto regressively, which receives feedback from a GNN discriminator. Disclosure: I’m only vaguely familiar with the neural architecture search literature. Pros:1.GA NAS appears to consistently find high ranking architectures compared to the baselines, often at the cost of fewer queries. 2.A fairly extensive set of experiments are included in the Experiments section and Appendix. 3.GA NAS is able to incorporate constraints into search. I’m concerned that a GAN based approach is therefore limited in its search ability. The assumption that such a large number of “good” architectures are available ahead of time seems rather strong. 3.The authors state that previous work has determined that other architecture search methods are not any better than random search. The gains in accuracies of the architectures produced by GA NAS over the baseline seem moderate at best. Where in the f GAN paper is it stated that the JS divergence is more robust than the assymetric KL? 2.Is X_0 counted in the number of queries reported? BANANAS has the same accuracy value as GA NAS. 3.Why is the variance of GA NAS’s accuracy so small in Table 1? 4.Table 2 caption could be more descriptive. I’m also concerned about the validity of the assumption that an initial set of known “good” architectures would be available to a search algorithm; the authors should clarify how these were selected. I keep my score. It also appears that the authors significantly misunderstand VAEs. The difference between GANs and VAEs is not JS divergence vs KL divergence. Using a KL loss for adversarial learning does not require switching to a VAE. Given the central role they play in this paper s motivation, a better understanding of these subjects is important.<BRK>A flexible but complex and expensive NAS method. Summary:The authors introduce a method for NAS that repeatedly trains a generator to sample candidate architectures. The method is evaluated on three NAS oracle benchmarks as well as constrained NAS settings. While there are some promising experimental results, I lean slightly against acceptance due to poor presentation, limited comparisons on most evaluations, and what seems like fairly limited benefits of the approach given its complexity and cost. The method can easily incorporate constraints on computation and memory. 2.As with most non weight sharing methods, GA NAS requires several hundred queries on each benchmark, which translates to GPU weeks of search time. It is not clear that the benefits over weight sharing methods, which are not quantified for most cases, outweigh this large search cost. For example, random search is just as easy to apply to constrained problems as GA NAS and should be used as a baseline. 6.There is no code in the supplementary materials. The citation style does not follow ICLR guidelines. Why is Q a good metric for speed given that the algorithm doesn’t know to stop after query Q since in practice it won’t know the rank N of the current architecture? 5.Many numbers in paragraph 4 for Section 4.1 do not correspond to any number in any table. 7.Table 4 should include at least one weight sharing method such as GDAS (Dong & Yang, 2019), which is much faster and performs reasonably well. # Post response updateThank you to the authors for answering some of my questions and clarifying the search and evaluation of GA NAS. This paper increases this complexity with a GNN and an adversarial training setup. Use of such additions require showing significant improvements over baselines like random search, which I do not believe is achieved. I thus stand by my initial rating.<BRK>After the revision, the description of the method is clearer (Sec 3.2), and the experimental results are clearer (Sec 4). Summary:The paper provides interesting results for neural architecture search. In particular, this paper proposes a search strategy for NAS problems, Generative Adversarial NAS (GA NAS), using importance sampling, which can be applied to micro/macro, constrained/unconstrained search problems. Pros: 1.The proposed method achieves higher performance to compare to previous methods with better robustness, reproducibility, and efficiency. 2.The idea of NAS based on importance sampling for rare event simulation in the method seems interesting. The proposed method at the same time could be broadly applied to micro/macro, constrained/unconstrained search problems. Cons: 1.I suggest the authors conduct further ablation studies to enhance the understanding of the approach and readability of the paper: (1) Comparison of computational resources (e.g.wall clock inference time) required for each query in Table 1 and Table 3. To be a fair comparison, it would be better to compare [number of queries * resource consumed per query]. (2) For the update algorithm of the generator, the proposed method uses JS divergence minimization referring to [29]. Section 3.1 mentions that JS divergence is more robust than KL divergence, but I think a further analysis could strengthen the point. For example, an explanation about the method and the concept of evaluation metric "rank" should be improved for the readers.
Reject. rating score: 3. rating score: 4. rating score: 5. rating score: 6. <BRK>This paper investigates the SGD with constant step size (SGD CS) on non conex optimization problems. The authors claim in the introduction that convergence of SGD CS on non convex functions is shown. To show convergence of an algorithm, the missing piece of the paper is that, starting from the initialization point, the algorithm can be guaranteed to find such a local neighborhood around the point of attraction. Without this guarantee, the analysis of points of attraction is meaningless in the sense of algorithm convergence. As pointed out by the work [Liu et al.2020], most of the minimizers are not isolated, instead they form a low dimensional manifold. 3: The main theoretical result, Theorem 2, relies on the fact that step size is sufficiently small. I don’t see the connection between the small step size theoretical result and the large step size experiments. 4: The paper frequently talks about large learning rates. Toward a theory of optimization for over parameterized systems of non linear equations: the lessons of deep learning.<BRK>1.What is exactly the overall advantage or difference between the SGDL and the vanilla SGD? Because the paper claims to theoretically justify the claim that “SGD is better”, can authors point out how they justify it theoretically? Does the proof assume that the initial point is close enough to the optimal solution? How can SGDL globally converge (converge from any starting point)? 5.In experiments, the paper uses decaying learning rate, so a large initial stepsize can quickly decay into a small number, so how does this become an advantage? SGD has better generalization which has been observed in many prior works. The main concern of my comments is still not clear.<BRK>This paper studies the smooth finite sum problem under suitable conditions in the non convex case. Based on the results, they introduce a modified SGD algorithm with a large initial learning rate (SGDL), and provide extensive experiments on various popular tasks and models in computer vision, audio recognition and natural language processing. 2, Extensive experiments are presented to show the effectiveness of SGDL. Furthermore, all $\nabla f_i(x^*)$ need to be zero. minor comment: In the first inequality of the proof of Remark 2, why the bound is not zero? Since under the condition $\tau \infty$, $||X_k x^*||$ should be no larger than $\epsilon$. After the rebuttal The authors partially addressed my concerns.<BRK>This paper presents a theoretical analysis of SGD with constant step size (SGD CS) and presents conditions under which SGD CS leads to parameter updates that converge to a local minima, including parameters of non convex functions. The authors then show, in context of some special functions, that the step size can be fairly large yet convergence is achieved. This is followed by a number of empirical studies of SGD with large (but annealed) step size (SGDL) on a variety of tasks. I find the connection of SGD CS with SGDL tenuous, and it is not clear that the theoretical analysis helps in selecting largest possible step size. However, I do find the following contributions of the paper valuable:a) Analysis of SGD CS sheds lights on conditions under which the minimizers (local) of objective functions act as attractor (or strong attractor) of SGD updates. * In Fig.6, the two curves in (a) and two in (b) … are they train and test perplexity results?
Reject. rating score: 5. rating score: 5. rating score: 6. rating score: 8. <BRK>Unfortunately, the description of the experiments is not very precise. The rankings in table 1 are pretty inconsistent between different metrics, and the corresponding figure 3 appears to be cherry picked, as the ScreamdSprites is the dataset where the proposed methods perform best. I also do not agree with the claim that "TCWAEs achieve good disentanglement" on real world datasets. In summary, while the paper provides numbers, it lacks new insight. In a sense, this is also acknowledged by the authors, who merely state in the conclusion that "our methods achieve competitive disentanglement on toy data sets"   that s not much, given the effort that went into the experiments.<BRK>This results in a two regularization parameter objective, whose superiority to existing approaches (using a single parameter) is not clear. Strengths: WAE with a disentanglement term was as far as I now not attempted before, the authors offer two well justified techniques to do it. Weaknesses: (1) The work is very iterative, existing approaches are only combined, (2) Superiority to WAE without this term is not surprising, and I failed to see a clear superiority to competing unsupervised disentanglement approaches. Overall, I tend to think the paper would require a more exhaustive investigation of disentanglement approaches, contextualized to the Wasserstein distance and issues raised regarding marginal versus non marginal divergences. I recommend rejection.<BRK>This paper extends the Wasserstein Autoencoder (WAE) work by splitting the divergence on the variational marginal into 2 terms, akin to what was done in TC VAE. A cursory search indicated the following paper which also addresses disentanglement with the Wassertein Total Correlation: [Xiao et al 2019]. Overall, I found this work to be a nicely complete exploration of a simple extension of an existing framework. They mostly rederive existing methods in the WAE framework, but results are promising and the paper addresses several datasets, compares to baselines well and seems well executed.<BRK>Summary:The paper is motivated by the need for a better trade off between the reconstruction and disentanglement performance of an autoencoder. The proposed solution is to use KL as a latent regularizer in the framework of Wassestain autoencoders, which allows for a natural interpretation of total correlation. The experiments are exhaustive and the results show competitive performance wrt disentanglement while improving reconstruction/modeling of the AEs. If a dataset is of dynamical nature, how difficult would it be to extend the current version of TCWAE to dynamical systems? Do the authors have any intuition/hint on what should change to make their method applicable to dynamical setups? Minor:  Consider changing the naming of the baselines either in tables or figures to make them consistent  Chen et al (2018)  > TCVAE Kim & Mnih (2018)  > factorVAE.
Accept (Oral). rating score: 8. rating score: 8. rating score: 8. rating score: 6. <BRK>Lately in continual learning the focus has been more on NAS type ideas and algorithms, but this work is a nice divergence from this direction. The idea of optimizing in a space orthogonal to the previous task is novel. The execution of the idea is nothing special since it s using standard linear algebra, but I gave the authors full merit to the idea itself. My higher score is mostly due to the fact that the experiments are limited. They should be included as otherwise the superior performance of the algorithms is questionable. See for example:https://arxiv.org/abs/2006.04027Ju Xu and Zhanxing Zhu.<BRK>Summary:The paper proposes one of the most scalable approaches to sequential continual learning with known task boundaries and related tasks, while taking steps towards enforcing data privacy and removing some of the task label constraints. Learning progresses only in directions orthogonal to gradient memory. Several recent evaluation methodologies are used to empirically validate the approach with significant success. Strong empirical results, although not apples to apples in all cases, see the comment posted earlier. Recommendation and Rationale:I strongly recommend acceptance because the method is simple, practical and the paper is well written both in terms of clarity and also analysis. Please discuss in the manuscript.<BRK>Summary:This work targets learning multi class classifiers in the continual learning setting. The key idea is to learn new tasks by taking gradient steps in directions orthogonal to the gradient subspaces marked as crucial for previous past tasks. Review:1.The paper is overall clearly written and the method is adequately described. I believe that the paper is strong and makes a significant technical contribution to the field of CL. GPM shows very little degradation in performance on past tasks and is very resilient to forgetting. So, while it minimizes negative BWT, it also restricts positive BWT. However figure 2 counter intuitively shows GPM to be both fast and memory efficient. Is this because these graphs in figure 2 are per epoch? 5.Lastly, no experiments with a single class per task have been performed. This setting is known to be quite challenging in general and induces significantly more catastrophic forgetting (see Kamra et al, 2017).<BRK>This manuscript proposes a new approach for continual learning problems. The key idea is to maintain bases of subspaces using SVD in the Gradient Projection Memory (GPM), in which the update direction is orthogonal to the gradient subspaces deemed important for the past tasks. Image classification experiment was conducted to justify its better empirical performance in practice. Overall, the paper is well written. I have the following comments. 1.The contribution of this manuscript is kind of incremental compared with OGD (Farajtabar et al.2020).From my understanding, the main improvement is using SVD to store the bases, which basically trades computational efficiency for memory. 2.Is it possible to report running time of the proposed approach and compare with other approaches? It is better to report these values. 5.The literature review is not sufficient. NeurIPS 2020. It addressed most of my concerns.
Reject. rating score: 3. rating score: 6. rating score: 6. rating score: 6. <BRK>For this, they consider the class of Double Q learning algorithms and describe a setting where estimation under this algorithm can lead to multiple fixed point solutions. They also propose another version that uses clipped target values. The two algorithms are evaluated on several Atari games and show, in some cases, improved transient behavior over double DQN. Therefore, it is important that the authors continue to mature this work so that, when it is ready for publication, it will make a significant contribution.<BRK>This paper analysed the underestimation bias induced by approximation error, by formalizing the underlying approximation, they theoretically proved the existence of multiple approximated fixed points which causes the converging to non optimal solution. Besides, they proposed the lower bound double q learning to overcome the underestimation bias. Table1 shows the TD error and the absolute state action value which didn’t demonstrate the small approximation error would cause significant estimation error which would cause the sub optimal fixed points. 2.The effectiveness of lower bound double q learning is doubtful.<BRK>This submission focused on the double Q learning and investigated its underestimation bias issue. The authors claimed that using a double estimator can lead to multiple fixed points. To alleviate this issue, the authors proposed a correction approach by using the lower bound w.r.t.the real return. I am a bit confused about the authors  explanation on the multiple fixed points in Section 3.2. Another concern I have is regarding the comparison w.r.t.multi step learning: The authors plot in Figure 3 the performance of n step bootstrapping vs. LB, which I really appreciate.<BRK>The authors claim that double Q learning, which is a well known approach to alleviate overestimation bias of Q learning, can suffer from underestimation bias, and can lead to non optimal fixed points. This paper provides theoretical evidence to support this claim. The main idea of this paper is to add real returns as a lower bound to the objective, and alleviate underestimation bias. Empirical results in Atari domains are presented. Or is this an average of multiple trajectories that start from s_{t+1}?
Accept (Poster). rating score: 7. rating score: 6. rating score: 6. rating score: 5. <BRK>Summary: Well written paper with solid experiments on an extension of two prior works. I have not seen the task of (few shot) recognition and visual reconstruction seen so far. This paper is a good extension of HoloGAN and has some novel points. Conditional version of HoloGAN. Combination of view synthesis and recognition. The authors claim that other models could be combined in their setup, I agree, but the empirical results are below state of the art. Update and final recommendation. Thanks to the reviewers and authors for their responses.<BRK>This paper presents a new dual task of joint few shot recognition and novel synthesis. 3.The experiments on evaluating the view synthesis module is comprehensive and the results are promising. The major contribution of this paper should be the bowtie architecture with the feedback loop, but this has also been well studied in the literature. I will be happy to increase my rating if these concerns can be addressed in the rebuttal period. The authors have addressed all of my concerns. Therefore, I increased my final rating.<BRK>This paper proposes a "feedback based bowtie network" FBNet for joint generative synthesis via a GAN based framework (specifically HoloGAN) and few shot fine grained recognition. The authors propose to use the synthesis network for synthesizing augmented images and additional losses computed by the image classification network along with conditional generation to improve the quality of the synthesized images. The authors of this work should clearly cite this prior work and reframe the novelty of their approach in relation to it. The authors should clearly re frame their novelty and make this correction in the final version if accepted. Hence its contribution is above the acceptance threshold.<BRK>The authors propose a model for joint few shot recognition and novel view synthesis. So I think that the position of novel view synthesis should not be lifted as high as the few shot recognition, and this work should be dedicated to the few shot recognition with a narrowed scope from "joint few shot recognition and novel view synthesis" to "few shot recognition". Synthesizing the intermediate feature maps should be more realistic in this case, because the pixels are mainly for human (reviewers), but the featuremaps are mainly for model (recognition module). In other words, the few shot recognition result should be the only final output, and the novel view synthesis output should only be considered as an intermediate result.
Accept (Spotlight). rating score: 8. rating score: 8. rating score: 7. rating score: 7. <BRK>This paper presents a impossibility result for value function approximation in batch mode RL. This problem arises because the data gathering policy can fail to get data from all states even though the features themselves are uncorrelated. The authors claim that this setting is interesting because these conditions only require polynomial number of samples in the standard supervised learning case, and I agree with that. I think this paper will be a good addition to ICLR.<BRK>The main results shows that realizability and feature coverage are not sufficient to guarantee a polynomial sample complexity. I believe the paper can have a significant contribution to the batch RL/offline RL community. Therefore, I recommend to accept this paper. However, the paper seems to focus on the former and does not have any analysis for the latter in the main text.<BRK>The paper is a challenging one to read, and I can t say I followed the whole proofs, but I understand the results. I m not familiar with this result, and without it, I don t understand a key moment of the proof. ### ConclusionI think it s an important paper, and is highly relevant to the large amount of work that uses off policy samples. ### OriginalityThe paper s results are new, to my experience.<BRK>It s an interesting paper showing provably efficient batch RL is impossible when only two assumptions holding: a) Realizability, and b) Feature Coverage. However, in the hard instance, the important states $s_h^*$ are even not in the support of the dataset (maybe also breaks the Asp Concentrability?), though I understand it s necessary to obtain the exponential lower bound. In conclusion, I argue that this paper is well written and answers the question perfectly with certain assumptions. My doubts may be incorrect, but I m looking forward to the discussion.
Accept (Poster). rating score: 8. rating score: 7. rating score: 7. rating score: 5. <BRK>The paper shows that so called "off policy” methods which, by their naming as such, it is implied that they should work in a sparse deployments environment are, in fact, not suited (and often not evaluated) for this regime. I am unclear on why each training iteration should start with a policy learned from behavioural cloning of the last deployment’s data instead of the model that was deployed which would be available at that time. Figure 4 clearly shows BC to be the better approach In practice but I would appreciate some intuitive reasoning for this (unless this is standard practice). As far as I understand it, parameters such as the number of offline policy updates per deployment or the weight of the KL divergence in the policy update step are crucial to good performance yet the paper does not explain how to choose them without engaging in the true environment. If that is correct than this seems to defeat the aims of the paper and question the overall methodology for deployment efficiency.<BRK>There are several things I like about this paper:  The topic is very important and underexplored, especially the deployment efficency w.r.t.the number of batch collections  Lots of relevant work is cited  The experiment make sense given the research question. Especially evaluations such as in Figure 2. The results show significant improvements of the proposed method over state of the art  The method  is "simple", in a good way, and has not that much moving pieces, compared to other RL algorithmsThe are a few things I think could be improved:  Because the paper considers "repeated batch" it should touch the subject of "if I can collect x batches, what batches would tell me the most?". I.e the algorithm as written right now is "greedy" in the sense that the policy will only act in the way it considers most optimal (and is not completely different from the behavior policy). From this  evaluations like shown in Figure 2 are novel and make a lot of sense. [1] Chua, Kurtland, et al."Deep reinforcement learning in a handful of trials using probabilistic dynamics models."<BRK>The new approach belongs to the family of model based online reinforcement learning algorithms and seems to primarily augment a prior approach, called ME TRPO, by using a helper policy trained by behavior cloning data collected after the most recent deployment of learner policy. The experiment results validate that the proposed approach achieves better data efficiency and deployment efficiency compared to prior approaches. PositivesThe experiments are very extensive and of high quality. A diverse set of tasks is used, along with a good range of prior approaches for comparison. Performance is the best in almost all tasks. It is a great step towards wider application of RL algorithms in the real world by recognizing practically important metric such as deployment efficiency. NegativesThe authors could make a clear statement on the contributions of the paper. The authors could make clear the approach is an extension of ME TRPO, which at present is not in the way the authors proposed the algorithm. While I don’t dispute the merit of deployment efficiency, it can be better motivated. The paper suffers from lack of algorithmic novelty, but the experimental efficiency outweighs.<BRK>The paper has been well written and is easy to understand. The premise of the paper is clear, and methods have been evaluated properly. Suggestions to improve the paper:  While the deployment efficiency metric has been clearly explained, the justification of this metric has not been elaborated on sufficiently. In what situations is deployment expensive? How common are they? However, that is neither discussed nor compared with. Why does a batch size of 100K make sense? Why limit it to 5 to 10 deployments? These numbers appear to be arbitrary. However, it is unclear why the algorithm is better suited to improve deployment efficiency compared to prior works. From the explanation provided in the paper, BREMEN is a good offline RL algorithm and is coincidentally better at deployment efficiency as well.
Reject. rating score: 4. rating score: 4. rating score: 5. <BRK>This paper addresses the problem of how convolutional neural networks (CNNs) achieve translation invariance, and the authors argue that this invariance es mostly learned from suitable datasets, rather than a result of the architecture. Experiment 3 for instance, encourages local translation invariance (within each quadrant), but not across the whole canvas. The paper is well written and can be followed easily.<BRK>* section 3: by "non pretrained network” do you mean the untrained network? Other comments: * The contribution of the paper is very unclear from the abstract, even getting past the first two sections I was still left wondering what I should be expecting to see in the rest of the paper.<BRK>This paper analysis and studies translation invariance in convolution neural networks. Only one network is used for the study, to validate if it is indeed the architectural design the gives translation invariance. COMMENT AFTER REBUTTAL PERIOD:Given that there was no rebuttal, I keep my initial rating.
Accept (Poster). rating score: 9. rating score: 7. rating score: 6. rating score: 5. <BRK>1) Summary  The authors proposed the negative data augmentation technique which is useful for generative adversarial networks, anomaly detection, self supervised learning frameworks. The idea is simple, and the technique was proven that it is powerful for several tasks. They performed several experiments, and I think the experiments were enough to show the technique s superiority. the support of a generative model learned from samples may "over generalize"... I am not sure that the sentence is true. This paper is well written, and concrete, I recommend that this paper should be presented in ICLR 2021.<BRK>This paper investigates how augmenting the negative examples, not just the positive examples, can improve a variety of representation learning tasks. Strengths:A major strength of the paper is its simplicity. In contrast to the number of advances in machine learning that lack intuition into why it works, this paper does a good job at offering some explanations and motivations for the approach. The experiments are convincing to show the generality of this idea. The experiments are on several different datasets. The introduction does a good job at establishing the difference to other data augmentation methods, in particular by using negative examples. Weaknesses:In some cases, the negative data augmentations may actually be inside the positive set. How would the approach scale with noise in the negative augmentations?<BRK>However, the authors need to address all comments of the reviewers and also discuss all missing related works in the updated version. The paper also provides theorems to prove the convergence of the proposed model on GANs and CPC. Overall, the paper is easy to read and the idea makes sense. The paper also misses to discuss and compare with recent works also on data augmentation for GANs. *Generative learning*W2 – The detail of how to incorporate NDA into GAN is not clear. Therefore, theoretically I am concerned a bit about the convergence of the model.<BRK>Experiments are comprehensive across different tasks  The usage of data augmentation seems interesting but with some questions (see below). The most famous mixup is also treated as "positive" samples for training. Is that because only particular augmentation can be used as negative samples, e.g.Jiasaw?The answer to this question is critical. However, the paper does not mention/ study much. For example, in figure 9, the paper proposes to push samples and its jigsaw version away. The proposed method tries to push them away. If justifications of these questions can be sufficient, I think it can be a strong paper.
Accept (Poster). rating score: 7. rating score: 7. rating score: 7. <BRK>This paper analyzes the geometry of several contextualized embeddings. and clearly answered (There s no anisotropy locally). For example, instead of "There is an apparent contradiction." **Minor issues**  "We find a low dimensional manifold in GPT/GPT2 embeddings, but not in BERT/DistilBERT embeddings." Why can t you claim the low dimensionality for BERT/D BERT embeddigs? You may simply state that this a widely used dataset. Please consider rephrasing "experiments"  > "analysis", as you are not conducting _controlled experiments_, but rather performing exploratory analysis of the embeddings.<BRK>#### Summary:The authors investigate the token embedding space of a variety of contextual embedding models for natural language. These include findings of (local) isotropy in the embeddings when appropriately clustered and shifted, and an apparent manifold structure in the GPT models. I think this paper presents a great jumping off point for further research on the subject, as it certainly raised quite a few questions with me. I support its acceptance but would hope to see the authors address some of the questions raised here. #### Positives:  Lots of analysis with several different techniques  Very interesting and relevant subject area  Thorough use of recent related work  The paper is quite well written and organized considering the inherent challenge of writing up research of this sprawling nature.<BRK>Contextual representations are highly isotropic within clusters of the representations. Pros:  The manifold analysis of word frequency is intriguing and intuitive. This is a valuable contribution because it explains previous findings in Ethayarajh 2019 and reconciles them with theoretical expectations. The authors offer no analysis for the difference in behavior between different models. Questions:  The authors claim to select the clusters that maximize MMS.
Accept (Poster). rating score: 6. rating score: 6. rating score: 6. rating score: 5. <BRK>The paper proposes a series of trick for policy extraction from Cross Entropy Method based (CEM/iCEM) trajectory optimizers. The motivation of the paper is clear.<BRK>This paper presents an approach to distill a model based planning expert into a policy to enable real time execution on robotic systems. 2.The paper is well written and motivated. What is the reason for this?<BRK>This paper presents a method to combine zero order optimizer with imitation learning. Overall the paper provides a clear description of the essential components of the algorithms and the result is also quite strong. The paper itself shoes SAC can already perform very well.<BRK>The whole pipeline is not very novel to me. What are the differences between that and the one without policy samples? The experiment results show promising results. CEM needs extra environment steps to search for a solution.
Reject. rating score: 5. rating score: 5. rating score: 5. rating score: 6. <BRK>The proposed solution is based on meta learning, following the path drawn by Li et al.AAAI 2018; the Authors propose to adversarially split the training set in meta train and meta validation sets, and then update the current model in a direction that fosters good generalization performance on the meta test. In particular, I like the idea of finding meta train and meta test splits in an adversarial fashion. This is crucial, since randomly splitting the training set in meta train and meta validation would not be helpful, since it would lead to episodes where meta train and meta test are iid. Could the authors comment on this? The overall writing could be improved. I still do not properly understand how the maximization problem at the core of the method is approached, and I believe that the paper needs some exhaustive proof checking to improve the overall writing. I look forward reading the Author response and iterating the discussion. Yet, I still believe that this work is not ready for publication. Random splitting and adversarial splitting perform very comparably (1% is not a lot), in my opinion casting some doubts on how meaningful the solution found to the proposed optimization problem is.<BRK>This paper proposes to unify adversarial training and meta learning in domain free generalization where labels of source domains are unavailable. Pros:+ The idea of adversarial train/val splitting in meta learning is interesting. + The paper provides an upper bound of the generalization error on unseen target domains, which is implicitly minimized by the proposed method. Major Cons:  The effectiveness of adversarial train/val splitting is not well justified. However, DFAS has the issue that the same domain may be used in both meta train and meta val, which significantly limits the domain transportation and may yield limited domain generalization capacity in comparison with MASF and MetaReg. More experiments and discussion are suggested to justify this point. Additional experiments are suggested to empirically show the difference. I like the idea in general and the problem is well motivated but it needs more work for a complete version. I would encourage the authors to further improve the quality of the paper.<BRK>The paper further gives theoretical bound on the generalization error of the proposed method. ##########################################################################Pros:The paper proposes a new approach for domain generalization. ##########################################################################Cons:The paper needs to update the model parameter, the initialized parameters and the train validation split, which may not converge or converges very slowly. Though empirical results show that the convergence of the method is fast, some theoretical demonstrations are needed for the convergence speed of different updates. The only novelty of the paper is performing meta learning on the train validation split with the largest domain gap, which is an incremental contribution over meta learning based domain generalization (demonstrated by the ablation study). However, L2 normalization (Finn et al., 2017; Dou et al., 2019) is a widely used techniques for meta learning and domain generalization, which is not counted as a novel contribution for the paper. The authors need to further demonstrate that the main contribution: meta learning on the train validation split with the largest domain gap, has huge performance gain. ##########################################################################I lean to rejecting the paper since the performance gain is mostly falls on the L2 normalization but the main contribution of the paper: performing meta learning across the train val split with the largest domain gap, does not have much performance gain.<BRK>The paper provides a novel way to combine meta learning and adversarial training for domain generalisation. Different from existing methods, the authors propose to split the training dataset into train/val subsets in an iteratively adversarial way, regardless of domain labels, by which the model can be trained to learn to generalise well from training subset to val subset via meta learning in each iteration. Reason for score:Overall, I vote for accepting. It’s ingenious that the paper proposes to ignore domain labels to enhance the learned model’s generalization ability towards domain shift problems. 4.The paper provides comprehensive experiments of different domain shift settings as well as theoretical proofs to evaluate the proposed method, which are quite solid and convincing. Cons:1.It would be better if the paper provides more details in ablation study, for example, it mentions that DFAS 3 finds the hardest val subset only based on loss by setting \alpha   0 in Eqn.
Reject. rating score: 4. rating score: 6. rating score: 6. rating score: 6. <BRK>The authors tackle the problem of efficient architecture search for continual learning. They propose to use reinforcement learning based neural architecture search to efficiently expend layers, select which neurons trained on previous tasks to reuse and which new neurons to train from scratch. My main concern is about efficiency, the choice of RNN generating architecture propositions and using the performance of each proposition as a reward can t really be called efficient as it requires multiple (200 in the article) full trainingC on each task. For the controller network training, why using an additional parameter for the exploration instead of directly following the policy parametrized by $\theta_c$ ? I would also like to see how a simple baseline like random search perform, using the same number of models as the NAS approach (i.e.training 200 networks sampled from the search space used by the NAS procedure on each task).<BRK>The (hard) problem of determining how to expand the network is tackled with reinforcement learning, largely building upon a previous approach (reinforced continual learning, RCL). The obvious downside to this approach (and to RCL) is the potentially very increase in runtime stemming from RL, which requires fully training many networks just to solve one additional task. This renders the approach impractical for large models; consistent with this, the authors only study models of modest dimensions. The present paper is mostly an extension of RCL. Regarding training time, Fig.7: while runtime in seconds is important, can the number of (controller network) training iterations of RCL vs. CLEAS be provided as well?<BRK>Summary: This paper presents a new method for continual learning by examining for each new task the NN created so far, deciding for each neuron whether to keep it (with the same weights), and how many new neurons should be added. Pros: They demonstrate improved performance in comparison with 3 previously proposed methods, in particular MWC, an improvement (invented and tested by whom?) over EWC of (Kirkpatrick et al., PNAS 2017), while using about the same number of parameters. Also the controller network needs to be apparently be trained for the whole task sequence.<BRK>The experimental results show that the proposed method outperforms state of the art methods on several continual learning benchmark tasks such as MNIST Permutation, Rotation MNIST, and Incremental CIFAR 100. The authors propose Continual Learning with Efficient Architecture Search (CLEAS), which is equipped with a neuron level NAS controller. The controller selects 1) the most useful previous neurons to model the new task (knowledge transfer) and 2) a minimum number of additional neurons. 2.It is not clear for me the rationale behind the sequential states of neurons and the authors’ claim that “This state definition deviates from the current practice in related problems that would define a state as an observation of a single neuron.”.
Accept (Poster). rating score: 8. rating score: 7. rating score: 4. rating score: 4. <BRK>This work also demonstrates that the learned model has generalization capacity so that the tuned model works on out of domain data. # Pros* An elegant solution to the fine tuning settings especially for the low resource settings. * Experiments are performed extensively on various tasks and demonstrates its effectiveness in generalization for out of domain settings. * Interesting analysis of the experimental results. # Cons* Basic idea is already demonstrated by Li and Eisner (2019), and I was not very surprised by this results. # DetailsIt is a very sophisticated way of avoiding overfitting especially when the data size is limited, and it might have an impact of broader application when exploiting pre trained models. Thus, I d recommend acceptance for this submission.<BRK>The paper proposes a method to avoid overfitting while finetuning the large pretrained models for downstream tasks on small scale datasets. In Section 3, Table 2 clearly shows that the method provides significant improvements under the low data regimes and the model also achieves significant improvements in most of the datasets when tested for out of domain generalization. Weak Points:Since the paper deals with low resource scenarios, I would have really appreciated if the experiment section also included some experiments on multilingual datasets while focusing on low resource languages.<BRK>This paper studies fine tuning BERT like pretrained language models (PLMs) on low resource target tasks. The authors hypothesize that the general purpose knowledge obtained by the PLMs from pre training might be irrelevant and redundant for a given target task. Empirical evaluations on sever datasets demonstrates the effectiveness of the method over previous research. The paper is presented well, and it s a good read. However, with the current set up of plainly applying VIB to fine tune a PLM, I find novelty rather limited. The main results in the paper seem to suggest otherwise, i.e., with a larger model, VIBERT actually has much less room to improve.<BRK>**Short summary of the paper**:The authors apply the Deep Variational Information Bottleneck (DVIB) to a NLP setting, using pretrained BERTas a fixed part of the encoder and fine tune subsequent MLP layers of the encoder as well as an MLP decoder. However, I m rather concerned about the general concept of "fine tuning across random seeds". **Contributions**:  Proposal of the use of DVIB with large scale pretrained models such as BERT in a NLP setting (low significance)  Extensive experiments showing higher generalization and robustness to bias compared to other SOTA regularization methods in NLI benchmarks and low resource transfer learning (medium significance)**Pros**:  The shown results show SOTA results in terms of generalization for a wide range of benchmarks with only marginal increase of model complexity (in terms of # of parameters & training time). **Style**:Overall, the paper is well written and structured.
Accept (Poster). rating score: 6. rating score: 6. rating score: 6. <BRK>Specifically, they use variance of GP prediction as a trigger to control when we should switch to a new action sequence. Strength 1 The paper is written well, and the organization is OK2 The idea of using GP for video generation sounds interestingWeakness1 The way of using GP is kind of straightforward and naive. Where are these modules in Fig.3 ? GP is more suitable to work in the latent space, is it? Moreover, the proposed method is like a fundamental work.<BRK>Can the authors provide some insights? Weaknesses and comments:There are quite a few typos in the writing, especially toward the latter part of the paper. I’d encourage the authors to do a thorough check to ensure the paper is typo free.<BRK>I would encourage the authors to provide a more thorough experimental section. ### SCOREI vote for accepting the paper. In general the paper is clear and well written. The results are positive and overall it seems like a valid alternative to current approaches that will be of interest to the video prediction community.
Reject. rating score: 3. rating score: 4. rating score: 4. rating score: 5. <BRK>The paper is fairly easy to follow and does not have serious presentation problems. The idea of having a hyperspherical latent space [1] is already been studied (and is a famous work), yet the author does not include any discussion with it. 2.There is nothing new I can learn from this submission. It requires extra handing on the momentum. This statement is very vague and is incorrect.<BRK>The authors aim at constructing another generative autoencoder: reconstruction loss plus matching aggregate posterior with prior penalty. For this they use a constrastive loss in latent space. This brings us to the following point:  lack of theoretical guarantees: There are no new theoretical guarantees or insights in this paper. Empirically lowering the FID score on cifar by a few points without any guarantees is imho not good enough anymore for research in generative autoencoders.<BRK>Description: The authors propose momentum contrastive Wasserstein autoencoders (MoCA), which is an extension of the Wasserstein autoencoder (WAE) that aims to match the prior p(z) and aggregate variational posterior q(z) through the use of contrastive learning, as opposed to earlier proposed techniques (MMD, GAN). This may also involve you having to run WAE GAN / WAE MMD yourself, since I assume that you were not able to quote uncertainty estimates from their paper. Even if uncertainty estimates were in the cited papers, I *strongly recommend* to run these experiments yourself to remove any confounding variables from the fact that these numbers were quoted from the literature (since the experimental setup could be vastly different compared to yours). Can a similar metric be derived for your case? What are your thoughts on this? However, I am concerned with the statistical significance of some of the results. I am open to raising my score if my concerns have been addressed.<BRK>**Summary**This paper considers training autoencoders with a hyperspherical latent space distribution. Generations of this autoencoder model are compared against wasserstein auto encoders and several VAEs and evaluated with FID scores on CIFAR10 and CelebA. Please discuss more related work with hyperspherical priors.
Accept (Oral). rating score: 7. rating score: 7. rating score: 7. <BRK>1) Summary  Mixup is one of the representative data augmentation techniques to improve the generalization of the network. The authors proposed the co mixup technique which is novel. Especially, the technique can mix several images with z, and the z is found by optimizing their objective function. It is novel, and the experimental results are convincing. 2) Strong points  co mixup approach was formulated well  clearly outperforms the other mixup like technique such as CutMix and Puzzle Mix3) Weak points  Training is slower than others, even if computing z is fast. How about 1000 images? This paper is a good one, and I look forward to acceptance.<BRK>This paper proposes a new mixup method that encourages diversity among the samples mixed from a minibatch of data in addition to saliency of each mixed sample. The authors formulate two objectives: 1. a BP set function (submodular + supermodular), and 2. a submodular relaxation obtained by modularizing the supermodular component. This approach outperforms mixup baselines on image classification and several other tasks (calibration, object localization, and robustness). The paper presents interesting ideas with impressive accuracy results. My biggest concerns with this work are clarity, thoroughness of experiments, and whether it is too computationally expensive to be used in practice. Significance:  The proposed algorithm s running time may be prohibitive for some applications. In the appendix the authors mention partitioning each minibatch and running the algorithm on each partition to make running time feasible, which suggests that accuracy improves at the expense of running time. Section 4.2 presents the algorithm as having linear running time, the exponential dependence on the number of labels $|\mathcal{L}|$ should be mentioned hereExperiments:  The results sections compare against good baselines across several tasks, but this would be stronger if it compared to non mixup baselines. A more thorough ablation study would analyze each term in the proposed objective function, compare to the mixup baselines applied to m>2 inputs, and compare to the original set modularization method. The paper proposes an alternative heuristic to the set modularization method of Narshiman and Bilmes 2005. The claim that Algorithm 1 is faster and produces better solutions would be stronger with a thorough empirical comparison to set modularization. How much time does co mixup take compared to training the network? EDIT: The author response addressed all my concerns and answered all my questions, in particular that the exponential running time is a worst case bound that is indeed loose in practice.<BRK>This paper proposes a new batch mixup method, co mixup, to improve the networks’ generalization performance and robustness. An iterative submodular minimization algorithm is used to solve the proposed problem through approximation. Promising empirical performance is reported on several tasks. The optimization problem formulated seems to be very reasonable by maximizing the saliency and diversity of the mixup data. The proposed method also demonstrates slightly better performance than other mixup methods. Overall, this is an interesting paper. There are however several issues that are not clear. The authors can clarify the following questions: 1. How to perform training with the generated data? 3.As stated in the paper, the A_c matrix measures the distances between locations of salient objects in the input examples. Does this require object localization? Or it simply computes the distances between the feature locations? Can the approximation guarantee an optimal solution to the original problem? 6.In the experiments, although the proposed approach demonstrates promising results, the differences between the proposed method and the comparison method, Puzzle Mix, are very small. The paper claims Co Mixup significantly outperforms all other baselines. How significant are the differences in Table 1? 7.The paper limits the comparison to mixup methods. How is the performance level of the co mixup by comparing with other regularization based methods? For example, VAT regularizaiton based methods.
Reject. rating score: 5. rating score: 5. rating score: 5. rating score: 6. <BRK>Despite the apparent mixed results, this paper should be a candidate for acceptance. Reading just by the accuracy scores, it seems like CAT often performs worse or about the same as baselines in multiple experiments. Results in section 5.2 lack explanation (i.e.what do the table columns/rows actually mean)  Minor formatting issuesOverall, I think the central problem that the authors are trying to solve is important and their work makes a reasonable contribution towards the solution.<BRK>I am concerned about the fairness of the comparison against baseline methods like MAX/ AVG. E.g.how the baseline models are trained. post rebuttalI would like to thank the authors for their efforts to improve the methods and the draft. Part of my concerns was resolved. For clean accuracy, CAT r did provide a better trade off. For the concern that the comparison to the baseline presents unfairness as the proposed method was designed for the composite attack with a larger perturbation space, I think the author agrees with my point to some extend. I decided to keep my original score deal to the remaining weakness in the paper.<BRK>Such comparisons are missing in Section 4.2 (pixel perturbation and spatial transformations). This seems to be a major weakness of the proposed method. The paper is well motivated with novel technical contributions (Section 3.1) supported by reasonably designed experiments. CAT seems to be more robust to composite attacks but not as robust as MSD on other attacks.<BRK>If there are some theoretical justifications/proofs, it would be interesting to see such discussions (e.g., the composited attack consistently leads to higher classification loss (inner maximization of adversarial training objective)). Although I appreciate authors for their comprehensive experiments, the current results are based on fairly small and easy datasets and it would be still interesting to see the results on more complex datasets such as Cifar 100 or mini ImageNet. post rebuttal update  The authors successfully addressed my initial concerns regarding more analysis and experiments on a larger dataset.
Accept (Poster). rating score: 8. rating score: 7. rating score: 7. rating score: 4. <BRK>In doing so, the authors addressed all of my major concerns. Hence, this paper is not a first attempt. The reason why the proposed representation performs well on these two very specific tasks is not clear and not discussed. Is the final representation significantly different from a mel filterbank? Instead of the proposed title, something like "Benchmark of learnable audio representations on a broad range of classification tasks" would be more truthful to the work.<BRK>For filtering stage, instead of learning all the parameters of the convolution layer, they let the model to learn only center frequency and bandwidth of the filterbanks that are initially assigned with Gabor filters. Then, they further ran a multi task classification experiment to obtain universal audio front ends. And, the results show the proposed learnable front ends is showing some generalization ability on most tasks. I think this contribution is not trivial, so if the authors can add more experiments (or plots) to show the difference between models with and without l2 normalization, then it would be helpful. The backbone model used in the paper is fixed, and showing that the proposed audio front ends shows similar trends with multiple backend models can verify better generalization ability of the proposed approach.<BRK>The paper is easy to read and authors communicate their contribution clearly. Review edit after authors  revisions   Most of my concerns have been resolved in the significantly improved revised version of the paper. However, I believe that the title may be somewhat too general: LEAF is evaluated only on classification tasks, and IMHO that should be indicated in the title as well. Would the conclusions change with a different encoder/head?<BRK>In this work, the authors introduce a learnable front end (LEAF) for audio. This paper is well written and easy to follow. I have summarized my comments below which will help in improving the quality of this manuscript: 1. Typically Mel FBs are utilized across architectures and one of the major shortcomings of the proposed work is the integration of LEAF with state of the art models (for example x vectors for SID). All the audio problems addressed in this work encounter varying levels of noise in the real world. I would recommend the authors test LEAF with different levels of noise. 2.In Table 1, Mel and LEAF has the same accuracy for Music instrument and needs to be bolded.
Reject. rating score: 4. rating score: 4. rating score: 4. rating score: 5. rating score: 6. <BRK>In this paper, the authors introduce a new quantitative diversity measure advocating its usage for generative models evaluation. In a nutshell, to measure the diversity of a particular set, the authors split it into disjoint train/val subsets and learn a DNN to predict the outputs of another randomly initialized DNN on the train set. Then the generalization gap of the trained DNN is computed on the unseen val subset, and the normalized value of this gap (averaged over several splits/initializations) is considered as a diversity measure. (2) The proposed measure is novel, the usage of random networks in a new context sounds interesting. (2) The computation of RND requires several DNN trainings, which is time consuming. Overall, my current recommendation is (4), mostly because of missing a crucial part of related work and unconvincing experiments. The authors report some numbers on Recall in Table 1 but it only shows that Recall is consistent with RND, being much cheaper to compute. Therefore, I do not see any reason to prefer RND over established diversity metrics.<BRK>However, the concept of diversity is related to multiple factors and I don t agree to define diversity independent of memorization: a good memorization of diverse data also contributes to the diversity of models and sometimes good memorization suggests high capacity of model thus may lead to high diversity. In the approach proposed, the memorization concept is implicitly wrapped into the size of the predictor model. But there is no analysis on the effect of the predictor model sizes on the diversity scores. For the experiment section, the evaluations are rather non systematic, only a few categories  RND scores are shown. A table of overall performance will be good. Measuring diversity of models is an important task, while this paper provides some interesting discussion on defining the diversity and proposed method to measure it. but the definition needs a bit refinement and the the author failed to prove that the propose method is a systematic metric on the diversity of models (only showed specific categories is not enough).<BRK>This paper proposes that generalization performance of distillations of random networks can be used as a good metric for the diversity of a data set: as a data set gets more diverse, it should be harder to learn to mimic a random computation on that data set. However, in proposing that RND should be used as a diversity metric, the paper does not sufficiently compare the proposed method to previously proposed alternatives. The paper should establish that the metric is meaningful and useful. 2.How does it compare to previously proposed metrics? A comparison to FID is done, but there are many other approaches for measuring diversity. 3.It is a goal for RND to match human assessments of diversity? It is argued that some complexity (such as noise) is uninteresting and should not be included in a diversity metric   but this seems to imply that the goal is to measure just the diversity that would be interesting or informative to a human.<BRK>This paper applies random network distillation (RND) as a method for quantifying how diverse samples from a generative model are. Intuitively, this is a more difficult task on a more diverse dataset, and so the distillation loss can be interpreted as a measure of diversity. The distillation loss metric is not compared to other diversity metrics. This would help demonstrate that the RND score is better aligned with diversity than other standard metrics. This deserves some scrutiny as the RND is a random feature detector, so it is not clear why it will generally favor semantic diversity. This should be expanded to determine that it generalizes. Overall, while the paper has some merits, it needs to compare its metrics to other available ones to better make its argument. It would be interesting to dive deeper into how context affects the diversity of the generated text.<BRK>RND as a Diversity MetricThis paper proposes a new modality independent diversity measure for generative models and examines this across image and natural language generation. If it’s not diverse, we would expect a large gap. I recommend acceptance. The largest weakness, IMO, is that the work doesn’t do enough study into the importance and nature of the random target network. Notes:* This has obvious failure modes. This paper should address more details about the requisite nature of the target network. Capturing the notion of diversity, distinct from information theoretic measures, is an important property. Questions to authors:* What is the importance of the random network architecture? How does the “architectural prior” impact the efficacy of your approach as a data diversity measure? I found Section 3.2 to be much clearer.
Accept (Poster). rating score: 8. rating score: 8. rating score: 6. rating score: 5. <BRK>Summary:This submission numerically shows that during exploring the neural network landscape,  GD flow keeps increasing the sharpness. As a result, GD with a fixed learning rate will exhibit two phases during the dynamics. Denote by $\eta$ the fixed learning rate. In the first phase, GD follows closely to the GD flow, and it finally converges to a region where the sharpness is roughly $2/\eta$. It is clearly written and the numerical evaluation is also sufficient. It reveals a very complicated dynamical behavior of GD for training neural networks, which has not been systematically investigated before. Cons:The relationship with the previous study on the dynamical stability of (S)GD is not sufficient discussed. A large number of numerical results in [1,2] already showed that the edge of stability happens for the convergent solutions, which implies that the edge of stability must happen at least in the very late phase of GD dynamics. The authors should explicitly mention that the edge of stability was already observed in these previous works.<BRK>This work contributes to our understanding of deep network dynamics   it is a precise and apparently robust phenomenon that was surprisingly not noticed before (perhaps because of the requirement of GD vs SGD). Thus I recommend acceptance. Weaknesses and desired clarifications:  It should be mentioned more prominently that these results are primarily for networks trained with MSE loss   and that a similar but weaker phenomena holds for cross entropy loss. The distinction between SGD and GD seems crucial for this phenomenon, so more discussion would be good. In particular, as noted in the related works, some papers using SGD claim an opposite effect. It would help contextualize this work to relate it to prior works which are consistent (or inconsistent) with this phenomena   especially works studying the Hessian of deep nets. In particular, after the 1st eigenvalue has saturated at 2/eta, does the 2nd eigenvalue also "progressively sharpen" up to 2/eta ? (And so on for later eigenvals).<BRK>The authors refer to this phenomenon of sharpness hovering at or above the 2/\eta bound as optimization on the "edge of stability." Quality and clarity: The work is of good quality. The results are for full batch GD which somewhat limits the applicability of the results to practice where SGD is more common. Fig.11(c) shows that across networks of varying width but trained at the same learning rate, wider networks end up with smaller values of sharpness at the end of training (here, say that all experiments are stopped at the same value of the training loss). I think a shortcoming of the paper is that the strong width (i.e.architecture) dependence of the phenomenon is not fully appreciated or discussed by the authors (e.g.for instance, by discussing in the main text that it only sets in for narrow networks) or investigated empirically. A comment on relation to prior work: the authors write that Lewkowycz, et al.(2020) imply that "actual progress would occur in regions where the sharpness remains strictly less than 2/\eta. (Note also that the paper tends to study wide networks.)<BRK>This paper presents an interesting observation for GD. That is, the sharpness of the learnt model in the final phase of the training (measured by the largest eigenvalue of the training loss Hessian) hovers right at the value 2/\eta while the training loss. The paper is easy to follow. Besides the empirical results in the main body, authors give insightful discussions in Intro and related work section. Specifically, authors propose a novel guess, that GD eventually transitions to “Edge of stability”, where GD can finally succeed with non small enough step size. I have two concerns for this work. 1) Authors did not investigate why sharpness finally hover over 2/\eta. Hope to have authors  feedbacks on this later. 2) Given people use SGD to train neural networks, discussions about the insight from the observation of GD to SGD will enhance the impact of this paper.
Accept (Poster). rating score: 7. rating score: 7. rating score: 6. rating score: 6. rating score: 5. <BRK>The paper makes a significant contribution by providing a theoretical explanation to the observation that resampling is generally more effective than reweighing as a debiasing tool. The ubiquity of the sampling problem, makes the impact of this work significant despite not being very original. The theoretical results look solid and I cannot see any issue with them to the best of my knowledge.<BRK>This paper provides a theoretical investigation into, and comparison between, two forms of correcting biased data: reweighting, and resampling. This is a very interesting problem to consider, and I applaud the authors on their general approach and conclusions, which are novel to the best of my knowledge. The paper is also reasonably well written too. I agree that they add to the paper, but my understanding from the introduction is that it was already known that resampling methods work better in these applications than reweighting. Otherwise, some of these could be moved to supplementary material in favour of further discussion on the theoretical results.<BRK>The setting and the results are very unclear. By observing the behaviour of resampling and reweighting in simple optimizations with SGD, the theoretical results show that resampling tends to be more stable. It would be interesting to see how resampling and reweighting behave with different learning rates.<BRK>Summary:This paper delves into a stability analysis of reweighting and resampling for overcoming imbalanced data in supervised learning. It is also well written, and contains some interesting discussion on their analysis. The main concern I have is why the variance for your results is so low from run to run. This could suggest the problem is too easy, or that there is a bug somewhere.<BRK>The problem to compare re sampling and re weighting during optimization with stochastic gradient descent is very interesting and important. The paper provides two explanations. So the experiments do not validate the SDE analysis. It makes no sense to me to compare resampling and reweighting with a fixed learning rate.
Reject. rating score: 4. rating score: 4. rating score: 5. rating score: 6. <BRK>It would be great to see ablation studies that show the effect of the planning horizon, search budget, and so on, which may be helpful to understand why the proposed method could perform better than the baseline. We can also explore which role of search is more significant between the search for exploration of collecting training samples and search at evaluation time. The value and action models are also trained as same as Dreamer. summary:This paper extends Dreamer, a model based RL algorithm trained through latent imagination, by additionally performing decision time planning in the learned latent space dynamics for action selection. Finally, additional online planning, either a simple rollout or an MCTS, is performed to select an action at each interaction with the environment. pros:  This work shows that performing a search in a model based RL can potentially benefit. Experimental results show that the proposed agent improves the sample efficiency of the Dreamer baseline in several domains. concerns:  The main weakness of this work is the novelty. The analysis of why search is beneficial during exploration seems to be not substantial. What aspect of the search of the proposed method can be helpful for better exploration (or other factors contributing to performance)? It seems that the dynamics of the continuous latent variables are stochastic. Since the number of latent states is infinite, there may be a need for special treatment to make MCTS tractable, but it is not clearly described. (e.g.double progressive widening is used?) Above Eq.(9): there is no definition of $\alpha$. Experiments could have been more thorough.<BRK>This paper proposes to integrate planning into Dreamer. The planning via MCTS is on the learnt latent dynamics and the policy learnt by Dreamer. One of the challenges addressed in the paper is to perform planning on continuous action spaces. The proposed method is evaluated on 20 control tasks from the DeepMind Control suite, and compared against the original Dreamer algorithm, and a baseline planning method that does only rollout simulations. However, the proposed idea is quite incremental. As the domain is continuous, therefore there is a bit challenge on the search tree s representation. Most techniques used in the paper is quite standard from existing works. In addition, there would be helpful if there are more ablation studies to look at the effect of the way MCTS is setup, .e.g.the amount of the fixed actions at branching, the number of simulations, etc. Those settings would affect how deep the policy tree is built, which roughly similar to the setting of horizon $H$ in Dreamer. Given that MCTS can do planning under uncertainty (POMDP), i.e.on inaccurate model estimation, it would be great if the proposed idea discusses on this possibility and could address the problem of performance degradation with a large long look ahead horizon. The experiment results are not very convincing. There are many unfinished experiments. A complex MCTS planning while consuming expensive computation, but performs worse than the baseline Rollout, and sometimes worse than the original Dreamer. More ablation studies might also be needed to make fair comparisons, i.e.while MCTS requires more planning time, could more computation budget be allocated for the baseline Rollout (more simulations or with larger tree settings) and Dreamer (more batch updates for action and value models)?<BRK>The authors extend the Dreamer algorithm to use a different policy optimization mechanism, either a form of Monte Carlo control or MCTS applied online in the continuous latent space at each decision step. This paper combines existing algorithms and compares two variants (“Rollout” vs “MCTS”) on continuous control domains. Experimental results in 20 control tasks suggest that the proposed approaches and Dreamer perform similarly on most tasks. On some tasks, the “Rollout” and “MCTS” action selection approach seems to have an advantage, but there is no clear trend showing that MCTS provides an advantage over the Monte Carlo control in these cases. The number of simulations and rollouts were fixed for these experiments, so it prevents more nuanced interpretation of these results. Overall it’s not clear that the alternative action selection mechanism for Dreamer proposed in the paper has any clear advantage overall. In the cases where there is an advantage (e.g.Hopper Hop, Quadruped Run), we don’t know whether the gains come from having a stronger policy improvement or from the modified behavior policy (which may help exploration), it would be interesting to investigate these questions in more detail. Overall, the paper is clearly written and combines existing ideas in a sensible way, but it’s not clear what the take away or potential impact of the paper is given there isn’t a particularly strong finding that comes out of this paper at the moment. Perhaps the authors could clarify their main takeaway for further discussion. Additional questions:* Is the policy prior for search updated based on the search policy (as in MuZero)? G is used to denote random returns in Eq 10, but V is used in Eq 5.<BRK>##########################################################################Summary:The paper is developed on top of the Dreamer architecture, i.e.learning a latent space dynamics based on the image inputs to train policies. The difference is that, instead of using the already trained policy, this paper used MPC or MCTS to sample actions during the exploration phase to reduce the bias. The authors demonstrated that their approach led to overall improved sample efficiency and final policy performance across many MuJoCo benchmark tests. The contribution, however, seemed minor since it is a small modification to the original Dreamer framework, and the improvement in the performance is not significant. Thus, my rating for this paper is weak acceptance. ##########################################################################Pros:(1) The writing is clear and easy to understand. (2) Comprehensive studies on many experiments to study the effectiveness of their method, unlike many learning papers which only selected a small subset of validation tasks. This gives us a full picture of the strength and weakness of the proposed approach. Cons:(1) As mentioned before, the theoretic contribution of the paper is small. It modifies the SoTA with a minor tweak, and the results are not that significant. (2) In this paper, the observations used are from the original state spaces of the environments. It remains to see if the claim still holds if the studies are executed in the pixel space. ##########################################################################Conclusion:Please add ablation studies in the image space as well and see if the conclusion still holds.
Accept (Poster). rating score: 7. rating score: 7. rating score: 6. rating score: 6. <BRK>The dense prediction means that only some of the predictions can be affected by the adversarial patch, and so the certification process can use this to reason about the robustness at the aggregation level. Opinion:The paper provides a simple, interesting approach, and describes it clearly.<BRK>This paper considers a problem of the defense against adversarial patch insertion attacks for image classification. Overall, I believe this is a strong paper, containing both theoretical and practical novel contributions and I think it should be accepted. Pros:1) The paper is well written and is pleasant to follow. 4) Experimental results are convincing and additionally include efficiency comparison.<BRK>This paper investigates provable defenses to visible adversarial perturbations. Experiments with small adversarial patches on CIFAR 10 and ImageNet point to the efficacy of the approach. BagCert appears to outperform related certifiable approaches to patch attacks. The main boon of this approach lies in the fast certification time, since a constant number of forward passes are required. This defense in its current formulation seems to be quite specific to image classification. In Figure 4, did the authors try to compare against row based smoothing techniques (in addition to column smoothing)?<BRK>This paper presents a provable defense method called BAGCERT against patch attacks which uses an invariant of BagNet for certification. The basic certification process is created by using a novel aggregation function. To further reduce the impact of the adversarial patch, the proposed method uses the certification condition as the objective loss to train the network.
Reject. rating score: 5. rating score: 5. rating score: 6. <BRK>##########################################################################Summary: This paper studies the theoretical aspect of a continual learning method called orthogonal gradient descent (OGD). However, my major concern is about the clarity of the paper (see cons below). The paper investigate an important problem in continual learning framework which is the generalization. In page 2, authors use "CL" for referring to continual learning but it has not been defined. 2  Although the proposed method provides several experiments, there are still many other methods and datasets that have been ignored.<BRK>The paper provides a theoretical analysis on the OGD based continual learning method. The experimental results are also very limited  and weak since it only compares with SGD, an obvious weak scheme that suffers from catastrophic forgetting, and does not compare with any other continual learning baselines. Even though the paper aims for a theoretical contribution, it is very limited only for OGD based scheme, which is not strong in practice. I would like to see other reviewers  opinion as well.<BRK>The primary drawback of the paper is that the authors do not compare the OGD+ algorithm to other continual learning algorithms (synaptic intelligence, elastic weight consolidation, etc.). It is not clear to the reviewer why improving OGD to OGD+ is itself a contribution. Minor comments:(1) Section 3.2: f^* is not defined as of this point in the paper. The primary contribution of this paper is the theoretical analysis of continual learning.
Reject. rating score: 5. rating score: 5. rating score: 6. rating score: 7. <BRK>This paper is also well explained and illustrated. The main weakness of the paper is the experiments. Based on the results, I am not convinced that the proposed W MSE is effective. On more challenging CIFAR 100 and Tiny ImageNet, the results are mixed when compared to BYOL. I m wondering if the authors have tried more positive pairs beyond 4 in the paper. This paper reports results on Tiny ImageNet while the existing methods report on ImageNet. Note that I do not penalize this paper for this point, as computational cost is high for using larger architecture and larger dataset. But it would be good to have a comparable baseline in the middle ground, e.g.ResNet50 architecture on CIFAR 100 dataset, where BYOL reports 78.4% top 1 accuracy with linear eval. Post Rebuttal Update I would like to thank the authors for their rebuttal, which has addressed part of my concerns. After reading the authors  rebuttal and other reviewers  comments, I m still concerned on the weak baselines and mixed results in this paper. Unfortunately, I will keep my rating. Concrete suggestions to improve this paper in the future:(1) Strongly recommended: use ResNet50 instead of ResNet18 for the small scale experiments, in this way you get directly the numbers from the literature (e.g.BYOL on CIFAR 100);(2) Nice to have: for the expensive ImageNet experiments, it would be nice to get comparable results using the smallest comparable architecture (e.g.ResNet50) from the literature.<BRK>This way, similar to BYOL, this paper removes the construction of negative pairs while improving the MoCo V2 slightly on not very challenging benchmarks. The authors can find my questions/comments in the list below. Does this make sense? I would be happy if the authors can comment on this. 3.I think my biggest concern with the paper is the lack of extensive experiments. It would be much more convincing to have experiments on ImageNet which is a standard experiment for unsupervised learning. Even in some cases, CIFAR100, contrastive loss does better than the proposed one. 5.My impression is that the contrastive method in table 1 and 2 represent the MoCo v2. I would replace it with MoCo v2 in these tables as there are many other methods that uses contrastive loss functions. I did not understand the point of using Euclidean distance experiments without normalization? What do they exactly prove? And we can always normalize the embeddings as it is done in the other methods. I would be happy to receive some comments on this from the authors.<BRK>##########################################################################Reasons for score:  Overall, I vote for accepting. Although I have some concerns with the validation I really like the method. BYOL authors presented an extensive experiment setup with several datasets. Why not use the same (at least some) databases and compare them with their reported results on the paper (and not use your own implemented version of BYOL)? In fact, some of those datasets are already used in this paper but as you use ResNet 18 instead of ResNet 50 (used in BYOL paper) the results are not comparable. Validation would be easier with other published methods  W MSE with d 4 seems to work better than d 2. What about larger d? Although the authors point out that the proposed MSE loss does not need negative samples, negative samples are needed in the whitening transformation. Which is the improvement provided by the MSE loss over the classical contrastive loss? The authors say that Contrastive Loss needs large numbers of negative samples. How many negative samples were used in the experimental setup? Minor Comments:  Please check: "On the other hand, Hjelm et al.(2019) have shown that the contrastive loss needs a large number of negatives to be competitive" ##########################################################################Questions during the rebuttal period:  Please address and clarify the cons above  #########################################################################Some typos: (1) Page 4: matrix and (z_i,z_j) correspond  > corresponds(2) Page 7: jitterering  > jitteringI think that the paperUPDATE AFTER REBUTTAL:The authors have covered most of my concerns about the paper and I think that the paper has been substantially improved. However, my biggest concern was about the experiment results and  I think the paper still lacks on validation comparison with other methods.<BRK>The paper proposes to first do representation "whitening", so that the representations are scattered in the space and not collapsing to a single data point; then compute distance metric on top of that (e.g.Euclidean, cosine similarity). Experiments are done on several toy datasets like CIFAR. + The proposed approach seems pretty simple. I haven t run the code to verify the results though. In SwAV, it shows that with multiple crops, the performance can be boosted quite a bit. I haven t tried on BYOL but I believe it could also be helping there. Though it can be viewed as concurrent work (I think W MSE is actually even earlier than BYOL), but the experiment session in the paper is not clear about this. Overall running experiments on these toy datasets are less satisfying, not only because it lacks comparison to other major approaches (like MoCo on ImageNet), but also because the signal we get from smaller datasets may not transfer well to more real world images. W MSE can run much faster because it only needs 4 (or even 1?) This is a potential advantage that W MSE has, but it is not clear from the paper. Other than experiments, I am also not too satisfied with the writing. I would like to see the paper more self contained in the next version.
Reject. rating score: 4. rating score: 4. rating score: 5. rating score: 5. <BRK>**Summary**:The paper presents a novel steps size adaptation for the L BFGS algorithm inspired by the learning to learn idea. I therefore vote to reject the paper in its current form. **Pros**:  It is clear that a lot of thought has gone into the project to come up with the policy. I think it might have merit but requires additional tests. 1) What would be the results of using no adaptation and rely on a step of 1 (or 0.9) constantly as a baseline? The benefit of using the double network for the policy is not clear to me. It might be easier to understand the influence of the policy in such a setting. Is this used in the implementation?<BRK>2.Strong and weak points of the paper    Strong part: 1) The network architecture for the step length policy is very interesting, and may be useful for other problems with similar needs. Week part: 1) The paper s goal is limited to design the step length policy for one specific optimization algorithm L BFGS. 3) The performance test setup is not realistic in neural network model training. 3.Based on the strong and week points of this paper, I tend to reject it. The precision seems different between these two algorithms and the author/s claimed one. Why not use the same precision requirement so that we can compare these algorithms directly? 3) Can this proposed architecture/idea be used as a step size policy for not just L BFGS?<BRK>** DescriptionThis paper makes two separate contributions to machine learning: (1) It proposes a neural network to learn the step size policy of L BFGS and (2) it uses the algorithm for training a deep neural network. For neural network training, robustness to the noise produced by mini batch training is important to understand. The choice of problem was puzzling. Clearly an MLP on MNIST is not representative of modern machine learning. I was left with the impression that the authors were being slightly selective in their choice of problems for showing the utility of their method. I would have liked to see more conclusive evidence in this direction and a clearer discussion of the regime where this method is likely to perform well.<BRK>The paper studies a problem of learning step size policy for L BFGS algorithm. This paper falls into a general category of meta learning algorithms that try to derive a data driven approach to learn one of the parameters of the learning algorithm. The paper itself cites in the introduction:  "...for large dimensional problems this procedure is likely to become the bottle neck of the optimization task". In fact, it is not very clear at all that the proposed method would work on a more general scenario when the evaluation dataset is wildly different from training the dataset. I would be curious to learn what is so special about L BFGS that made the authors chose it. After all, the paper (and L BFGS) deals with deterministic optimization problems on a full batch which limits the applicability of the paper.
Accept (Poster). rating score: 7. rating score: 7. rating score: 6. rating score: 6. <BRK>This work studies the double descent phenomenon in ridgeless regression with deterministic or random features. The work provides a lower bound on the generalization error that requires weaker assumptions than bounds given in previous work and applies to many interesting learning methods. Strengths:  The generalization bound presented this work requires fewer assumptions than previous such bounds while also being stronger. The work is theoretically rigorous and helps to shed light on why various learning methods perform well. The authors thoroughly investigate the applicability of their bound with specific discussion of each of their assumptions.<BRK>The paper studies the phenomenon of double descent for ridgeless regression. These results extend our understanding of double descent and point that under most general settings it is impossible to avoid for ridgeless linear regression. The paper is well written and the analysis and comparison to prior work provided appears thorough. I have not verified all the proofs in the appendix. Thank you to the authors for their response and update.<BRK>Summary:The paper focuses on the theoretical understanding of the so called double descent phenomenon, which may offer insights into the practical success of deep learning methods and has been observed in both overparametrized neural networks and kernel machines. In particular, the authors derive a nonasymptotic distribution independent lower bound on the excess generalization error of the ridgeless linear regression under mild conditions on the input distributions and feature maps. The sharpness of the lower bound has been demonstrated by some numerical experiments. Overall, I vote for accepting.<BRK># ContributionsThis work studies linear regression with feature maps (or kernel regression) without regularization in order to theoretically explore double descent phenomena seen empirically when training over parametrized networks. This paper provides lower bounds on the out of sample error or generalization error caused by the label noise. They also consider analytic feature maps including random ones and thus imply results for random deep neural networks. The main text of the paper is well organized. 3.Or just as importantly are there examples of proof techniques that are used here that are substantially novel over those in the prior work that may be beneficial to the community in general in understanding these over parametrized regimes? Since these two settings are fairly intertwined it would be nice to understand how results in current work compare to the results in this paper.
Reject. rating score: 3. rating score: 4. rating score: 5. rating score: 6. <BRK>The written need improvement in some part of the manuscript. Pros: A new method (GSN: Graph Substructure Network) is proposed to a topologically aware message passing method that better utilize graph substructure information. It is a good idea to pass messages differently depending on their local topologies. In addition, node features are not considered in graph isomorphism, which can lead to incorrect subgraph matching. The experimental results on some of the datasets (such as, MUTAG, PTC, Proteins and NCI1 in table 1) do not appear to be significantly better than those of the previous approaches when considering the variances of different runs.<BRK>The core idea is to count the number of certain substructures, such as cycles, cliques, and triangles. Then the proposed MPNN encodes such substructure counting information into the message passing. Experimental results show that the proposed method can obtain better performance than the comparing methods. + The proposed method leads to better performance on different graph classification datasets. The experimental results can show the effectiveness of the proposed method. Counting different types of substructures can be very time consuming. As mentioned in this work, the worst case can have O(n^k) complexity. When the graph size is large, we may need to count too many types of substructures. Other existing methods, such as graph pooling methods, which can be considered to implicitly encode structural information. Update after rebuttal I have read the authors  rebuttal.<BRK>Then these features are used in a standard MPNN. This idea is interesting and clearly explained in the paper but I think the paper could be greatly improved after addressing the following issues: 1  the authors should clarify their position with regards to invariance. Is it true? c  a similar idea as the one presented in this paper was presented in : Coloring graph neural networks for node disambiguation  by George Dasoulas, Ludovic Dos Santos, Kevin Scaman, Aladin Virmaux https://arxiv.org/abs/1912.06058 [arxiv col] The main advantage of the current paper as opposed to [arxiv col] is to propose an explicit coloring thanks to the structural features. 3  the experimental evaluation is not convincing. Such an ablation study would show the benefit of adding the MPNN on top of these features.<BRK>This paper studies the expressivity of graph neural networks, and proposes a new approach to improve GNN’s expressivity by encoding nodes and edges with features via subgraph isomorphism counting. The proposed solution contains some merit, and the experimental results on graph classification task demonstrates the superiority of the proposed approach. Pros:1.This paper addresses an important problem in GNNs, which is to improve the expressivity of GNNs. Although the paper claims that in practice it is not that bad, the worst time complexity is still high. 2.Discussions on how to select substructures on bigger size of graphs are expected.
Reject. rating score: 3. rating score: 3. rating score: 3. rating score: 4. <BRK>This paper proposes to obtain multi scale features by `resize  > convolution  > resize (inverse)’. Extensive experimental results on COCO validate the effectiveness of the proposed approach. Comparison with recent approaches. Cons:Lack of novelty. The use of multi scale features is not new [a, b]. In object detection, dilated conv in TridentNet is not the only approach using multiple branches. The ablation study in the experimental results did not compare with existing works, like TridentNet, and [c, d] to justify why another multi scale approach is needed.<BRK>This paper proposes a new general feature fusion operation, Multi Scale Fusion Module (MSFM). By adding MSFM layers between feature extraction layers, it is observed that the detection result is improved with minor added parameters. Cons:Major comments:(1) The quality and clarify of the paper needs to be improved, for example, Table 3 has shifted horizontal line. (2) Ablation studies clarifies on the effects of the change of configurations, but does provide much evidence on why MSFM module helps with the detection task. (2) Good to report the variances/confidence intervals of the metrics as well.<BRK>The paper proposes a multi scale feature fusion block and inserts the block into ResNet backbones for object detection. It is very similar to the inception block in IneceptionNets. However, the flaws are obvious as follows. The paper is only a small modification over Inception. The novelty is far below the bar of ICLR. 3.Related methods are not compared.<BRK>In this paper, the authors study the problem of scale friendly feature fusion for object detection. Specifically, the authors propose to process features at each layer of a feature pyramid network at multiple scales and fuse them back into a single scale. Strong results showing significant improvements, around ~2AP, over baselines, including strong detectors like RepPoints. Overall, the paper is very well written. I didn t find any typos or grammatical errors, which is very rare for a thorough reviewer like me. Weaknesses:  The novelty is limited. It would have been nicer to see in Table 4 the details on the type of the backbone used. The authors did not provide a rebuttal but kindly thanked the reviewers and stated their intention for improving the paper with the reviewer comments and submitting it for a future venue.
Reject. rating score: 4. rating score: 4. rating score: 7. <BRK>The PGBN extracts topic information from the input, which is then used to enrich the information that is available to the transformer. Experiments using pretrained GPT 2 and BERT show that incorporating topic information outperforms the respective baseline transformers both in language modeling perplexity and on the GLUE benchmark. Moreover, the topic model augmentation can be applied to pretrained transformers. The model is then evaluated on GLUE, which is not a test of long term dependencies. It remains unclear whether providing topic information of preceding segments is enough to allow the model to draw information from these segments that is useful for a task, beyond mimicking their style. Moreover, I do not find the results convincing that this methods works. I find that it convolutes simple concepts and it could be a lot easier to understand the proposed method if written in a different way.<BRK>This work is the first to integrate the contextualized topic information via a deep probabilistic topic model into the Transformer architecture. 2.Three different types of contextual topic information are provided to capture long range semantic dependencies into the Transformer models. The ablation study is weak. 3.No experiments show the effect of hyperparameters choices from the topic model, such as the number of layers of PGBN as well as the topic number of each layer. 4.Since this work is motivated to capture longer range dependencies, it is not clear how the topic information helps. A comparison of the model performance for different lengths of input sequences would be helpful.<BRK>This paper introduces an interesting idea of enhancing the contextualised word embedding learned by Transformers with long range semantic dependencies via topic learned by Poisson Gamma Belief Network (PGBN), a deep topic model. The experimental results show incorporating topic information can further improve the performance of Transformers. Overall, it is an interesting paper. The ablation study shows that the performance of base models increases with the topic information. Although there are existing works on increasing the size of the input, such as Hierarchical transformers, the authors propose to used deep probabilistic topic model to leverage the semantic information via latent topics.
Reject. rating score: 4. rating score: 5. rating score: 5. rating score: 5. <BRK>2.Algorithm 1 is not complete. The authors should explicitly write down Step 6: w_i \leftarrow SGD(.) I agree that the global PL condition is quite strong for a nonconvex problem and is indeed unrealistic. That is, it should be a problem intrinsic property. It is because the convergence result is in expectation (due to the randomness of the algorithm), in order to use a deterministic local PL condition, a natural way is to bound the iterates within this local region with high probability, but this is often the hardest part. However, as I commented above, these assumptions are too tailored and stringent.<BRK>This paper proposes homotopy SGD (H SGD) which solves a sequence of unconstrained problems with a homotopy map and homotopy parameter. The authors analyze the algorithm for solving nonconvex problems satisfying PL condition. I would suggest the authors to include more "real life" problems to show the merit of the new approach in practice. Overall, I think that presenting H SGD as an alternative to SGD makes the motivation of the method unclear. But in the regular optimization setting, it is not clear why one needs to utilize homotopy. I think such explanations are useful and should be given throughout.<BRK>This paper proposed a Homotopy Stochastic Gradient Descent (H SGD) algorithm by applying homotopy strategy to explore the nice local structures of problems. It is quite novel that authors brought the convergence to a sub level set up. It still remains unclear whether it is acceptable as $\mu$ could be $1e 6$, $1e 7$ as shown by the author. Further, achieving the global linear convergence to a sublevel set is not new, it is even achievable for the widely used SGD with momentum for the quadratic function without any assumptions on it. The global convergence to the optimal solution which also has been well studied in other literatures with or without PL condition, such as,https://arxiv.org/pdf/1812.03934.pdf.<BRK>There is maybe only one   while vanilla SGD and HSGD require a PL condition among a different set of points; there is thus a chance that the "empirical PL" would be better for the HSGD. Main reason to accept the paper : I believe that homotopy is a nice idea that is still under explored in the optimization/machine learning literature. However, one can not know this in advance; and one can not know this even during the run of the algorithm. This is simply not true. Specifically, the boundedness of the variance is very rarely satisfied in practice and is not required for the state of the art SGD analysis under relaxed, strong convexity [1,2]. 5) The theory is not complete. Without such a result, one can not argue that HSGD is better than any baseline, such as vanilla SGD.
Accept (Poster). rating score: 7. rating score: 7. rating score: 5. <BRK>POST REBUTTAL UPDATE: I am increasing my confidence in this paper from 2 to 3   I still believe the paper can use some more clarity but enough points have been explained and updated in the draft for me to feel more confident in my evaluation. I think the ideas in this paper are quite interesting   for this reason I continue to recommend acceptance. Below Eq 3: “the training data may contaion on a few samples of the variable” – do you mean only a few values of the variable may be observed? My hesitance comes on two fronts: I may be lacking some background in the relevant group theory/invariance literature, and it is hard for me to understand a number of important ideas due to the information density in the writing (a result of ICLR restrictions but also some fault of the authors). I shouldn’t have to work so hard to understand the experiments Is that a Reynolds operator?<BRK>Summary:A method is given for training neural networks in the presence of a group of transformations, such that the network weights are invariant with respect to any transformation on the inputs which doesn t contradict the training data. The training method also seems quite lightweight, and shouldn t require significant additional resources to check for the presence or absence of symmetry. Critiques:The explanation of the results has a lot of room for improvement, and I would recommend the authors revise the writing to follow standard best practices, such as defining/explaining new variables when they are introduced, giving the steps associated with novel algorithms, etc. I m recommending a weak accept, but this can be improved by clarifying the presentation and making the results easier for readers to understand. Thank you for adding that, I have changed my score accordingly :)<BRK>This paper proposes an interesting and potentially quite impactful and valuable idea, which I believe is novel. This impedes the flow and sacrifices clarity. The experiments show that the proposed method outperforms networks that are fully invariant or non invariant when the true data is partially invariant. ...Overall, I recommend stripping out some of the mathematical details and using more words and diagrams in the main text to describe the underlying issues/motivations/methods. 2) The framing of the paper seems to oversell the method in a way that makes the contribution less clear. 3) The writing is not very clear. If so, it should be discussed as a limitation. This was confusing. 2) The authors claim that their method can discover invariances without any data supporting them. And their abstract claims: "Any invariance to transformation groups is mandatory even without evidence, unless the learner deems it inconsistent with the training data." If so, this should be explicit. Overall, I found the framing in the work to be "the model discovers invariances by itself without any data!"
Accept (Poster). rating score: 7. rating score: 6. rating score: 6. rating score: 5. <BRK>Summary:The paper proposes a continuous convolution equivariant with respect to SO2 for trajectory prediction. The experiments about physical consistency and the equivariance error answer important questions about the method.<BRK>To me, this paper is overcomplicated as of a solution to the problem. The theory part is well written and easy to understand.<BRK>  SummaryThis paper presents a novel Equivariant Continous COnvolution (ECCO) method for vehicle and pedestrian trajectory prediction. I think the only non equivariant approaches are those one shot approaches that predict all actors in the scene simultaneously.<BRK>The authors present a novel approach to trajectory prediction, where they use rotationally invariant continuous convolutions. Did you have to do some augmentation with scene rotations and imposing of the constraints? Or T in Section 4.5. The explanations are very vague indeed.
Reject. rating score: 3. rating score: 6. rating score: 6. rating score: 8. <BRK>The paper provides theoretical analysis and numerical experiments to characterize the structure of hidden layers and a set of optimal solutions. After warm up with two layer neural networks, the paper provides main theoretical results for deep linear networks and deep ReLU networks. Finally, numerical results are presented to verify theoretical analysis. However, the comparison is not done appropriately in the following sense. These previous results analyze the gradient dynamics of deep linear networks, whereas this paper analyses the set of optimal solutions. The set of optimal solutions is more well studied in the literature of loss landscapes of deep linear networks, where stationary points, saddle points as well as the set of optimal solutions are well studied. What this paper proves would be interesting if these results apply for the characterization of stationary points, saddle points, local minima, or gradient dynamics, instead of the set of globally optimal solutions. However, in the present paper, the alignment of the layers is explicitly imposed by the constraints or the regularization on the norm of each layer’s weight. This is trivial since with this constraints, the norm is minimized with the alignment, of course. The results on deep ReLU networks are also trivial because of the unrealistic assumptions on the data, as I explained in the following. Again, this setting would be interesting if the results are about gradient descent dynamics or loss surface, instead of optimal solutions. Again this might potentially be interesting if this is about stationary points, gradient descent or local minima, because this is nontrivial for those points. The assumption of XX^T   I_n is a strictly stronger version of the assumption of d >  n. Therefore, the results on deep ReLU networks are also trivial. I would recommend the authors to carefully read the papers in the literature of implicit regularization versus explicit regularization, and in the literature of gradient dynamics and loss landscape of deep linear networks. For example, characterizing optimal solutions would be interesting for deep ReLU networks if we cannot easily construct optimal solutions, which is the not case in the setting of this paper as I described above. The explanation with the mini batch is also wrong: optimization problem and the optimal solution are defined for the full dataset, and not for the mini batch. In Figure 4, for MNIST, it has only 60% test accuracy. We know closed form solutions easily for deep neural networks in this paper s setting (as explained above), but this should not work as it is using very strange models so that we can easily have closed form solutions. It is memorizing the direction of the training data and over fitting a lot. Linear models work better.<BRK>  Overview of the paper  The paper theoretically studies the structure of optimal weights in deep linear and ReLU neural networks. Using the convex duality formulation, the findings in the paper includes 1) alignment of the weight matrices in deep linear networks; 2) alignment of the weight matrices in deep ReLU networks when the input is rank one or whitened. Some experiments are provided to verify the theory. Contributions and strength  I think the main contribution of the paper comes from the convex duality formulation of the training problem, which is new. The paper largely devotes to the theoretical results, while the presentation is clearly organized. The setup in the paper removes this non convexity (for deep ReLU networks, certain assumption is imposed to enable the duality theory), which undermines the impact of the work. It is not clear to me what are the implications of the theory proposed in the paper to practitioners. The overview section (section 1.1) is a bit deviate from the rest of the section, in that, the training objective is not the minimum norm training problem in the following sections 2   4. 2.The duality theory requires (X, Y) is feasible, which is a pretty strong condition, given the network is linear. This in turn says the data are noiseless and are obtained from a linear model. An interesting direction to work on is using the dual formulation in Lemma 1.1 to analyze the situation where noise is present. In lemma 2.1, the equal sign is used to indicate the two optimization problems are equivalent, which is a bit confusing (reads like the constraint on the left equals the objective on the right). Figure 2 is not centered compared to Figure 3. ———————— update after reading author’s responseI raised my rating to 6 due to the clarification on assumptions in the paper.<BRK>This work uses dual formulations of Neural Networks with ReLU activations. It starts explaining the dual formulations with simplest single layer unregularised linear neural networks with a single dimensional output layer. Then gradually extends the models to deep, regularised models with ReLU activations. There is also an assumption on the data to be of rank one or whitened. The experiments are limited and not essential, since they only show that the theory can be confirmed with experiments, albeit they also demonstrate the limitations of simplified models studied here. This work builds on a recently published work by Ergen and Pilanci, where more limited NNs have been studied, although with a similar dual formulation approach. The main contributions of this paper are the proofs of Theorems 4.1 and 4.2 (given in the appendix). The theorems in section 3 are also novel, but the simplified case of the results in section 4. While very interesting, these results are not applicable in practice, because as the experiments in section 5 show, the models studied here are too simple. However, this is a good step forward towards building a more complete framework to study better NNs. I did not find the paper easy to read. Several other results have proofs in the appendix, but it is not clear from the main article which proofs are available in the appendix.<BRK>The authors consider training neural networks with a variety of losses and regularization (such as weight decay). The authors introduce a novel convex dual formulation which allows them to characterize optimal solutions as being extreme points of particular convex sets. For multi layer linear networks, the authors prove that the optimal weight matrices have rank equal to the number of outputs of the network, and whose singular vectors align with those of neighboring layers. The conclusions of this paper are strong and apply to a wide variety of neural networks. The extension of linear spline interpolation results from 2 layers to more than 2 layers is a significant contribution. The results of this paper themselves form a substantial contribution to the field and motivate clear followup work to test the performance of the provided formulae to larger and more realistic datasets, along with weakening the whitening assumptions. Analytical expressions for weights of trained deep ReLU neural networks is a significant development, and the authors may want to provide more commentary on its significance (possibly with a small review of the most closely related works in this sense). While I am not an expert in this subfield, there appear to be multiple significant contributions.
Reject. rating score: 4. rating score: 4. rating score: 5. rating score: 8. <BRK>Overall I found the ideas in the paper somewhat interesting. 1) In the abstract, the authors claim that in their first step there is no loss of topological information. Do the results in the paper demonstrate this ? 4) In section 3.3, the authors introduce the orthogonal loss wherein they force the weight matrices to be orthogonal. There is not discussion on this as well. Similarly for Figure 4 for which there is no analysis or discussion. 8) How are the authors deciding on the values of the different hyper parameters i.e.gamma and alpha in the Appendix ?<BRK>The paper presents a nonlinear dimensionality reduction (NLDR) method which is claimed to be invertible. The method inserts an existing idea (LIS) to a two stage neural network implementation. The proposed method is tested with several known benchmark datasets as well as three synthetic datasets. Clearly the classes are more mixed by using the proposed method. The writing is fairly good. The main problem is that the paper is not self contained. * I cannot find the definitions of d_X and d_Z in the LIS loss<BRK>The authors display the failure cases in Fig 4. My major concerns are listed as follows. Moreover, this paper seems to be a combination of previous works (LIS + sparse coordinate transformation), and thus it is important to clearly state the real contributions. 4, Figure 4 is important as it visualizes the embeddings of different methods; however, there isn’t any discussion related to figure 4. 5, There is an interesting phenomenon that the L 1 th layer preserves the most NLDR results, which should be elaborated.<BRK>Proposed idea is indeed novel and interesting actualization of geometry preserving dimension reduction shown in Figure 1. Strengths:Proposed methods combines multiple ideas work on structure preserving manifold learning, invertible and distance preserving sparse representation learning. Each of the steps above are achieved by NN structure and novel loss functions that impose orthogonality, sparsity and isometry constraints and so on. Empirical results on synthetic and real world datasets support the approach and shown efficacy of the method. Weakness: Invertible mapping learned is computed explicitly but can also be learned end to end during training. Can the RIP property provide a lower bound for choosing s.Recommendation:Paper is a clear accept as it introduces a novel method achieve Figure 1 using NN.
Reject. rating score: 4. rating score: 5. rating score: 7. rating score: 8. <BRK>Finally, a novel algorithm is provided as a baseline to solve the CAD reconstruction problem. ### Detailed Review The following is the detailed review of the paper, organized into strengths and weaknesses subsections. This topic is of considerable importance to the CAD community. ##### Reproducibility Since the entire dataset and the baseline is released in an open source programming environment, it should be easy to reproduce and verify the results. Introduction of a new dataset to the research community needs to demonstrate that the tasks to be solved on the dataset is not trivially addressed by the known state of the art. In summary.<BRK>The paper also provides a reference method for sequence estimation based on imitation learning as well as several baselines and their empirical evaluation with respect to the posed benchmarks. ### Strengths**[S1]** The paper is written well and takes a comprehensive look on a new dataset from the perspective of the data itself, semi synthetic generation of new data, evaluation metrics and benchmarking on the data as well as a reference method and its performance analysis. **[S3]** New datasets are often valuable contributions to the community, in particular when they allow the discovery of new insight (however see W1 below)### Weaknesses**[W1]** The main contribution of the paper seems to me the database itself and the associated gym. **[W2]** The submission claims a "novel, neurally guided method", but I do not see the novelty laid out clearly: outside of "using common sketch and extrude CAD modeling operations from real human designs" in section 2, the novelty is vague and the relation to the state of the art imprecise. As it is, I am concerned that the submission is not appealing to a sufficiently large audience at ICLR.<BRK>Furthermore, the paper proposes comprehensive evaluation metrics and a baseline method for predicting the sequence of 3D CAD design from a CAD model. The dataset and environment are highly valuable to the research community. A good baseline method with comprehensive evaluation metrics. This would not be called "raw geometry” as in the abstract. As the aforementioned CAD reconstruction task is not very practical, adding another practical task such as reconstructing the sequence from raw geometry data is highly recommended. Clarification on this is recommended. Is this due to the limited capacity of the proposed network?<BRK>I have not worked with CAD models, and I am not in expert in reinforcement/imitation learning. The paper then introduces a new training/test dataset and an environment for training agents, and evaluates a reasonable set of agents rather extensively. As a slight criticism, I found that too many details are moved from Section 5 to the supmat. As far as I can judge based on my limited knowledge, the paper (together with associated dataset and environment) is likely to spur new research and to be impactful. I therefore give it a strong rating, but I cannot exclude that I have missed some flaws that would be identifiable by an expert.
Reject. rating score: 4. rating score: 4. rating score: 4. rating score: 5. <BRK>This paper presents a method with two goals: (1) estimate if a given tool can solve a given task, and (2) generate a tool that can solve a given task. The networks are trained together, so that task success probability has some impact on the tool encoder s latent space. Results show that the model is largely successful in both tasks, but no challenging baselines are here. What is the geometry loss L_g? What does this mean exactly, and why does it help? My understanding is that this label is available on every training example. When I think of "imagination", I think of interpolating or extrapolating into space that was not quite seen at training time. The task is simple enough that imagination may not be necessary. How about a nearest neighbors baseline? e.g., generate a scenario that requires a tool in between one of the tools seen at training time. Maybe train with long sticks and short L s, and create a task that demands a long L. This would show that the model has a useful latent space and that the imagination (i.e., latent space traversal) is beneficial. The rebuttal appears to state that all baselines and evaluations suggested by the reviewers are irrelevant to the goals of the paper. I see the two main hypotheses clarified in the rebuttal, but I do not see a convincing argument for avoiding comparisons to related approaches to the task at hand.<BRK>The authors try to tackle the problem of tool synthesis by using a classifier to guide the learning and exploitation of a generative model through activation maximization. While the idea of synthesizing tools step by step continuously is interesting, the technical and experimental design make the problem over simplified and thus raise some concerns. The overall concern for the paper is two fold: 1) Some overclaims. For the tool selection task, the task unaware and task driven model achieve similar result, which means information from task predictor is not helpful in this case. In tool imagination task, the gradient doesn t flow back to the learned representation in the task unaware case so suboptimal performance understandable. It would be better if the test setting is more diversified with more degree of freedom in the simulation dataset to showcase the capability of proposed model. How many images of tools are used to train 3D reconstruction model? Is the position/orientation of the tool fixed in the tool selection and tool imagination evaluation? Is it possible that a tool that cannot finish the reaching task at one location can reach the target when positioned at another location/direction? Are there any failure cases in tool imagination and what might be the reason? Although the authors resolve some of the concerns in the rebuttal, there are still limitations in the method and task design.<BRK>This paper explores the idea of tool synthesis in an unsupervised generative learning setting. Tool synthesis is particularly difficult; only one paper was published recently on this particular topic [1]. It has not yet passed the peer review, but I recommend the authors consider citing it for future readers for completeness purposes. Cons:  What is the representation of the task or the tool? This is probably the most significant difference that could tell the present work from the prior work, especially if the results are promising. Instead, the model is more likely to learn an association between the given task and a trained shape (or 3D shape space if learned better). This is a very challenging problem in tool manipulation in robotics (e.g., see [2]). Furthermore, what about trajectories? Even if an algorithm can synthesize the tool and chooses an action, one still needs to properly manipulate tools with the planned trajectory to complete the task. This seems to be completely left out in the paper. Why not directly use 3D meshes as input for the algorithm, instead of using two views to reconstruct? The experiments are far too simple. The authors do have extra space on page 8, but did not include additional results.<BRK>Summary By combining a task success classifier with the latent space of a tool shape generative model, this paper shows that an activation maximization approach can generate tool shapes which can succeed at particular tasks. The architecture is taken from previous work, as well as the method for maximizing activations. For example, how do the supervised affordance learning approaches perform in the tool imagination task when combined with task success prediction? In the tool imagination task, it would be useful to see a couple of things from the task unaware approach to make sure the conclusions are sound. Is the model reaching the same level of feasibility for task aware and unaware versions? Second, what do the imagined tool trajectories look like for the task unaware approach? Reasons for score The topic explored is interesting, and the experiments are simple and illustrative, but there are remaining questions about the baseline comparisons required to make strong conclusions. The rebuttal suggests that there are not many other baselines against which to compare. However, I am still missing details of the latent space generated by the task unaware approach.
Reject. rating score: 3. rating score: 3. rating score: 4. rating score: 6. <BRK>The authors propose a novelty detection module to help unsupervised class incremental learning. The paper investigates the effectiveness of the proposed method on MNIST, SVHN, CIFAR 10, and CIFAR 100. Actually the label technically is not “correct” but assumed to be correct for the class incremental learning.<BRK>As far as incremental classification is concerned, I don t understand the definition of the metric given in section 4.3 and therefore I m not sure I understand what the task is really about. Experiments are there to show the interest of this method for anomaly detection or incremental learning. I find it difficult to formulate an opinion on this paper because I don t think I have managed to understand the detail of what is actually done. Here it is not a data but a batch of data that is considered.<BRK>A set of exemplars is used to evaluate accuracy changes, based on which novelty is determined. The ideas is novel, but I is less scalable and the approach currently lacks key analysis and comparisons with the incremental learning methods and open set approaches. Pros`:+ An novelty detection approach that considers the changes in accuracy of the previous tasks as a new task is learned by assigning a new label to the incoming episode. Only a single approach, BiC, is considered for comparisons. I would recommend authors to check a nice survey on this topic: "Recent Advances in Open Set Recognition: A Survey"  The paper is not well written.<BRK>This paper proposes to tackle the problem of unsupervised class incremental learning, where the training data is composed of a sequence of "exposures". + The introduction of class imbalance works well with the confusion based novelty detection and its contribution is experimentally verified on various datasets. There is another closely related type of incremental learning: unsupervised continual learning. If not, how the proposed method prevents the catastrophic forgetting from happening? In brief, this paper proposes interesting idea of having confusion based novelty detection approach to tackle the unsupervised class incremental learning, but it needs more experiments and discussions to make the paper more complete and ready for ICLR.
Accept (Poster). rating score: 7. rating score: 7. rating score: 7. rating score: 6. <BRK>#################################Summary:The paper presented a new setting of online contextualized few shot learning to mimic human learning. This setting combines continual learning and few shot learning, and additionally considers context switch. Specifically, a learning method is presented with a sequence of samples that might come with labels. An extension of Prototypical Network (Snell et al.) was explored for this new setting. #################################Pros:* A novel setting of continual few shot learning that considers context switching. * New datasets to support the proposed setting. * The paper explored several baselines for the proposed setting, including an interesting extension of ProtoNet. I’d expect some more discussion of the evaluation metric.<BRK>Additionally, this work creates a new data set and adapts an existing one to this novel setting, and provides a method that can deal with it, as validated empirically. ############## Recommendation ##############I recommend this paper for acceptance. ############## Arguments ##############The main contribution of this work is to propose a novel problem formulation for continual few shot learning, which is closer to realistic continual learning. I would encourage the authors to explain this in more detail, and more honestly assess how realistic their problem formulation is. The fact that a similar data set can be created from Omniglot interesting. Same for the provided videos. This could also enable a more comprehensive evaluation of the proposed approach in this submission.<BRK>The paper proposes a new learning paradigm that combines both few shot learning(FSL)  and continual learning (CL) to provide a more realistic learning environment rather than the traditional train test retrain approach in FSL. Overall, I think it is a very nicely written paper with some issues with evaluation settings that might be exaggerating the performance of baselines. + The use of contextual memory (incorporating both spatial and temporal context) is very interesting and is a promising approach for both FSL as well as a general learning architecture for visual event perception. Two environments are proposed, along with a novel dataset.<BRK>Summary:This work aims to make a realistic learning setting by combining few shot learning and continual learning in the online setting. Building on this, they propose a new dataset, RoamingRooms, that incorporates such context. The authors propose a new method, Contextual Prototypical Memory, to tackle this problem. This new paradigm combines the two settings and improves on their shortcomings to make it more realistic. Additionally, this is all done in an online setting. 3.The proposed model is simple, with components added specifically to make use of the additional information in the dataset (the RNN) or to output additional information required for the task (the new class detection branch). Even if this method detects new classes by thresholding the probabilities for novel class detection, it should be used as a baseline method. Notes:1.There is a lot going on in this paper.
Accept (Poster). rating score: 7. rating score: 7. rating score: 6. rating score: 6. <BRK>SummaryOne problem of few shot episodic learning is a poor sampling resulting in negative impacts on the learned model. Cross Episode Attention Module (CEAM) is proposed to alleviate the effect of poorly sampled shots and Cross Episode Consistency Regularization (CECR) is proposed to enforce robustness of the classifiers. Strength  The paper proposes a novel idea of how to improve few shot learning by exploiting inter episode relationships. Using multiple episodes and exploiting inter episode is a new attempt. (including supplementary materials) Weakness  I could not find a significant weakness of the paper. RatingI like the overall idea of using inter episode relationships for few shot training. Another strength of the method is that no additional hyper parameter is used to tune the performance.<BRK>Why does CEAM take S as argument twice? I think cross episode relationship is an important topic of study in the context of few shot learning, and the experimental results in this paper are strong. A more thorough study of the motivating problem and how & why MELR works would greatly improve this paper. From my perspective, a particular important strength of this paper is its ablations. I m fairly convinced that the presented implementation is likely the best way to implement the idea of cross episode attention + distillation. It would be great if that could be incorporated into the final version of the paper. A possible explanation for the drop in performance when using 3 or more episodes is due to the relative decrease in episode diversity. Cons:1.The paper is motivated by the supposed “poorly sampled episodes” problem. Results on wider datasets could corroborate this hypothesis. It is not clear why the attention module output in CEAM is added to the embedding F if the goal is to ignore bad examples and emphasize good examples.<BRK>MLER consists of two main modules. The first module (CEAM) applies the attention mechanism to two sampled episodes and it alleviates the negative impact of badly sampled instances. Pros:  This paper proposes a new meta learning method to alleviate the negative effect of poor sampling of support sets. Experimental results show that MELR can improve the baseline method (Propnet). These results show some evidence of the effectiveness of two proposed modules (CEAM and CECR). Hyper parameter candidates used for MELR are not described. Why does this happen? How much does the proposed method depend on hyperparameters such as $T$ and $\lambda$? (1) and (9), although argmax is taken in the loss function $L$, it is not correct when using the cross entropy loss.<BRK>### SummaryThis paper proposes a way to exploit relationships across tasks in episodic training with the goal of improving the trained models who might be susceptible to poor sampling in for few shot learning scenarios. The proposed model consists of two components: a cross attention transformer (CEAM) which is used to observe details across two episodes, and a regularization term (CECR) which imposes that two different instances of the same task (which have the exact same classes) are consistent in terms of prediction. ### Considerations:   I like the idea of exploiting the information across tasks to improve the performance of episodic meta training. This is an interesting direction that should might definitely help disambiguate in the case of poor sampling. The paper might be a good contribution to the scientific community, but I ll wait for the authors  response on my doubts before my final decision. I guess it is only for simplicity, but it might be beneficial to consider other types of relationships. I suggest the authors clarifying this point in the paper.
Reject. rating score: 4. rating score: 5. rating score: 5. rating score: 6. <BRK>It was in this context that I suggested label smoothing   that training on smoothed labels might address a sparse G matrix, but it seems like this point was not communicated clear enough by me and not understood by the authors. The authors suggest some applications of the model centric view of the data, but do not present any experiments regarding these applications. I believe the paper might be more convincing if those experiments are added in the next version of the paper. At this time however I will still have to vote for rejection. The paper uses the Frobenius Theorem to show that every point in the input space R^n can be associated with a submanifold which constitutes "the model s view of the data." My main concern with this work is that the import of this work is not clear. I also do not understand the difficulty in using fully trained models to perform their experiments.<BRK>This matrix is similar to the Fisher Rao metric, but for the input and not the parameters of the model. Some comments and suggestions that I believe will help:  The proofs can be moved in the appendix to save space. The motivation of the paper is not very clear to me. There are a lot of terms which are not defined explicitly. I suggest the authors to include some figures, which will help the reader to understand the proposed idea. What is exactly the data leaves and the foliation? I understand that this is done in order to avoid degenerate matrices, but why is interesting to study such a non trained model? 5) I think that the proposed idea may be interesting. However, I believe that the current version of the paper needs to be improved such that to make the idea accessible.<BRK>This paper provides an information geometric view on deep ReLU neural networkclassifiers which use the softmax function. For a fixed model it defines the``local data matrix  based on gradients \( \nabla _x \log p(y|x, w) \) ofconditional log probabilities combining ideas from both scores and the Fisherinformation matrix. This matrix gives rise to a Riemannian manifold with aspecific foliation of interest. It would be nice if the authors could make itclearer what is the interpretation of the local Fisher matrix, i.e.what itactually models. It is not clear howthe 1 2 examples presented for each of the experiments have been selected. It would be nice if the authors could comment on how an example where thedata is distributed to fill the entire data space can be foliated into C 1leafs with the data being contained in one of those leafs. The paper s goal or contribution does not become clear fromthe abstract.<BRK>In this work, the authors showed that deep ReLU networks can model the low dimensional manifold structure of the dataset. The authors first define a local data matrix G which is analogous to Fisher matrix. The authors visualize this data manifold on MNIST data. It is always a better idea to explain rationale and motivation of a Proposition before stating and proving it, e.g., while reading Proposition 1 for the first time, I have no idea the need for looking at kernel space! 3.Although I am very impressed with the theoretical result presented in the paper, in practice constancy of rank may not be a valid assumption and as mentioned in section 4 that towards the end the fisher matrix becomes close to a null matrix, the impracticality of the paper comes from the usage of partially trained model.
Reject. rating score: 3. rating score: 4. rating score: 4. rating score: 4. <BRK>This paper proposes a sampling free technique based on variance propagation to model predictive distributions of deep learning models. Estimating uncertainty of deep learning models is an important line of research for understanding the reliability of predictions and ensuring robustness to out of distribution data. The paper proposes an upper bound to the variance estimate of predictive distributions. However, the paper does not explain how the upper bound can be ensured. For this, I would recommend a formal analysis of the uncertainty estimates by inspecting the confidence intervals through coverage properties. In my opinion, this is a very strong assumption. Again, in my opinion, this is a significant limitation of this approach and I recommend that authors highlight these points. We see underestimation of uncertainty with \rho < 0.15 and overestimation with \rho   1. However, since these are based on synthetic data, I would suggest that the authors formally assess the fit using metrics like confidence interval widths and coverage. o	The authors note that “Estimation of ρ is possible by observing the outputs of middle layers several times under the approximate predictive distribution.”.<BRK>This paper proposes a variational approximation of Neural Network uncertainty dependent on the  bayesian interpretation  of dropout. Currently, rules are derived for a small set of neural network layers. The chief issue with this work is the  generality  of the work. Thus although the proposed approach claims to evaluate uncertainty, it is rather estimating the uncertainty introduced by dropout using a variational approximation, and assuming this is the same as uncertainty. This additional hyperparameter gives some doubt whether the proposed approach is something which can be easily integrated into existing models. In Table 1, how do the authors address that their approach (which is meant to be an approximation to MC) appears to outperform MC? I would strongly like to view comparisons to other works in quantifying network uncertainty.<BRK>The paper proposes a sampling free approach for estimating predictive uncertainties in Bayesian neural networks trained via Monte Carlo dropout. In particular,  given a dropout trained neural network, the paper develops a deterministic approximation to the test time predictive distribution that is otherwise approximated through Monte Carlo simulations. The paper is clearly written and proposes a solution to an important problem. Deterministic approximations like the one presented here are promising. This is achieved by propagating  uncertainty (first and second moments of the inputs) through the network. While the application to amortizing MC dropout is interesting, it appears to be a direct application of previous work. * Amortization of the posterior predictive distribution is not a new idea. While this is technically true, this is also true for other distillation based amortization techniques.<BRK>Thank you for the interesting paper! SummaryThe authors focus on the important problem of efficient uncertainty quantification. More specifically, they propose a methodology that approximates the variance across samples from an MC dropout model with a single forward pass. Strengths  Efficiently (from a FLOPS standpoint) propagating model uncertainty in a BNN is an important research area, particularly for compute constrained use cases. I see that "This is because the approximation accuracy of the Taylor approximation is not necessarily high as shown in Section B", but I did not find Section B or Figure 3 to be clear. WeaknessesAs noted below, I have concerns around the experimental results. Why do you think this is the case, particularly if the standard deviation was used as the uncertainty signal for the OOD decision. For more clarity it could be helpful to update this to say that the epistemic model uncertainty is represented in the prior distribution, and upon observing data, those beliefs can be updated in the form of a posterior distribution, which yields model uncertainty conditioned on observed data. p. 7: "Estimation of ρ is possible by observing the outputs of middle layers several times under the approximate predictive distribution. The additional computation cost is still kept quite small compared to MC dropout." Without details, this seems like a key component that can yield arbitrary amounts of uncertainty.
Reject. rating score: 4. rating score: 6. rating score: 6. rating score: 8. <BRK>This is important for reproducibility and is needed for justifying the soundness of the algorithm. Alg.1: y_{1}, ..., y_{K} have already been used a the training set labels. IMHO, the rebuttal raises further concerns about the technical quality, and the paper still requires stronger justification for acceptance. Importantly, I don t see why this condition is needed as there is no justification on how it is connected to the disagreement test. "According to the definition, there exists a class of functions F that is complex enough such that OOD(P, F) is the complement of the support of the training distribution. In addition, if OOD(P, F) is the complement of the training distribution, then ID examples not in the training distribution are included in OOD(P, F).<BRK>Deep one class classification. Considering that real world data has a lot of labeling noise, most such functions as f* would end up memorizing the train data and would have bad performance on ID data in the test set. RETO has at least one short coming over the outlier detector techniques: it needs to wait some time until it can collect enough test data.<BRK>I think the authors did a great job with the overall structure of the manuscript. One of the main strengths of the paper is the extensive experimental validation. So it could help, for the assessment of the manuscript, to mention when this assumption would be violated in real world settings and how the proposed approach would be affected by such an assumption or a violation thereof. I would have expected to see at least one method from that field in the comparisons. Yet there are some methodological details that could be clarified, next to the points listed above.<BRK>The authors present a method RETO that achieves state of the art performance in a transductive OOD detection setting. Benchmark data such as CIFAR, SVHN, and MNIST are used to compare conventional and proposed baseline methods, such as k NN, Vanilla Ensembles OE, Mahal, Mahal T, and MCD. However, there is not enough discussion on how the proposed method can achieve such a high level of accuracy compared to the conventional methods; early stopping is used in RETO, but is it promised to reproduce the same level of performance in other tasks?
Reject. rating score: 4. rating score: 4. rating score: 4. <BRK>The paper proposes a method for an improvement of generative adversarial models via post processing its latent variable distribution. To be more precise, the method proposes to train an additional neural network that outputs an important weight for each point of the latent space, thus reweighting the final distribution in the space of images. I think the idea to put the filtering stage into the latent space is indeed worthy. "Discriminator rejection sampling."<BRK>By leveraging the idea of importance sampling, the authors train an additional network. The authors propose one more method, latentGA, following the path in the latent space that maximizes the learned importance weights. The paper also raises an interesting question of whether the existing enhanced sampling methods help when the target distribution is not sufficiently disconnected. The paper misses some essential experiments to be faithfully compared with existing methods. Given all the above, I am leaning towards a reject and my main concerns are as follows.<BRK>## Summary:The paper proposes a new algorithm for improved sampling of GANs. The proposed method tries to fix this issue and is motivated by rejected sampling. However, instead of using density based algorithms for rejecting samples, the authors take a fixed pre trained generative model and train a neural network that learns to reject samples from the latent space. While some of the experiments are convincing, I do not buy some of the arguments made in the paper. Comparing to these algorithms would make this paper much stronger.
Reject. rating score: 5. rating score: 5. rating score: 6. rating score: 6. rating score: 7. <BRK>**Post Rebuttal**I thank the authors for the extensive experiments and answers. Unfortunately, I still feel that the contribution is rather marginal. **Summary of Contributions** The paper points at the expressive bottleneck of GNNs as the weak distinguishing strength of learned aggregators. Then, based on the analysis, 2 aggregation schemes with enlarged strength are proposed. **Strengths**   *Simplicity of analysis*   The paper presents a simple formulation to the aggregation operator via the matrix coefficients which provide an intuitive view of the requirements from the aggregator functions. al.(2020)*   Although the paper states results regarding the requirements from strong aggregators, as very similar result has already been introduced in Corso et. **Recommendation**The paper posses an interesting and simple view of aggregators in GNNs however I have concerns regarding the significance of contribution.<BRK>The improvements reported in the experimental section seem to be not really significant. Summary of the paper: The main objective of the paper is to improve the expressiveness of the GNN by exploring powerful aggregators. Weaknesses: The strength mentioned above (multiplication of hidden features values and the aggregation) is also a weakness: I have an impression that already known results are presented in a much more complex way. The paper is not easy to follow in general ( e.g., the sentence "The difference is that each dimension of hidden features is aggregated with an independent weighted aggregator which works like a comb".)<BRK>The work presents a framework to categorize GNN aggregators based on their distinguishing strength. I am not sure about (2). Strengths:  The paper is mostly well written. The output of the aggregation is a matrix, not a set. Since the authors already use OGB, I wonder why only three data sets have been chosen. While there seems to be some novel insight in this work, which might be of interest to the community, I think the paper needs to be improved in (1) clarity of presentation and (2) experiments. Issues with (1) can maybe be fixed within the rebuttal period.<BRK>Summary: 	This paper explores the representation power of graph neural networks. Unlike recent work on choosing among simple aggregation functions or combinations thereof, the authors here recognize that these aggregators are the bottleneck in the representation power and generalize simple aggregator functions commonly used in literature to an aggregation coefficient matrix. Weaknesses:* The study of the expressiveness of GNNs is a very popular topic right now and not enough context is provided about related work on this topic and other approaches, mainly focusing on GIN and GAT in the development and while a few other GNNs are considered in the experimental results, they are not discussed or explained enough. However, it is not satisfied by existing GNNs.” This statement should be explained more and supported.<BRK>The authors propose two new layers for GNNs. Furthermore, it is shown that current approaches have very low distinguishing strength and that CombConv and ExpandingConv, by their construction, yield higher expressive power. Additionally, proposed layers are compared with current approaches on 4 data sets. This paper constructs an interesting theoretical analysis of GNNs and finds the bottleneck of these networks to be in the coefficient matrix of the aggregation scheme. While I was not able to check all proofs, it seems like a solid mathematical analysis. What I find somewhat sobering is the experimental section.
Reject. rating score: 3. rating score: 7. rating score: 8. <BRK>This paper proposes a new procedure for continual supervised learning from a non IID data stream that assumes ability to maintain some of the stream examples in a buffer and use the buffer to improve updates of a prediction model. Strenghts:  the paper presents very detailed experimental resultsWeaknesses:  the paper is not easy to read  the underlying assumptions about the data stream are not clearly defined.<BRK>This paper covers an interesting topic of continual learning of the stream of data. Pros: In this paper, the authors provide an incremental learning approach that prevents catastrophic forgetting. Their approach can work on both balanced and unbalanced data. cons: The authors need to improve the presentation of the manuscript by providing more explanation. It could be confusing for readers who are not familiar with the topic.<BRK>The paper presents an extensive amount of experiments indicating that in this scenario, the proposed method improves significantly on existing approaches in terms of accuracy and memory efficiency. Conclusion:I believe this article should be accepted as a see it presents interesting findings in the area of incremental learning on non stationary data streams. How the model would handle these situations?
Accept (Poster). rating score: 7. rating score: 6. rating score: 6. rating score: 6. <BRK>They make various claims about how such a dataset can benchmark AI models with regards to their ethical “understanding”. I can also see that using it to fine tune language models and test them as done in the paper, can give an idea of how the language representation is aligned with or represents well ethical concepts. If not, I would suggest being clearer about the way the dataset can be used. In addition, I personally do not like using language such as “With the ETHICS dataset, we find that current language models have a promising but incomplete understanding of basic ethical knowledge.” Or “By defining and benchmarking a model’s understanding of basic concepts in ETHICS, we enable future research necessary for ethical AI”. I would like to see the authors make more precise claims in that respect. I think the area of ethical AI is important, releasing a well constructed dataset is an important step forward and overall this paper should be of interest to the ICLR community. These details should be added. Post rebuttal comments:My concerns are resolved.<BRK>I appreciate the work the authors did by collecting a large dataset that can be used as a benchmark of ethical assessment across different moral concepts. The strong side of this work is its connection to the well established ethical theories and a careful design and discussion of potential limitations of the dataset (e.g.cultural differences and ambiguous judgements). However, there are certain weaknesses in the paper. The results discussion seems not strong enough and more detailed analysis of the results would help this paper a lot. It is not clear what conclusions can be made about the existing models in terms of their ethical performance. Are the models ethical already or not that much? The authors claim that larger models are significantly better than smaller ones but do not report variances of performance and/or results of statistical tests.<BRK>The authors use a dataset based on this concept but do not mention it explicitly. The dataset tackles many tasks and, as we ll discuss in the review, perhaps too many. The paper uses classification tasks to test whether models build on large language models can encode ethical and utilitarian judgements. 129 152Originality and Significance Despite these concerns, the paper if focuses on important questions. Unfortunately, the ICLR format and length constraints limit the ability of the authors to fully expand on these important questions. The issue is in trying to square this with the high agreement rates. I think these are interesting questions, but the lack of space devoted to these questions (it could be its own paper) might let someone believe that the answers in this dataset are "correct".<BRK>This paper presents an interesting data set aimed at testing neural language models’ capability for “natural language ethics”   determining which natural language statements are more ethical than others. It’s an interesting and important task and the paper includes a useful data set that will probably see broad adoption. As a result it may not be ideally suited for the ICLR audience. I do feel like the paper could be improved through more clarity and analysis in the experiments, and dialing back at least one claim. The dataset construction appears to follow well established subcategories of ethics, including questions for each subcategory. The experiments are not described in enough detail. The fact that I don’t know which data was used for fine tuning also leaves me a bit confused about the adversarial filtration. So the filtering is adversarial, but the train/test split is not. “this is the first work we are aware of that uses empirical data to inform notions of fairness, ”   had a hard time understanding, could you say more what you mean by ‘inform’
Reject. rating score: 4. rating score: 5. rating score: 5. <BRK>I vote for rejection at this moment. This paper proposes a new knowledge distillation method called Neighbourhood Distillation to speed up knowledge distillation process. Experiment on several benchmarks shows the proposed method can be beneficial in several application. Overall, I think the idea to speed up the process of knowledge distillation is a direction that could be discussed in the future,  while the training cost of current KD in experiments in this paper is almost affordable and the speedup is not very impressive. Besides, several concerns need to be addressed. 2)  The goal of this paper is to speed up the KD process, however existing cost of KD experiments in the paper is affordable. 5)  The GPU time (H) is not a very objective index, could you provide the flops of each method?<BRK>This paper introduces Neighbourhood Distillation (ND), a new training pipeline for knowledge distillation (KD), which splits the student network into smaller neighbourhoods and trains them independently. The authors breaks away from the end to end paradigm in previous KD methods and provides empirical evidence to reveal feasibility and effectiveness of ND. 2) The idea is simple and intuitive. Benefit from parallelism and small training components, such training schema can speed up the convergence of standard KD. [1] proposes a similar blockwise knowledge distillation method. 4) In Sec.5, only the width search experiments are conducted, which is more like layer wise or block wise pruning. However, architecture search is a general method that can not only search the widths but also the operations. 5) All the experiments are done on ResNet series. Different teacher and/or student architectures, such as VGG, ShuffleNet etc., should be considered. Which blocks are chosen in Fig.1(a)?Does the shallow and deep blocks have the same phenomenon when perturb small number of blocks?<BRK>This paper studies knowledge distillation in the context of parallelly training sub networks (called neighbourhoods) instead of commonly used end to end training paradigm. The authors explore the applications of the proposed neighbourhoods distillation in improving sparse networks,  searching a good student structure given the teacher and knowledge distillation merely using synthetic data. Both CIFAR and ImageNet datasets are considered in the experiments. The proposed method is interesting. Besides, more details on how to inject noises to a network or its neighbourhoods would be useful. Neighbourhood distillation (ND). And the authors claim that ND can be performed in a much faster manner, compared to conventional KD. How about computational resource cost when training them parallelly? Does the speed up is also benefited from parallel training?
Accept (Poster). rating score: 7. rating score: 6. rating score: 6. rating score: 6. <BRK>The paper proposes a method to implement each layer with different precision (mixed precision quantization). 1) results are a slight improvement over SOTA methods. This is to be expected given the maturity of this topic. 2) typo in Table 2 ("wight precision")3) It will be good to relate this work to [1] that also studies mixed precision quantization using a pre trained floating point network.<BRK>The method proposed by the authors is sound. It leverages insights that have been employed in a neighboring area (pruning via regularization) and re purposes those to the problem of quantization. One issue I have with the proposed method is that the parameter space is expanded by a large amount. Since for every scalar weight, we end up with a collection of binary weights. Doesn t this make training more difficult? However, one of the main early claims is that all works trying to find per layer precision do so manually. It would be nice to contrast with such works as well. Minor issue: comp x  is used in the results (tables) without being defined. This has to be explicitly stated at least once (maybe in the captions).<BRK>This paper introduces a new method to quantize neural networks in a differentiable manner. Proposed method applies the group lasso on the bit planes of the weight parameters to let certain LSBs in each layer to be zero ed out. STE is used to train the binary representation of each bit plane and the sign of weights during the training. I think that the idea of introducing group lasso to prune the entire bit plane is very interesting and the paper is well written, but some additional analysis will be helpful. 1.I think the result must be compared with more recent papers, such as LSQ (Esser, Steven K., et al., 2020).<BRK>This paper basically proposed to learn the quantization bits (precision) in each layer. Specially, weights are constructed with binary representation as $W_s   \[W_s^1,...,W_s^b\]$. And a group sparsity is imposed to all $W_s^i$ for all weights in a layer, leading to certain $W_s^i \to 0$, thus cancelling the bit allocation in $i$ th. Experimental results is promising. Pros:1.It is interesting to see that weights are represented in binary format, while each bit is trained in a full precision scheme. After determining the quantization bit in ("fake") quantization training (although $W_q$ is quantized but $W_s^i$ is not exactly binary, which is the exactly weight we want) using Eq.5. 3.Why is necessary for $W_s$ to be separated into postive and negative part ($W_p$, $W_n$) in processing ?
Reject. rating score: 3. rating score: 3. rating score: 3. <BRK>Summary:The paper presents an approach that for every object identifies the factors that have a high impact on the models  uncertainty. On the other hand, the paper is half baked, there are conceptual flaws, the evaluation protocol is weak, there is an incorrect interpretation of experiment results;  My comment on novelty is the following: I would say that algorithmic novelty, is the weak point, however, bringing the attention of the community to important problems is not less important than new algorithms (and even is more important IMO). Evaluating model calibration in classification.<BRK>Lower bounds are defined for scalars, not distributions. Page 5: before equation7 the text reads “it is possible to find the latent variable that would decrease the uncertainty…”. It is not stated how it defines a latent variable 	Page 5: The process described in equations 8,9 is essentially making a gradient step (of the entropy) in each of the M_E possible directions and then measures which step reduced the Entropy.<BRK>This paper proposes to identify sources of uncertainty by disentangling representations in latent spaces for object classification tasks. 2.It is not clear how the model in Figure 2 is trained. ### Originality#### Pros  The paper proposes a  method to identify sources of uncertainty by disentangling representations in latent spaces in object classification tasks.
Accept (Poster). rating score: 7. rating score: 7. rating score: 6. rating score: 6. <BRK>Recommendation I think this paper should be accepted for publication, as it serves as a much needed reality check for some of the more frothy publication trends in GNN research at the moment. * (+) The experiments range over a substantial set of datasets, models and ablations, providing ample evidence that this is not just a fluke.<BRK>There should be randomness coming from MLP training. Why is that? Please justify this through experiments.<BRK>This paper shows modified label propagation can perform better than GCN. And this postprocessing is based on the traditional label propagation algorithm. It shows that this simple method matches GCN performances on various datasets. 1) To my understanding, this method can be viewed as proposing a  better  initialization for label propagation.<BRK>YesIs the organization of the paper well? Based on this discussion, they introduced Autoscale and FDiff scale. I agree that C&S + Autoscale or FDiff scale can achieve comparable performance with a small number of parameters.
Reject. rating score: 3. rating score: 3. rating score: 4. rating score: 4. <BRK>The authors should include the comparisons with state of the art second order optimization solver such as K FAC. Some experiments have been conducted. This strategy often results in much large variance and slow convergence in practice.<BRK>The authors derive the convergence of SPPA to a stationary point in expectation for nonconvex problems, and perform numerical experiments to showcase the effectiveness of the proposed method compared to SGD and its variants.<BRK>However, the constraint would make the stationarity in (14) no longer hold. 3.In Appendix A, the authors make two assumptions in (13) and (14). However, the idea in the development of the algorithm seems standard. The term $\sqrt{\lambda_t}c$ should be $c/\lambda_t$.<BRK>Regarding the convergence result, there is no advantage of SPPA against SGD and the author should have a discussion about this.
Accept (Poster). rating score: 7. rating score: 7. rating score: 6. rating score: 5. <BRK>The paper systematically reviews different hyperparameter settings and training strategies used for PGD adversarial training on CIFAR 10. More details on the attack default settings should be provided. Finally, I’m trying to reconcile the reported results with those from Madry et al.(2017, https://arxiv.org/pdf/1706.06083.pdf) who report 45.8% adversarial accuracy under a PGD attack   which is lower than all except one result reported in this paper (if I’m parsing it correctly). Same for AutoAttack: what were the hyperparameters, in particular for AutoPGD?<BRK>The task of finding good hyper parameters for adversarial training is a challenge. So I like the idea of performing a study on the effects of the different settings. This is my major concern of this paper. Most of the results shown in the paper are quantitative. It would have been more consistent to check Table 2 and Table 5 with WRN 34 10/20 so that the reader finds exactly what happens when the respective changes are made.<BRK>The authors show that although the proposed settings were found on the PGD AT model, they generalize well to the TRADES defense as well. This already highlights the importance of selecting the right hyperparameters for adversarial training. Hence, the finding in this paper is not too surprising. All the results reported in the paper are for single runs, however they may be a result of variance due to random initialization. I have not increased further due to the limited novelty of the paper.<BRK>Since I m currently not actively working on the practice of adversarial robustness, the other reviewers are likely a better judge on the usefulness of the results of the paper for the community. Strength  The papers extensive empirical results are useful to identify good hyperparameters, and it obtains some interesting findings, such that small differences in weight decay can make a big difference in performance, in contrast to standard performance. Weaknesses  The papers novelty is low; essentially it is a rigorous study on how to choose hyperparameters for a specific adversarial training setup (i.e., adversarial training of CIFAR 10).
Reject. rating score: 3. rating score: 5. rating score: 6. rating score: 8. <BRK>This paper proposes to extract interpretable rules from a learned neural network. The authors claim that they are the first to propose rules connecting 1) multiple neurons together, and 2) do this at a dataset level. I mean, it seems possible that for some input the rules could produce a different output from the neural networks. Another undesirable property of ExplainNN seems to be that it relies on a dataset to derive its rules. Is it possible that when run with a different dataset and the same learned network, ExplaiNN would produce a different set of rules? Writing wise, the paper is presented well enough. I would advise the authors to refrain from using the main body of the paper as a listing of contents and simply pointing to the appendices. The pictures in the experiments section were difficult to make out. I couldn t figure out from the image whether the husky s pointed snout had been identified as a defining feature.<BRK>Paper SummaryThe authors propose a method to explore how neurons interact within a neural network and derive rules of interactions that can help interpret the inner workings of the neural network and open up the black box. * On a related note, I felt the evaluation presented by authors while extensive is rather qualitative in nature. While the prototypes of identified rules across different datasets look relevant and interesting, it is not clear whether they are the best set of rules.<BRK>The paper proposes an approach to explainable supervised learning by extracting sets of rules for two individual layers within a neural network. ### Update after author response ###Thanks again for the clarifications   After reading the author responses, the other reviewers  comments and the new version of the manuscript, I increase my score for the paper, as the authors now better state the relationship to Circuits and GRAB, and provide a significantly improved evaluation. A smaller comment to the related work is that there might be missing references for model destillation, such as [2]. This is not to say that prior work already learned sets of rules among layers, but   if relevant   it should be evaluated to what extent the proposed method is superior for explaining neural nets to end users. Would this be computationally tractable? Are two layers better to interpret for humans? A minor question would be why you define "prototype" as on a single neuron basis (in the introduction)?<BRK>Or if you fit ExplainNN with different seeds on the same network? The examples look very impressive, but my main concern is with whether the examples could have been cherry picked, in the sense that most of the thousands of rules produced may not be useful. The method itself is interesting enough and the examples sufficiently compelling (even if cherry picked) that I would recommend the paper to almost anyone interested in neural network interpretability.
Reject. rating score: 4. rating score: 5. rating score: 5. rating score: 5. <BRK>The experimental evaluation does not give any insights into which components of the approach actually help in performance. # Review UpdateI thank the authors for their thorough response and the additional experiments. I would have liked to score the paper higher, but at this stage I believe the paper is still not ready to be published. The authors acknowledged in their update that the review process helped them to understand their own work better. This is not made clear in the paper. I believe this looks like the right direction to take.<BRK>Bayesian methods (MCMC or variational inference) for parameter estimation would seem to provide a natural basis for comparison as well   why were these not considered? The paper is well presented, the method is straightforward to implement, and the choice of experiments in the natural and physical sciences is interesting and relevant. Why was IPOPT chosen, instead of more standard gradient descent methods? The authors mention a connection to Bayesian methods, which I believe could use further elaboration.<BRK>* **Recommendation**: I recommend reject the manuscript in its current form. While acknowledging the novelty and significant of the application, I find the method a relatively straightforward combination of existing techniques (deep uncertainty and sample re weighting), without sufficient in depth analysis (either theoretical or empirical) on the merit the combination for the intended application. There are also some potential theoretical concerns that needs to be addressed.<BRK>I appreciate that writing cross disciplinary work is challenging, however, I strongly encourage the authors to improve upon the clarity and structure, and to bring the terminology closer to the standard in the ML community. As I understand it,  such models are useful in physics applications, for example. Second, why was no comparison made to directly estimating the parameters of the generative model on the targets (predicted features) in the polarimetry dataset?
Reject. rating score: 4. rating score: 6. rating score: 6. <BRK>Another major assumption is P* and the fact that the test data is coming from this distribution. The paper can also improve its writing, clarity and accuracy of some minor points such as the sampling bias issue brought up in point 5. This makes me wonder what the paper is trying to prove in this case. This makes the problem trivial as it is almost saying that if I satisfy the fairness objective and make sure that my test data is satisfying the very same objective then the performance of the model will boost.<BRK>2.If these assumptions do not hold, then the fairness definition sometimes does not make sense to me. For example, when assumption 3.1 is violated, the optimal classification accuracy for group A is 90%, the optimal classification accuracy fro group B is 85% and there is a policy \pi that achieves the optimal accuracy for both groups. For example, using importance sampling to correct the distribution difference between the training data distribution and the target test data distribution. The paper discusses the conditions that minimizing the risk under a biased data distribution with these fairness constraints will lead to Bayes optimal policy in the target test distribution. The paper provides insight on when the optimal fair policy and the optimal utility policy coincide from a biased data point of view, i.e.ensuring some fairness on biased data might learn the optimal policy for the unbiased data. I do not understand why the authors present this example 2.6. 1.2 Assumption 3.1 assumes that the unconstrained risk minimizer on unbiased data distribution is algorithmically fair.<BRK>In this paper, the authors show that under some assumptions, they can use the fairness constraint to understand some parts of the distribution shift between train and test, and as a result they can improve accuracy at the test time. If the optimal classifier satisfies some fairness constraint, then enforcing that fairness constraint on biased data should always increase the accuracy, right? The authors then assume in the test data the optimal classifier is fair; thus, this assumption leaks some information about the distribution of the test data (weights of these groups at the test data). By projecting the optimal classifier to the fair subspace, we can recover the optimal classifier under some assumption.
Reject. rating score: 3. rating score: 4. rating score: 6. <BRK>Finally, after reading the paper, I think the paper proposes a new neural architecture for similarity learning rather than focusing on metric learning. #### Pros: Drawing inspiration from dynamic systems, the paper proposes a novel architecture that is not only dynamically involved with timesteps but is also coupled (between two instances), which has the capability of bringing two arbitrary sequences close. The main issue I found in this paper is its presentation. Why is there such a discrepancy between the results in the paper and state of the art results please (as it is more convincing to build upon state of the art results when possible).<BRK>The paper notices the similarity between a dynamical system and an RNN. In fact, the paper proposes an improvement to the *siamese* GRUs. Despite the fact that the proposed idea is quite simple, it was hard to follow the paper. Secondly, the proposed architecture intermixes two sequences. Therefore, it is not possible to learn embeddings. The experimental section is very scarce. Furthermore, there are not enough benchmarks for the metric learning on this dataset. This work needs more experiments on datasets with several published results. # ConclusionThe idea is interesting, but the experimentation is very weak. I increase the rating by one point. The logical flow of the paper is hard to follow.<BRK>I liked the formulation and motivation of the paper, explaining the sequence metric learning problem  and drawing parallel between synchronized trajectories produced by dynamical systems and the distance between similar sequences processed by a siamese style recurrent neural network. The authors mention a drawback of the proposed architecture is that each pair has to be passed through the network instead of just computing once each representation and then the distance for each pair and that this could be balanced by the use of virtual metric learning during training.
Reject. rating score: 4. rating score: 5. rating score: 5. <BRK>Summary: The paper presents a privacy preserving transformation technique for image data. The main idea is to use adversarial representation learning to obfuscate sensitive attributes. Strengths:1) The paper considers an important problem of preserving privacy on images and presents a heuristic approach to address this problem. The prior work on this topic has  focused on removing information from representation, and the paper extends this to also generating new information. Being an experimental paper, a stronger evaluation (say on multiple datasets) might also help.<BRK>The paper introduces a framework to privatize sensitive attributes of data using adversarial representation learning. Some comments and questions: Can you provide more evidence as to why replacing the sensitive information with something else (which is what the generator does) is useful?<BRK>Summary:This work proposes a privacy preserving transformation mechanism based on adversarial representation learning. The proposed work is an extension of generative adversarial privacy (GAP). Reason for score:The contribution of the paper is incremental, and the experimental results are limited to one dataset and one baseline. More detailed comments:          In related work, please add more recent works in adversarial representation learning and explain the contribution of this work compared to the existing ones.
Accept (Poster). rating score: 8. rating score: 7. rating score: 6. rating score: 6. <BRK>The paper aims to get a better understanding of differences between wide and deep neural networks through an empirical evaluation. The paper is well written and should not be difficult to reproduce. The most interesting and somewhat surprising finding is that even though two networks with different number of parameters and layers but with the same accuracy make very different mistakes, and there is a pattern to it.<BRK>This paper explores core questions related to how depth and width of deep neural networks affect the learned representations.<BRK>In this paper，the author studies the effects of width and depth on neural network representation. This paper conducts lots of experiments on CIFAR 10, CIFAR 100 and ImageNet with different network architectures and apply the CKA to measure the similarity between representation of each layer. I can’t fully understand how the similarities between representations of each layer are measured. 2、	Exquisite figures that well displays the experiment results.<BRK>This paper explores characteristics of resnet networks that arise with different capacity. How do these behave when increasing depth vs width and do these relate at all to the different classification results?
Reject. rating score: 4. rating score: 4. rating score: 5. rating score: 7. <BRK>This paper considers the stability of the stochastic gradient decent algorithm under different conditions. They give a tight bound for the stability of SGD with decreasing step size in the non convex case. 2, They analyzed the stability of SGD and give the lower and upper bounds for the stability in many cases. cons: 1, In Theorem 3, they give a lower bound for the stability of SGD with decreasing step size in the non convex case. However, this lower bound is larger than the upper bound in Hardt 2016 when T goes to infinity and the other parameters fixed. This contradiction implies that the results in Theorem 3 should be incorrect. 2, They claim the O(1) uniform stability of SGD in under the Hessian Contractive condition. But the uniform bound of $||w^*   w^{*\prime}||$ is not proved. After the rebuttal. The authors partially addressed my concerns. I have read other reviewers  comments.<BRK>Based on this notion, in [Data Dependent Stability of Stochastic Gradient Descent, Kuzborskij and Lampert, ICML 2018] used (Hardt et al., 2016) and extended them to the distribution dependent stability setting for both convex and non convex cases. I suggest that you please include this work, discuss it, and show how you are similar or different than them and how tighter or relaxed your bounds are compared to them. However, I consider this present manuscript is a fairly well written manuscript but it is not complete yet. 2.In the context of uniform stability, [London, B. Generalization bounds for randomized learning with application to stochastic gradient descent, 2016], “partially” addressed how data independent component such as step size affects the stability. Theorem 6 in your manuscript mentions about fixed stepsize but other than that I do not see any discussion on step size which is in my understanding is an important component in the performance of SGD. 3.Please correct me if I am wrong: In the proof of Theorem 6 did you use the uniformly bounded gradient? In my understanding, uniformly bounded gradient with strong convexity leads to a contradiction. as one of the instances.<BRK>This paper studies stability of SGD which is a popular optimization algorithm. In particular, the authors show by constructing specific problems that the existing stability bounds for SGD applied to convex problems are tight within a constant factor. The authors also provide new conditions weaker than strongly convex assumption in both convex and non convex case. I have doubts in the proof of Lemma 1. It seems that the identity\[E\|\triangle_{t+1}\| (1 \alpha_t\lambda)E[\|\triangle_t\|]+\frac{\alpha_t}{n}\|x_i x_i \|\]is not correct. As far as I can see, one can only show\[E[\|\triangle_{t+1}\|] \frac{(n 1)(1 \alpha_t\lambda)}{n}E[\|\triangle_t\|]+\frac{1}{n}E\big\|(1 \alpha_t\lambda)(w_t w_t ) \alpha_t(x_i x_i )\big\|\]Therefore, I think Lemma 1 may not be right. As a result, the lower bounds on the stability bounds may not be right. 2.The self corrected condition depends on the dataset S. However, for the uniform stability, one needs to consider all datasets S. Therefore, the divergence bound in Theorem 2 can not be used to get stability bounds. Furthermore, $\xi$ should be zero if one considers all the dataset. Is it clear that $(1 1/n)^r\leq 1$? I also cannot understand well the deduction in eq (20). Furthermore, if this inequality only holds in expectation, then one can not use the Definition 6 which requires $w_t$ to be close to $w^*$ almost surely. In conclusion, although this paper considers a very interesting problem, there are some issues on the correctness of the deduction.<BRK>More specifically, the paper investigates the tightness of the algorithmic stability bounds for SGD given by Hardt et al.(2016).Furthermore, the authors propose the Hessian contractive condition, which characterizes deep learning loss functions with good generalization properties, when being trained with SGD. Pros:1.The paper concerns the stability based analysis, which is one of the approaches used for explaining the strong generalization performance of deep neural networks in practice. 2.By investigating the tightness of the stability bound for various types of problem (convex, non convex), the paper shows that in general, using stability framework seems to hit an obstacle on the way to explaining generalization. Hence, further conditions are needed to guarantee generalization. By this point, the authors propose Hessian contractive condition which is satisfied by potentially many machine learning loss functions and is sufficient to guarantee a better generalization properties. 3.The paper provides empirical evidences / experiments which make the theoretical claims more comprehensible. Cons:Apart from the strong points, I still have one concern about the clarity of the paper:1. The lower bound in Theorem 1 is larger than the upper bound in Theorem 3.8 in Hardt et al.(2016) when the Lipschitz constant is smaller than 1. Can you explain this mismatch?
Reject. rating score: 4. rating score: 6. rating score: 6. rating score: 6. rating score: 7. <BRK>The authors of the paper propose a hyperparameter tuning algorithm that uses a simple non probabilistic model to predict the performance of any new hyperparameter configuration on a target task. Without more details on how the tuning of the surrogate architecture exactly happened, it does not seem possible to apply this method to other meta datasets, making the paper incomplete a non reproducable. Honestly I would think the paper would be stronger without this use case described, and more more focus given to the results and the zero shot case. In the current version the architecture is relegated to the appendix and the tuning absent. * In general the paper is not really self contained, instead a lot of the important details are contained in the appendix. I would also suggest to be clearer about what the main contribution is. However the idea is simple and promising.<BRK>Pros:This work proposes a solution for the zero shot hyper parameter optimization problem without access to observations of losses of the target response. A battery of experiments and the completions comparison against state of the art HPO solutions are conducted to demonstrate the performance of the proposed approach. Cons:This work may be treated as applying meta learning to hyper parameter optimization. That is, a model is introduced to learn how to learn a group of optimal parameters based on the given meta features. The meta features of datasets are directly computed by Dataset2Vec [1]. This paper points out that prior methods rely on engineered meta features, but actually [1] can be applied to improve them as well. I suggest the authors discuss the situation where prior methods and [1] are combined to reveal the advantage of the proposed method. "Dataset2vec: Learning dataset meta features."<BRK>Summary: The authors propose a new zero shot hyper parameter optimization method based on the meta learning framework. In this paper, the authors construct a new meta dataset to evaluate the proposed method by compare it with various HPO methods (both zero shot approach and sequential model based approach). The algorithm in the appendix doesn t have a section on learning meta features, how (and when) are these updated in the algorithm? Although the range of \hat{s} is defined as {0, 1}, in (8) \hat{s} appears to be a continuous value. The authors argue that  "Without loss of generality, we use the Euclidean distance to measure the similarity between the extracted meta features, ... and \gamma   1".<BRK>In this papar, the authors formulated a new objective function for HBO, which included an additional regularization term based on the dataset similarity. The authors used the distance between the meta features of selected datasets to measure this dataset similarity and assumpted that similar datasets should have similar hyper paprameters. I have three major concerns as following. 1.In Appendix B.1, it mentioned that using the similarity regularization defined in Equation 4. alone is not sufficient to measure the dataset similarity, so an additional similarity metric Equation 9. is used. In the current version of the manuscript, many essential details are put in the appendix, such as the optimization of Eq.5.<BRK>The authors propose a novel approach to zero shot transfer learning for hyper parameter optimisation (HPO). One thing that could probably be improved a bit is the coherence of the text, I mean, I had to jump a lot between the Appendix and the paper to understand some necessary details. The experimental evaluation appears solid and comprehensive. It appears that the response model used some basic settings that were not optimised for the task, but I still wonder how sensitive the learning of the response model is to the choice of hyper parameters. To summarise I think the experimental validation suggests that this approach has potential to improve the state of the art in HPO. Also in the experimental evaluation it could made clearer that the results are not sensitive to the tuning of the, pretty complex, HPs of the response model itself.
Reject. rating score: 4. rating score: 5. rating score: 6. rating score: 6. <BRK>I do believe that the question of whether we can construct an arbitrary generalization curve is very important and that it should be studied and explored more deeply, but I m not convinced by the set up in this paper. First, from the perspective of the complexity of the model. While adding a dimension to the linear regression adds a parameter, I m skeptical how this relates to the complexity of the model in how we view complexity in machine learning and in the research area of double descent in particular.<BRK>Then, we can check that 0 is the best estimate when all features are just pure noise, and it seems that there is no motivation for us to learn anything from the random noise. The room can be left for the analysis of the bias term. Showing multiple descents does not add much value because it never beats the trivial estimate 0 in this setting.<BRK>The motivation for normalization in the paper is that the closed form error, $||(A^\top)^+x||^2$, sums over d dimensions and so the generalization error has to be normalized by d^2. This does not seem right.<BRK>However, showing that the generalization error can be controlled even for a simple model as this is nonetheless important. 2.The paper is well written, the problem it addresses is clearly discussed and the development of the proposed method is well detailed. I would have liked to have some numerical examples to illustrate the design of the generalization curve for a simple case. 2.In the setting in the paper you draw the new elements either from a normal distribution or from a mixture distribution when you increase the dimension.
Accept (Poster). rating score: 7. rating score: 7. rating score: 6. <BRK>The authors proposes block minifloat (BM), a floating point format for DNN training. Equation 6 was a bit confusing for me. EDIT: the authors have clarified that the hardware area results take into account the need to support multiple formats, which addressed my biggest issue with the paper. Where the paper make a strong contribution is in the hardware implementation of BM, something which neither Drumond or Yang really got into.<BRK>This paper proposes a family of numerical representations for training neural networks based on minifloats that share a common exponent across blocks. An exhaustive exploration of the design space, and the discovery of some new low precision representations that offer high accuracy as well as computational density. Furthermore, $X_i^a$ seems to indicate that the definition of $a$ depends on itself. In Figure 6 – are the accuracy measurements actually taken from the hardware simulation?<BRK>This paper introduced a new representation (Block Minifloat) for training DNNs with low precisions of 8 bit or less. Overall, the paper is well written. Particularly, the hardware evaluation gives some impressive results.
Reject. rating score: 4. rating score: 4. rating score: 4. rating score: 5. <BRK>Might add more details about the shift and scale, so that the reader does not have to read another paper. The motivation and idea in this paper are clear and simple, so the reader is easy to understand it. The middle layer has a better representation than the end on the few shot image classification task. 4.The results are positive. The idea in the paper is not very novel. It will be good to study more ensemble methods. (2011).[6] Wang, Yan, et al."Simpleshot: Revisiting nearest neighbor classification for few shot learning."<BRK>The authors propose a simple approach, which obtains competitive results with the state of the art of few shot learning. This structure suggests that the submission would likely be better suited for a more technical venue. Overall, a lot of polishing of the paper is needed prior to publication. “Our ensemble contains multiple encoders (encoders of different network structures).” At this point, this is not very clear: is the method used on top of a traditional ensemble ?<BRK>Summary:The authors propose to tackle the problem of few shot learning (FSL) using ensembling diverse classifiers. None of the introduced ideas in this paper is novel. The authors of [3] also used intermediate layers for better classification results. The experimental section is informative and clear.<BRK>The ensemble method is able to create an ensemble of classifiers. This topic is very straightforward and would be very easy for the audience to understand. While the results might not be that convincing enough. The biggest concern is the contribution of this paper, to be more specific, the proposed method might not be useful and might need to be tuned in other few shot settings. The topic is inspiring and interesting while it is not clear how the ensemble could help FSL tasks.
Reject. rating score: 5. rating score: 5. rating score: 5. rating score: 7. <BRK>This paper addresses the task of adversarial defense, particularly against untargeted attack. \+ The proposed method is simple, and involves a little modification to the classifiers and small computation overheads. Post rebuttal  I appreciate the responses from the authors, which partially address the concerns I was having. However, I am still not fully convinced that the proposed method is significant enough.<BRK>In section 4.1, the authors say "Thus, the undefeated Adversarial Training defense cannot be used as a baseline becauseit uses adversarial samples during training for all types of attack." The authors claim that this method achieves better results compare to baseline methods. I m not sure why the authors did not include any adaptive attacks like this one in section 5.<BRK>Summary:This paper proposes target training to defend against adversarial attacks on machine learning models. Detailed comments:While the idea of applying target training to defend against adversarial attacks is interesting, I have the following questions regarding the proposed method (performance, limitation, etc.). How does that compare with normal training and adversarial training? 4.Would Algorithm 1 (Algorithm 3) also be working against attacks that do not (do) minimize perturbations? How to choose between these two algorithms?<BRK>Cons:(1) The paper is not well written especially for the intro part (no more information is provided compared to their abstract). Also, some formula need more explanations to help better understanding, e.g., loss_f in the minimization 1. Some typos: (1) In the last paragraph of page 2, ...many gradient based attacksSome.... This should be a typo. (2) What s the difference between the two optimization problems in page 2 and 3 ?
Reject. rating score: 3. rating score: 4. rating score: 5. rating score: 6. <BRK>I recommend to reject this manuscript. However the theorem itself is just a direct outcome of the Nyquist–Shannon sampling theorem, and it is generally true to not only neural processes but also to all the other approaches. In addition, the analysis is limited to only scalar valued function on a 1D interval. The writing could also be improved. Concerns:  The definition of neural processes in the background section is confusing. In the original paper, the neural processes were however defined as random functions. In the background section, the words say  some sources define ... .<BRK>This paper addresses an interesting and timely problem, which is to understand how Neural Processes work to learn a representation of a function space. Offering a closer investigation into a recently introduced framework, this work will likely be of interest to the ICLR community. When you say "frequency decomposition" it carries a clear mathematical meaning, and it is a much stronger statement than what the paper reports empirically.<BRK>The paper tries to analyze the behavior of Neural Processes in the frequency domain and concludes that such Processes can only represent oscillations up to a certain frequency. While drawing a parallel between Neural Processes and signal processes, I think that there is some weakness in the experiments of the paper.<BRK>The work examines properties of Neural Processes (NP). However, it is not fully clear to which extend the claims translate to other data or generalise well. The paper provides a strong theoretical foundation of the method and authors support their claims by  empirical stimulation.
Reject. rating score: 3. rating score: 4. rating score: 6. rating score: 7. <BRK>In this paper, the authors compare several kinds of MBRL models, including observation prediction, to show which approach is better. In conclusion, The models that contain prediction on both a reward and an observation outperform the ones with only reward prediction. It might show the way for further MBRL models. [Weaknesses]  The number of parameters on the model that predicts only rewards is much (about a hundred times) smaller than other models. There are no clear and consistent differences between models with an observation prediction, which reduces this work s novelty. It is better to use four different SOTA models or four modified models from one backbone model instead of modifying two models. [Comment]  There is no bolded item in Table 2; it seems to be omitted, or the result is not strong enough.<BRK>Also, the main online results from Figure 2 show in some environments that the oracle baseline (that knows the true environment dynamics) is worse compared to the MBRL algorithms. The authors mention in the text that this is due to a limited planning horizon I still do not understand why the MBRL algorithms work better given that they also need to cope with a limited planning horizon. The authors conclude, based on their experiments, that their hypothesis can be confirmed. A clear execution and presentation of the setting described above would allow to draw conclusions about whether predicting observations actually helps *across* different architectures, but not the setting studied by the authors that uses separate architectures for joint reward observation prediction and reward only prediction. There are just 3 seeds per experiment so it is not clear which joint reward observation model is best, although it seems indicative that the reward only prediction model is worse across tasks compared to the joint models. Statistical significance is only getting worse in Section 4.3 which studies the offline setting: because here only one performance value is reported *at the end of training* as opposed to *in the course of training* (making it even more difficult to judge the statistical significance, since RL algorithms are known to produce rapidly performance changing policies in the course of training). ClarityThe clarity of the paper is low. However, only empirical reward distributions are presented, i.e.I don t see how one can draw conclusions about state space visitation from there.<BRK>I also think it s great that less conclusive or less succesfful experiments (e.g.on optimism) are still included in the appendix rather than file drawered, though I m puzzled that the claim regarding implicit optimism is made at the end of section 4.3. considering what appendix A2 shows. It s surprising that the paper does not take any of the prior work it cites as the exemplar model for reward prediction, similarly to how it does with pixel prediction. On the other hand, if these models are credible competitors from past work, that work should be cited. The paper makes the point that using reward only as training signal can be substantially cheaper (section 4.4), and that latent dynamics models are faster than pixel space dynamics models (table 5). Does this mean we might expect cost normalized performance to show some interesting patterns? [I think the paper provides a useful case study and I appreciate the caveats added to the conclusion, but based on additional discussion with the other reviewers, I think that the paper s more general claims remain unsupported and insufficiently moderated.<BRK>  Summary:      This paper presents a study on the trade offs of image prediction for model based RL      They find that image prediction loss is important and, surprisingly, reward prediction accuracy can be negatively correlated performance while image prediction accuracy is considerably more well correlated. The authors do a good job at pointing out the caveats and shortcomings of their study. Adding in a very different environment, i.e.Visdoom, would strength the analysis. My fellow reviewers have some concerns that while I don t agree, I think they could be avoided with some changes in the presentation to the paper:* The models used. There was considerable concern about how much smaller that models is than the others. The 0% line in Fig 6 and Fig 8 is very similar to this hypothetical (if it exactly) so I believe this will pan out as expected.
Accept (Oral). rating score: 8. rating score: 7. rating score: 7. <BRK>Summary: The paper presents a new way algorithm to compute the straight through variant of the Gumbel Softmax gradient estimator.<BRK>Summary:* This paper proposes a Rao Blackwellized version of the straight through gumbel softmax gradient (STGS) estimator. * The estimator exhibits lower variance at lower temperatures in the experiments. Strengths:* The method is simple and the computational overhead is very small compared to the original STGS estimator. Questions:* The main argument of this paper hinges on the claim that lower temperatures result in lower bias of the gradient estimator.<BRK>This paper introduces the Rao blackwellization technique to reduce the variance of the straight through gumbel softmax gradient (STGS) estimator wrt the parameters of discrete distributions. I don’t have a lot of nitpicking to make for this paper, as it is quite well executed.
Reject. rating score: 2. rating score: 3. rating score: 3. rating score: 4. <BRK>This can be easily addressed by simply using average pooling or sum pooling. The quality is very low. For example, even in the introduction, where is the ending of the second paragraph? “After this H0 is again passed through”, simply not readable..3). Could you please provide more intuitions on the solution during the rebuttal? 4).The results reported seems much worse than results reported in other paper. We can easily see the best result in PTC, best result is 80.41±6.92 while the submission gives only 76.2 ± 4.2  From that perspective, I did not see any advantage in the submission. I would suggest the authors add additional section to discuss that. 6).The draft includes an github link to share the code, however that link indicates the author affiliation information. Clearly I could not access the code on the github as I would have risked infringing anonymity.<BRK>This paper proposes two fully connected layers based neural graph pooling methods for graph neural networks, named Neural Pooling Method 1 and Neural Pooling Method 2. Experimental results on four datasets (PTC, PROTEINS, IMDB BINARY, IMDB MULTI) of two tasks (bioinformatics, social networks) show that the proposed graph pooling method can improve the performance by 0.5% 1.2% accuracy while decreasing the std. Weaknesses:  My biggest concern is that the proposed approach lacks originality and novelty because it is a simplification and variant of SOPOOL from Second Order Pooling for Graph Neural Networks (Ji and Wang, 2020)   Based on the author s writing, it is unclear what is the second order statistics for graph pooling, why it is important to have second order pooling, and how the proposed method can capture the second order statistics. The improvement of the proposed methods compared with SOPpool is marginal. For example, On PROTEINS, the accuracy is improved by 0.5% with the same std. On other datasets, the improvements are only at most 1.2%. To show the proposed approach is better, more datasets or tasks should be used. There is not enough discussion and analysis of the results. Especially, there should be some analysis to compare the method 1 and method 2: For different datasets, when one method is better than the other? The problem and notation is introduced formally in section 3.1, but is repeated again and again at the beginning of section 3.2 and section 3.3 Questions:  Do both of your method 1 and method 2 capture second order statistics? Is this correct? Have you tried your graph pooling approaches on other underlying GNN models? "Neural Pooling Method" is too general and thus not particular enough to summarize your method.<BRK>Pros :1.This work studies an important topic but less explored topic, graph pooling. 3.Experimental results are interesting. Cons:1.The main concern is the lack of novelty, and the technical contribution is very limited. Wang and S. Ji. Just one more layer? 2.It is actually a stealth exchange of concepts that the authors attribute the success of the methods to the neural networks. 3.About the experiment of this manuscript:a. Just 4 of the 9 data sets in this article are used   ‘Z. $H^{ ^T}Q$ and $QH^{ ^T}$ are not equal, which can be corrected by slightly changing the diagrams. b.	Repeat a paragraph and an equation many times. For example, in section 3.2, there is only one long sentence in a paragraph.<BRK>In this paper, the authors proposed two graph pooling methods, i.e., Neural Pooling Method 1 and 2. The difference is that, instead of a single score, it has multiple scores for each node, which leads to a matrix for graph representation. In general, the novelty of this paper is limited. Some other concerns are listed as follows:It is not clearly motivated why the topology information can be preserved by the two proposed pooling method. The process in Equation (6) can be viewed as a weighted summation. Is it designed in this way on purpose to capture the node size information? The same issue exists in the Neural Pooling Method 2. It would be better if the users could adopt more datasets should for experiments.
Reject. rating score: 3. rating score: 5. rating score: 5. rating score: 6. <BRK>The authors study the problem of learning the rule for the game of life with a convolutional network.<BRK>Finally, the authors show that the systems  convergence probability is sensitive to the initial weights.<BRK>Can the authors make any predictions relevant to the lottery ticket hypothesis based on their experimental results?<BRK>Can the authors improve the quality of the numerics there?
Reject. rating score: 2. rating score: 3. rating score: 3. rating score: 4. <BRK>Forward reach set computation can be used as a basic primitive to verify properties of deep neural networks used in a robotic setting. Starting from the outputs of the neural network, and  then work it s way to the inputs. This is an interesting way to look at the problem itself,  but as the authors point out it is an intractable problem. My concern about this paper is I don t see the use of a pre image computation algorithm as being very useful. A forward reachability tool works pretty well for the size of neural networks considered in the paper. Moreover, almost any safety constraint that needs to be verified with system dynamics in the loop always should ideally work forward in time. The authors do make a convincing case for the ACASXu example.<BRK>The presentation and the writing of the paper should be improved. There are some concepts that are not defined early on and maybe never in the paper. What is the mathematical definition of a preimage? Its hard to fully understand but it seems that the method suffers scalability issues. Can this be formally analyzed? Is it a fundamental problem? The NN used in the experiments are very tiny. I would consider experiments that reflect more realistic situations in the real world. It is not clear how to verify the performance of the method. Is there a reason for that?<BRK>Three experiments are proposed where the authors claim that the computed pre images help interpret the network decision making. * The final example of using this method in practice for ACAS systems is interesting, but it is difficult to follow what “success” would mean for this experimentMinor points: * The following sentence on page 3 seems to be missing something “Preimages are most insightful and useful when the inputs and outputs have definite interpretation – application areas where the need for massive networks is less.”. Overall the paper is interesting, however I am not certain of the novelty as some related work is not discussed. * The paper is a nice mix of theoretical results which lead to practical applicationsQuestions and Concerns:* The authors state that maxpool can be rewritten in terms of a linear component and a ReLU, but this is non obvious.<BRK>There are many issues in the paper that can be improved. The title is not appropriate, this work does not address safety applications. It is worth noting that the word safety is not defined and not used in the main body of the paper. It is difficult to follow the presentation of the paper since mainly the applications are presented and then some contributions given, in the same presentation as the abstract. Moreover, the uniqueness of the solution needs to be studied.
Reject. rating score: 3. rating score: 3. rating score: 4. rating score: 5. <BRK>Based on the experiments presented in this paper, my understanding about the hypothesis testing is that it is about performing a complex task relevant to the hypothesis, and answering whether the hypothesis turns out to be true or false. How about "Montezuma s Revenge"? Unfortunately, I found this paper paper does not adequately provide neither novel methods, results, nor insights. I would like to reexamine the paper once the authors respond focusing on which parts are particularly novel. Is this because of fundamental flaws in RL formulation (sparse reward, credit assignment problem, etc.)<BRK>While the problem is interesting, I found the paper difficult to read as the task is ill defined in the section 3 where many notation definitions are missing and some notations are reused in different contexts with different definitions (e.g.o sw,h in the first sentence of last paragraph of section 3 and o is an observation from the world in the beginning of section 4, a is {false,true} in section 3 but can also be a move in the world later).<BRK>This paper considers the general problem of testing hypotheses about the world by a kind of reinforcement learning, much as a person might learn by taking actions and observing their outcomes   equivalently learning policies that can generate observations to validate a hypothesis. The exact formalization of a hypothesis, from it s textual expression is missing. Furthermore the stated need to "predict the hypothesis" is a phrase that doesn t ring true. "Prediction" in this sense may simply be a colloquialism, just adding to the confusion. The references are vague or not clearly relevant. In summary, without clear definitions and careful adherence to notation, critical evaluation of the paper is encumbered.<BRK>While the problem of testing hypothesis with environment interaction is interesting, one expects a more principled solution for this problem. Also, the reward curves in Figs 10   12 do not look normal. ​ ​# Reason for decision​While the introduced problem is interesting (building upon Denil et.al), my main concern for recommending weak reject is that the main contribution seems to be just problem specific reward engineering. ​  The paper is clearly written and nicely defines the hypothesis verification problem.
Accept (Poster). rating score: 7. rating score: 7. rating score: 6. rating score: 6. <BRK>The paper proposes a novel noise contrastive estimation (NCE) objective that incorporates hard negative samples without similarity supervision, e.g., assuming unsupervised learning. To this end, it modifies the denominator of the original NCE by (a) re weighting the negative samples based on the euclidean distance from the anchor point, and (b) considering a de biasing of the effect of positive samples (that should be near to the anchor). I also liked their theoretical analysis on the method. Experimental results can be a weakness of the paper for some readers in several aspects: e.g., lack of large scale experiments, and the mixed results on comparing the method with the "debiased" baselines. Section 5.1: There could be a more justification on why the proposed method works not better than "Debiased" on CIFAR 10. Figure 4: Why the results on CIFAR 10 are not presented? It would be nice to give more information for the readers on why the method works less effectively on that dataset. It was a bit confusing for me to follow Eq.3 or 4 before I noticed that q is conditioned on x. The paper could further provide a practical guide on how to choose tau^+.<BRK>Summary:This paper investigated how to sample informative/hard negative examples for self supervised contrastive learning without label information. To tackle this challenge, this paper proposed an efficient tunable sampling distribution to select negative samples that are similar to the query when the true label or similarity information is not accessible. As for Principle 1, the authors said that upholding Principle 1 is impossible with no supervision, and they proposed to uphold Principle 1 approximately. My main concerns are (1) how to distinguish hard negative examples and same class samples as Fig.1 depicts, (2) how to sample $v$ in Eq.(4) to estimate the expectation. Both empirical and theoretical analysis is provided. \+ The proposed method can be easily implemented with few additional lines of code. \+ Experiments are conducted on several datasets (STL10, CIFAR100, CIFAR10). The proposed method works well even with a small number of negative samples. \+ This paper has a high writing quality. Taking Figure 1 for example, how to distinguish  oak  from other types of trees without labels? Since memory bank/queue based methods like MoCo [2] has a relatively large number of negative samples (i.e., 65536), is it possible to improve the performance of MoCo, or reduce the number of negative samples? *******************************************Final decision: I would keep my score unchanged.<BRK>In this paper, the authors mainly study how to sample good/informative negative examples for contrastive learning. The key challenge is the unsupervision in contrastive methods. This work mainly focuses on how to sample informative negative examples in contrastive learning. Targeting at this problem, they propose a new hard negative sampling with theoretical analysis. Besides, they also do interesting ablation studies, in Section 6, for example, to investigate the effect of the more harder samples and debiasing. Weakness.This is an interesting and important problem. It is unfair for the authors to do experimental comparison, while I might suggest that if possible the authors might just do an analysis comparison without any experimental results. For example, just like the Figure 1, the authors might show some sampled negative sentences by typical method and their method. For the proposed hardness, maybe they could show some sampled sentences with different hardness, as an additional visualization.<BRK>* Page 5 "representation generalizable": generalizable to what? I m not sure what follows with the ball packing relates to generalization. Unfortunately, this paper does no empirical analysis on the labels in q, and I worry that readers may be mislead that something close to Fig 1 might happen in practice. This paper essentially extends / iterates on [1], using a different proposal distribution for the infoNCE loss. This is a fine idea and it seems validated by the experiments. In that case, they cover several types of distributions. There are existing works (e.g., [3] called CMDIM) that use labels to generate a mixture distribution of positive samples that come from the same instance as well as from other instances of the same class. They also omit same class from negative samples. Really, contrastive learning should be able to leverage any information available to generate similar tasks for the model to solve. Rejection sampling would be easy to implement, so I m not sure I m comfortable with discounting it so easily here. * Page 5 you introduce the debiasing idea, which could be made clearer in this work. * The explanation at the end of 4.1 is very confusing to me or not clear.
Reject. rating score: 4. rating score: 5. rating score: 5. rating score: 6. rating score: 6. <BRK>The paper adapts reversible computing techniques to compute gradients. The techniques presented are not new though the Julia based DSL is new. The results presented are for differentiating through a GMM.<BRK>It is useful to have further descriptions of reversible computing for an audience that may be unfamiliar with the topic. The novel aspects of the work are not made very clear in the paper. ICLR is about machine learning, but there is no evaluation of machine learning workloads.<BRK>* Presentation issues that distract from the main point of the paper. * The authors can present their DSL language more formally. ## General discussion and Questions for the authorsI liked the high level idea of this paper, however the presentation and writing need to be drastically improved in order to be accepted for publication.<BRK>I reviewed a previous version of this paper for NeurIPS. I really like the idea of reversible programming and I think that a clear introduction of reversible programming and its use in automatic differentiation could be of interest to the machine learning community. As a paper, the first section is great, but then the authors leave me with many questions: How do checkpointing and reversible programming differ in memory usage?<BRK>The plots can be in logarithmic scale. Through reversible programming, NiLang gets rid of the need for checkpointing and hence is amenable to CUDA execution. Explain that it s simply treated as an always true condition. But in the bundle adjustment benchmark, NiLang outperforms ForwardDiff and Tapenade, especially with CUDA acceleration.
Reject. rating score: 5. rating score: 5. rating score: 6. rating score: 6. <BRK>The paper proposes a problem of disentangling representations generated in pretraining models, such as BERT. To this end, the authors proposes a method that employs the mask technique on transformer weights or hidden units to find the subset of features correlating with a specific task. The main contributions of the paper is the introduction of binary masks to identifying some subnetworks, which may correlate with specific tasks, within pretrained models. Overall, the paper is well written and is easy to follow. The experimental setup is not convincing. So, the authors should answer the following questions: (1) Why do the authors just selected these two specific genres to conduct the experiments? (2) Do the authors conduct similar experiments on other genres and what about the experimental results? In fact, the finetuned baseline performs very well according to Figure 3.<BRK>**Summary**:The paper proposes a procedure to extract disentangled representations from pretrained BERT models. The experiments show that the proposed method outperforms baselines such as unmasked BERT, unmasked but finetuned BERT, and unmasked but adversarially finetuned BERT. * Second, the paper does not show improved results on any benchmarks. Could you discuss this more? * Does using the pretrained model (vs. one trained from scratch) help? * Have you considered masking a subset of the weights/activations (e.g.only in the last layer)? * Do you have any intuition about the learned masks? How much overlap is there between the masks learned for each attribute?<BRK>This paper proposes a masking strategy to identify subnetworks within language models responsible for predicting different text features. Pros:  Paper is well written and the idea is explained well. Experiment results are convincing and support the claims. Achieving comparable results to SOTA without the need to train or finetune models is interesting especially from a computational point of view. Cons:  I wish the authors performed their first experiment on more domains: books, music, etc. From current results, it s hard to confidently conclude that this approach is generalizable.<BRK>The masks for every layer are trained using a combination of a triplet loss, attribute classification loss, and one that encourages masks for different factors to be different across all layers. StrengthsBuilding models that are robust to spurious correlations in data is important for a variety of reasons and learning disentangled representations is a promising way to achieve that. This paper shows good generalization performance on datasets with such characteristics. The paper is well written and the overall approach is easy to understand. It would be interesting to see if the model is stable to recover from fine tuning on data with spurious correlations and still produce disentangled representations. The paper seems to lack some specifics about exactly what layers/weights are masked.
Reject. rating score: 5. rating score: 5. rating score: 7. rating score: 7. <BRK>The paper reintroduces the straight through estimator with bias variance analysis. It further discusses its relationship with some constrained optimization methods in convex optimization and In general, the novelty of the paper on the methodology side is not high. Why should it be defined in this way? The paper does not have a conclusion/discussion part which makes it incomplete. With the concerns listed above, the paper in its current version looks not fully ready for publication. POST REBUTTAL UPDATES  Thanks to the authors for the response and the efforts in the updated draft. I raised my rating according to the author s response.<BRK>2.The paper covers a large body of relevant work, covering theory behind straight through estimators and stochastic binary networks. The novelty of the paper isn t clear, if this paper is an analysis paper, the empirical evidence is weak. 2.The paper is very hard to read, and it is difficult to understand the clear motivation. It misses key details in the experiment section. 3.The utility of the proposed MD estimator is unclear,  it would be helpful if the authors would clarify the interpretation of Table 1 with their write up under "Classification with Deep SBN": the authors state that their method performs as well as the empirical ST, while the table shows it performs worse than their baselines. The paper needs major revisions in terms of notation issues: the vectors should be bold, to distinguish the from scalars.<BRK>This provides insights in the specific assumptions (implicitly) made by these estimators and allows to analyze their bias and performance. The experiments mainly concern section 4 and show somewhat the effect of Bias Analysis IV), but it would improve the paper if (toy) experiments would have been conducted relating to the other analysis in section 2.2 as well, which could then justify the practical relevance of these results. While the paper motivates the reintroduction of ST as principled methods, the experiments give some insights but do not fully convince of the practical use of the proposed extensions. It is positive that the paper is largely self contained, but this makes it a bit difficult to distinguish novel derivations from recapitulations of results from previous work. 3.RecommendationMy current recommendation is to accept the paper. 4.Arguments for recommendationThis paper helps in the theoretical understanding and unifying view of a variety of straight through estimators, which may help advancing these estimators for training networks with binary weights and activations, which is a relevant and difficult problem. Line 305: a reference is missing: ?? Some grammar could be improved, e.g.in abstract  we … obtains, …, explains<BRK>Summary: The paper presents a principled derivation and analysis of the straight through (ST) estimator, which is often used to train networks with binary weights and activations. Furthermore, the paper seems theoretically sound. The empirical experiments confirm that the ST performance improves as the number of latent bits is increased as suggested by the theoretical analysis. Another strong point is that the paper will make code available on Github, which can improve the reproducibility of the experiments. A weak point of the paper might be its limited potential impact on future works since it mainly provides an analysis of prior empirical ST approaches. A venue that allows longer submission may be a better fit for this work. Minor important points:  Missing reference in Line 305.
Reject. rating score: 3. rating score: 5. rating score: 5. rating score: 6. <BRK>The results presented in the paper appear occasionally simply too good to be true, while several aspects of the presented method are poorly defined, or totally undefined. Algorithm 1, which provides the main tool to be used in order to answer Problem 1, is defined as returning a subgraph of G_T that is isomorphic to G_Q. 2.The pseudocode states that a binary prediction is made based on the *average* score of all value of subgraph prediction function f(z_q, z_u); there does not appear to be any way of returning an subgraph as output, apart from this prediction. This threshold is mentioned in Section 2.3 by two names, and then it is never mentioned again. No discussion is offered on what values it has in experiments. Table 4 in the Appendix also refers to a number of layers.<BRK>This paper presents a sub graph isomorphism method using neural networks and embeddings that are supposed to preserve important topological structure of subgraphs. The approach of using a neural network to learn to perform tasks such as link prediction, label prediction etc., through meaningful node embeddings, is a welcome extension when applied to sub graph matching. That being said, the paper skips over large volumes of progress in sub graph matching work. Also, the embedding design decisions could possibly take away important conclusions from these methods and lastly, compare against solid benchmarks instead of much older and outdated ones. In conclusion, this work is quite interesting. However, at the moment it is not clear for this reviewer how to put it in perspective with respect to decades of systems and theoretical work on sub graph isomorphism. Thanks to the authors for providing a response to the review comments.<BRK>Their main subroutine is a subgraph embedding that is trained to encourage the property that if A is a subgraph of B then the embedding emb(B) coordinatewise dominates emb(A). The results of this computation are then used to guess an alignment between the vertices of the query and (a subset of) the vertices of the target graph. I believe we re past the days in which applying deep learning to a new problem is considered a novel contribution in its own right, but it s still an interesting topic to read about. * The post inference alignment problem is a way to bring significant knowledge about the problem domain into the solution, after the learned part of the system completes. However, I didn t see discussion of how to perform the lookups efficiently in the proposed scheme. Questions:Q1: Could the authors speak a little to efficiently querying for the best dominating target subgraph at runtime? Q3: In section 3.2, you comment "This benefit is a result of avoiding the loss of information when pooling node embeddings and a better inductive bias stemming from order embeddings." Summary of recommendation: I think this is a borderline paper. The approach seems reasonable.<BRK>This paper proposed a new algorithm for performing subgraph matching under deep learning framework. However, I have the following concerns about the paper. While this is a rational number for synthetic and small scale graphs, a feasible k can be much larger in other scenarios. I would like to see how the choice of k can be influenced by different datasets and what is the connection. 2) The result of the matching can rely heavily on the quality of the construction of the graphs. However, in real cases, a query graph may be constructed using some heuristic ways (e.g.k nearest, \epsilon ball or Delauney in graphics). I would recommend the authors consider about more practical cases rather than synthetic ones. To me, it also seems that the proposed method cannot be readily extended to edge features. I suggest the authors to at least discuss the feasibility of extending this method to edge features. I would consider raising the rating if the authors can address the aforementioned questions well.
Reject. rating score: 3. rating score: 4. rating score: 5. <BRK>This paper studied the memorization issue of generative modeling. It proposed a benchmark for a public generative modeling competition and observed how participants attempted to game the FID. Actually in GAN evaluation, FID is not the only metric that are widely used. Also, it is not clear how MiFID really evaulates the generilization ability. The writting of the paper is not good. The introducation spent too many paragraphes on describing the background of GANs but failed to clarify the motivation/intuiation clearly.<BRK>The paper investigates memorization/overfitting in GANs and proposes a new metric (MiFID) to identify memorization in trained GAN models automatically. The topic of memorization in GANs is an important one and certainly one that does require more work and research. Results:  Regarding memorization methods (Table 1): why are AEs considered cheating? Why is the training time relevant for memorization in GANs? Why use a new dataset of only dog images? What was the overall goal of the competition? Why not use other metrics such as LPIPS [3] or SSIM [4]? Technically speaking the memorization distance is not a distance since it is not symmetrical, maybe call it "divergence" instead  For the formula/calculation of MiFID: I might misunderstand: but why use the "min" and not the "max" of the cosine distance?<BRK>In addition to other factors, the choice of the memorization margin (as described in section 3.2.1) seems to depend not only on the reference set but also on the “sets” of generated images. Furthermore, to assess the extent of memorization w.r.t.the FID score, the authors propose a new metric — Memorization Informed Frechet Inception Distance (MiFID) — which takes into account sample memorization w.r.t.a reference set. The authors conclude on a few notable observations — (1) unintentional memorization in generative models is a serious and prevalent issue; (2) the choice of latent space used to compute FID based scores can make a significant difference. While this is a good starting point, it’s unclear how well insights from the study may generalize to approaches that operate outside of these constraints.
Reject. rating score: 4. rating score: 5. rating score: 5. rating score: 5. <BRK>In the view of motivation, I don t think the motivation is strong enough and is convincing. In my opinion,  rewarding the correct predictions may be a good way, but penalizing the incorrect ones should also be important. 2.In the view of experiments, though the authors add Table 7 in the appendix, which is the result for training 90 epochs, I still doubt why Eureka Loss does not work better than recent works when training 200 epochs (which is also a common setting recently). And it seems that using CE at the beginning of training is important, and +CB$^+$ works the best. Moreover, In table 2, the results on "few" are especially not very good comparing with others, which makes it harder for me to believe that rewarding the high likelihood area really matters a lot for tail classes. It seems that the experiment results are not strong enough to support the proposed opinion.<BRK>The submission makes an intriguing claim that retaining focus on correctly predicted rare classes can improve performance for training with class imbalanced datasets. Experiments have been conducted on 2 image classification datasets and 1 dialogue dataset. For all such comparisons, has it been ensured that the architectural and training details are fixed across comparisons? Some typos:“down applications” —> “downstream applications”“a effective number” —> “an effective number”“thus the likelihood” —> “so that the likelihood”“deferred courage” —> “deferred encouragement"Overall, the paper is clearly written and reports exhaustive experiments (with the caveats/questions above). While the motivating experiments in Section 2.2 are not very compelling, in part due to the very minor improvements, the key intuition that the classification of rare class hard examples should be continued to be encouraged (so that their predictive confidence doesn’t drop as these examples are weighted down by some of the other methods) sounds interesting, although some of the phrasing about “rewarding well classified examples” can be a bit awkward. My main concerns as of now are about experimental details, which are described above in the questions. I m still not sure if the experiments are particularly compelling. Apart from this, taking some of the comments from the other reviewers and the authors  responses into account, I am retaining my initial rating.<BRK>Summary:  This paper made a finding that weighting up  correct predictions for rare class examples also can help to improve the performance of imbalanced classification. In light of this finding, it proposes the Eureka Loss to add additional gradients for examples belong to rare classes in the high likelihood area when correctly predicted. Pros:  The paper is clearly written and easy to follow. The experiments are thorough and demonstrate the effectiveness. I could intuitively understand that the Eureka loss function would encourage the examples to have likelihood of either 1 or 0. Have the authors visually checked the examples with a likelihood of 0? post rebuttal update I thank the authors for the responses. While I still think the idea is potentially interesting and original, I could not increase the score given the fact that this manuscript is naturally incremental without theoretical justifications.<BRK>This paper deals with learning imbalanced class distributions. Then, based on the findings, it proposes a new learning objective called Eureka Loss, which can be viewed as a combination of the frequency based and likelihood based methods to reward the classifier when examples belong to rare classes in the high likelihood area are correctly predicted. Overall, it is well written. 2.It clearly discusses the existing two methods (i.e.frequency based methods and likelihood based methods). 3.The motivation for the design of the new learning objective(i.e., Eureka Loss) is based on the empirical finding that the high likelihood area of the rare examples is important to improve the performance. The finding is mainly on empirical observations, which may lack theoretical support. Why is the high likelihood area of the rare examples is important for generalization? 2.For the experimental settings, e.g.iNaturalist 2018, the i.i.d.assumption does not hold for the training and test set. Since the reason in 2, I guess the hyper parameter selection becomes difficult.
Reject. rating score: 3. rating score: 5. rating score: 6. rating score: 7. <BRK>##########################################################################Advantages of the paper:  The paper is overall well written. 2  Authors claim novelty on algorithm 1  design a new bilevel optimization algorithm . Do authors have heuristics to choose the stepsizes $\beta$ in practice? How do authors know that the proposed algorithms converged for this choice of stepsize? IMO more insights and comments on the theoretical results would be very helpful for the reader.<BRK>This paper proposes two novel algorithms, named deterBio and stocBio, for the nonconvex strongly convex bilevel optimization problems, and presents a comparison with several existing algorithms to demonstrate their superiority by experiments. + The paper is well organized attached with sufficient supplementary materials for different propositions, and give analysis about the gradient complexities of proposed algorithms and related methods.<BRK>The paper presents two algorithms   one for the deterministic and one for stochastic bilevel optimization. 2019.My recommendation is to reject this work with a 4.<BRK>The paper propose two algorithms for solving bilevel optimization problem where the inner objective is assumed to be strongly convex.
Accept (Poster). rating score: 7. rating score: 7. rating score: 7. rating score: 6. <BRK>The paper proposes a pragmatic approach to unifying end to end speech recognition models for whole utterance and streaming models. Streaming models are defined as not using any future audio, while whole utterance (or "full context") models can look at the entire audio recording. This is shown to be the case for two end to end model architectures, ContextNet and Conformer. The result is of high practical relevance. While there is a clear reduction of overhead of model validation and deployment process, it is not clear from the paper whether training the joint model is cheaper than training two individual models (since each utterance is trained twice per minibatch). Exaggerated self referencing that gives the incorrect impression that Google invented everything is not appreciated by all members of the community. The term "inplace knowledge distillation" is a little confusing.<BRK>This paper proposes an unified framework for both streaming and non streaming ASR and the knowledge transfer between them. The results show that both latency and performance are improved. Since usually the performance of streaming ASR is inferior to the full context version, the unified training scheme could enforce the model to fit both tasks well, thus could serve as some kind of regularization. 2) The weight sharing proposed in this paper could make it more efficient for deploying both streaming and non streaming ASR at the same time. 2.The source of improvement on latency is not well explained. Adding some related work on knowledge distillation could make it a more complete story.<BRK>This paper proposes a unified single neural network architecture to realize both streaming and full context ASR systems. The two modes interact with each other by teacher student training, where the teacher is the full context mode while the student is the streaming mode. One of the drawbacks of this approach is that the current technique is too specific to the ASR topic, and it may not get much attention from the general machine learning audience. Also, the paper does not have some novel machine learning algorithms, and it would also not gain much attention from the general machine learning audience. Advances in neural information processing systems. ": How about discussing Watanabe, Shinji, et al."Hybrid CTC/attention architecture for end to end speech recognition."<BRK>This submission proposes a framework for training online and offline ASR models. Experimental results suggest that at least on Librispeech this approach provides tangible benefits for online ASR models. Regarding (b), it seems that MultiDomain data set is very challenging or not enough tuning was performed to illustrate the benefit of your approach. Clarity: The clarity of this submission suffers from a mostly verbal presentation of very technical operations. Significance: Given experimental results, the significance of this particular submission is minor. At least on one of the data sets the results appear to be good. Cons: Technical elements are described in a very verbal fashion which may lead to misinterpretation.
Reject. rating score: 3. rating score: 3. rating score: 4. rating score: 5. <BRK> Summary This paper proposes a simple approach to discover interpretable latent manipulations in trained text VAEs. The method essentially involves performing PCA on the latent representations to find directions that maximize variance. The authors argue that this results in more interpretable directions. Strengths   The method is simple, and can be applied on top of existing text VAEs. Weaknesses   There are only mostly qualitative results presented. Human evaluation seems nonideal since it is only tested on 12 people. The method is only applied to one text VAE mode which specifically uses BERT/GPT 2 , so it is not clear if this will generalize to other models (e.g.models trained from scratch).<BRK>This paper presents a PCA based latent variable language model for unsupervised latent variable interpretation. Cons:1.The novelty is quite limited. Applying an existing well known technique to obtain interpretable latent variables is not advancing this domain in the right direction. 2.The explanation of latent variable in this paper is self justified. The self defined baselines cannot be convincingly conveyed that latent variable are interpreted. 3.In the quality evaluation, the authors do not show how clearly to modify the discovered latent variable to alter the sentences. 2.Other than the current quantitative and qualitative analysis, do you think any other quantitative evaluation will be helpful?<BRK>This paper studies latent manipulations in text autoencoders. The authors propose that compared to random and coordinate directions, moving in the PCA directions of encodings of training examples will produce more interpretable text manipulations. As the idea is straightforward, I d like to see more in depth analysis and more solid evaluations. When are these latent directions applicable and when are they not? The only evaluation in the paper is human evaluation of whether a latent direction shift produces interpretable generations. It s conducted on 20 sentences, which is too small to draw any conclusions. The results on the Wikipedia dataset are very poor.<BRK>The author propose to use PCA like method on latent space of VAE models to unsupervisedly detect interpretable direction. The idea is reasonable and practically useful for large scale pretrained VAE model, i.e.OPTIMUS.This paper has a clear idea and a thorough discussion with related works. I have some concerns about the model. The proposed model seems requiring a large scale pretrained model. From the PCA side, it does not require a Gaussian space.
Accept (Poster). rating score: 8. rating score: 7. rating score: 6. rating score: 5. <BRK>This article deals with the problem of partially labeled dataset: if some entities are missing, how SOTA approaches are going to behave? Then, they wonder which part of the missing performance is due to the lack of labels and which part is due to the incorrect labelling of discarded supervision. The experiments are well explained and interesting on synthetic dataset. Then the authors propose a new cross entropy loss to test their hypothesis on real data by sampling high confidense negative samples as ground truth. It is a way of performing distillation on the model using negative sampling. Consistant & relevant work that deserves to be publised in ICLR.<BRK>This paper conducted an empirical analysis on the unlabeled entity problem in the NER task. It concluded that there are two reasons to affect the NER model s performance:  the reduction of annotated entities, and treating unlabeled entities as negative instances. Experiments showed that the latter reason gave a much more negative impact on the NER models. The way of constructing synthetic datasets should be explained clearly (Section 3.1). It is not clear how does the negative sampling applied on the dataset (in sentence level?entity level?<BRK>To alleviate the performance degradation, this paper proposes a negative sampling approach that considers only a small subset of unlabeled entities in order to reduce the impacts of unlabeled entities. This paper analyzes the performance degradation by evaluating synthetic datasets and finds that all the unlabeled entities are treated as negative instances is the main factor of the performance degradation. There are some previous studies related to the unlabeled entity problem. The experimental results show that the proposed method achieves better performances compared to previous studies on real world datasets and achieves competitive performances compared to the state of the art methods on well annotated datasets.<BRK>This paper investigates the unlabeled entity problem, which is generally observed in the manual annotation setting and distant supervision as well. The main observation of this paper lies in two aspects: 1) comparison between the reduction of annotated entities or treating unlabeled entities as negative instances. Most interestingly, the authors show the observed difference between pre trained language models and LSTM based models. Based on the observations, they propose a general approach to eliminate the misguidance brought by unlabeled entities and such a simple design shows good performances.
Accept (Poster). rating score: 6. rating score: 6. rating score: 6. rating score: 5. <BRK>SummaryThe paper proposes a constellation model that performs feature clustering and encoding dense part representations. The few shot experiments on the mini Imagenet, CIFAR FS, and FC100 datasets show the effectiveness of the proposed method.<BRK>This paper proposes ConstellationNet for few shot learning, which is inspired by constellation models. The ablation study leaks the case when the codebook is not used. If the proposed method can be called a constellation model is somewhat questionable. It is known that the convolutional features correspond to (implicit) object parts.<BRK>Adapting popular traditional models to contemporary settings is a promising idea, and the proposed method reaches SOTA performance on standard benchmark. STRENGTHSExploiting object parts and their relationships is a promising direction for few shot learning, where one may aim to share knowledge about objects structures and common parts.<BRK>The proposed contellation module is an improved version of non local block [a], which contains a cell feature clustering module and a self attention module for modeling pixel wise (cell wise) relationships. Inserting this block to the backbones could improve the performance for few shot learning setting. In my mind, the most important difference between the proposed constellation module and non local block is the cell feature clustering module.
Reject. rating score: 3. rating score: 4. rating score: 5. <BRK>However, the method is not novel and the numerical examples are too simple. The integral equations solved in this paper are very simple.<BRK>Are the last parameters \zeta_1 and \zeta_2 fixed or trainable? The way it looks in the equation now is since you refer to it as the ‘second network’ that they are trainable, but in Section 3 you mention they are fixed. Some numerical results on this would be useful. Right now it is not clear for me how the proposed method is better than classical methods. A small comment but the English of the paper should be improved.<BRK>The primary contributions as claimed by the authors are the use of Legendre polynomial based activation functions and creating a differentiable approximation for the integral equation by using Legendre polynomials and Quadrature methods to analyse the integral. The paper in its current form is not addressing how and why neural networks improve performance over the traditional methods and is also missing relevant comparisons and ablation studies.
Reject. rating score: 3. rating score: 3. rating score: 5. rating score: 6. <BRK>During the rebuttal, I concluded that this submission is highly confusing, rather misleading. Stochastic rounding has also been published before and shown to still miss the accuracy mark with >0.,1% accuracy gap for some important benchmarks. Rounding discussions the paper focuses on become highly secondary.<BRK>(This is both according to the authors Sec 4, experiment setup; and according to the QPyTorch paper arxiv:1910.04540, Sec 3 intro.) Doing it for every MAC operation could likely be even more expensive than just doing 32 bit MAC operations, since it involves the generation of random numbers, division, etc.<BRK>2).Today, SGD is often used with momentum, could the authors comment on the precision of momentum accumulation. The authors of this paper summarized the techniques nicely, however, the novelty limited.<BRK>The authors demonstrate that the nearest rounding is the culprit for the worse performance of half precision training compared to single precision, due to cancelling small updates. How can this be the case? ### Detailed feedbackThe paper is relatively well written and easy to follow.
Accept (Poster). rating score: 7. rating score: 7. rating score: 7. rating score: 5. <BRK>This paper presents a general purpose pre training approach for jointly encoding utterances and relational tables in the task of table semantic parsing, where a natural language utterance is transduced into an executable query (e.g., SQL) over relational database tables. Additionally, the 90 productions used in generating synthetic data are curated from utterances in the Spider dataset. I was wondering if the authors had considered using a more compositional grammar (e.g., the general grammars in Wang et al.2014 or Herzig and Berant, 2020) with larger logical form coverage. The experimental results suggest that the key is to combine SSP+MLM, it would be interesting to study the relative impact of the two objectives, e.g., by varying the amount of their respective pre training examples.<BRK>This work explores a pretraining strategy (similar to https://arxiv.org/abs/1606.03622) to the problem of table question answering. More specifically a synchronous context free grammar (SCFG) is first learned from training data (with manual alignment of entities/phrases). Then the SCFG is used to generate more full supervision data for Roberta model pretraining. The training objective is a combination of two parts: SQL Semantic Precision (SSP) predicts elements in SQL given the question on the synthetic data, and masked language modeling (MLM) on the natural (training) data. New state of the art results are achieved. Oblation study shows the impact of  SSP, MLM, training time, dataset etc., which is nice.<BRK>## After author responsesBased on the revision of the draft and the authors  responses to the review, I am raising my score to 7 from 6. ## Overall summaryThe paper proposes a pre training method useful for training neural semantic parsing models that translate natural language questions into database queries (text to SQL). ## Weaknesses  It is unclear how much effort is needed to construct the SCFG. There could have been more quantitative analyses of the method and its component parts in the paper. ## Questions  What would happen if you use the synthetic Spider data and use it to train the semantic parsing model for Spider? Does the grammar provide for multiple natural language utterances that will translate to the same SQL? There, the gains don t seem so dramatic. What happens if the task specific training data is also used with the MLM or SSP objectives in pre training? https://arxiv.org/abs/2004.10964 gives evidence that it can be useful to fine tune RoBERTa on the downstream task s data using the pre training objectives.<BRK>##########################################################################Reasons for score:  Overall, I vote for rejection. I like the idea of pre training for table semantic parsing. However, the pre training conducted in this work is different from standard unsupervised pre training in that the data used for pre training is task specific, i.e.for the task of natural language to SQL generation, and the evaluation is only conducted on natural language to SQL generation tasks. 2.Even though the authors pre train on table based NL to SQL generation, as a pre trained model, the model should be tested on other semantic parsing problems. I agree that with the data augmentation process, the pre trained model can see many newly composed questions and SQLs. However, the development of such a pre trained seems to be too tailored for the task and for the datasets. In such a tailored way, the improvements are also marginal to me.
Accept (Poster). rating score: 7. rating score: 6. rating score: 6. rating score: 6. <BRK>Based on that, the author developed a contrastive “push and pull” framework to: (1) keep proximity between learned and ImageNet pretrained features; and (2) push the feature embeddings away from each other across different images, using the feature diversity observation as the inductive bias. Although the proposed formulation looks similar to recent contrastive representation learning, this work seems to be well motivated, and find some unique interpretations for their push and pull in the specific context of syn to real generalization. The authors also proposed two variants: a multi layer contrastive loss applied to intermediate feature map groups; and a cross task dense contrastive loss that operates on the batch level for dense supervision. It has been known that CNN tends to focus on textures and hence learns poorly from the less texture rich synthetic data. This provides a feature level clear and measurable illustration of syn real domain gap and implies the insight what good features should behave like in syn to real generalization. They also proved again that the diversity of features correlates with generalization performance. While I feel overall positive, a number of questions and concerns remain for the authors: 	In Section 2 proof of concept, note that VisDA 17 training and validation sets have different sizes. Is that the same as adding an attention layer in the nonlinear projection head? Where is the dense NCE loss applied? Was it adopted by both classification and segmentation experiments by default, or only in the segmentation? That was not specified anywhere in the paper.<BRK>SummaryThis paper focuses on the domain generalization problem where the source domain contains synthetic data. An interesting phenomenon is observed in this paper: the diversity of the learned feature embeddings plays an important role in the generalization performance. Then, this paper presents a method to address the syn to real generalization problem by combining augmentation, contrastive loss and attention pooling techniques. In general, the observed phenomenon is interesting but the proposed method is not a well motivated solution. From Figure 2, it is clear that the synthetic data are very different from real data in the view of feature diversity. This phenomenon is interesting and maybe motivate more works regarding syn to real problems (not only the domain generalization problem). However, the solution presented in this paper is not novel and is not connected with the new view very well. 2.The experiments are not enough. For general interest, this paper should also present results when using the proposed method to address ordinary domain generalization problems. 3.There are many typos in this paper (even in the abstract: "that leverage" should be "that leverages"). 4.It is unclear why we should use augmentation and A pool. The motivations behind the two techniques are unclear. Why can they improve the accuracy of the proposed method?<BRK>This paper was motivated from an observation the common lack of texture and shape variations on synthetic images often leads the trained models to learning only collapsed and trivial representations without any diversity. The authors made a hypothesis that the diversity of feature representation would pay an important role in generalization performance and can be taken as an inductive bias. Seeing that, they proposed a synthetic to real generalization framework that simultaneously regularizes the synthetically trained representations while promoting the diversity of the features to improve generalization. The framework was further enhanced by the multi scale contrastive learning and an attention guided pooling strategy. However, the authors did not clarify where and how they use loss in their experiments. Experiments on VisDA 17 and GTA5 supported the hypothesis: though assisted with ImageNet initialization, fine tuning on synthetic images tends to give collapsed features with poor diversity in sharp contrast to training with real images. This indicates that the diversity of learned representation could play an important role in synthetic to real generalization. I also feel more analysis and insights could have been provided for the segmentation experiments in 4.2. Currently there is no more information beyond Table 5. For example, some feature diversity measure like Table 1 could be reported for segmentation too, since revealing the feature diversity inductive bias is the main novelty in this paper.<BRK>Training on synthetic data and generalizing to real test data is an important task that can be particularly beneficial for label or data scarce scenarios. The paper aims to achieve the best zero shot generalization on the unseen target domain real images without having access to them during synthetic training. Overall this paper is good both conceptually and experimentally. It is also well written in general. My main concern is that the current baseline comparisons in experiment are not fully consistent nor satisfactory. However, if looking more closely, even the baseline ResNet 50 mIoU sees a big gap between Yue et al.(2019), and the proposed method as well as the two others. It is unclear and unconvincing to me why the same baseline can perform so differently among those methods, and I think this might potentially undermine the experiment reliability/reproducibility and deserves more clarification from the authors. Given there is some extra space, the authors may want to visualize some classification and segmentation results, displaying both success and failure cases. Typos:a novel framework that leverage   > should be “leverages”the diversity of learned feature embedding play  > should be “plays”
Reject. rating score: 4. rating score: 6. rating score: 6. rating score: 8. <BRK>Summary: the paper attacks the problem of model selection for individual treatement effect (ITE) models when the domain of learning and prediction differ. The "regularizer" would be scoring to which extent replacing factual outcomes by their counterfactual predictions would preserve conditional independence relations (induced by the causal graph) in the prediction domain. why not use the standard ? It seems in a sense "too good to be true". Can you provide intuition on why it is so ?<BRK>##########################Summary:This paper proposes a novel interventional causal model selection (ICMS) score to select individualized treatment effects (ITE) models under the unsupervised domain adaption (UDA) setting. The problem is fundamentally challenging as counterfactual outcomes cannot be observed. The authors propose Theorem 1 that the conditional independence relationships in the interventional DAG are equal to that in the interventional distribution for the target domain, followed by augmenting the target domain data with the model s prediction of the potential outcomes. This paper tackles an important and challenging task: adapting ITE models unsupervisedly with unobserved counterfactual outcomes. The core assumption of causal structure invariance makes sense to me and the experimental results show significant improvement for all SOTA ITE models used.<BRK>The paper introduces a model selection metric for ITE models under the unsupervised domain adaptation setting. The causal structure learned from the source domain is used to estimate the causal risk on the target domain. + The problem is significant in machine learning and the idea is incrementally innovative: causal inference under UDA setting. In equation (7), the definition of NCI is not clear.<BRK>Summary:The present paper proposes a novel approach for model selection for individual treatment effect (ITE) estimation in the unsupervised domain adaptation (UDA) setting. Will it be made public at some point? This necessary condition states preservation of all conditional independencies of the causal DAG by the interventional distribution of the target domain. The framework proposed in this work constitutes an interesting new approach to model selection exploiting ideas from causal invariance and also allows to adapt existing ITE estimation methods to tackle covariate shift problems.
Accept (Poster). rating score: 6. rating score: 6. rating score: 6. rating score: 4. <BRK>This paper proposes using adaptive stochastic search as an optimization module within deep neural networks to perform general non convex optimization. This is used as a block within deep FBSDEs,which in general do not have a closed form optimization solution, and use the resulting network to show state of the art results in solving high dimensional PDEs on a 101 dimensional portfolio optimization problem. Pros : 1.Using adaptive stochastic search allows the inner optimization module to take multiple iterations without unrolling the computation graph (unlike meta learning methods), since the initial value used by the module is arbitrary, and not provided by the network. The authors empirically show a 5x speed up in using their approach compared to unrolled differentiable cross entropy and better qualitative performance than unrolled gradient descent for training an energy function for a simple regression task. 2.Authors confirm that their approach of using the optimization approach within a deep FBSDE works as expected by getting the optimal solution for cartpole, then use their method to beat random and constant strategies on a high dimensional portfolio optimization problem, for which it is not possible to use methods that unroll the computation graph (due to memory issues). Cons:1.More experiments for Structured Energy Prediction Networks with more challenging functions would give a better indication of the limits of the proposed approach in comparison to prior work. It is also unclear if the proposed approach would show worse performance for non toy datasets where a few iterations of unrolling gradient descent or differentiable cross entropy are sufficient. This is especially since the only non toy experiment was performed in a domain where other methods (differentiable cross entropy and gradient descent) couldn t be run.<BRK>### Summary The authors present the idea of adaptive stochastic search as a building block for neural networks, as an alternative to other "inner loop" optimization methods like gradient descent. Claims are supported by empirical evidence. 1.Proposed method is robust to changes in hyperparameters like the number of inner loop iterations during inference. The paper doesn t present this relevant information in page 4. In Figure 2 clarifying if the loss is train set or test set would be nice, plotting both would be even better. Not comparing to existing task adaptation/meta learning methods and benchmarks (for example First Order MAML s gradient)### Suggestions1. (Edit: the authors considered it)<BRK>These blocks have the form of x_{i+1}   \arg \min_x F(x, x_i, \theta), and can be thought of as a neural network layer. The approach presented in this paper relies on adaptive stochastic search as a differentiable optimization procedure. It is benchmarked in a variety of fields (1) energy based learning, (2) robotic control, and (3) portfolio management. *Cons:* First of all, the paper does not present or mention the practical implementation of their algorithm, which I believe is the only contribution of their work. As it is right now, there is no section of their technical contribution   just mention of the existing adaptive sampling. Regarding my first point, since this paper presents a simple idea/existing idea in another context, I would expect more details of the practical implementation of their algorithm and sensitivity to the different hyper parameters. Both of those are lacking in the main paper. One of the main reasons for the presented method is that it can tackle non convex objectives; however, the environment presented is convex. Finally, the authors make a lot of emphasis on the difference between their optimization approach and the one that meta learning does.<BRK>The authors suggest a way to backpropagate through neural network with an embedded optimization problem. The results are interesting but require a more compelling presentation and in depth analysis. It is also hard to evaluate whether the presented approach really is better from the given computational experiments. Since the authors explicitly treat nonconvex optimization problems they could also make more explicit that a nonconvex optimization problem does not necessarily have a unique optimum. The argmin is therefore a set valued map and not differentiable in the classical sense (even when the optimum is unique almost everywhere in the parameter space it is possible that the optimal value is discontinuous in the parameter). For non global optimizers (such as gradient descent) starting in such a way that we do not end up in the wrong local optimum is crucial. Regarding the violin plots for the portfolio optimization I am not sure how much they benefit the paper. The goal of the paper should be to demonstrate the superiority of the suggested method over baselines. More analysis about the quality of the gradients obtained by the suggested method compared to other methods would improve the paper:  How many unrolling steps do what to the gradient variance?
Reject. rating score: 4. rating score: 5. rating score: 5. rating score: 6. <BRK>This paper proposes Q learning based algorithms called Elimination Based Half Q Learning (HQL) and Full Q Learning (FQL). In the one sided feedback setting, the proposed algorithm improves the regret bounds over existing methods in terms of the dependency on the size of state action space. Numerical experiments are provided to show the performance of the algorithm. The algorithm improves the regret bound in terms of the dependency on the state action space. For example, the episode length seems to be too small (H <  5). The writing quality could be improved.<BRK>Motivated by OR problems, this paper extends Q learning algorithm to one sided feedback and full feedback settings. The novelty of this paper lies in applying reinforcement learning algorithms to the inventory control problem. Although the regret has no dependence on the size of state and action space, the time and space complexity do. However, the assumptions of this paper is a bit too strong, which is the major weakness of this paper.<BRK>* The model assumptions should be highlighted. The one sided feedback/full feedback models are unconventional. The main contribution of the paper is a new algorithm leveraging the model structure, so that the regret no longer depends on the size of state and action space. The application of Q learning in inventory control seems promising.<BRK>This paper proposes two algorithms, Half Q Learning and Full Q Learning for the classic inventory control problem. It establishes cardinality independent regret bonds for the two algorithms in terms of length of episode and horizon. The inventory control problem in consideration is important. Exploiting the feedback structure in this problem is interesting. The experimental section could be improved, e.g., compare the algorithms with larger H values and running times. This way, it is easier to see what assumptions are reasonable and satisfied by the target application. The experimental setting appears to be quite simplistic. The reviewer would suggest experimenting more scenarios to evaluation the performance. It would also be interesting to compare the running time of the algorithms.
Reject. rating score: 4. rating score: 5. rating score: 5. rating score: 5. <BRK>The paper claims to present a novel GAN based model explainability, for generating one vs one explanations, by incorporating to be explained classifier as part of the GAN. They use GANs to produce a baseline image which is a realistic instance from a target class that resembles the original instance. Why do you need a GAN? And based on this analysis you could explain why, for instance, the digit  0  is classified as a  6 . 2.Related with the previous point, your analysis is very limited. Could  0  be classified as an  8  or  9 ? It is not clear from your analysis. There are no comments on these cases. 3.I am not sure how to interpret Figure 2. The authors  contribution is not clear. 2.The experimental validation is limited and not convincing. What about more complex datasets, like CIFAR10, LSUN, etc.? Could your approach explain the mis classification in these cases?<BRK>This baseline can be integrated with a variety of attribution methods, including integrated gradient, DeepLIFT, Occlusion, and deepSHAP, and shows consistent improvements over zero baseline and minimum distance training sample for one vs one explanations. The experiments are conducted on three datasets – MNIST, SVHN, and apple2orange. The way the authors address this problem is interesting by leveraging GAN models. It would be more convincing to show the effectiveness of the proposed approach on natural images and a large number of classes, like CIFAR and ImageNet, as used in the previous work such as IG. 7) I understood that the authors focused on one vs one explanations. But I am interested to hear the authors’ thoughts on how to extend the proposed approach to one vs all explanations. After Rebuttal: I thank the authors for the rebuttal. I have also read the other reviewers’ comments.<BRK>Summary: This paper looks to use GANs to generate baselines for attribution methods. Strengths  As far as I know, the authors  contribution of one vs one attribution (compared to one vs any attribution) is novel. Whilst other works have alluded to this or ran heusristic experiments, this paper does a good job of formalizing the notion. The ability for GANMEX to live on top of any other attribution method makes it an attractive addition to existing attribution methods. Thank you for visualizing the baselines generated by GANMEX, quite helpful :)Weaknesses   A computational complexity analysis is required to gauge the practical utility of generating baselines with GANMEX. Questions  While GANs seem like an attractive choice of deep generative model (DGM) for this problem, can you comment on or experiment with other DGMs (i.e., VAEs or specifically VAEACs [1])? However, any DGM that has latent class separation should suffice. You would be able to perform optimization in the latent space [2, 3, 4] and achieve similar class separation, as described in Figure 1.<BRK>Summary\This paper proposes a new  baseline  for attribution methods tailored to deep neural networks. The choice of a baseline has been controversial in the literature, and a good method to select a baseline remains an open problem. Now one can compare attributions from such a model for a normal baseline and a baseline from GANMEX. Consider an MNIST model, a one vs one attribution would attribute why an input is say a  2  and not a  4 , i.e., it is contrastive against a particular target class and not all classes. This paper proposes to use a StarGAN for generating these baselines. The paper then evaluates explanations derived using the new baseline and shows that they explanations  perform  better. Overall, I think the paper tackles an important problem, but I have several concerns with the motivation, the appropriateness of the baseline definition in this work, and the evaluation. I problem I had reading it is that there are a few sentences that are stated as fact without any justification. Such statements should probably be reformulated. Ideally, the paper will set out a list of desirable properties; then show that the baseline derived from GANMEX satisfies these. It is still not clear to me why a notion of minimum distance in a different target class is the right one. Can the authors say more about why this should be the case? I suspect the gini index will have the same problems as those discussed in the Tomsett et.
Reject. rating score: 4. rating score: 4. rating score: 4. rating score: 5. <BRK>Summary:The paper generalised Type II ML regression models for scenarios where different noise dimensions cannot be assumed independent, but instead one needs to model the full covariance structure. This is clearly an important problem and it is well motivated in the work. The development is restricted to a specific, relatively simple, model family that is frequently used in several fields but that is not at the core of the ICLR community. For example, there is no discussion on whether the result derived here would have uses also in other model families. This is sufficient, since no clear comparison methods accounting for full covariance are available.<BRK>The authors propose a methodology for type II maximum likelihood on a hierarchical Bayesian model for EEG signals. The model, as claimed by the authors, is fully Gaussian and therefore tractable. To address this, the authors propose a mechanism for, what they claim is, efficient optimisation. A single set of experiments using synthetic data was considered, where the proposed method was compared against other benchmark. I also would like to emphasise that the discussion of the paper states that  "This paper proposes an efficient optimization algorithm for jointly estimating...." and "The benefits of our proposed framework were evaluated within an extensive set of experiments ". None of these claims are true or at least they not validated by any supporting evidence in the paper. Perhaps with the stated future work and stronger experimental results (real data), this paper can be improved.<BRK>The paper proposes an efficient optimization method for estimating the full noise covariance in a hierarchical Bayesian framework. I think the proposed method is an effective tool to estimate the full noise covariance especially for the problem setting in this paper. But the overall novelty and contribution are not strong enough for the ICLR community.<BRK>Joint Learning of Full structure Noise in Hierarchical Bayesian Regression ModelsSummary:The paper argues that modeling the full covariance structure in a sparse bayessian learning setting leads to significantly better results in eeg inverse problems. The proposed method is evaluated on simulated data. 4.Experiments are reasonable and presented clearly. 2.Has this particular problem (sparse bayesian regression with full covariance noise) not been considered by others? (I think both ML II and MCMC and possibly other methods have previously been used.) 4.Experiments on simulated data highlighting more clearly the *algorithmic* advantages of the proposed method would be appreciated.
Reject. rating score: 4. rating score: 4. rating score: 4. rating score: 7. <BRK>**After Rebuttal** I would like to thank the authors for their rebuttal. They further empirically evaluate the robustness of explanations generated after proposed changes on real life datasets. 3.The evaluations in the paper are weak, it is trivial that if perturbations are from data distributions the attack proposed in Slack et. The data generating model itself a black box model and involves more uncertainties in explanations.<BRK>Related work: Saito et al.address this problem concurrently https://arxiv.org/pdf/2006.12302.pdfAll in all, I find the work to be a useful step forward, but believe that it would benefit from more thorough analysis before publication. The % of the time that the sensitive variable appears in the top position will also depend on how aggressively the biased classifier is used. I d like to see some sensitivity analysis to the number of samples.<BRK>The main proposal of the paper is to alter the way in which LIME, SHAP, etc generate the perturbations needed to compute the explanation. I could not find this information in the paper. Although I liked the idea exposed in the paper and enjoyed reading the background and related work, the experimental section and the conclusions interpreted from results seem a bit preliminary. The paper focuses on the impact on the robustness to attacks, but more discussion and empirical results about the impact on explainability of the original method would be required.<BRK>This work also shows that the IME method is more resilient to adversarial attacks in comparison to LIME & SHAP, while both LIME and SHAP would benefit from the proposed data generators. The result on the robustness of IME method is good. Authors have submitted modified version of the code i.e.gLIME & gSHAP which use the proposed improved data generators. Using training data distribution may perhaps improve the overall quality of explanations as well i.e.beyond making them robust to adversarial attacks, it might be good to discuss any such benefits in the paper by considering explainability metrics such as monotonicity, faithfulness, etc.
Reject. rating score: 5. rating score: 5. rating score: 6. <BRK>https://arxiv.org/abs/1709.02341  The equations are not numbered in the paper, but the first equation in section 3 is a little unclear given the paragraph before it on how it would be obvious that this follows. Post Discussion  The discussion with the authors improved my understanding of how the paper fits with recent work. Q appears to be overloaded many times in the mathematics of the paper and makes it a bit difficult to follow the theory and section 2.<BRK>Conference on Robot Learning. This should be explicitly mentioned if the authors put this statement in conclusion. The theoretical contribution of the paper is to find the relationship between EFE and negative value function and proposed a prior preference learning method. If this is not equivalent to Q function, it cannot be very clear.<BRK>The paper sheds light on a novel interpretation of active inference from the point of view of RL and demonstrates a theoretical connection between the two. It is quite hard for a reader not truly familiar with the field to follow. We can assume that PPL refers to Algorithm 1, although the latter is never referred to in the text. Also, it is not clear what "conventional global preference" refers to. Also,  it would help put things in perspective to compare the authors  approach to classic RL/IRL algorithms.
Reject. rating score: 4. rating score: 5. rating score: 5. rating score: 6. <BRK>The paper studies an early exagerration constant \rho that trades off the attraction vs repulsion parts of the manifold learning algorithms at the early steps of optimization. The paper provides a comparative analysis of objective functions for several manifold learning algorithm through the lights of effect of \rho. It shows that the embedding of UMAP and ForceAtlas2 could be roughly recovered using t SNE with a special choice of \rho. The paper also proposes an explanation why UMAP works better when initialized from LE and why t SNE gets better results from early exaggeration. The paper seem very incremental in nature, since Elastic Embedding (Carreira Perpiñan, 2010) already studied the effect of \rho in a very similar manner to the findings proposed in the paper. The authors added the analysis between t SNE and most recent UMAP and ForceAtlas2 algorithms, but most of the novelty about the effect of \rho and the connections to LE and early exaggeration are quite apparent. Most importantly, the paper doesn t propose any new algorithm or any improvements beyond insights. For example, it is not clear to me, ho the authors define "high repulsion embeddings" and how much lower the repulsion strength should be.<BRK>Authors of this paper conducted study on existing neighbor embedding methods from the perspective of the attraction replusion spectrum. Pros:1)	Extensive experiments demonstrate the phenomena that neighbor embedding methods can achieve the continuous manifold structures or discrete clustering structures by varying the balance parameter. 2)	Detailed implementations of existing methods are taken into account for the analysis, so it is a good guideline for users of these methods on real applications. Cons:1)	The novelty of this paper seems limited. The attraction replusion property is not new to the existing neighbor embedding methods. 2)	Authors conducted various experiments to show the property, but the conclusion is only empirical. 3)	Authors may need to demonstrate how these observations can make improvement over the existing methods or motivate new models. 4)	To better demonstrate the continuous manifold or discrete clustering structures, authors can take various real data with clear underlying manifold structures that have been studied in the literature. Simulation data is good, but it is less real since the noise may significantly affect the experimental results.<BRK>Summary:In this paper, a unified view of embedding methods for visualization is presented. The main message is that, Laplacian eigenmaps and t SNE are governed by a single formula, and the difference of them can be seen as a difference of a hyperparameter value. We can also approximately recover two different embedding methods   UMAP and ForceAtlas2. Using a few benchmark data sets, the relationships of these methods are visualized. The main result   a general equation that covers several embedding methods   is technically interesting. However, it would be not clarified what kind of benefits we can get from the finding. Another concern is that it is not easy to judge the visualization results (e.g.Fig 2) because they are qualitative and subjective. Minor comments:  $\sim$ in Eq.(3) seems to mean the equality up to constant but such usage would be not common. It would be better to add an explanation.<BRK>Summary: the authors study a number of neighbor embedding methods in terms of attraction repulsion forces. The authors show that t SNE, UMAP, FA2, and LE can be (approximately) unified as a common approach that use different levels of tradeoff between these two terms. Review: a main portion of the technical contribution of the paper is simply writing down the gradients of the commonly used DR methods and providing an intuitive comparison of gradient terms. However, there is not much rigorous mathematical result to support the claim. I am aware of the complexity of such a strong theoretical result (e.g.the work of (Arora et al.2018)).However, the current version of the paper does not offer any concrete result and remains on an intuition level observation. The experiments partially support the claims, but similar observations have been made before in related work. For instance, (Amid and Warmuth 2019) qualitatively sort DR methods by means of their "global score" (which is a notion of how well a method preserves cluster information).
Reject. rating score: 4. rating score: 4. rating score: 5. rating score: 5. <BRK>This paper proposes AUBER which applies reinforcement learning algorithms (DQN) to progressively prune attention heads from the lower layer to the higher layer in pre trained transformer models (BERT) in order to improve the model fine tuning. The state of DQN is the L1 norm of the value matrix of each attention head, the action space for each layer is the total number of attention heads with an additional quit action (H + 1). Experiments on 4 tasks from GLUE shows the effectiveness of AUBER against baselines. Here are a few concerns. It is better to have a discussion compared with those algorithms. The 4 tasks evaluated are small. Does AUBER also help in these settings? Thus, it is better to report the comparison of training time between these baseline systems. Pruning attention heads in BERT is interesting. Is there any way to apply the current approach to other components in BERT?<BRK> Paper Summary:This paper proposed to prune heads of multi head attention in BERT to achieve regularization for tasks with a smaller dataset. The authors use DQN to learn a policy to prune attention head layer by layer. They demonstrate improvements on 4 small tasks from GLUE. ReviewThe central idea of this paper is interesting, but the experiment is not convincing enough. This paper needs much stronger experiments to support their claim. Experiment is weak. Lack of comparison with other regularization techniques, for example those mentioned in related work. The training overhead of this method is not discussed. It involves repeated finetuning after each layer is pruned.<BRK>Summary:The paper focuses on reducing over fitting for the BERT model by pruning the attention heads. from top to  bottom or the reverse. +ve   The paper is easy to follow and the authors have presented the details in smaller sub sections which makes it easy to understand. Using RL to determine the head pruning sequence makes if better than the previous greedy methods like Michel et al.and Voita et al.Concerns  Training routine seems very time consuming as the network is fine tuned after removing every single attention head. The authors can try to remove the heads in a batch in order to reduce the training time. For reducing over fitting simple methods like increasing the dropout probability or reducing number of transformer blocks could also be tried. It would be good if the authors can provide comprehensive comparison with simple techniques in order to justify the multi step training for AUBER.<BRK>Summary: This work proposes learning to prune attention heads in BERT in order to achieve better accuracies on downstream tasks, especially when there are a small number of training examples. The authors employ reinforcement learning, or more specifically deep Q learning, to learn the pruning policy in a layer wise manner. Experiments show that after pruning through the proposed method, the performance improves. The use of RL for pruning policy is reasonable, and to my knowledge this is the first work on BERT models. The comparison in Table 3 may not be fair since different numbers of heads are pruned under different strategies, therefore the models have different capacities. 3.Only four of the GLUE tasks are included in the experiments, it will be more interesting to know how the method works for other GLUE tasks with relatively large training data. 4.The performance of finetuning GLUE tasks could be unstable. Are the results reported in the paper from a single run or average of multiple runs?
Reject. rating score: 3. rating score: 4. rating score: 4. rating score: 5. <BRK>Weaknesses:Ordered form less to more specific:1) The intention of the paper is unclear. The intention of graph convolutions is to explicitly encode assumptions about the relationships present in the data, in this situation why would I prefer a soft constraint to a well motivated, explicit one? If structural constraints were unduly restricting the expressiveness of the models, I would expect to see this borne out in the empirical results, but this is not the case across the chemical reaction and citation network experiments. 3) The paper struggles with clarity at points. the citation network or pixels in an image, or does it describe adjacency of the observations as defined by the observed labels or other meta information. Reasons for score: I vote for rejecting the paper, as while I really do appreciate that the results of the paper are presented honestly, I think there are concerns with the current draft.<BRK>**Summary**:This paper investigates different ways of incorporating topological information about the data in the machine learning models. The experiments demonstrate that for data with a certain topology type, the introduced loss can provide performance when used together with existing methods. In this light, I feel there is not too much new insight in the paper to warrant a publication at ICLR. Most importantly, a comparison to [2] is missing. **Conclusion**:While the work has interesting motivations and is well written, it has not done a convincing job at demonstrating the effectiveness of their proposed method or shown a thorough experimental analysis. While I appreciate the author s effort in adding more experiments, I do not think the added experiments and reply properly addressed the concerns shared by other reviewers and myself. I think this paper is not ready for publication.<BRK>2.The related work is well explained. 2.The comparison of the paper is problematic. Can the authors confirm that the comparison is fair and meaningful (e.g.eliminating other confounding factors like controlling the number of parameters)? For example, we can control adjacency matrix received by graph neural networks. In between, we can corrupt input graphs (e.g.randomly adding or deleting some edges) before feeding it to graph neural networks. It mainly justifies which method can perform better in downstream tasks instead of justifying the importance of manifold. 4.Some baseline methods are not considered, for example the methods learning latent graphs: Semi supervised classification with graph convolutional networks and Glomo: Unsupervised learning of transferable relational graphs. Based on these cons, I think a more rigorous comparison is needed.<BRK>The principle objective of the proposed GR VAEs is to use the learned latent space features as the input for improvement on downstream tasks especially for classification. Pros  + This paper is well written and the idea is clear and easy to follow. Also, if we directly combine the raw data feature with embedding by some manifold learning technique, and input it into the vanilla VAE, can we get similar result (the graph topology is preserved) as GR VAE has? I think the goal of the experiment is to demonstrate that, the embedding learned by GR VAE is superior comparing to other kinds of features such as from the vanilla VAE, so I expect the embedding by GR VAE is applied to different models (e.g.GraphSAGE, GCN, DeepWalk) instead of just DGI (at least I only notice that DGI is adapted), and on each model the result from using GR VAE can outperform others using raw data features or other kinds of finetuned features.
Accept (Poster). rating score: 8. rating score: 6. rating score: 6. rating score: 6. <BRK>The proposed solution outperforms the baselines3. 4.The paper is well written and easy to follow. A large body of work on weak supervision has been completely ignored by the authors. These are some relevant papers:  Using weak supervision beyond classification (for ranking): https://dl.acm.org/doi/10.1145/3077136.3080832  Learning from multiple weak supervision signals: https://dl.acm.org/doi/10.1145/3209978.3210041  Some theoretical justifications for weak supervision training (for ranking): https://dl.acm.org/doi/10.1145/3234944.32349682. It is different from the proposed solution in the sense that a small set of labeled instances are used to assess the quality of weak supervision signals as opposed to labeling the weak supervision functions themselves. Some functions may not be easily assessed by human annotators and this may influence the model.<BRK>These will be significantly easier to judge by a human than other text classification tasks (e.g.the "Salman Rushdie" example in the introduction)  It would be interesting to compare the proposed approach to a setting where a user is asked to generate LFs based on a description of the tasks and a few examples. The user experiments are interesting and  provide very good insights about the method but there are several open questions about the experiments:  The paper defines an LF family based on the existence of unigrams and exhaustively generate LFs from that family. Edited after authors responses: I would like to thank the authors for the detailed response and the changes they have made to the paper. However, it is not clear to me that this would generalized to other text classification tasks (not to mention language tasks beyond text classification)  It is interesting to measure and show the effort of validating labeling functions vs. labeling samples.<BRK>This paper proposes a new framework for interactively selecting labeling heuristics in a weakly supervised setting. The main idea of the proposed approach is to combine weak supervision and active learning. Compared to the previous work which relies on human manually create labeling functions (the abstraction of the weak supervision), this work defines a family of labeling function and uses an active learning method to interactively identify a set of labeling functions that maximizes the utility based on the usefulness by the users. Main comments:The idea of using an active learning approach to select useful labeling functions from a large pool of labeling functions based on usefulness is interesting and the paper presents compelling solutions to a practical problem. It s crucial to provide more details here. 2.For the final subset of labeling functions, the authors defined the three scenarios. The distinctions of these different scenarios are not clear. Snuba has some image applications.<BRK>This paper proposes a new approach for active learning by interactively discovering weak supervision. The paper is well motivated and the proposed active learning strategy is derived based on theoretical guarantees. If so, how can the model distinguish between the candidates in the beginning? The experiments have been performed on several text classification tasks, where the label function family contains only uni gram indicators. Can the proposed method be extended to more complicated (and useful) label functions such as regular expressions? Since here the feedback from human is just whether a uni gram is indicative to the label, which can be easily learned by a neural model. "Learning to Ask for Conversational Machine Learning." EMNLP 2019. It also seeks human supervision beyond the instance level. After author response:The authors presented new experiments on image datasets during the rebuttal, which demonstrate the flexibility of the proposed framework. It is still unclear whether and how this method can help more practical problems.
Reject. rating score: 4. rating score: 5. rating score: 5. <BRK>The single model regularization terms proposed in the paper seem to encourage the model to become overconfident. This will reduce churn but is clearly not desirable in most practical applications. This inflates the performance estimates on this data obtained for the proposed methods compared to the baseline, which does not have these hyperparameters to play with. I would also like to see results for churn when standard forms of regularization are included, such as L_2 or L_1 regularization. As these should also increase the stability of the learning process, it stands to reason that they will also reduce churn (particularly when the hyperparameters are tuned on the test data). has an effect on accuracy. The paper eliminates churn due to data augmentation by removing data augmentation. Instead, the random number generator used for augmentation should be initialized in a deterministic manner. Why is SChurn_1 denoted a percentage in Table 1?<BRK>The fact that it increases in Table 1 suggests to me that we are indeed observing a confounding effect where the main cause is the drop of accuracy rather that the removal of data augmentation. Pros:1.The paper is well written and clear. 2.Studies dissect many components, e.g.churn caused different sources of variation, ablation study of the proposed co distillation+entropy. The optimization of these hyper parameters can lead to misleading results if hyperparemeters of baselines are not optimized accordingly with similar budgets. In light of the response of the authors and the other reviews, I still recommend rejecting the paper, with a rating of 5. When modifying data augmentation or data order, it is not possible to determine whether the change of churn is strictly due to data augmentation/data order or to accuracy drop. One way of avoiding this confounding effect would be to conduct hyperparameter optimization in a way to enforce a given level of accuracy (ex: 88%). Note that the main differences here beside the fact that data augmentation is fixed or random, is that the accuracy is lower by 1.5% 2.5%. We see again an increase in churn related to a decrease in accuracy. I understand it is more effort as I recently went through the process, but it is possible. We boldfaced the results in table 2 with the best mean performance, which we believe is a standard practice. ResNet is trainable in a deterministic way using PyTorch for instance.<BRK>From what I can tell, churn and disagreement are the same thing, and churn has a different English meaning. Disagreement seems like the better term for this. The authors identify several sources of randomness, from underlying hardware differences to parameter initialization and more. Why is Table 2 referred to before Table 1? The second is to use co distillation, a form of online ensemble learning. The paper does a decent job of making an important point about churn, investigating its prevalence, and proposing a solution. For a narrow result like this, specific to one metric of neural networks, I would like much more empirical validation that the authors provide. Only three data sets and three baselines does not seem like enough, given that the experiments provide the main take home message of the paper. For example, imagine a facial recognition system. In the current paper, it s mostly assumed to be undesirable. To an extent, I agree, but I d like to understand more clearly why it is undesirable. I don t think the authors need to cite quite so many papers about the much more general problem of reproducibility in science. This finding is interesting and instructive: "churn observed in Table 2 is not merely caused by the discontinuity of the arg max". This finding is fascinating: "Even with extreme measures to eliminate all sources of randomness, we continue to observe churn due to unavoidable hardware non determinism."
Reject. rating score: 5. rating score: 5. rating score: 6. rating score: 6. <BRK>This paper proposed a new regularization method via patch level interpolation. Then patches from its neighbors will be used to interpolate to each patch in that given image. Moreover, in this paper it has demonstrated such regularization can be extended with virtual adversarial training and mixup training. Although the proposed training strategy is simple, and the proposed patch interpolation is able to achieve better performance when compared with some baselines. It is still not comparable to existing semi supervised training works such as Mixmatch or FixMatch. In the last section of the paper, the author shows the extension with mixmatch  by surpass the mixmatch with little improvement. (Note, fixmatch seems simpler than proposed method in terms of computational cost)Thus the proposed method does not convinced me for its effectiveness.<BRK>The paper proposes a regularizer called Patch level Neighborhood Interpolation (Pani). The idea is to construct a graph over patches, and interpolate new patches during training. The paper applies Pani to othter two regularization methods VAT and MixUp. The paper shows better results through experiments. The writing of the paper can be improved. Here are some questions or weaknesses below, which may be all due to the writing issues. The appendix is not attached to the paper. I ve read the updated paper and other reviewers  comments. Figure 1 is very confusing. In general, I d like to maintain my initial rating as a borderline paper. First, I realize that authors are not familiar with the policy, because they did not attach the appendix to the manuscript but uploaded as a separate file. As a result, their updated paper did not address issues in the review effectively. Is there a visualization of $r$? Second, in terms of other data augmentation, the authors merely say "As data augmentation is a common trick, consistent improvement can be easily expected across all methods". Figure 2 shows multiple design choices, but the paper does not say any values of them in Table 1. Table 1: While it says "without data augmentation", aren t Pani and VAT data augmentation techniques? Would the proposed method still have advantage? The paper should conduct such an experiment to justify this by applying Pani on more layers given that the cost is low as studied in Figure 2.<BRK>The paper proposes a general regularizer called the Patch level Neighborhood Interpolation (Pani) that constructs patch level graphs at different levels of neural networks. Specifically, it is based on the k nearest patch neighbors at each layer and linear interpolation for each patch. By applying this proposed regularizer framework into two special cases and get Pani VAT and Pani MixUp. Numerical experiments are comprehensive and convincing. Two special Pani based algorithms within a batch are proposed with applications in image classification. Experimental performance in terms of accuracy and running time shows that the Pani regularizer can improve the algorithm. Cons:The motivation of combining patch based k NN and linear interpolation is not fully clear. No theoretical guarantees for this improvement are provided, which could be strengthened in the revision. Overall, the work is important and interesting which could provide insights to other related works in the area.<BRK>The proposed Pani seems to be novel. It can explore the information of the neighboring relationship between samples and can be regarded as the meta regularization. For the general formulation of Pani, how does the number of the nearest neighbor patch graphs affect the results? As I am not familiar with this topic of the paper, there are lots of regularization methods, it would be better to add more details about the regularization methods that do not neighboring relationship among samples.
Reject. rating score: 4. rating score: 4. rating score: 6. rating score: 6. <BRK>This paper proposes an efficient algorithm to obtain a given node s embedding based on its local PageRank scores. The proposed approach uses a hashing technique to hold similarities between nodes effectively. Besides, it exploits a local partition approach to compute PageRank scores.<BRK>###### Summary ######The paper proposes a node representation learning method for undirected networks based on the Personalized PageRank algorithm. ###### Weaknesses ######  The experimental evaluation is weak and does not allow us to draw meaningful conclusions about the proposed algorithm. The proposed algorithm does not show significant performance improvement on the link prediction task.<BRK>Summary:Contributions of this paper are twofold. Second, it presents InstantEmbedding, an efficient implementation based on PPR and random projections. For DeepWalk, in particular, complexity is proportional to the parameters defining the random walk, not n as shown in Table 1. Can you explain why DeepWalk cannot be described as a local method since it is based on truncated random walks? There is no clear reason why the proposed approach will be good for solving this task.<BRK>The authors propose an InstantEmbedding approach that learns d dimensional embedding for each node in sublinear time. As the paper claims that this approach is a local embedding but globally consistent, users can quickly learn vector representations of nodes based on the local structure, being free from possibly growing the rest of the graph. (Originality and Contributions)The proposed algorithm: InstantEmbedding is simple. And, that could be a significant part of the Locality. More importantly, the message passing scheme in GCN is scalable to multi graph or even hyper graph, which is not readily clear in the proposed method.
Reject. rating score: 4. rating score: 5. rating score: 5. rating score: 6. <BRK>The paper is bad written, very difficult to extract, and understand what the authors want to express. The descriptions are very unclear, figures are not illustrative. I suggest the author polish the paper in a better form. I agree with the motivation and observation that the high level tasks that requires object level understanding are similar to the BERT. I am curious about the relations and methods of using the self supervised learning as an auxiliary task. Overall, I believe there are huge improvement space for the writing.<BRK>This paper studies the temporal and spatial reasoning in videos. So the novelty is a major concern. Is there a pretraining phase for the masked object representation prediction? Any reasons why not compare with other baselines in CATER like "Learning Object Permanence from Video" by Shamsian et al? While the author resolve some of the concerns, they are encouraged to further polish the paper and use more evidence to support their claims.<BRK>Results show considerable performance improvement and data efficiency compared with prior methods. Pros:+ This paper is easy to follow. + Results on performance and data efficiency are impressive. It is not. Also note that whether such a family of model indeed possesses a certain level of reasoning is still debatable, as some researchers found in conversational reasoning (e.g., [1,2]) and in particular, advocated by Gary Marcus. Some sections of the writing are more than necessary.<BRK>Summary: the paper propose to tackle visual reasoning problem in videos. This would make the main message of the paper even more salient and bulletproof. on image QA and R3D (Girdhar & Ramanan, 2020) on CARTER, which is a video reasoning task / benchmark. The solution chosen makes intuitive sense   discover objects using MONET and encode object encodings to make the final predictions using transformers.
Reject. rating score: 3. rating score: 4. rating score: 5. rating score: 6. rating score: 7. <BRK>Specifically, based on arithmetic coding and k d trees, AriEL maps sequences of discrete data into volumes in the latent space, and can then retrieve sequences by random sampling. Results show that it can generate more diverse and valid sentences. AriEL takes the frequency of sentences into account, and the frequency implicitly models the statistics of human language. My major concern is the evaluation. The evaluation metrics in the paper are very limited and not convincing. They fail to assess the strengths of the proposed sentence embedding. I will mention a few that confuse me most:\(1) In the second paragraph of Section 3.2, what does “but the probabilities defined by the latter are used as a deterministic Russian roulette” mean? And in Table 1, what does “comply with the bias” mean?\(3) In Section 3.4.2, I can hardly believe the definition of “grammar coverage”. But again, these are to be proved by downstream tasks. ***UPDATE: The authors have addressed some of my comments. I appreciate their efforts for making the paper clearer. Also, the paper would be much better if its writing and organization can be improved.<BRK>It leverages essences of arithmetic coding and kd tree to encode/decode sentences with a fixed region of the space. The idea is interesting. However, there is some disadvantages of the proposed encoding which are not always mentioned in the paper. First, due to the topological difference between the proposed encoding and other spaces (e.g., Euclidian space), the proposed encoding could not be treated as embeddings in some usual meanings, e.g., it is hard to calculate the "similarity" between two encodings by arithmetics on real numbers as many deep learning methods implicitly does. Second, the resulting encodings will be affected directly by the capability of the actual representation of real numbers.<BRK>Specifically, AriEL uses a language model to split the latent space into volumes. Originality: The idea to incorporate more structure into the learned encodings via arithmetic coding and K d trees is an interesting one. dataset.Cons:   I think the paper would be more significant if the authors would have demonstrated the efficacy of their method on more benchmark datasets; for example, WMT (Kaiser et al.2018) or WikiText (Merity et.al 2016).I am curious as to how the performance of some of these models are *so* bad: for example, the Transformer obtains a validity score of 4.7% on the toy dataset, and the AE/VAE get roughly 0% accuracy on the prediction task. Or is this specific to an RNN? I will keep my score as is, since I think that the paper would greatly benefit from more practical/rigorous empirical evaluations to demonstrate the usefulness of the approach.<BRK>The algorithm is based on arithmetic coding. In general, I believe this is a well written paper. Other than the writing, I still have concerns about the motivation and evaluation. In the first point of contributions, the author states that the proposed method improves the retrieval of learned pattern with random sampling. Does this mean when the coding is randomly sampled, we can see more valid sentences comparing to other methods? If so, the validity in evaluation is the key to claim this contribution. This low score strongly contrasts with our knowledge that a well trained Transformer language model is very strong at producing valid sentences. Upon reading the experiment section, I couldn t find an explanation. When comparing with VAE, it s important to compare the interpretability of the latent variables, which is the main purpose we train a generative model. However, if the interpretability is not a major motivation of volume coding, then I m concerning whether it s meaningful to compare with VAE. Considering all these factors, I decide to give a weak acceptance to this paper.<BRK>This paper proposes "volume encoding" for sequence modeling. Unlike traditional autoencoder or variation autoencoder model family, the proposed model AriEL applies KDTree to map the input sequence to a quantized multi dimensional space as the code, and supports tasks such as reconstruction and generation. From its root, data like language are naturally discrete. AriEL, on the other hand, represents this type of data into the discrete mode. **table 1**Can the authors clarify on how to quantitatively or qualitatively measure the correlation between these generations and the training dataset bias? **table 2**It s pretty well known that VAE type of reconstruction may have grammar errors like repeated words. The AriEL model seems to be able to avoid such error but at a cost of wrong n grams like "small large". Could the authors give some intuition here? **conclusion**With all the questions above, I still vote an accept for the proposed model for its novelty.
Reject. rating score: 5. rating score: 5. rating score: 6. rating score: 6. <BRK>Formalizing the offline meta RL paradigm, where we meta train on pre collected (offline) data for several RL tasks and adapt to a new task with a small amount of data. 2.Introducing MACAW: an algorithm for offline meta RL that has the desirable property of being consistent (i.e.converges to a good policy if enough time and data for the meta test task are given, regardless of meta training). To do so they rely on MAML (which provides consistency) and AWR (a simple, popular offline RL algorithm) and add a couple of changes: some hyper network like parameterization to add capacity and adding an extra objective in the policy update to enrich the inner loop. Pros:      1. Most of the experiments are well executed, using good baselines and as well as providing understanding through ablations      3. The offline meta RL formulation should include behavior policy as part of the task definition. MACAW is a nice simple algorithm with good guarantees. Cons:    1. In particular, the paper largely borrows the meta RL formulation from the online setting where task MDP.<BRK>This paper proposes a method for "fully" offline meta RL. Specifically, they assume there is no interaction with the environment at all neither during meta train nor meta test and this method only sees previously collected data at all times. Their method is built on top of AWR [1] in which policy updates are weighted by the advantage term. In batch RL, even though we assume there is a fixed data for training ( no interaction with an environment whatsoever), the final performance will be measured by interaction with the environment (i.e.policy will be evaluated in an online setting). However, this paper assumes that there is no such interaction with the environment exists. The question is how the setup in this paper is sensible and important at all? Per paper definition, if an algorithm can find a good solution to test tasks regardless of the meta training task distribution, is called consistent. Did you follow PEARL setup for the experiments or ProMP/MAML? PEARL might get better result with hyper parameter tuning.<BRK>This paper introduces a new problem setting in meta reinforcement learning, namely metaRL. Since we re in the *fully* offline metaRL setting this should be possible? Pros:  The paper is very well written, and easy to follow. Would MACAW also work on the standard metaRL setting, where the agent is interacting with the environment during meta training and therefore responsible for collecting the data itself? Why is MACAW listed as a "sensible" approach? UPDATEI have read the other reviews and the author s response. In the experiments on 4 MuJoCo benchmarks the proposed method outperforms two baselines, Offline PEARL and Offline MT+FT. Some last comments:  Several other reviewers also raised concerns that the "fully" offline setting might be unrealistic. Throughout the paper, you stress the importance of the algorithm being consistent. It would still be great to add an experiment where MACAW is adapted online at test time (entirely without offline data) like in C.2 but on in distribution tasks. All the ingredients are there though and I think with some extra work the authors can easily improve their paper. What I would like to see is, if there is a significant shift between training and task distribution, can MACAW recover from a bad initialisation? A main motivation for the offline RL setting is that you can use real world data and train a metaRL agent using this.<BRK>The paper proposes the problem of fully offline meta RL. Here, the idea is to leverage offline experience from multiple tasks to enable fast adaptation to new tasks. The paper distinguishes two settings of offline meta RL, one where only the training data is collected offline and testing corresponds to sampling online trajectories, the other where both training and testing data are collected offline. The paper also proposes a method for the fully offline meta RL problem based on the MAML method. A solution is proposed for the fully offline meta RL problem. The modifications to the policy functions are backed by theory and is also empirically verified to be helpful in the experiments. Extensive ablations on the various modifications to MAML+AWR confirm that the utility of the approach for the fully offline meta RL problem. But given my hesitation with the utility of the fully offline setting as well as the experiments, I am recommending a weak accept. It will also help if the authors can provide more motivation for the "fully" offline meta RL setting.
Reject. rating score: 4. rating score: 5. rating score: 7. rating score: 7. <BRK>Furthermore, the comparison is performed with old quantization methods. The paper proposes WaveQ which proposes a sinusoidal regularization approach for quantizing Neural Networks. In particular:  The theoretical analysis provided does not apply to the proposed method. The accuracy results provided in the empirical section incur significant degradation as compared to the baseline.<BRK>This paper proposed a regularization term to control the bit width and encourage the DNN weights moving to the quantization intervals. Pros:1.The paper is well written and easy to understand. 2.The idea of using the sinusoidal period as a continuous representation is novel and it makes it natural to use gradient descent methods to optimize the bit width.<BRK>The paper proposes using a sinusoidal regularizer for neural network quantization. Because the period of the function is highly related to the required bit width, it can be used to determine the bit width while keeping good characteristics   continuous and trainable. The proposed method is widely adaptable and easy to use with quite promising results. However, this paper is still valuable and does not hurt novelty. 2.Many recent quantization papers use w / max(|w|) to normalize weight.<BRK>This is where the current paper introduces a neat trick. The authors do compare to a  "decrement the bitiwidth of a single layer" baseline, which is a good start. However, this still leaves the problem that the $\beta_i$ would be real, not integer, as required for quantization.
Reject. rating score: 4. rating score: 5. rating score: 6. rating score: 6. <BRK>**Main Claim:**In this work, the authors propose to use the Feature Statistics Alignment paradigm to enrich the learning signal from the discriminator in a sentence generation GAN. **Contributions:**This work introduces an novel and interesting idea of Feature Statistics Alignment in training GANs. The experiment results show that the proposed model outperforms existing models. **Strong points:**The idea is novel and interesting. There’s a major flaw in the evaluation metrics.<BRK>Summary: The paper proposes an improvement to sequence generative adversarial networks (GAN) to cope with the common training issues of GANs. In particular, the rationale behind the proposed method is justified. The paper aims at addressing a major issue in training GAN for sequence generation: how to strengthen the learning of the generator compared to the discrimination network which is easier to train? The spotted typos are fixed in the revision. The comprehensive ablation study is interesting and helps to understand how each module (feature alignment, Gumbel Softmax, batch size) contributes to the enhanced performances.<BRK>Summary:The paper addresses the task of improving GANs for sequence generation and proposed a method based on the relativistic discriminator. The proposed method employs a Feature Statistics Alignment (FSA) paradigm to reduce the gap between real and generated data distributions. It also outperforms baselines on human evaluation based on the acceptance, grammaticality, and meaningfulness of the generated sentences.<BRK>[Summary]This paper proposes a new GAN based text generation method that incorporates feature statistics alignment and gumbel softmax for reparameterization to deal with mode collapse and unstable training. But I have some concerns as well. Text generation is important problem. [Weakness]  The authors insist the use of Gumbel softmax in GAN tranining is under explored. There are more method using Gumbel softmax [Gu et al.2019] and a similar softmax with temperature annealing. Some related work  were missed such as DialogWAE [Gu et al.2019] and ARAML [Ke et al.2019].In particular, DialogWAE uses GAN and Gumbel softmax for text generation even if it focuses on dialog generation.
Reject. rating score: 5. rating score: 6. rating score: 6. rating score: 7. <BRK>The authors try to identify several problems in the Transformer model and modify the model architecture. The author should demonstrate the training efficiency of the proposed model. This trick cannot be viewed as a contribution to the paper. 5.Regarding experiments and comparisons. Overall:The general problem that the authors want to solve is not very clear or very well motivated. The experimental comparisons and baselines are not adequate.<BRK>Overall:I found this paper interesting and relatively easy to follow. Pros:1.Simple idea and the flow of the paper is easy to follow. 2.An extensive set of experiments to verify both the usefulness of the Feedback Transformer and the limitations that the authors hypothesize to be true for transformers. Cons:1.The introduction of sequential ness to Transformer is good but obviously would slow things down especially as the sequence gets longer. The authors reported on this very briefly, but I think it is an important enough aspect to warrant more analysis.<BRK>### SummaryThis paper modifies transformers with feedback memory. It seems that the proposed needs to take a much longer time to train as the authors mentioned it in a sentence on page 8. The authors can give more results and discussions so that future users can know whether to choose transformers with feedback memory according to their situations. * (optional) In Table 3/4, how about feedback transformer that keeps the same number of layers and similar parameters as Trans XL. It is just an optional discussion as feedback transformers seem to need much time to train and the rebuttal time is limited.<BRK>The main topic of this paper is modification and enhancement of Transformers originally proposed in Vaswani’17. This paper focuses on the limitations of the Transformer architecture as an autoregressive model. Although the proposed method does not obey the original concept of Transformers, the findings from this paper s experiments are very impressive.
Accept (Poster). rating score: 7. rating score: 6. rating score: 5. rating score: 5. <BRK>Summary: This paper deals with the problem of bounding the amount of errors that a model can make when attacked by an adversary limited to small perturbation of an input image. As opposed to previous papers, the authors suggest to perform formal verification to prove that the samples are either robustly classified or abstained on (as opposed to testing the classifier + detector adversarially which would have left open the question of whether or not the attack against the pair was done in an adequate way). The crux of the paper lies in redefining the way accuracy should be computed in the case of an abstain option and adjusting verification and robust training methodologies for it. There might be some potential extension to this paper in the direction of OOD detection. Out of Distribution examples tend to make the classifier perform poorly. Opinion: The paper is quite interesting to read, and as far as I can tell is the first one to apply verification methods to the detection of adversarial examples.<BRK>The paper has merits, it is formally sound and it improves the SOA. However, my rating is not so high for the following reasons:  Only a short paragraph at the end is devoted to the natural vs robust error trade off and the comparison is done only with IBP (another defense method of 2018). What is the point of developing classifiers that are robust to AEs when they are not robust to other normal images (like those found in the test set)? I do not agree with that; it should be considered successful only if the input is classified as the correct class y, period. The "abstain" class is just a reject option, which is not the same as the correct classification. One can always resort to rejection to improve the error rate in the non rejected part of the dataset. What would be the error reject trade off curve of the base classifier in these same datasets?<BRK>##########################################################################Summary: In this paper, the authors propose an additional "abstain/detection" loss term into training, so that the classifier can either robustly classify or detect an adversarial attack. Hyperparameters in the objective trade off between clean and adversarial accuracy. I like the idea of adding an "abstain" error term in the loss during training, and I like that the authors have derived some (simple) bounds on the min max formulations. ##########################################################################Pros: The authors are trying to break the loop of adversarial training and subsequent adaptive attacks, by developing "provably robust" methods. The idea of incorporating an "abstain" loss term at the training stage, and trading it off with the true error in classification is interesting. Loss for misusing abstain class or mislabeling the true label could be something more general.<BRK>This paper aims to train networks that can map a possibly $\ell_\infty$ perturbed input to its class provably or map this input to the “abstain class” provably. This is achieved by training on the IBP output boxes together with a new loss function. The method diverts from the classical setting in which the classifier needs to be robust, by allowing the classifier to abstain. While this is an interesting idea, the paper seems to be rushed and not carefully written. The notation could be improved and presentation could be simplified. al.[4]) are missing in Table 1 and thus are not compared against. Further, some questions remain open after reading the paper:Questions:  Would the idea to have an explicit “abstain” together with the loss function work also for randomized smoothing [1]? Have you tried to use certification methods using tighter relaxations like k ReLU[2]? How would your method perform when using COLT [3]? Comments:  in equation 3, it seems to me as if the second ‘+’ for $\underline{z}_l$ should be a ‘ ’, similar for $\overline{z}_l$  in equation 6: $\ell_{\text{xent}}$ should be defined before usedThe comparison in the evaluation is not complete, questions remain open after reading the paper and evaluation is missing, hence this is a reject for me.
Accept (Poster). rating score: 7. rating score: 7. rating score: 7. rating score: 7. <BRK>This paper proposes a new class of stochastic processes determined by a distribution over Neural ODEs. The overall structure of the paper is clear. I find the newly defined process interesting and applicable to many real data sets.<BRK>This work presents a new method that combines neural ODEs, which uses neural networks to flexibly describe a non linear dynamical system, and neural processes, which uses neural networks to parameterize a class of functions. Also, is it the case that data from multiple contexts are trained in parallel? The decoder and the other pieces of the model are clear. I find this work to be an interesting and important extension to the existing literature on neural processes.<BRK>This paper proposes a new algorithm that can adapt incoming data points by applying Neural Ordinary Differential Equations (NODEs) to Neural Processes (NPs). The correctness of their claim and Clarity:This paper is well written and almost correct, but the details about the experimental setting look missed. Additional feedback:Thank you for submitting it. Weaknesses:1) Task details are not clearly described.<BRK>The proposed NDP has two main advantages: 1  it has the capability to adapt the incoming data points in time series (unlike NODE) without retraining, 2  it can provide a measure of uncertainty for the underlying dynamics of the time series. Does it imply that NDP can only work well in a specific conditions? There are no explanations and clarifications for these in the paper. In general, there is no mention on how to train the NDPs.
Reject. rating score: 4. rating score: 4. rating score: 5. rating score: 7. <BRK>11.Minor issues. The presentation has a strong theoretical flavour. At times, the paper is difficult to follow and lacks clarity. Figure 1 could be put at the top rather than in the middle of text.<BRK>The paper seems to be not finished yet. *ablation studies on MIAN*Which one is the counterpart of MIAN? I conjecture that the paper is not finished yet. ***paper editing issues*The methods in the table are not cited in both the table and the main text.<BRK>The paper is well organized. The overall contribution is not that large, as  the proposed information regularization method is simply based on recently proposed related work. (3) and Eqn. The experimental analysis seems to be inadequate. The presentation can be greatly improved.<BRK>I vote to accept the paper. I carefully read other reviewers  comments and responses. I enjoy reading the analysis of advantages over the existing solutions, which is well reflected in the experiments.
Reject. rating score: 3. rating score: 4. rating score: 6. <BRK>#### SummaryThis paper proposes a new SGD algorithm with heavy ball momentum and adaptive coordinate wise stepsizes, called SEHB. Firstly, the paper does not motivate the algorithm at all: it simply lists the ideas that the authors combine in this work, and the derivation of the stepsizes is done under the assumption that $\gamma_t 0$, which corresponds to the case with no momentum. Secondly, it proposes a momentum modification and tries to analyze it, but the theoretical momentum parameter has to decrease exponentially. Of course, this issue is common in regret analysis, but it is an issue nevertheless. The next issue is that only deep networks for vision were tested, while some methods fail to outperform Adam on NLP tasks, GANs, etc.<BRK>Summary: This paper proposes a new variant of Stochastic Heavy Ball method, combining Euler s method to adjust learning rates adaptively. They give a convergence analysis of the regret bound under the online convex optimization framework and conduct experiments on MNIST and CIFAR10. In Theorem 1, the bounded domain assumption is strong and unjustified: It can not be assumed since there is no projection in the algorithm. What is it expected to be in general?<BRK>Based on the idea of Euler’s method, this paper proposes an algorithm that adaptively adjusts the directional step sizes. The algorithm also incorporates Heavy Ball momentum with a tunable momentum parameter. A convergence analysis of the algorithm is provided in the case of a decaying learning rate. Note that all the other adaptive methods, such as Adam, Addelta, PMSprop etc, use two independent hyper parameters (e.g., step size and beta_1 in Adam) to control the two seemingly independent aspects of the algorithms. Minor:Title of Section 3.2: convergence analyze  > convergence analysis
Reject. rating score: 5. rating score: 6. rating score: 6. rating score: 6. <BRK>Overall, the NeuroChains method appears to do a good job at extracting sub networks from large DNNs and these sub networks make predictions of the original DNN significantly easier to interpret. In the paper there are experiments that aim to verify three claims: (1) NeuroChains can find sub networks containing less than 5% of the filters in a deep convolutional neural network while preserving its outputs in some small region of the input space; (2) every filter selected by NeuroChains is important for preserving the outputs and removing one of them leads to considerable drop in performance; (3) the sub networks extracted for small regions of the input space can be generalized to unseen samples in nearby regions. The experimental results suggest that NeuroChains can, as claimed, extract sparse sub networks that match the outputs of the original network for some small region of the input space. 2.The descriptions of the method and the experiments were very clear. As a result, the experimental results could be reproduced based only on the information in the paper. Therefore, the claim that all filters found by NeuroChains are important is not well supported by the experimental results. Therefore, I don t think that Figure 4 provides strong evidence that the sub networks extracted for local regions of the input space generalise well to nearby regions.<BRK>content:It is about pruning for explanation. The goal of the methods presented is, given a sample x, to extract a network, which isan unmodified subset of the original network,and has similar predictions to the original network in a region around x. The authors evaluate faithfulness in the sample itself. strength: paper concept is well explained. Clarity of the idea. The outcome are sample dependent very small sub networks. weaknesses: The validation of the method. First of all, they do not validate the impact on the region of a sample x sufficiently, and that must be done because it is a central claim of the paper. The not satisfactorily attendance to that claim is the main reason to reject this paper at the moment. page3: "(3) it is for data from a local region instead of the whole data distribution." It seems that these visualizations, while nice to show, are not really central to the questions raised by the authors. In that sense the evaluation of local faithfulness is not complete. one needs to perform evaluation on what happens with predicted labels of adversarial samples close to x. I think you need the space for more interesting content. Post review: The reviewer thinks that the authors did a thorough job of addressing the reviews.<BRK>The key of the proposed approach is to apply a multiplicative weight to each filter and layer in the network and enforce these weights to be sparse. The proposed objective is also very fast to train since the local region used is very small. Despite the similarity to existing post hoc network pruning/slimming work, I think the goal of finding this small network is very different from that of pruning so thus this work could be of interest to the ICLR community. The paper is also generally well written. Could this be used to identify problems and provide more insights on how the network works on out of distribution examples or examples with wrong predictions that other methods do not provide? (I’m trying to be pedantic here to generate discussion and for me to understand the goal this paper is trying to address)+ the proposed algorithm has several hyperparameters that seem to have been manually selected (?). Overall I think this is a good submission.<BRK>This submission extracts a very sparse subnetwork from an RNN. The submission proposes to multiply every filter with a weight/gate and optimize these weights to extract the subnetwork. Up on that, the method also multiple weights to entire network layers so a network layer can also be pruned. I have a few questions/comments about the proposed methods1. If this is the case, is the analysis of these filters still applicable to the original network? 3.It seems that gate values of the subnetwork are part of the subnetwork in predictions. Actually, some important filters may not even be selected. What are new conclusions from the analysis if I omit the points you want to make?
Reject. rating score: 5. rating score: 5. rating score: 5. rating score: 7. <BRK>This paper introduces a framework that leverages transitive similarities between images and text to align multiple languages, without bi lingual parallel training corpora. The paper is well written and easy to follow, and the notation is clean. The experiments are convincing with interesting case studies. One shortcoming of retrieval based methods is that its applicability is limited by the size of corpora. In terms of unsupervised translation, this problem is even more severe. Some other minor concerns are that:(2)The authors evaluate word level translation based on ground truth obtained using statistical patterns. This somehow makes the comparison with generative models (e.g.Sigurdsson et al 2020) unfair. (3)A missing and important related work.<BRK>"cross modal similarity as well as a cross image similarity"  > "sentence image similarity as well as a image image similarity"You could improve the mathematical notation greatly. I recommend that the paper do not be accepted for publication in its current form for all the reasons mentioned above. My first comment has to do with the framing of the paper. I am not sure I would call the proposed method one for "machine translation". This is not a generative translation model, but rather a retrieval model that retrieves similar sentences in a foreign language. I will provide detailed comments on these points below. This paper does not address any of these problems. In: arxiv. All issues considered, since the method is a crosslingual retrieval model that uses images, the authors should compare to other baselines proposed specifically for crosslingual sentence retrieval. However, authors could retrieve images for sentences in Tatoeba/BUCC using a crossmodal retrieval model, and train their proposed model on the retrieved image sentence pairs.<BRK>This paper introduces a framework that leverages visual similarity to align multiple languages, using images as the bridge between them. The cross modal alignment between language and images is used to estimate and guide the learning of cross lingual representations. A connection or analysis to InfoNCE should be addressed. The $L_v$ is to distinguish the image and its distortion with other images. Another thing I may misunderstand is that the experiments in this paper are claimed as translation, but I think it (sentence level) is more like retrieval. I am also wondering the details of how to evaluate the retrieval task on other generative translation baselines. Some missing references:[1] https://arxiv.org/pdf/2002.02955.pdf[2] https://arxiv.org/pdf/1811.11365.pdfIn summary, this paper presents an interesting topic, but the proposed method is of less novelty and the experimental design needs more improvement.<BRK>The authors propose to leverage images to train an unsupervised machine translation (MT) model. Their main idea is that the similarity of images can be used as a proxy for the similarity of sentences describing the images. The sentences, in turn, can be in different languages, and knowledge about their similarity can be exploited as training signal for an unsupervised MT model, i.e., training without parallel sentences. The authors evaluate on two different tasks: word level translation and sentence level translation. (However, the sentence level translation is retrieval based, i.e., no sentences are being generated.) They compare to multiple baselines, which seem to be chosen well. Overall, the idea the authors are presenting is convincing, the experiments are clear and well designed, and the model makes a lot of sense. I don t see any major shortcomings of this paper. This should be corrected.
Reject. rating score: 5. rating score: 6. rating score: 6. rating score: 6. <BRK>In the literature, there exists neural network based models for learning a physical engine which have good predictive accuracy but poor sample efficiency, as well as symbolic models which are highly sensitive to deviations from their fixed physics engine. This particular grammar is parameterized by a few latent variables related to unobserved properties of the physical environment, such as mass and charge. Finally, they develop an Expectation Maximization algorithm, in order for estimating these latent variables as well as inferring the underlying physical laws of the system. The paper is well written in general, i.e.the main idea of the paper is easy to grasp and all the related technical concepts are explained in a very simple way. On the other hand, it is no surprise that a carefully constructed grammar for physics is sample efficient. My final remark is about the EM algorithm in the paper. I think that the details about the EM should be written more clearly and thoroughly.<BRK>The paper proposes an Bayesian symbolic physics (BSP), an intuitive physics model that jointly infers symbolic force laws and object properties (mass, friction coefficient). Some preliminary experiments are shown for the method s effectiveness and data efficiency. **Strength**: The paper fills an important missing position in the spectrum of intuitive physics models, as Figure 1 argues. The force law grammar, to my knowledge, is something novel in this area, and represents a reasonable inductive bias that balances expressivity and physical plausibility (with two further physical constraints: dimensional analysis and reference invariance). From the inference side, the proposed EM approach is also reasonable. It d be great to see more scenarios (than graviton) with more diverse settings (e.g.object mass, initial position), some quantitive numbers, and comparison with some baseline (if possible). Also, is it possible to use MCMC for formula inference? The paper currently only compares with one side of the spectrum, i.e.more neural approaches.<BRK>This paper proposes a fully Bayesian approach to learn an intuitive physics model by combining symbolic regression and statistical learning (MCMC). Pros:+ Personally, I like this paper as it approaches the learning of the intuitive physics model in the right direction, highlighted in figure 1. It is a nice combination of symbolic and learning based approach. The current related work section only focuses on the machine learning based intuitive physics model but does not cover symbolic regression in general. Prior work has demonstrated that SR can indeed learn the physical law in a much more complex setting [1 2]. For researchers who are opposed to the idea of intuitive physics, they would ask where the prior knowledge comes from. The question is, what the benefit of learning? For instance, can the learned model from MAT dataset be transferred to simple scenarios created by bullet like engine with a similar friction based interaction, but not identical? [1] Distilling free form natural laws from experimental data, Science 2009[2] AI Feynman: A physics inspired method for symbolic regression, Science Advance 2020<BRK>The paper proposes a fully Bayesian approach to symbolic intuitive physics that, by combining symbolic learning of physical force laws and statistical learning of unobserved properties of objects. The paper is clearly written with clear explanation of the proposed EM style method. However, one of the main claim that the method "enjoys the sample efficiency of symbolic methods with the ac  curacy and generalization of data driven learned approaches" is not well supported in the experiments. While in Sec.4.1 it shows the proposed method is more data efficient than the neural baseline, it s not clear how the method generalized to complete different scenarios. This (10x more data efficient) is hard to tell from figure 5 as it doesn t show when the neural baseline reaches the same performance. The error bars in figure 5 is somewhat not very indicative of the stability of the method and some of them for the neural baseline are extremely large. Is  m  supposed to be r in Algorithm 2?
Reject. rating score: 3. rating score: 3. rating score: 4. <BRK>The goal of this paper is to learn cross modal associations between a person’s face and a voice. Table 2 is not a fair comparison. The authors use the triplet loss which has been widely used for this problem before (Kim et al.2018, Cheng et al.20  https://dl.acm.org/doi/pdf/10.1145/3394171.3413710).<BRK>The paper suggests a new benchmark for the evaluation of both matching and retrieval tasks. It is not clear if the authors applied data augmentation to the Vox2 training set.<BRK>The TriNet uses L2 normalized triplet loss, which is also not new and can be found in many previous work, e.g.[1].Simply applying this normalized triplet loss to cross modal matching is not a significant contribution. As the paper points out, the training and testing data of TriNet are different from other methods, which means the results are not comparable.
Accept (Poster). rating score: 7. rating score: 7. rating score: 6. rating score: 6. <BRK>The presented idea is to learn a probabilistic (Gaussian) vector representation of the implicit reward function, so that the policy (Q) function can be trained in scale. Weak points  The engineering details are not well explained. I agree with authors that an arbitrary function approximator can be used for encoder. There are some typos (e.g., the the, figure ...)It would be good to explain more the training procedure in the paper. As an example, it is not sure why the presented encoder is called as auto encoding reward when the input is demonstration and the output is the policy.<BRK>The paper should show the result of such a simplification of the method presented in the paper. In this paper, the authors propose two neural networks that are expected to resemble an autoencoder. The addition of $q_\phi$ is sensible overall if one wants to have a (normal) distribution over rewards for every (s,a). The reward learned by the algorithm is not compared with any other approaches. The overall quality of writing in this paper is at a high, professional level. Please provide a link with IL and IRL that precede this sentence. The low dimensional embeddings are not even given as input to the decoder. That was a great outcome.<BRK>The paper is well written. Generally, I was particularly impressed by the authors’ explanations of related literature in sections 2 and 3. For the final version of the paper, I would encourage the authors to provide simulations on standard IRL problems used in the literature and quantifying performance e.g.with the cumulative loss of the Q values wrt. Overall, it would be very helpful if the authors can provide some more details about the implementation and provide evaluations that quantify the inferred reward functions in the rebuttal. With the additional information, this could be a very strong paper that advances BIRL.<BRK>One could also evaluate AVRIL s ability to compute high confidence bounds on imitation policy performance, as in the evaluations for other Bayesian IRL methods like B REX [3]. I have updated my score. While I think this paper is a promising initial step, there are a few issues:The main issue is that the action prediction experiments show small improvements over the AUC of prior methods on the healthcare dataset, and that AVRIL performs slightly worse than prior methods on several of the control benchmark tasks. The interpretability analysis on page 8 does not highlight the benefits of learning a distribution over reward functions.
Reject. rating score: 3. rating score: 5. rating score: 6. <BRK>Communities in multi round bargaining games with evolutionary dynamics are evaluated in three main setups. The second enables agents to refuse to bargain with deceitful agents. Finally, in the third setup, a global punishment system is shown to be able to drive out deceitful invaders. The main take home message is that, when lying is an option, agents(  communities) need to be prepared for it. I agree with the authors that we need to start looking at more naturalistic setups (e.g., communities instead of two player games). + The setups are well explained and easy to follow   ##### Cons ######  Much of the motivation of this paper relies on the argument that there are strategies that functionally dominate cooperation. Along the same lines, the view that functional pressure needs to apply to only individuals, and that these are fully selfish, also requires evidence or needs to be clearly marked as a rather strong assumption. (p. 3)  The lines in Figure 3a and Figure 3b are very hard/impossible to read.<BRK># Overall reviewThis paper attempts to address a question in the emergent communication literature: what preserves / maintains the stability of emerged communication protocols. The authors manipulate the prevalence of lying behavior in a community of agents playing a variant of a Nash bargaining game. The main take away is that explicit punishment, from the environment and from truth tellers not wanting to communicate with liars, can prevent the spread of exploitative lying behavior in the community. My main worry about the paper is that the conceptual motivation is a bit unclear. The authors present the paper as addressing when emergent communication can be stable. But in all experiments, the communication protocol is fixed: mappings from market needs to proposals. Did the authors experiment with that approach, or can they say more to motivate theirs? * The y axis in the main results are "average scaled reward". How exactly is the scaling done? # Minor typographic comments* abstract: "From the perspective of Darwinian, ..." needs to be finished after "Darwinian".<BRK>**Summary:** This paper studies multi agent communication with an aim to mimic conditions for the emergence of language in society. I am not an expert on evolution but the paper doesn t say enough to justify the choice. Is this right? A credit mechanism is also introduced to measure the performance of agents. **Strength:**  The setting is interesting. In general, understanding the dynamics of language evolution is a very interesting topic. **Weakness:**  The language part of the story is confusing since there is no language used in the experiments. Implications of the results are unclear as it is not obvious that the setup captures all the important nuances present in human society. Then there is the question if the way natural selection and genetic evolution are modelled, reflect the real world.
Accept (Poster). rating score: 7. rating score: 7. rating score: 7. <BRK>The paper starts by that observing the local minima obtained in a multi task scenario are connected with a linear path of low error regime to the local minima of each task in a continual learning scenario in contrast to the path between the different minima of tasks incrementally learned, provided the both training of multi task and continual learning have started from the same initialization. Motivated by these observations, the paper proposes a new solution to the continual learning problem.<BRK>############## Recommendation ##############I recommend this paper for acceptance, but urge the authors to substantially revise their manuscript to make it more approachable. ############## Arguments ##############The question of whether and how the solutions to multi task and continual learning are connected is highly relevant. The proposed algorithm is clever and simple: it leverages past data not only to approximate the loss of the previous tasks on the new solution, but also to add a regularization encouraging a low loss linear path between the solutions.<BRK>Positives1  I quite enjoyed reading the paper. Towards this end, the authors empirically identify that all the solutions of CL (i.e.solutions obtained after each task) and MTL are connected by a linear region of low error. The finding, albeit empirical, that the solutions of multi task and continual learning are linearly connected could prove to be very important for future research in CL. Based on this observation, the authors propose a memory and regularization based CL method, MC SGD, that ensures that the final CL solution is linearly connected to all the task’s solutions.
Reject. rating score: 3. rating score: 4. rating score: 4. rating score: 5. rating score: 5. <BRK>The authors take an attempt at offline RL thanks to a mix between behavioural policy regularization and model based policy optimization. They basically combine two algorithms: AWAX and, depending on the level of safety given an epistemic uncertainty evaluation, MOPO may be additionally used to fine tune the policy. (2016).Safe policy improvement by minimizing robust baseline regret. High confidence off policy evaluation. Even more problematic, most, if not all, the references on offline RL are from this author, and therefore lacks diversity. In particular Model based offline RL [Iyengar2005,Nilim2005], and model free offline RL [Thomas2015b] have a rich history. "If our initial policy does not achieve expert level performance, and we are confident that we can learn an effective model with the available data, then ..."  > it is unclear how these decisions are sorted out. Finally, the experimental results do not savethe day. In comparison with CQL, it is not better (but it is a strong baseline). So, it s not improving the state of the art. It would have been informative to show the behavioural performance in each setting. Typo and minor comments:  AWAC is used without citation or explanation first (2.1)  "The most common off policy model free algorithms are actor critic algorithms that alternate between policy evaluation and policy improvement in order to learn an effective policy." "Otherwise, we use the fully trained AWAC policy.<BRK>1.SummaryThis paper proposes an improved offline RL (batch RL) algorithm combining the state of the art behavior regularization actor critic method (Nair et al., 2020) with a model based RL technique. N trained probabilistic dynamics models generate fictitious trajectories with uncertainty penalized rewards after pretraining the policy with the behavior regularization solely on the offline data. Both these generated data and the original offline data are used for the further behavior regularized actor critic training. The results of  ours  in Table 1 are the maximum combination of AWAC and AWAC+MB2PO, which includes five best results from AWAC out of nine best results from  Ours. This is the reason why I had no choice but to give a harsh rating. However, I still lean to reject this paper for the following reasons. In this sense, the proposed method does not outperform CQL. 2.I agree with the other reviewers’ common concerns on the novelty / main benefit of this paper. It would have been better if the authors used this functionality of ‘official comments’ during the review process for better reproducibility.<BRK>### SummaryThe paper describes a novel method to combine two different school of thoughts for improving offline reinforcement learning. Paper is well written. 2.I would also like to thank authors for honestly describing the scenarios in which fine tuning led to degradation in the performance of the proposed method### What can be improved1. This though raises another question as to how much contribution   the results from have on their own. There maybe excellent points the authors   use to improve upon AWC results, however with the time budget allocated to   this review it is impossible to read and understand AWAC and then objectively   judge the improvements bought about by this scholarship. 2.Again, I would like to commend authors for the future work directions   mentioned in the conclusion. For example, one of the classic use   cases for offline RL would be that we are not aware of the situation, whether   the data is ‘expert’ policy or ‘naive’ policy, so it is very challenging to   decide whether or not to fine tune the results one obtains from AWAC for   example. 4.The details of the models used for fine tuning are absent, I did see which NN   models are used to build dynamics models. I am sure authors would agree that   a “neural networks that outputs a Gaussian distribution” would cover large   literature at conference like ICLR. if  not do we need to learn reward model ?<BRK>I think it would be nice to have some experimental results highlighting that the proposed approach indeed fixes this issue, perhaps by showing the uncertainty estimates and different resulting actions for different methods in problematic state. They now look decent and I have improved my score as a result. I however encourage the authors to improve their paper. **Summary**This paper proposes to combine two approaches for offline reinforcement learning: behavior regularized methods and uncertainty aware model based methods. First, a conservative MDP is constructed by estimating a transition functin and substracting an uncertainty penalty from the reward for each state and action pair, similarly to MOPO. AWAC (Nair et al., 2020) is used to learn a policy $\pi$ using offline data and the penalized reward, where $\pi$ is constrained to be closed to the behavior policy in terms of KL divergence. The best result of the proposed approach, i.e.with or without the second step, is shown to be competitive with the state of the art. 2) The experiments look good to me and show the performance of the method. In the experiments the best out of the two results is used. For example the architecture of the models used is not described. The idea looks interesting and should be investigated further. It is however nice to have information about existing methods leveraged in this work. I am not sure what the best solution is but I would like to suggest using "they" rather than "we" when the paper describes previous work.<BRK>*Summary* Authors suggest Model Based Behavior Regularized Policy Optimization (MB2PO) for fine tuning policies trained with behavioral regularized model free RL. I enjoyed reading the paper, I think it s a neat idea, and authors did a good job explaining them. Away from the details, looking at the results I have an important concern about the usefulness of the method. So authors claim that if AWAC has not achieved expert performance. So if we cannot be (at least to a good extent) confident that fine tuning will make the performance better, what is the main benefit? Specially if we are in mostly off policy setting, and cannot really get the performance of the model by running the policy multiple times on the environment. I am happy to increase my score, if authors clarify my concern (/mis understanding?).
Reject. rating score: 6. rating score: 6. rating score: 7. rating score: 7. <BRK>The paper analyzes the theoretical properties of noise injection in StyleGAN like networks, and proposes an extension to in particular to StyleGAN2 that results in somewhat improved metric scores. While the cited OT/GAN work presents interesting and relevant viewpoints about the difficulties in GAN training, I am not sure if it is particularly more relevant here than any number of other theoretical works. It may be noted that StyleGAN and DCGAN are not even formulated as to minimize a Wasserstein divergence. The paper then coins a term "adversarial dimension trap", which I am not exactly sure why this terminology was chosen. The gist of the observation seems to be mostly well known, i.e.the generator can only cover a zero measure region of the data space whereas the data is more spread out. That said, I am not thoroughly familiar with previous theoretical work on GANs and the particular formulation here may be novel. The paper then introduces a fairly general form of stochastic noise injection into the network layers and calls this fuzzy reparametrization. Here some connections to are drawn to "fuzzy equivalence relations" which (apparently?) Then, the key theory is developed. This then leads to a proposal of a practical algorithm. It would be better to spell this out as an algorithm listing. As for the content of these formulas, I am not sure if I understand what the operations or the reasoning behind them is. The presentation is further made confusing by the language. I understand that the authors may not be native speakers, but the readability is much below the usual standard of ICLR papers and the paper would benefit from improving this. As for the results, it does appear that there is some improvement in some of the metrics, and the proposed method may in principle be useful. It is not hard to believe that adding some extra flexibility to the noise injection might improve the results, at least in a limited number of scenarios. In this sense the paper may be on to something. What is the meaning of using PageRank to reduce the number of LSUN Cat images? How is PageRank related to choosing images and what s the difference between that and just taking the first 100k pictures in the set? And for that matter, I am not sure if we learn anything from randomly limiting the set to 100k images, when we don t know how it worked for the full set. Perhaps this node could be expanded into its own architecture diagram as well, given that there is no shortage of space in the appendix. _UPDATE AFTER REBUTTAL_The authors have improved the paper somewhat by expanding and clarifying the discussion on some key parts. However I remain very borderline and I am not sure if I am fully convinced by all the claims. One specific issue: I think the authors should make it more clear in the paper that the experiments are done in 128 pixel resolution, in light of R1 s questions.<BRK>Pros:This work introduces the problem of adversarial dimension trap, which leads to punishment on the smoothness and invertibility of GANs. This work proposes to learn fuzzy equivalence relation of the features and uses reparameterization trick to model the high dimensional feature manifolds. A novel form of noise injection is proposed to overcome the adversarial dimension trap. Prior noise injection methods can be explained as a special case with certain hyper parameters. This method is universal for the families of GANs, including WGAN, DCGAN, etc. Experiments on three datasets are conducted. The results of both image synthesis and GAN inversion are desirable with plausible texture details. Cons:My major concern is about the experiments. In Table 1, it seems that the reported FID and PPL differ from the scores reported by StyleGAN2 [1]. Such a huge discrepancy is wired. Recall that [1] improve the FID from 4.40 (StyleGAN v1) to 2.84, but the baseline, which should be the same, performs even worse than StyleGAN v1. The differences between the PPL scores reported in this paper and [1] are even more significant. Why is the PPL about ten times better (even without the proposed method)? The authors need to provide more details to explain how they calculated the FIDs and PPL. It seems that the authors calculate these scores in a non standard manner. Besides, I suggest the author evaluate the Precision and Recall [2] on FFHQ. "Improved precision and recall metric for assessing generative models."<BRK>To summarize, this paper proposed a new noise injection method that is easy to implement and is able to replace the original noise injection method in StyleGAN 2. The approach is supported by detailed theoretical analysis and impactful performance improvement on GAN training and inversion. The results show that they are able to achieve a considerable improvement on DCGAN and StyleGAN2. Based on my understanding, the fuzzy reparameterization technique realizes something that StyleGAN2 cannot achieve, and resolves some fundamental limitations of StyleGAN2. How many more compared to the additive noise implementation? Since FR can be seen as a generalization of StyleGAN2 noise injection, we would naturally expect that the proposed method should perform better than StyleGAN2. However, this is not always the case in Table 1. I guess more ablation studies can also be done on $\sigma$, such as interpolating between StyleGAN2 implementation and FR implementation, or a linear layer with the same number of additional parameters but has no constraint as in Eq.6, 7, 8, 9. I am curious to see how better this method performs in terms of inverting real images in the wild. I also believe the inversion in z space allows me to appreciate more about the inversion improvement. Overall, I vote to accept this paper due to its good performance improvement over prior standard noise injection implementation. Meanwhile, I hope the theoretical analysis can be made easier to understand for a researcher that lacks the related background. [Update after reading authors  comments]Based on the authors  and other reviewers  comments,  I keep the score unchanged.<BRK>In this paper the authors highlight two major drawbacks of GANs. 1) The optimal Generator is discontinuous and 2) the  adversarial dimension trap  caused by the relatively lower dimension of the latent space compared to the real world data which makes the generator not Lipschitz and/or the generator fails to capture the real world data distribution and is not invertible. Secondly, the authors provide a form of generalization of noise injection in GANs called fuzzy reparameterization, which leads to a solution by letting the generator map onto an arbitrarily low dimensional skeleton of the feature spaces and filling up the remaining space with random noise. The solution consists of two stages, first a map from feature space onto the skeleton set is learned followed by noise injection adapted to the local geometry of the orginal feature manifold. Experiments:Experiments were done on FFHQ faces, LSUN objects, and CIFAR 10 datasets with models DCGAN, StyleGAN2, and bald StyleGAN2 which is StyleGAN2 without noise injection and path length regularizer. StyleGAN2 + FR also outperforms on the image inversion experiments. Pros: This paper provides a theoretical framework for noise injection for GANs which is novel and interesting for the GAN community. The experimental results are extensive and convincing and support the theoretical analysis. Cons: In section 4.3 the algorithm eq.6 8 is not very clear to me. E.g.what are the parameters A,b, alpha and r and how are they motivated? PixSum is over the feature maps? A more detailed description with comments would be helpful for the reader. I could not find the FR implementation in the supplementary file, it looks like it contains only the original StyleGAN(2), DCGAN and DCGAN with additive noise models.
Reject. rating score: 4. rating score: 6. rating score: 6. rating score: 6. rating score: 7. <BRK>I understand the bonus proposed is an empirical surrogate to promote optimistic values (with respect to the seen data) — but ideally, do we not need optimistic values that include q*. 2.What do you see are the key advantages of the proposed approach? al.2019)4. uncertainty propagation by bootstrapping from optimistic estimates (as done in Jin et.al.2019)The paper also highlights that due to the uncertainty propagation achieved by incorporating ideas from (3) and (4) the induced exploratory behaviour is more effective, and hence the sample complexity of learning is reduced. Therefore, instead of the more common 200M frames used for training deep RL agents, 20M frames are used for training in the experiments here. #### SUGGESTIONSMy concrete suggestions to improve the paper while keeping the core idea the same is two fold:1. a more thorough empirical section comparing to methods designed for uncertainty propagation2. a rewrite which reflects that the components of the core idea are existing proposals in literature in the case of both bonus estimation and the EBU algorithm. The relevant work is discussed and the paper gradually builds up to the core proposal. 4.Section 5.2: incentive —> incentivize## POST REBUTTALI really appreciate the author s engagement and response during the discussion phase to help me understand the paper better, and the revisions incorporated in the paper. Here are my main concerns that I hope is useful for the next version or final submission. But, currently the choices are presented in a confusing way, and the actual contributions of the algorithm are unclear: most importantly, is it a Frequentist approach to exploration or Bayesian? (c) BEBU is essentially EBU as proposed in Lee et. al.2019 with bonuses added. I do not think the text presents it so. under the Bayesian viewpoint, which is the crux of the architecture used here (Bootstrap DQN), the attempted theoretical connections in the paper (Theorem 1) and the practical algorithm proposed (based on the ideas of Optimistic LSVI), do not provide a clear picture. 2.While I understand a complete empirical comparison to the many existing deep RL methods can be very expensive, the literature review does miss methods proposed with a similar ethos — propagation of uncertainty. I think they need to be discussed and compared against	(a) Bayesian Deep Q Networks : Azizzadenesheli et. Maybe this is a step in the right direction, and the extensive empirical results do seem to suggest it is effective, but the presentation is unclear. Some of the proposals made are unclear and left unexplored/discussed.<BRK>This paper studies optimistic exploration for deep reinforcement learning. Their algorithm outperforms other SOTA exploration approaches. Pros:  The optimistic exploration problem is very important for the RL community. However, how to apply these theoretical insights into practical algorithms is still an open problem. The paper proposes a method that uses bootstrapping to construct confidence bonus, and connects the theoretical findings to practical algorithms. The experimental results on 49 Atari games are rich enough to show the improvement of the algorithm, with visualization results and ablation studies. Other comments:  The connection with optimistic LSVI is interesting but not satisfying. After the rebuttal and discussion period, I believe that the contribution of the paper are mainly empirical. Lastly, it is unclear whether the algorithm is a frequentest approach or Bayesian. I believe that the former two concerns can be addressed in the following way, and don t weaken the contribution of the paper. From the theoretical perspective, the feedback from the authors does tackle this problem. I am eager to see the discussion in the final version. From the experimental view, the authors also claim that all the algorithms in the experiments use eps greedy to help exploration. In that case, the improved performance of their algorithms are mainly due to their bootstrapping and backward update methods, instead of eps greedy trick. Besides, I think applying several widely used tricks (such as eps greedy) to the algorithm is acceptable, which makes the results comparable to other methods that also use the tricks. Maybe such discussion should be added to the paper. Overall, I believe the main contribution of the paper is from the empirical perspective. The experimental results look nice in general. I am a little disappointed that the authors missed the comparison with two related literatures in the initial version. As a result, I change my score to 6, and I hope the above problems can be addressed in the next version.<BRK>This paper focuses on deep reinforcement learning and proposes a practical exploration algorithm called Optimistic Exploration algorithm with Backward Bootstrapped Bonus. Based on the Optimistic LSVI algorithm, the authors propose a new optimistic exploration bonus for general cases, similar to the optimistic exploration bonus in the LSVI algorithm under the linear MDP setting. Experiment results show that the QWR algorithm has better performance than previous algorithms. The main contributions are delivered:  Proposed optimistic exploration bonus for general cases; New deep reinforcement learning algorithm. However, I still have some concerns about this paper. First, the actor value function Q is the expected cumulative reward with discounted factor \gamma. However, it is usually set 0<\gamma<1 for infinite horizon MDP and set \gamma 1 for episodic MDP or finite horizon MDP. Second, in the LSVI algorithm, the agent chooses action by the totally greedy policy with the state actor value function. In the OEB3 algorithm, the agent chooses action by the \epsilon greedy policy. Even the optimistic exploration bonus in the OEB3 algorithm is the same as the LSVI algorithm under the linear MDP setting, theoretic proof of LSVI may not support the OEB3 algorithm. It is better to do more experiments with a broader action set and show the OEB3 algorithm s performance in those experiments.<BRK>The authors of the submission "Optimistic Exploration with Backward Bootstrapped Bonus for Deep Reinforcement Learning" draw inspiration from the theoretical reinforcement learning literature to propose an optimism based bonus for deep q learning. This is a very nice poin: the uncertainty propagation should be done backwards. The authors are right in my opinion to claim their work represents a welcome addition to the emerging literature that proposes the use of uncertainty bonuses around the value function as opposed to myopic uncertainty bonuses only at the immediate reward level. There exist other recent works that introduce similar ideas (bringing in bonuses for the future uncertainty as opposed to solely penalizing immediate  ) are some missing citations in the related work section, most notably "On optimism in model based reinforcement learning" (using value optimism in model based RL and deep RL), "Efficient model based RL through optimistic policy search and planning"(optimism and GPs in model based RL), and also SUNRISE which looks awfully related to BEBU UCB. The experimental results of this work are strong. It is nevertheless unclear how much of these results are the consequence of accessibility to massive computing resources. I would like to see the paper positioned more faithfully within the relevant optimism at value level literature, even though these works are model based in nature. It would also be very useful to have algorithm boxes for the different methods or method templates that the authors describe in the text.<BRK>The algorithm builds on the idea of optimistic exploration developed in the theoretical optimistic LSVI algorithm for linear MDP. OEB3 learns an ensemble of bootstrapped Q values and estimates the UCB bonus as the variance of the Q values. Experiments have be done in Mnist Maze and Atrari games comparing OEB3 with previous exploration methods and some OEB3 variants. Table 2 nicely summarizes the algorithm design difference of related algorithms and highlight the two important components of backward update and UCB bonus in OEB3, and the improvements clearly helps OEB3 to outperforms similar exploration approaches. However, it may be miss leading to say that OEB3 outperforms all existing bonus based methods since it actually performs worse than some prior methods in Montezuma’s Revenge. I think this is a nice work given its connection to theory and the nice empirical performance, but I think the following questions about steps in the OEB3 algorithm should be answered. The paper claims to use bootstrapped Q learning, but in the algorithm description, at each time only one episode E is sampled for all head. This is different from the original bootstrapped Q learning design where different heads are learned using different samples with bootstrap. Are there some steps missing in the algorithm description or no bootstrapped samples are used? If all heads are learned from the same sample, does the diversity among different heads only depend on their initialization? However, in evaluation the action seems to be selected by majority vote. In algorithm description, epsilon greedy is actually used in OEB3 during training. The use of epsilon greedy is different from other exploration methods which guide exploration by bonus or other randomness. In experiments, do other baselines also use epsilon greedy?
Accept (Poster). rating score: 7. rating score: 7. rating score: 5. <BRK>An important aspect is that the temporal dimension is processed separately, with an additional convolution, instead of simply using a 4D convolution. The paper is written understandably and the evaluations are sufficient, IMO, to confirm the claims given by the authors. The presented method is very interesting in my eyes. Together with the presented striding, based on farthest point sampling, and the transposed convolution, the method seems to be very versatile, as it was shown in the results.<BRK>There is no discussion about the limitations of the proposed method in the paper. The presented experiments demonstrate the effectiveness of these convolutions by using PSTNet for action recognition and semantic segmentation on point cloud sequences, showing improvement over relevant recent work. The architecture operates on ordered point cloud sequences but does not leverage timestamps, so there is an underlying assumption that point clouds are sampled at a fixed rate that does not change from training to test time. + I like the idea of decoupling spatial and temporal convolutions.<BRK>•	The experiments are exhaustive and impressive. The extensive experiments verify the effectiveness of the proposed method and achieve state of the art results on multiple benchmark datasets. Strength: •	In this paper, the authors introduce a method named PST convolution which could directly handle the sequential point cloud data. No matter the point tube or the anchor points extracted in the spatial and temporal domain, the main idea is finding a good neighbor for the current point.
Accept (Poster). rating score: 7. rating score: 6. rating score: 6. rating score: 6. <BRK>In this work, the validity and stability constraints are implemented inside a depth first search traversal of building the RNA. Related works also contains recent works in RNA. 3.Overall the paper is well written. I liked the illustrations for explaining the methods. It clearly shows the effectiveness of the generative model used. ##########################################################################Questions during rebuttal period:  Please address and clarify the cons above  #########################################################################<BRK>Cons:  This paper falls into the common pitfall of not controlling for homologous sequences between the training and test set. For each sequence in the training set, what is the sequence with the largest sequence ID in the test set by alignment? I think a comparison with an algorithm like INFERNAL (http://eddylab.org/infernal/Userguide.pdf) is necessary. This is the most simple context free grammar generative model. These would be sequence motifs that are preserved across organisms with constrained sequence and function that may not necessarily follow Watson Crick basepairing rules. Why is this the case?<BRK>The tasks proposed in this paper are not comprehensive enough to entice a (comp) biologist to be convinced one way or another, but it does provide an introduction to the problem for the ML field. Comments:* While the trends in their results are clear, it’s difficult to know which values of Diversity, FE DEV, and Validity are good enough to say that they have made significant progress relative to the field. The main issue is that all model comparisons are only with their own VAEs.<BRK>First of all, it is unclear from the text whether this is also done during training and how the probabilities at masked stages are treated (are they re normalized after the masking is applied?) * I appreciate that the authors include a comparison between decoding with and without the heuristic masking rules. Should not the MFE structure always have smaller Free Energy than any structure output by the model? I would appreciate it if the authors also included decoding results for untrained decoders with and without constraints.
Reject. rating score: 5. rating score: 5. rating score: 6. rating score: 6. <BRK>The paper also bounds the rank of DAG in terms of the rank of its skeleton and moral graph. The main contribution of the paper is a strong justification for learning SEMs under a low rank assumption by showing that graphs with many hubs are low rank. Therefore, learning SEMs subject to rank constraints rather than sparsity constraints can be useful for graphs with hubs. Therefore, even for graphs with hubs learning SEMs subject to sparsity constraints might still give better results than learning SEMs subject to rank constraints? The first half where the authors obtain bounds on ranks of DAGs is the main contribution of the paper and is clearly interesting. Therefore the authors propose using "structural priors" to obtain these bounds. However, this is not clear since in experiments the authors only show the SHD as a function of "average degree" and not "maximum degree".<BRK># Review## Summary of the paper and the review  Paper summary: The paper explores the possibility of exploiting the potential low rank nature of the underlying causal graph when conducting causal structure learning. However the study has some problems in its argumentation and experimentation. I am looking forward to authors  improvements and responses regarding these issues. I believe that it is very important that this work investigates the properties arising from the nature of the underlying distributions and how this should relate to the models and algorithms. The authors discuss the structure of causal DAGs in relation to widely observed complex network structures such as scale free networks. Given the centrality of scale free graphs in their argumentation, is there a specific reason for the lack of nonlinear SEM experiments with scale free graphs? This might require an extensive search for $\hat{r}$ and $\lambda$. Almost none of the structural information required by the theorems seem to be easily accessible enough. Even when we have some structural information, it is not always clear how this information can be used in constraining the search space when this information is anything less than the knowledge of the causal structure itself.<BRK>This paper attempts to exploit the low rankness of the adjacency matrix of the DAG in Bayesian network structure learning. The paper is very solid in presenting mathematical facts and detailed algorithms. In fact, the experiments in Section 5 already uses the ground truth rank information in NOTEARS low rank. There are also many works on combining low rankness with sparsity, which I suggest the authors to consider as future steps. It s not ideal that the algorithm requires the knowledge of rank beforehand, but it s okay if this point is clearly communicated in the paper. I would keep my current score.<BRK>In particular, this paper provides how to exploit the property of the low rank for recovering a underlying causal structure. This paper is well written and delivers its main contribution really well. However, my major concern is about the simulations of the paper although I acknolwedge that most of relevant papers exploit a similar settings. The paper solve a very important problem of causal inference. 2.The paper is really clear and convincing. Hence, in some points, the targeted graph is unrealistic in large scale settings (d is large). Nevertheless, as an emerging field of learning DAG models in polynomial time with complete search, it should be accepted.
Accept (Poster). rating score: 7. rating score: 6. rating score: 6. rating score: 4. <BRK>It may be so that we end choosing all points in the original dataset for a given approximation and test error. A characterization of this using generalization bounds would be helpful. Results on MNIST and CFIR shows the efficacy of these results.<BRK>I believe the idea is not novel but the authors are the first to formalize it. The main concerning point is linking their work to too many areas instead of focusing on the main scope of the paper which is explicit data compression/distillation.<BRK>The theoretical results are only for Linear KRR. The empirical performance of the proposed algorithm is evaluated by experiments based on synthetic data and some standard benchmark data sets. Overall, I think the paper is well written and the proposed method is of potential value to existing literature.<BRK>I do have many comments on the claims made in this paper, and I hope the authors can answer them. A Deterministic Streaming Sketch for Ridge Regression.
Reject. rating score: 5. rating score: 5. rating score: 6. rating score: 6. rating score: 8. <BRK> The paper proposes to modular PDE solutions in networks into spatial and temporal modules, where spatial modules are rapidly adapted by meta learning. Modularity with meta learning has been explored before, and the physical learning framework has also been learned before. This work seeks to combine both but does not seem have to have impressive performance. ### Post Rebuttal UpdateI thank the authors for their response. However, I still have some remaining concerns about novelty, since to me this paper reads as another application of MAML to domain X, to speed up performance in the particular domain.<BRK>This paper proposes a framework for physics aware meta learning to tackle the few shot learningchallenges in physical observations. The authors claim that by incorporating PDE independentknowledge from simulated data, the framework provides reusable features to meta test tasks witha limited amount of data. 2.The paper is well written, and the idea is presented clearly.<BRK>The paper describes the approach to meta learning of spatiotemporal predictions for sparse data with auxiliary spatial derivative modules. For example, in the extreme weather dataset,  the top 10 extreme weather events since 1984 are selected; is it possible to use some of the remaining data for pretraining on non extreme events? This could remove the bias of synthetic dataset hyper parameter tuning and could serve as  a baseline.<BRK>The key contribution of the paper is not very known to reviewer. Can the authors highlight some of the key contributions of the paper?<BRK>I found this paper to be well written, and a solid contribution to the field of physics informed machine learning. The appropriateness of this choice is probably responsible for the improvements offered by the proposed approach.
Reject. rating score: 3. rating score: 4. rating score: 4. rating score: 5. rating score: 6. <BRK>### Contributions ###* The authors study a new kind of problem, the robustness of few shot learners against adversarial attacks * The paper propose a new meta learning algorithm ADML that uses adversarial and clean examples during meta training (both for train train and train test). A clearer motivation would be desirable. ### Originality ###In principle, combining gradient based meta learning such as MAML with adversarial training (similar to the proposed baseline MAML AD) is a natural fit that does not require novel research. This would make it an uneven comparison to standard meta learning, that would do actual K shot training. ### Recommendation ###Because of the lack of motivation for adversarial meta learning, lacking intuition for ADML, and particularly the robustness evaluation against a very weak attack, the paper is a clear reject to me. Moreover, also RPGD was used as a transfer attack and as such is quite weak.<BRK>The paper focuses on few shot learning with adversarial training samples in both during meta training and meta testing phases. Instead of mixing up clean and adversarial examples in both inner gradient optimization and outer gradient optimization the paper suggests using them in a mutually complementary fashion. The paper is overall definitely interesting because adversarial attacks in few shot learning is not a studied topic, to the best of my knowledge, and the proposed approach appears to be effective, especially compared to a meaningfully defined baseline called MAML AD. First, only FGSM is used as the adversarial sample generation method and other well known techniques are not considered. The same attack approach (FGSM) is used for both training and testing. Post rebuttal: I d like to thank the authors for the rebuttal and for pointing out some additional results that I had initially missed. While they re definitely welcome, it is still not clear to me why the main work is based on FGSM and why simple defense techniques have not been considered, both of which significantly weaken the manuscript.<BRK>The authors demonstrate robustness to a range of adversarial attacks on miniImagenet. Pros:  As far as I m aware, this is the first proposal to meta learn for adversarial robustness  Method is agnostic to the model and the form of adversarial attack  Reported results demonstrate robustness to a range of adversarial attacksCons:  The method is not really explored in any depth  The method is limited to MAML  Considered attacks are relatively weak [see 1]Recommendation: rejectionMotivation:While the idea of adversarial meta training is well motivated and generally sound, the specific method in this paper is primarily proposed and not really explored in any depth. al.Adversarially Robust Few Shot Learning: AMeta Learning Approach. Finally I have a concern with the evaluation protocol: the algorithm suggests that adversarial examples are generated by drawing a fresh batch of data. Should it not be exactly equivalent to MAML in this case?<BRK>The paper gives no sensitivity analysis of the hyper parameters. 4.The experiments are not thoroughly conducted. Indeed, image classification is an important task and a benchmark for meta learning. Still, it would be more informative to test the proposed method with multiple types of data, say graph data as mentioned above. Actually, a recent work "Adversarially Robust Few Shot Learning: A Meta Learning Approach, NeurIPS 2020" adopted nearly the same setting and datasets and achieved superior results. 4.Page 6: the paper considered several attack mechanisms including FGSM, FFGSM, RFGSM, and RPGD.<BRK>This paper presents ADML (ADversarial Meta Learner), a method for general meta learning when adversarial samples are present. In a sense, ADML extends MAML to deal with adversarial/contaminated samples in training. The paper is well written and the extension of MAML onto the adversarial setting is simple yet effective. One minor comment regarding the evaluation though is that there could a couple more ablation studies for ADML. Also for the clean clean case, since MAML still performs slightly better than ADML, is it possible to modify/extend ADML such that it will still match MAML s performance when there s no adversarial examples in the training set?
Reject. rating score: 4. rating score: 6. rating score: 6. rating score: 6. <BRK>To do so it trains a base policy that is optimized so that it can maximize its expected reward against a variety of opponents using only a few updates of its parameters (i.e.a few gradient steps of a straightforward optimization problem). The various opponents that are used for the training of this base policy are generated in two steps. Then, given a base policy and a "hard to exploit" opponent, more diverse opponents are sequentially generated (in a procedure coined diverse OSG) by optimizing their expected reward against the base policy while maximizing their "diversity" with the "hard to exploit" opponent and the already generated diverse opponents. More formally, this diversity between two policies is defined (and optimized) as the MMD between the distributions over the trajectories that they generate (when the policies are seen as MDPs "playing" against the given base policy). After exposing this training procedure, the authors evaluate L2E on 3 toy games, showing that the trained policies are indeed able to benefit from little adaptations to a variety of heuristic opponents and perform better than some baseline methods. To the best of my (admittedly limited) knowledge, the suggested approach is significantly novel. If not, the derivation of the gradient computation does not really constitute a "theorem". ## Reasons for scoreWhile I really want to emphasize that the problem is very interesting and that I like the premise of L2E, I think the paper, in its current form, is missing the target. In fact the empirical results suggest that the base policy is breaking even against a random opponent (before adaptation) at Leduc poker, which seems rather weak to me. Most importantly, I would really love to understand how the base policy is updated in a "real setting", after training (see my cons #1).<BRK>**Summary:** This paper proposes the Learning to Exploit (L2E) framework that can quickly adapt to diverse opponent s unknown strategies. **Reasons for Score:**Overall, I vote for a score of 5. **After rebuttal:**The responses address most of my main concerns, and I have increased the rating from 5 to 6. OSG removes the requirement of preparing the population or task distribution in meta learning, which can be expensive. 2.Section 3.3.1 shows promising results that the proposed MMD regularization term can generate diverse opponents. However, this paper compares the baselines trained based on the random opponent population, such as the MAML baseline in Section 3.2. Hence, it is unclear how much more effective and diverse opponents that OSG can generate compared to state of the art opponent generation based algorithms (e.g., population based RL (Jaderberg et al., Science 19)). However, the TRPO baseline in this paper does not perform the pre training ("The TRPO baseline does not perform pre training ..." in Section 3.2).<BRK>## SummaryThe paper motivates the problem setting of quickly adapting to exploit sub optimal opponents. To do this they use L2E which can generate exploiter and diverse opponent agents and updates the base policy based on trajectories from exploiter base policies. I appreciate that Nash was included as an opponent. Rebuttal Edit (Increased score form 4 to 6)Thanks for the discussions. I think testing this is key to back some of the claims made in this paper. Who is the opponent in Figure 4? I also broadly agree with the other reviewers suggestions / concerns. It seems that we need the exact opponent and base policy at all times in L2E. The literature that is cited in the paper is in a harder setting than this   where the opponent’s policy has to be estimated. The MMD term seems very expensive and I am not sure it will scale, particularly with population size. In the experiments section I would like to see more details on the actual training.<BRK>Summary: The authors propose an opponent modeling in 1 vs 1 games called the Learning to Exploit (L2E) framework, which exploits opponents by a few interactions with different opponents during training so that it can adapt to new opponents with unknown styles during testing quickly. In particular, the authors propose Opponent Strategy Generation (OSG) that produces effective opponents for training automatically through adversarial training for eliminating its own strategy’s weaknesses and diversity regularized policy optimization to improve the generalization ability of L2E. Pros:1.Significance: the authors propose an opponent modeling called the L2E framework, which can adapt to unknown opponents quickly with a few observation of interactions2. There were no similar results of the Leduc poker task also in the Appendix. I would like to know the reason.
Accept (Oral). rating score: 8. rating score: 8. rating score: 7. rating score: 7. <BRK>### SummaryThe authors propose “GCSL” (goal conditioned supervised learning), an algorithm that bridges the gap between reinforcement learning and imitation learning. The paper is clearly written and the authors very cleanly explain their key ideas and insights. I think this current submission is already relatively convincing as an accept, as it is clearly written, has well explained motivations, strong experimental results, and extensive ablations in the supplementary pages. I do have a few clarifying questions on the experimental results, but am regardless confident that this paper meets the ICLR acceptance criteria. The authors clearly showcase the experimental use cases of their technique by demonstrating its benefits in terms of stability to hyperparameters and in leveraging expert demonstrations. This seems rather counter intuitive to me, why do the authors suspect this is the case?<BRK>In the paper "Learning to Reach Goals via Iterated Supervised Learning", the authors propose a new approach to build conditional policies for reaching tasks that can reuse the previous failed attempts as new examples on how to reach the state that was actually reached during the failed execution. The paper is overall well written, clearly illustrated and appropriately structured. The relabelling will just reinforce this behaviour and the algorithms will quickly converge in a local optimum (which is actually far from being optimal). This can happen for instance, if the policy has a strong bias. Overall, this is a nice paper with an interesting algorithm, which could be made better by more discussion on some related work and potential limitations.<BRK>The analysis and ablation of learned behaviors with different data collection and relabelling settings is well done. the authors of the paper propose a new way to learn goal reaching policies by utilizing the previously collected trajectories in an iterative manner. This work is very close to hindsight relabelling methods [Schaul et al., 2015; Andrychowicz et al., 2017; Rauber et al.2017], but authors state that their method is more stable and does not estimate a value function. Their novel approach, called goal conditioned supervised learning (GCSL), learns to reach goals from a target distribution by running the policy and collecting suboptimal trajectories, and then relabeling these collected trajectories during training to perform supervised learning on them to update the policy.<BRK>Some aspects are not clear in the paper and the experimental section has to be improved. The comparison with other techniques is fair, but considering this very particular objective while other objectives may be easier to learn and more interesting. I think that this aspect is opening different questions: A) the first one is the interest of learning such policies. This is a crucial aspect since I do not understand how a policy that is not using the horizon as input is able to reach a goal at a particular timestep.
Accept (Poster). rating score: 7. rating score: 6. rating score: 6. <BRK>In this work, the authors provide a method for a posteriori calibration of DNN uncertainty with emphasis on constructing a classifier that has PAC uncertainty guarantees. Thus, for a task like ImageNet, the authors inference model would require the set Z to be quite large (quoted at 20000). Accounting for and re calibrating the poor uncertainty of neural networks is a central problem for deep learning especially when operating in safety critical domains and so the work is certainly attempting to attack a worthwhile problem. Thus, the PAC guarantees in this paper would seemingly be invalidated.<BRK>Summary: This paper proposes a method for obtaining probably approximately correct (PAC) predictions given a pre trained classifier. Strong points:  	The proposed method provides a provable guarantee on the reliability of a pre trained methods prediction, which is a very nice property to have in the reliability/safety problem. I have no idea what they are Experimentally, the method shows improvements over a naïve baselines, and demonstrate that it can obey a given error or safety threshold in practice, an important propertyWeak points + Clarifications: 	I am confused about the application of this method to safe planning. In particular, it seems to me like the proposed intervals only hold their PAC guarantee when the test time distribution matches the training distribution. Is there a reason why the greedy approach to Fast DNN inference you take is desirable?<BRK>This is a paper that focusses on the timely and important problem of uncertainty quantification for the predictions of deep neural network classifiers. The authors propose constructing calibrated outputs that have provable correctness guarantees, using PAC style arguments. For this to work, one needs good guarantees on the DNN s estimates of its confidence   and creating such guarantees forms the crux of the paper. Pros:+ Paper is well written+ Important and timely problem, motivating arguments are well constructed+ Paper appears to be mathematically sound though I did not check all the proofs in the appendix.
Reject. rating score: 6. rating score: 6. rating score: 6. rating score: 6. <BRK>A large part of the paper is also given over to a high level discussion of "resolution dilemmas." But I would have loved to see a more quantitative treatment of the "resolution dilemmas." This seems intuitive; but there are no examples illustrating such interpretability. The authors have clarified some of the points raised, and I appreciate that. I also continue to appreciate that the empirical results are promising. The discussion of coherence in the appendices says something about this, but not in a way that I would understand how to operationalize.<BRK> Summary In this paper, the authors investigate how to learn graph representations from structural landmarks. The idea of landmark has also been discussed in a recent work [1], in the context of unsupervised learning. Empirical results demonstrate the effectiveness of the proposed method on public benchmark datasets. Hopefully, the authors could address my concerns in the rebuttal. The discussion on spatial resolution seems to be around the assumption for graph classification: bag of structures (BOS) or some more complex model that considers relations between discovered structures. Overall, it is difficult to see the connection between section 2 and 3.<BRK>This paper studied an important problem of graph mining on graph classification tasks by investigating the challenges that previous graph models encountered and solving them with the proposed method. 3.The analysis and survey of related work with respect to the two mentioned resolution types is comprehensive. However, I have concerns about the lack of analysis for learned structural landmarks, since in the paper only the choice of its number is well discussed. I suggest providing a more comprehensive analysis for them. 4.Although the results are competitive compared with other baselines, the authors didn’t explain why the method performs well on some datasets while not performing well on others.<BRK>Idea about projecting graph information into structural landmarks is intriguing. To help make stronger case, I would suggest to do proper ablation study as it is not clear how much gain is coming unsupervised learning. Pros:Overall I like intuition and the method about capturing the spatial resolution and structural resolution in a strategic manner. Author have some strong empirical performance especially on Protein, PTC and IMDB M dataset. Also, a general suggestion would be to add more descriptive caption for each Figures in the paper. Can authors discuss the computation complexity of their method?
Reject. rating score: 4. rating score: 6. rating score: 6. rating score: 7. <BRK>For instance, "we establish a theoretically grounded and practically useful framework for the transfer learning of GNNs", which would deserve some empirical/theoretical evidence. A comparison, based on another kind of features are needed. Yet, the paper is only exploring the transferability of the proposed GIN. R1: However, the authors could not show how to measure the structural differences between the source and target graphs? However, it does not work well as node degree.<BRK>The authors proposed a transfer learning scheme for graph neural networks. ANALYSI ,  structural equivalence , and  structural different  are typos. Clarity:Overall, this paper reads fine. To benefit most GNNs in real world applications, the transferability of GNNs needs to be analyzed with node features as well.<BRK>The paper introduces a theoretical framework for analyzing GNN transferability. The main idea is to view a graph as subgraph samples with the information of both the connections and the features.<BRK>This work considers the unsupervised learning for graph neural networks. The work has solid theoretical analysis and extensive experimental studies. To encode the structure information, the K hop ego graph is used to generate a k hop ego graph for each node. Here are several small concerns:1. Would the authors explain how the ordering works.
Reject. rating score: 3. rating score: 4. rating score: 5. rating score: 8. <BRK>While the paper discusses a number of ideas that are highly relevant to AI and should be better known by the AI community, the paper has also some weaknesses and is not currently in a form that is ready for publication. Formulating the general problematics around theoretical questions on the origins and evolution of complexity would also enable better linking with the literature in the sciences of complexity and theoretical biology, which is not sufficiently made in the current version of the paper. It is not clear what is the logical rationale of the listing of principles considered in the paper: what is the scientific objective of this list of principles? where do they come from? Also, there seems to be some (at least apparent) inhomogeneity between them, e.g.point 1 says "there should be no built in notion of an individual", but point 2 says "the evolution of new emergent individuals create novel opportunities..."  > point 1 reads like an assumption while point 2 reads like a desirable emergent property. I recommend the author(s) frame more precisely their work in terms of scientific objectives and links with the conceptual history of alife approaches to AI. (Eds.).(1995).The artificial life route to artificial intelligence: building embodied, situated agents.<BRK>The work follows the idea of artificial life on a computer and proposes to use RNNs as building blocks with simple rules of exchanging information between those individual RNNs on a grid. The main weakness of this submission is the lack of the mathematical formulation of the algorithm that is used for generating the complexity. This is not sufficient to explain the mathematical procedure used to arrive at figure 2. There are many non linear dynamical systems that can generate complexity (some of them are mentioned in the manuscript). The authors present beautiful pictures of weights produced by their system. However, I do not understand if the authors are claiming that their dynamical system is somehow more meaningful than other more traditional counterparts. I have read the revised paper and the discussion with other reviewers.<BRK>This is an interesting paper that proposes a particular Alife framework to study the question of how to create an AI generated algorithm. In the proposed system, each cell in a 2D grid like environment is controlled by a different neural network. As far as I understand it, these neural networks are randomly mutated without rewarding the overall system for any particular objective. The authors study some of the dynamics that emerge from this framework. While the main ideas in this paper are exciting (especially they idea that there is no direct concept of an agent),  it currently feels more like a workshop paper than a fully fledged ICLR paper. It would also be useful to further highlight the implications of this work for the larger ML and ICLR community. Still think the approach needs some more work to appeal to the broader ML community and in its current state would be best suited for a conference focused on Alife]<BRK>The paper describes a framework for artificial life, where basic building blocks are artificial neural networks (ANN) elements (matrix multiplication, and other linear algebra operations), intended to be open ended and without any guiding objective. Language is clear at times, the paper is well organized and the there is no overuse of math notation. The framework advanced by the authors seems solid enough to experiment on the emergence of life. It is very re freshing to read a paper that is not only about surpassing other methods by a marginal score in benchmark datasets. 3.The whole idea that life is objective less can be controversial. Authors should be more careful when dealing with such motifs.
Reject. rating score: 5. rating score: 5. rating score: 6. <BRK>This paper formulates CNNs with high order inputs into an explicit Tucker model, and provides sample complexity analysis to CNNs as well as compressed CNNs via tensor decomposition. Experiments support their theoretical results. Sample complexity analysis of CNNs and compressed CNNs is an interesting topic. This paper is well written and is easy to follow. The latter analysis is not new given a rich literature on this topic, see e.g., Bahadori et al.(2014); Yu and Liu (2016); Kossaifi et al.(2020).Lebedev, V., et al."Speeding up convolutional neural networks using fine tuned CP decomposition." It would be more interesting to study the sample complexity analysis for the estimator from some polynomial algorithm.<BRK>This paper provides theoretical analysis of the estimating power of CNN (3 and 5 layers). The writing is generally clear. The convergence results are a nice contribution to the growing literature on neural networks  theoretical analysis. Although the results are interesting, I feel that important aspects of CNNs were not analyzed nor discussed in this paper. One of the main uses of CNNs is to perform classification where there is a softmax layer after the fully connected layer. In Theorem 1, can you give more explanation as to the meaning of model complexity $d_{\mathcal{M}}$? Is this the effective number of parameters?<BRK>Since [3] and [4] both have formulations of higher order CNNs/CNNs using Tucker decomposition, it seems to me that the current formulation proposed in this work lacks novelty. The paper also conducts numerical experiments to verify its theoretical results and provide some empirical studies to show that increasing the expansive ratio of a bottleneckPros:1.This work provides a theoretical analysis for higher order CNNs via analyzing its Tucker formulation using tensor methods. 4.As mentioned above, because a) the proposed formulation of higher order CNNs lack proper comparisons with existing works and has limited novelty and b) the finding from theoretical analysis lack sufficient experimental supports, the contribution of this paper is limited and it would be nice for the authors to provide more justifications for its novelty and better designs of experiments to convey the message. Many of the works also provide theoretical analysis (e.g.generalization bound in [5]) for the proposed formulations.
Accept (Poster). rating score: 8. rating score: 7. rating score: 6. rating score: 6. <BRK>### Final recommendationOverall, I believe this paper should be clearly accepted to ICLR, as the strengths outweigh the weaknesses. This paper is not the final solution to the object permanence task, and it has a lot of possible improvements, but it is a good step. Why is that? The tracker learns based on the class. In some cases the method feels very specific for the presented task and dataset. It would be convenient to use the same names as in the rest of the paper for the layers and the inputs/outputs. Why two transformers are necessary, actually?<BRK>Although I like the idea presented in the paper, I think there are several aspects that will strengthen the paper. 3.Thorough ablation is provided for the proposed multi hop transformer. It is important to know how well the tracking component itself performs. 2.More ablation on the framework should be provided. The authors have addressed my concerns with additional ablation studies and experiments to verify the effectiveness of the proposed multi hop transformer.<BRK>This paper proposes a multi hop transformer method for the video based object permanence task. Overall, the paper is well organized and easy to follow. The paper extends multi hop reasoning techniques that are widely used in NLP domain to video domain, which may inspire other researchers working on other video based tasks that require multi hop reasoning. 3.Experiments on the CATER dataset achieve state of the art performance on the object permanence task.<BRK>However, I have major concerns about the way the algorithm is explained in the paper. Taking these into consideration, I don’t think that this work is in a proper form to be published since several sections should be rewritten entirely. In conclusion, I recommend the rejection. Additionally, a harder version of the existing CATER dataset is created, to alleviate the temporal bias existent in the previous version of the dataset. Since my main concern regarding the writing part was addressed I change my rating and I agree with the acceptance. Same question for Transformer_s.
Reject. rating score: 2. rating score: 5. rating score: 5. rating score: 7. <BRK>The authors claim that "anyone with access to the classifier, even without access to any original training data or trigger, can construct several alternative triggers that are as effective or more so at eliciting the target class at test time." However, this paper creates the so called trigger from the poisoned calssifier. In most cases, this model is not occupied by the adversary. Moreover, the proposed method constructs the alternative triggers by first generating adversarial examples for a smoothed version of the poisoned classifier and then extracting colors or cropped portions of adversarial images. But the motivations of building the  adversarial robust version for the poisoned model and generating the adversarial examples are not well presented in the paper.<BRK>Summary:This paper demonstrates that backdoor poisoned machine learning models can also be vulnerable to alternative triggers. Specifically, adversarial samples that are generated against models robustified with Denoised Smoothing often show backdoor patterns. Experimental results suggest that alternative triggers can be equally or even more effective than the original trigger. Pros:1.This paper studies an important question of the vulnerability of backdoor poisoned machine learning models. Cons:While it is a novel finding that poisoned machine learning models are not only vulnerable to the initial triggers, I have the following questions on the proposed method:1. Could there be other ways to create triggers? There is a lack of discussion on this issue. 3.In terms of parameters, the perturbation size \epsilon is set to 20 and 60 (in l2 norm) in the experiments. Specifically, with a smaller value of \epsilon, can the attack still achieve such high success rate?<BRK>Summary:This is an interesting study on the analysis of poisoned classifiers and backdoor attacks. In particular, after creating several poisoned classifiers, and smoothing them using a Denoised smoothing technique, one can generate adversarial examples. Reason for score:The process of generating alternative triggers and finding effective triggers for each backdoor attacker is mainly manual and needs human intervention. This makes the proposed solution very challenging. This is important since the trigger generation is directly related to the backdoor pattern of the generated adversarial examples. Did the authors investigate this matter? Does changing the location or appearance of the original trigger affect the backdoor patterns in adversarial examples? In the experiment section, the authors only showed two samples to prove that clean classifiers are not easily broken. It would be better if the authors also showed the results on larger datasets with more number of classes. The authors mentioned the highest success rate is picked for different triggers.<BRK>The basic premise of the paper is that poisoned classifiers are broken in a fundamental way   not only are they vulnerable to attacks based on the original trigger image, they are also vulnerable to attacks by adversaries who do not know the original trigger. The paper tackles an interesting and well motivated problem, and shows that a single line of attack is much more worrisome than previously thought. 3.I like the fact that the authors incorporated user studies into the evaluation. ## Questions to the Authors ##1. It would be nice to actually demonstrate this empirically   construct a DP classifier with a poisoned backdoor, and show that the authors  method is still effective.
Reject. rating score: 5. rating score: 5. rating score: 6. rating score: 7. <BRK>Summary: This paper studies the heavy tail phenomenon in SGD. But no rigorous theory has been developed to explain the cause of this phenomenon. In this paper, the authors studied SGD iterates in solving a quadratic linear regression problem with i.i.d (Gaussian) data. The authors also developed other results on the law convergence and variance bounds. My main comment is that the problem studied in the paper is very idealized, I.e., a regression with iid Gaussian data. I understand that this makes the proof cleaner with compact results. However, this may not have direct implications on the heavy tail phenomenon in deep learning. Overall, I consider the results in this paper as rigorously exploring the heavy tail of SGD in linear regression under iid Gaussian data. I suggest the authors consider other more practical problems, and the results need not be related to deep learning.<BRK>The main theme of this work is to study conditions under which SGD iterations result in random variables with heavy tail random distributions. Specifically they focus on the step size, batch size and problem dimension. However, the authors argue that since even in this simplest setting SGD results in a heavy tail distribution, the more complex problems should not be expected to behave otherwise. The experiments verify the correctness of the theoretical results, but they are also of limited value. For example, the deep learning section studies a shallow fully connected network on MNIST and CIFAR10.<BRK>This paper studies the relations between the heavy tail phenomenon of SGD and the ‘flatness’ of the local minimum found by SGD and the ratio of the step size $\eta$ to the batch size $b$ for the quadratic and convex problem. They show that depending on the curvature, the step size, and the batch size, the iterates can converge to a heavy tailed random variable. They conduct experiments on both synthetic data and fully connected neural networks, and illustrate that the results would also apply to more general settings and hence provide new insights about the behavior of SGD in deep learning. I do not have time to check the proofs and are not familiar with this topic. The input data is Gaussian, which is quite restrictive. The authors addressed my concerns. I have read other reviewers  comments.<BRK>This paper gives a theoretical study of the tail behavior of the SGD in a quadratic optimization problem and explores its relationship with the curvature, step size and batch size. 1.This paper is the first one to study the originating cause of heavy tail phenomenon in SGD and give a rigorous proof of the relationship between tail index and the choice of step size and batch size. Experiments are conducted on both synthetic data and neural networks. The design of experiments is reasonable, and the results of the experiment not only support the claim that the tail index is deeply linked to the curvature of the loss and the ratio of step size and batch size but also give an insight on more general cases besides the quadratic optimization. This paper completes its proof under the settings of quadratic optimization and infinite streaming data, which may limit the applicability of the theoretical result.
Reject. rating score: 3. rating score: 4. rating score: 4. rating score: 5. rating score: 5. <BRK>Thus, I m having trouble appreciating the contribution of this paper given the existence of much stronger results. Existence of much stronger results:I don t get why majority voting is claimed to be the "state of the art" technique. The method developed in this paper, however, only works on kNN. So I don t see much empirical value nor any significant theoretical contribution.<BRK>**Update**After reading reviewer2 s comment, I realize that there is literature proving much stronger results that I was unaware of. **Summary:**First, the paper identifies k Nearest Neighbor (kNN) and radius Nearest Neighbor (rNN) to be naturally effective baseline certified defenses against data poisoning attack. Since kNN/rNN are not used as frequently in practical settings, the proposed solution may not be as useful to systems that require the use of neural networks.<BRK>Similarly, a_i and b_i are better to be a_i(x) and b_i(x). Even if the authors fix their theorem, I still don t find the theoretical contribution of this paper significant. Their Theorem 3 tries to achieve such a bound but I think this theorem is not correct (I mentioned the issue with this Theorem in the comments bellow).<BRK>I only see a few sentences before Theorem 3, and that does not support my understanding a lot. The theoretical angle proposed in this paper is interesting. The grouping idea is only intended to prove the theoretical results. The authors seem to point toward that modification is equivalent to one time addition plus on time deletion. Does that count as poisoning? Therefore, I hope the authors could provide a more clear definition of the defining poisoning size. There are indeed cases where due to the attack, some test examples become correctly classified, while originally they are not.<BRK>It is not clear how such a restriction can be mitigated in a practical setup. Due to the above concern, I m borderline on work. The theoretical results are neat. The paper studies the voting mechanism in the nearest neighbor models, and presents a relationship between the poisoning instances and the difference between the majority votes and the second majority votes.
Reject. rating score: 3. rating score: 3. rating score: 3. rating score: 4. <BRK>The paper tests several neural network architectures (as well as random forests) on these datasets and concludes that neural networks are generally not able to perform deductive reasoning. What are the hyperparameters of the neural networks? See below for a very partial list of works. Second, the paper contains many assertions that neural networks are incapable of reasoning.<BRK>This paper s contribution is introducing a set of tasks and datasets that require deductive approaches as opposed to common induction based models. The paper tackles an important and interesting problem that helps to shape the future of the neuro symbolic research area. In summary, the authors have initiated a good step toward defining the simple deductive reasoning tasks; However, the work has not placed well on the body of current neural and neuro symbolic techniques, tasks, and datasets and therefore the contribution is not enough for the publication in ICLR.<BRK>This paper studies the limitations of deep neural networks to model deduction based inferences. This is done by crafting simple datasets and experimentally showing that some (details are not provided) RF, NN and RNN models fail on these. The paper is hard to follow at places.<BRK>Experiments are performed with random forests, neural networks (MLP?), and recurrent neural networks. The proposed tasks are very simple for humans, but are discrete and deductive, rather than the inductive setups NNs typically work with. 2.The paper has a lot of space (is only 6 pages), but does very little to explain the models. *Post rebuttal*All reviewers agree that this paper is not up to the mark. While the revision does include several additional related works, they are not very well integrated with the rest of the discussion on the paper.
Reject. rating score: 4. rating score: 6. rating score: 6. rating score: 6. <BRK>Reasons for score:  The paper misses comparison with a large set of competitive techniques based on incremental learning approach and provides no justification for this omission. The authors also prove two theorems, which further support feasibility of their approach and provide some hints on the expected behavior. Therefore, in order to make comparison complete I would suggest to include some incremental learning techniques as well as autoencoder solutions and clearly demonstrate how the proposed method compares to them in terms of recommendation quality, flexibility, and computational efficiency. The second major point here is the evaluation methodology. So how are they trained? Avoiding test data leaks is absolutely crucial for a fair comparison. The quality of this cannot be measured with RMSE. In fact, there s a strong evidence that models that perform well in terms of RMSE metric may not be good at all in terms of more appropriate metrics like precision, recall, nDCG, MAP, MRR, etc. Please, consider adding more appropriate metrics into the work.<BRK>Extensive experiments are conducted to demonstrate the effectiveness of the proposed framework both in transductive and inductive settings, as well as the scalability. ### ClarityThe presentation of the paper is good. 3.The experiments are extensive, most of which are convincing. Though the authors point out the difference between the proposed IRCF and Zhang s work, they do not give adequate materials to support their arguments. How about other variables, e.g., $B, H, M_2$, etc. This seems weird to the reviewers. The idea is clear, and the technical solution is interesting and solid with a theoretical guarantee.<BRK>The goal is to possess expressiveness (against feature driven methods) as well as generalization (against one hot encoding based methods). In IRCF, there are a matrix factorization model for support users and a relation model for query users. Pros:1.The paper is well written and easy to follow. 4.The idea of using a set of pretrained embeddings as bases may be generalized to other inductive tasks. Thus, it is hard to assess the novelty without a related work section. 3.It seems that you assume C is a conical combination coefficients in Eq.4. Why not to use the unnormalized scores in Eq.4, which matches the Theorem 1 better?<BRK>The authors also provided theoretical analysis to highlight some mathematical insights out of this framework. The proposed method is evaluated on three real world datasets and compared with several baselines. Overall I find the work was well reasoned and executed in a relatively good shape, thus recommending acceptance. Although the experimentations are executed in a good shape, there are still some gaps between the current setup and real world recommendation requirements. It is acceptable given these metrics are consistent with the optimization objective, however, the notable gap between pointwise prediction setting and the real world online top K ranking setting needs to be called out. Another concern about the current evaluation protocol is, it enforces the temporal dynamics on the user side and assumes item representations remains the same   again it is consistent with the proposed method (i.e., Q remains the same) thus expected to favor it.
Accept (Poster). rating score: 7. rating score: 7. rating score: 7. rating score: 6. <BRK>These tasks are selected to require hierarchical modeling of long distance dependencies, and specifically to *exclude* pre training or a need for pre trained models (making many of the NLP tasks which motivate these papers unsuitable). I think this paper addresses a desperate need in the literature, which is a way of making sense of all these Transformer variants that have been proposed. In this sense, I like what it s doing. My main reservations concern the somewhat artificial nature of the tasks and the generalizability of the results. Papers such as the Longformer have primarily evaluated on NLP tasks like coreference and question answering; however, the "long range" abilities on these datasets only add a few percentage points on the given metrics, and they rely on pre training to be successful. So I m not sure how to judge this benchmark suite versus others that might exist. RESULTS: This leads to my second point, about the results. I agree with the authors that, with some tuning, many of these approaches could possibly substantially better at various of these tasks. But again, given the somewhat artificial nature of the tasks, what are we likely to actually learn? I wonder if Path X has fundamental identifiability issues due to its scale: for example, there are now more geometries the model can learn from the sequence, and possibly there are somehow just too many of these for learning to effectively figure out which is correct. All this said, I tend to come down positive on this paper: I think it brings some clarity to this space and will be a very useful starting point for others in the literature.<BRK>The tasks span a wide range of sequential data modalities. *Weaknesses*: It’s not entirely clear to me that LRA is best positioned as a benchmark, rather than an analysis tool; the authors themselves seem to also note this (“Hence, the results provided in this paper are not meant to be a final authoritative document on which xformer is the best”). For instance, a NLP researcher might care more about performance of ListOps, since it’s possible that these results have more relevance for the type of structure found in natural language. Despite this lack of clarity of goals, I think that this toolkit and study offer useful contributions. While it remains unclear to me that this paper has a useful benchmark contribution, the analysis of existing models is valuable. Furthermore, the observation that inherent tradeoffs in performance and speed make no model the one size fits all option is important; in light of this, I’d like to see the authors move towards making their toolkit better for determining what the “right” option is for a given user’s use case. Do you have a sense of how much this can potentially impact performance? Do you think that future developers of xformers should “hillcimb” on LRA?<BRK>This paper presents LongRangeArena (LRA), a new benchmark for evaluating models such as Transformers on tasks that require long range processing. Overall, this paper makes an important contribution and despite some limitations should be accepted to ICLR. The new benchmark is likely to promote research on this important topic. The proposed tasks are either synthetic (ListOPs) or made artificially hard by forcing byte level processing or flattening images to vectors. This means that there is no real value in performing well on these tasks, other than being a potential proxy to other tasks with similar characteristics, assuming such exist. I think relating this benchmark to real tasks would have made it much stronger. The paper would benefit from discussing the differences between the different models. A very minimal discussion is given on the last page, with a pointer to a survey paper. While I realize this is not the main focus of the paper, the different models are an important part of it, and without such even basic comparison, it is hard to appreciate the analysis presented in the results section (e.g., the last sentence on the Results on ListOps). The authors discuss some probing aspects of the first 3 tasks, but do not come back to these later in the paper, which was a bit disappointing.<BRK>This paper proposes a suite of long sequence processing tasks to benchmark efficient transformer variants in terms of their accuracy/speed tradeoff. Ten existing efficient transformer models are evaluated on these tasks and their accuracy/speed tradeoffs are compared on a relatively fair basis. Pros:1.This work is well motivated and timed. 3.This benchmark is accessible for academia : from table 2 it takes at most 10G GPU memory for a normal transformer. I think it is a limitation of the benchmark here. For example, all efficient transformers fail on path x, but that don t mean that path x is not useful. 2.While the benchmark proposed here considers image and text, it d be interesting to add audio processing as well, such as hot word detection. Is there any evaluation of training statistics like memory and time to convergence? Typo:This is the most comprehensive and extensives  > extensiveOverall, I think this would become an important bechmark for comparing efficient transformer approaches, and I would recommend its acceptance if the issues I mentioned above can be mitigated.
Reject. rating score: 4. rating score: 5. rating score: 5. rating score: 5. <BRK>The authors present a novel method dubbed AutoBayes that tries to find optimal Bayesian graph models for "nuisance robust" deep learning. The authors also propose the use of an ensembling approach to further improve robustness of the "best" model. which are never explained. No discussion of time/space complexity or computational demands, which is surprising given the apparent combinatorial complexity of the nested for loops in Algorithm 1. This paper seems to assume that the reader has read every relevant paper and is arriving at this work in sequence.<BRK>**Weaknesses**Scalability: unlike existing methods in AutoML, AutoBayes does not seem to attempt to optimize the process from which graphs are selected (i.e.pruning of graphs that are unlikely to work well), resulting in the need to enumerate all possible graphs (where the complexity is doubly exponential with respect to the number of variables). Fairness of experiments: for each component of the graph, the network structure is the same; therefore, compared to a structure X >Z >Y, we have fewer parameters if we use X >Y. I think a better presentation and clarity in the main paper would greatly help acceptance. **Overview**The paper proposes AutoBayes, which enumerates all the plausible graphical models between data, label, and nuisance variables, remove redundant edges with d separation rules, and learns neural networks to represent the parent to child information.<BRK>This inclusion would also help the presentation of concepts inthe paper. Showcasing thepresented work in this light, (i.e., as a naturalcombination of BN structure learning with macro level neural architectureoptimization) would be particularly novel and compelling. Finally, it is important to discuss the complexity of the presentedalgorithm. This algorithm scalesfactorially in the number of nodes in the BN. The terminology is very clumsy: "Bayesian graph models," "Bayesiangraph," and "graph model" are not established terms and, as such,should be defined so the reader knows what type of ML method is being discussed.<BRK>Summary: The paper presents AutoBayes: a new approach for nuisance robust deep learning which explores different Bayesian graph models to search for the best inference strategy. ##################################################################Strenghts:  The idea of automatically exploring various graphical models to select the best performing one is interesting. For instance, one major problem addressed in this paper is models robustness to nuisance factors, however this was not discussed in this section. Hence, it would be good to include an experimental evaluation on this point. In practice, how can this be evaluated? Steps to derive Equation 7 are not straightforward and can be more clarified.
Reject. rating score: 3. rating score: 4. rating score: 6. rating score: 8. <BRK>#### Summary:The paper proposes an active testing approach that actively selects test instances to estimate the performance of a (black box) machine learning model. The key idea is to train a Bayesian Neural Networks (BNN) with a small amount of labeled test data and evaluate how well the model under test agrees with the BNN on samples for which the BNN has a high confidence. More instances to be labeled by an oracle are selected with active learning, i.e.select the data point that minimizes the uncertainty of the metric prediction. Furthermore, appropriateness of the experimental setup and novelty of the BNN training are unclear and proper discussion of related works is missing. The paper is well written and easy to understand. The title suggests that the paper addresses machine learning in general. However, only a simple deep neural network for MNIST and CIFAR10 is used in the evaluation. Hence, it remains unclear if the approach can successfully be applied to other machine learning methods and datasets. Active testing is appealing when it can be assumed that the test data has a different distribution than the training data. I am referring to the phrase  the traditional method where the metrics are computed using their mathematical formula with all the labeled data *up to the current iteration*, and the labeled data is picked randomly from the whole test dataset .<BRK>Summary: The authors have proposed using an active learning approach to estimate evaluation metrics for a given model. The approach learns a sampling function that decides which observations need to be labeled, which are then fed to a Bayesian neural network (BNN) that aims to estimate the distribution Y|X. The authors select which observations to sample by maximizing the mutual information between the model evaluation metric and the BNN parameters. Pros:+ Active testing of models is a difficult problem in high dimensions. I think a more reasonable comparison method is the active testing approach from Sawade 2010 that estimates the distribution p(Y|X;\theta) using a neural network. In this case, do all the methods have unacceptably large errors? 5.When estimating evaluation metrics for classifiers, it is important to characterize the theoretical guarantees of the estimates. Are there confidence intervals associated with this approach?<BRK>Authors proposed ALT MAS, a data efficient testing framework that can accurately estimate the performance of a machine learning model, a novel approach to train the BNN to accurately estimate the metrics of interest, and a novel sampling methodology to estimate the metrics of interest efficiently. The performances of proposed methods are demonstrated through the empirical effectiveness of our proposed machine learning testing framework on various models under test for a wide range of metrics and different datasets. The problem setting addressed by the authors is one of the hot topics. The experimental results on the two data sets show good performance on the proposed method. Experimental results using MNIST and CIFAR10 show that the proposed method can consistently provide better accuracy than the conventional method. Although the proposed method is based on early stopping, we could not be sure from the paper whether the proposed method can consistently reproduce the same performance on other tasks.<BRK>The paper is quite clear and is well motivated by a practical situation of having a classifier that is a black box and that is evaluated by various metrics that one would like to assess as efficiently as possible. The paper demonstrates that the simpler task of using active learning to identify examples for labeling that would most reduce the uncertainty in metrics of interest is more effective than the more general task of using active learning to learn to predict the labels that the original classifier would give and using those results to calculate the metrics of interest. The only cons that I see are that the paper lacks some obvious explorations that I think would be quite valuable and informative:1. 3.Remark 3.2: Simplicity seems an insufficient reason to have the binary classifier and Bayesian Neural Network have the same structure. Is it obvious that using the sum is the best way? 5.In algorithm 1, step 6, note that $\mathcal{C}_{\eta}^{t 1}$ is being trained on $\mathcal{D}_l^{t 1}$.
Accept (Poster). rating score: 7. rating score: 7. rating score: 6. rating score: 6. <BRK>The authors described 3 methods to estimate the uncertainty. With SGB, the estimation is achieved by training multiple models using data sub samples. These two methods both have the disadvantage that the training time is multiplicative of the number of trained models. The main idea is to use a subset of trees in a GBDT as a model sample, so that we can train a single model but still able to estimate the uncertainty. I find the paper clearly written and well organized. Nevertheless, I think the authors make a good case about the trade off between a fast training time and a good uncertainty estimation. Uncertainty estimation is an increasingly important topic in machine learning application. This paper propose a fast approach and a more accurate approach for uncertainty estimation when using GBDT.<BRK>I think while the results are not super impressive, they are encouraging, and I would like to see more research exploring this area. Pros:1.I think error detection and OOD detection are an under explored area in the context of tabular data. 2.The main idea of the paper is to build ensembles of GBDTs with two flavors of gradient descent. The intuition that ensembles should improve uncertainty estimation is natural, and something worth exploring. I think this is an important contribution to the literature so that the community is aware it does not work for error detection. Cons:1.The virtual ensemble doesn’t quite give as good performances as SGLBs which the authors were emphasizing quite a bit.<BRK>This work examines a probabilistic ensemble based framework for deriving uncertainty estimates in the predictions of gradient boosting classification and regression models. As the authors have said, predictive uncertainty is sometimes a must have feature for high risk application of machine learning techniques. The authors conducted a range of experiments on both synthetic and real datasets and investigated the applicability of ensemble approaches to gradient boosting models that are themselves ensembles of decision trees. In general I think this paper is trying to tackle a significant problem, so it would be good if the authors would give some real world examples of possible applications when uncertainty estimates in the predictions is essential.<BRK>This suggests that models are not independent and intrinsically give a much higher weight on the first model for instance. The motivation behind the building of the virtual ensemble (the choice of T/2 in the text for instance) is not very clear either. (d) It would have been interesting  (and more convincing) to have results (of Table 1) for other non gradient boosting techniques to convince the soundness of the proposed approach and how it compares with other techniques. Overal comment:The paper is well written and the amount of experiments is impressive.
Reject. rating score: 4. rating score: 5. rating score: 5. rating score: 6. rating score: 6. <BRK>Why was medium used instead of small? The core idea is simple   which is a strength in my view   and does not require retraining the base language model, which could be important as language models become more expensive to train. However, the clarity and experiments in this paper fall short: the experimental setup has issues (detailed below), the effect on perplexity is quite large but relegated to the Appendix, several claims are speculative and lacking corresponding experimental evidence, and it is unclear how the additional heuristics affect performance. The method seems promising, but with the current experiments it is difficult to draw conclusions about how the method affects performance and which parts of it are necessary; given that this is an empirical paper, I would therefore not recommend acceptance in its current form. What if large was used? Does the conditional LM need to be a large scale pretrained model (it would be nice to see a baseline of a simpler conditional LM)? #### Heuristics  Several heuristics are used: $\alpha/T_i$ weighting, $\omega$ weighting, nucleus filtering, keeping tokens over a threshold, repetition penalty, and rescaling the logits to positive (used in only one experiment). How does each of these affect performance? There are no ablations, and given the small differences in some of the experiments it is unclear whether performance would actually be worse if we changed one of the heuristics. One outcome may be that the method only works for a careful balance of hyperparameters, which could be fine, but we don t have a sense of the variation. The perplexity gets much worse as the gedi training is introduced (i.e.$\lambda$ decreases), e.g.going from 25 to 45 on IMDb. This result is in the Appendix, and perplexity is never evaluated/reported in the other experiments. Crucially, the GEDI training does not appear to help over just re weighting with the conditional LM ($\lambda 1.0$ vs. $\lambda 0.6$). Could the authors comment on this result? How well does domain transfer work for less similar domains? How is perplexity affected for the models reported in Table 2? How small can the conditional LM be?<BRK>##########################################################################Reasons for score:  My score is marginally below the acceptance threshold. Safer LMs seem to be the outcome of the controllability of the LM by canceling out the opposite part of the target label using the Bayes rule. However, as a conference paper with limited pages, it would be better to make one or two points among them and providing more in depth with valid setups of experiments. I put additional notes about the experiments below. The authors also mention that this is a sort of contrastive learning, and they used contrastive generation. In this work, there is no such auxiliary optimization during training time, but the posteriors are re weighted using the Bayes rule. 4.In Section 3.1.1, various heuristics are used. I expected to see the effect or ablation of each heuristic and how important each of them is in terms of generation quality. Also, the baseline models such as CTRL, CC LM, and PPLM seem to be not using the same heuristics, which seems to be not fair. 5.The output in Table 6 makes me doubt how the experiments are badly controlled. The outputs from positive and negative sentiment are totally different and almost random text, meaning that the content of the generators is not controlled properly. 6.In Table 2, I don’t see any significant improvements of GeDi against PPLM in its attribution score (i.e., positivity) and transferability to the target domain. 7.In Table 3 and 4, have you performed the same experiments with PPLM and CTRL?<BRK>SummaryThe paper considers the problem of attribute based sequence generation, particularly in language models. Authors propose a framework “GeDi” which learns a generative classifier for controlling generation from a large language model. Reason for the scoreI vote for rejecting the current version of the paper (marginally below acceptance threshold). While the premise of the problem is well motivated, I think several sections of the paper are difficult to follow. I would strongly encourage the authors to include a pseudo code of the algorithm to improve the presentation of the central idea. The paper includes several experiments, though I think some critical ablations are missing. + The proposed algorithm of using generative classifiers is computationally efficient compared to strong baselines like CTRL and PPLM. The experimental results suggest that the algorithm also allows for better control over generation from a LM while maintaining linguistic quality. The experiments on detoxification are critical to the thesis of the paper, however it seems that experiments in Section 5.2 consider only GPT 2 baselines? I think a strong baseline based on prior work, like a CTRL generator conditioned on the positive label (as mentioned in Introduction), would help evaluating the gap between proposed approach and current algorithms.<BRK>The main idea is to use a smaller, compared to the LM to control, language model trained with control code (Keskar et al., 2019) to generate a per token score $P(c|x_{1:t}$) to steer the original Language Model distribution. the proposed method is more efficient than WD (Ghazvininejad et al., 2017) since it does not require a forward to the discriminator for each to token in the vocabulary, and computationally more efficient then PPLM (Dathathri et al.2020) which requires several updates per token. [Cons/Question for the authors]  I have read through the paper, but I could not find any significant test (e.g., annotator agreement, t test etc.), are the reported human evaluation results significant? Both detoxification and topic control has no baselines to compare with. For detox, why not using Universal Triggers (Eric et.al.2019) for making the model generate toxic text, instead of using prompt from the dev set of the same dataset used for training GEDI? it is well known that top p and top k greatly improve the model generation, are there performance drop if using top p? how GEDI compare to CTRL, PPLM  in this setting? [Reason to accept]The proposed method is a simple and effective way to control the generation of large language models. This is an important and timely problem, especially for language detoxification. [Reason to reject]The experiments are a bit unclear, looking forward to the author response[Suggestions and some more questions]  With reference to the sentence: " In addition to class conditional generation, CC LMs can be used as generative classifiers by applying Bayes rule to compute $P(c|x_{1:T})$, as is done by Keskar et al.(2019) for source attribution." Could you please add the inline formula, $p_θ(c|x) \approx p_θ(x|c)p(c)$, it saves one jump to the paper and makes the paper more readable :)  Could you please elaborate on why GEDI would be 10k fold less computation as compared with a unidirectional classifier? Could you include a more detailed computational cost analysis?<BRK>The paper proposed a method —  GeDi — to generate guided and controlled text from a large language model (LM). The proposed method guides generation at each time step by computing classification probabilities for all possible next tokens via Bayes rule by normalizing over two class conditional distributions (i.e.contrastive discrimination); one conditioned on the desired attribute, or control code, and another conditioned on the undesired attribute (i.e.contrastive attribute), or anti control code. The paper explores ways to increase generation speed and claimed that with the proposed techniques the generation speeds more than 30 times faster compared to PPLM model. The paper explores different heuristics to impose the guided generation including bias parameter, weighted decoding and filtering heuristics. Re: “so long as the LM and GeDi share the same tokenization”: can you please elaborate the constraint on ‘same tokenization’? However, by contrasting the predictions of opposing control codes via Bayes rule, the bias towards movie reviews can be cancelled out.”: The word cinematic can reveal a neutral/negative sentiment, is there any possibility that pushing the sentiment towards positive might degrade the accuracy of the overall generation? Re: GeDi training (λ < 1 in Equation (10)) and standard generative training(λ   1 in Equation (10)). : How the value for λ   0.6 was chosen? What is the impact of other values for this hyper parameter? Re: “In order to have prompts that are more likely to trigger aggressive generations but less likely to be explicitly toxic, we pass candidate prompts through a RoBERTa (Liu et al., 2019) model trained to classify toxicity, and only kept prompts where RoBERTa was less confident about the toxicity label.“: how did you measure model confidence about the toxicity label?
Reject. rating score: 3. rating score: 3. rating score: 4. rating score: 6. <BRK>Broadly speaking the work is quite unclear, and takes several passes over to have a basic sense of the approach. Figure 5 is presented in the section "example architecture" which might lead one to believe the authors implement this network (which it appears they do not). It does not seem ready for publication.<BRK>This paper aims at defining a new architecture, Dynamic Recurrent Neural Network, that would be based on discrete differential equations of basic linear system transfer functions known from dynamic system identifiation. This is not what is represented here. But RNN could be state of the art in other problems. For example, I know this work  Learning Dynamical Systems from Partial Observations , Ayed et al.2019, which has a section called  Benefits of Continuous Time.<BRK>In other words, produce a train and test trajectories and prove that the error on the test set is small enough. Otherwise, I remain unconvinced on the value of the interpretability. I am not ready to accept this paper in its current form, but if the authors could prove (1,2) or 3 or 4 or 5 then I’ll happily change my rating.<BRK>This paper aims at proposing Dynamic Recurrent Network to understand the underlying system properties of RNNs. By first showing five basic linear transfer functions in dynamic systems theory, the paper formulates DYRNN units. Though not fully explained, this paper provides a method to partially explain the RNN insights through FC layers learnt weights. The overall idea is interesting, and experiments are clear to demonstrate the proposed method. It will be better to test the method on some benchmark datasets so that it will be easy to compare with the state of the art.
Reject. rating score: 4. rating score: 7. rating score: 7. <BRK>This work propose a new GNN architecture to help GNN break its limitation on only working over homophilic networks. The technical is to use introduce graph global attention. I think the paper is written okay. This work has limited novelty. Moreover, if I understand it correctly, the limited difference between this work and [3] is most likely the global attention, which has very limited contribution. in [4]?I guess the computational issue comes from the global attention. However, I still think the complexity is a concern of this work. I do not think that Eq.(3) can be implemented within the complexity that the authors claimed. Moreover, if the authors use another way to compute the attention scores, that way should be very clearly stated instead of written in a different form. Given the high complexity, I cannot clearly see the advantage of this work in comparison to [1], as the non local attention has been proposed in [1] already.<BRK>Main IdeaIn this paper, the authors study the problem of GCN for disassortative graphs. Strength:The authors study a very interesting problem of GCN/graph embedding or disassortative graphs. The proposed model is very intuitive generalization of graph wavelet methods. Weakness:Though the authors mentioned the use of sparsification of attention for speed up, however, it mentioned that t is set to zero. It is interesting to see how scalable the proposed method is as it needs to have global attention to possibly all nodes. The authors only carry out experiments on three disassortative which are all very small. It would be interesting to see more experiments on disassortative graphs. Alternatively, it would be interesting to have an experiment on synthetic graphs where the \beta can be controlled and varied smoothly to see how it affects the performance of different algorithms. The authors picked only node classification of evaluation tasks.<BRK>Although this paper does not belong to my area of expertise, I was able to understand the paper clearly because of its lucid exposition. Experimentally, the authors show a novel GNN design with an attention module that has comparable performance to the MLP and outperforms other GNN designs. I believe that this will be a valuable contribution to many practical problems. Therefore I would like to defer this paper to my fellow reviewers.
Reject. rating score: 4. rating score: 4. rating score: 4. rating score: 6. <BRK>The figures are unclear and hard to understand. #### Post Rebuttal(Copying from the discussion below)I would like to thank the author(s) for their response. The concern was not about the viewpoint of the observer but the new viewpoint from which the question has to be answered. The paper is poorly organized and hard to follow. For example "viewpoint of the driver in the other car" like in the example provided by the paper.<BRK>There are many views in the scene that provide the correct answer "1" for this question. Comments (Technical, Major Flaws of this paper): (1) The idea of addressing VQA in multi view settings is reasonable but it is not entirely new. This leads to concerns about the validity of the proposed dataset. in Figure 4 as an example.<BRK>Why did authors decide not to include such examples in this work? ### Weak aspects and suggestions* The problem is interesting, though my main concern is regarding the novelty and contribution of the paper.<BRK>3.The paper is well written and easy to understand. Cons: 1.The motivation of why we need to learn mental rotations is not very clearly expressed, the practical examples given in the introduction are not sufficient.
Accept (Spotlight). rating score: 8. rating score: 8. rating score: 7. rating score: 6. <BRK>The NN has a "core" that is shared between all neurons, and a neuron specific readout. They train the core on multiple animals and find that it can generalize well: it can be used in a new animal and (with sufficient training of the readouts) achieve high performance. This is nice work overall.<BRK>Expert execution in terms of details of the technical work. The analyses are interesting and provide useful insights. I think a short comment on this scope limitation would help to further contextualize the paper. I recommend that the paper be accepted. I was confused by the following sentences: “… both readouts assume that the receptive field of each neuron is the same across features” “… readout has c + 7 parameters per neuron …” (I only see c+6.)<BRK>The paper presents an experimental study on predicting the responses of mice V1 neurons with computational models. 3.Introduce a novel readout mechanism that allows models to be shared fully across neurons which in turn helps transfer learning. I think this paper is interesting and it should be presented at ICLR.<BRK>(v) The authors report that task driven cores (such as VGG 16 pretrained on imagenet) perform badly in generalizaing across animals. In particular, they propose a novel readout mechanism that is parameter efficient and drives the core to learn better and generalizable features of the visual inputs. Pros:One of the major positives about this paper is the presented dataset. It seems to be relatively large and well curated. This is fine, but there is little justification as to why they chose the current "core" architecture.
Accept (Poster). rating score: 6. rating score: 6. rating score: 6. rating score: 6. <BRK>Novelty of this paper seems to be technically marginal. While the aggregation of adversarial robustness on meta learning framework is new, detailed architecture explanation with novelty is lacked. This makes this conclusion lack sufficient credibility.<BRK>It is an interesting paper empirically addressing adversarial robustness of model agnostic meta learning (MAML). The paper investigates where to incorporate robust regularization in MAML in order to improve adversarial robustness, and based on that *efficient* robust MAML methods are proposed. Interestingly, contrastive learning is incorporated and derive a more robust MAML model. The key point is not properly emphasized.<BRK>To achieve this, the authors provide extensive investigation and solid answers on when, how and why their method works. Overall I vote for accepting. Pros:1.Overall the paper is well written. 2.The paper presents visual evidence of when to incorporate robustness regularization in MAML by leveraging input attribution maps of neurons, which is reasonable and interesting for me.<BRK>It is not clear the results in this paper carries out to the RL setting. POST REBUTTAL COMMENTS  The authors provided additional experiments on CIFAR FS and Omniglot.
Reject. rating score: 4. rating score: 5. rating score: 5. rating score: 5. <BRK>Even if we assume all the problem above can be well explained, the improvement of the proposed model seems not statistically significant. Cons:1.The proposed methodology is not novel. I think it would be great if the authors can put this result at least in the Appendix. Hence, I do not think the current paper is completely novel. Nevertheless, the proposed methodology seems to be the correct answer for GNN to adapt to both homophilic and heterophilic graphs. I believe the paper can be greatly improved if all the concerns above can be addressed. Finally, the authors claim the performance of most baseline methods are found in the literature.<BRK>The paper proposes a graph convolution operator that is inspired by the well known approximation of a graph filter using polynomials of the graph Laplacian. The reported experimental results are positive, even though in many cases the improvement does not seem significant. The main difference with GCNII is the lack of the identity mapping. Authors should deeply discuss the differences between their proposal and other works in literature, clarifying their novel contribution. Also, since you run the experiments on GCNII, it would be interesting to see its performance on the bipartite dataset with \theta   1.5		 In Table 3, the results from literature do not report the variance. In general, it seems like the results of the proposed method and baselines are pretty close, and in many cases inside the variance range.<BRK>Generally, this paper is well organized and easy to read. As pointed out by [1], Chebyshev polynomial is a good approximator to approximate graph filters. It is better to add more justifications (e.g., numerical analysis) about the proposed approximation scheme. Meanwhile, the paper also reports the best results reported in the literature. Hyperprameters: In Appendix B.4, the authors claim that they follow the hyperparameter recommendation in the original paper of baselines. However, it seems that some of the given hyperparameters are not the best hyper parameters.<BRK>SUMMARY:This paper addresses the problem of vertex classification using a new Graph Convolutional Neural Network (NN) architecture. Numerical experiments with real datasets showcase the merits, including superior classification performance, of the proposed architecture. STRONG POINTS:The paper is timely and fits nicely the scope of the conference. The numerical experiments are convincing, offering insights, and demonstrating some of the advantages of the proposed architecture. I acknowledge that the architecture considered in those papers may not be exactly the same as the one proposed by the authors in this paper. More importantly, the paper focuses on NN architectures, so I think it is reasonable to have that on the title. ADDITIONAL RECOMMENDATIONS:Being able to obtain additional theoretical results would make the contribution more solid.
Accept (Poster). rating score: 8. rating score: 7. rating score: 7. rating score: 6. <BRK>Domain Net, as a much larger scale and more challenging dataset, could be considered. Thus, I recommend an acceptance. A high standard testing protocol is a very important contribution in DG research.<BRK>In this paper, the authors implement a test bed to evaluate domain generalization methods in a unified way. It is nice to see that the authors provide three kinds of model selection methods. Does this mean existing methods themselves are not good? I have some questions regarding the test bed details. 1)	Did the authors implement the existing methods or use the source codes provided by the authors?<BRK>All in all, I found the manuscript to be a compelling read that contains an interesting alternative viewpoint on the role and limitations of DG.<BRK>Strength: + The paper highlights an important point. UPDATE  Thanks to the authors for their feedback. I appreciate the efforts on clarification and loose end tying. However I suspect the splits are not the same. For example, some previous benchmarks have a fixed split by default, while I understood Domain Bed use multiple random splits? This paper is in part making a very strong negative result claim that a wide range of existing methods don’t work when implemented “properly”.
Accept (Poster). rating score: 7. rating score: 7. rating score: 6. rating score: 4. <BRK>I found this paper is well motivated and principled in its application of Monte Carlo methods to text based games. I have read the author rebuttal and appreciate the changes made to clarify the distinctions and handicaps used by each of the algorithms. My largest issue with the paper is that it does not make a clear distinction between pure RL methods (DRRN/TDQN/KG A2C) and methods that leverage additional functionality/handicaps to make planning possible (MC!Q*Bert/PUCT RL/MC LAVE RL). To this end, I d strongly encourage the authors to include a discussion of which Jericho handicaps are used by MC LAVE and make a clear distinction in the presented results between the planning and the pure RL agents. The delta hyperparameter for neighborhood size is not given in Table 4   it would be good to understand what value was used in practice and how large was the effective neighborhood.<BRK>I think such a qualitative analysis would be useful in better understanding how the "language driven exploration" term enables the agent to attain the empirical gains reported. 3.I am not entirely convinced that the action spaces of the IF games present a suitable testbed for the semantics based information sharing that MC LAVE attempts to achieve, as the action space vocabulary for valid actions in each state seems quite limited and tending to repeat the same key verbs, based on the presented examples in Table 3. The paper reports empirical improvements over existing methods on various interactive fiction (IF) games in the Jericho suite. It is not clear to me that the improvements are, as repeatedly emphasized by the authors throughout the paper, due specifically to MC LAVE leveraging semantic similarity of useful actions to focus on the most promising actions: It is possible that the additional term is simply injecting additional randomness to the action selection rule, which benefits exploration.<BRK>This paper introduces Monte Carlo planning with language action value estimates to guide exploration. The modification is small, but it is intuitive and shows consistent gains over the baseline without this modification. Some concerns I have are:1. what is the variance for the experiments in Table 1? 2. sample size of 3 is very small, can you increase this and report the mean/variance? Do the authors have qualitative observations as to why this method helps on these two games? 4. this paper is missing what I think is a very relevant citation in Branavan 2012 (https://arxiv.org/abs/1401.5390), which uses language from a game manual to guide MCTS.<BRK>(i) It appears that MC LAVE is using the valid action handicap in Jericho as a *hard constraint* (Eq.6 and Algorithm 1)   this means that the MC LAVE only has a search space of on average < 100 actions per step. The abstract and intro claim state of the art across all games. (ii) The second issue is that MC LAVE assumes that the simulator is deterministic and can conduct rollouts and reset within the span of an episode   standard planning assumptions but incompatible with all other baselines (except for MC!Q\*BERT) which do not use this handicap. 2.The paper in general is well written and easy to follow, the qualitative analysis and the additional diagrams in the appendix illustrating the variations in policies are appreciated.
Reject. rating score: 4. rating score: 5. rating score: 5. rating score: 6. <BRK>##########################################################################Summary:The authors proposed regret minimization (RGM) and structured RGM (SRGM) algorithms for generalization across biomedical domains. ##########################################################################Major comments:While the paper has its own merits, unfortunately, it has several issues that need to be addressed. My main concern is that I am not sure the experiment settings are truly realistic. For example, the number of labels for the HOMO task is not stated in the paper. Can you provide a more concise definition of standard and structured environments?<BRK>The authors propose a new method, the regret minimization (RGM) algorithm, to handle such a problem. The authors further extend the RGM to the structured inputs, leading to structured RGM (SRGM), which can be trained using gradient perturbation. 3.The manuscript is easy to follow although I believe it could be improved further. Some symbols in the paper are not defined as well. 3.In Table 2, why we have both RGM and SRGM?<BRK>2) While the authors described the hyperparameter search in the appendix, it would be nice to know more about the auxiliary models and losses. The authors claimed improved performance over existing methods on several empirical experiments using biomedical benchmarks. Strength: 1) RGM and a perturbation scheme for structured environments with few instances.<BRK>Summary: The paper proposed a new  regret minimization (RGM) algorithm on structured environment. 3.The presentation of Section 3 could be improved   notations could be better explained. Can you add some reference? The paper has solid theoretical analysis, and also demonstrate that the predictor obtained by RGM satisfies IRM constraints in the general case.
Reject. rating score: 5. rating score: 7. rating score: 7. <BRK>## Paper summaryThis paper proposes *Unsupervised Robust Representation Learning* (URRL), a framework that combines several data augmentation schemes and a similarity based loss. The goal is to improve the robustness of visual representations to image perturbations. A further goal is to maintain the robustness properties of pre trained representations after fine tuning the network to downstream tasks. The method is evaluated on clean data performance and adversarial robustness, as well as calibration error. The proposed augmentation approach appears to be very similar to the methods used in MoCo, AugMix, and related work. The paper suggests that augmentations need to be sampled from both of these families in a particular way (Eq.2) to overcome the issue that one of the families affects the color histogram too much. This approach is only a small and heuristic variant of existing methods. Further, the performance improvement of URRL over MoCov2 is small, less than one percentage point for most metrics. However, this method is not compared against baselines. Regularization approaches for improving transfer performance have been proposed before, e.g.see [Li et al., 2018](https://arxiv.org/pdf/1802.01483.pdf). These should be compared to the proposed method. ## ConclusionWhile the paper is well organized and clear, in its current form it does not meet the originality and significance standards of ICLR and lacks crucial baselines. Provide a more formal argument for the proposed augmentation framework to show that it is a significant conceptual advance over the current knowledge that augmentations are important. 6.Provide a more formal motivation for the similarity regularization, including a formal comparison to $L^2$ $SP$. 7.Compare the similarity regularization to a well tuned $L^2$ $SP$ decay baseline. ## Update after rebuttal:The authors provided convincing evidence that the proposed similarity regularization performs better with regards to robustness than $L^2$ $SP$. I still think the contributions are borderline because the proposed regularization is not backed up by theoretical arguments and the augmentation approach is incremental.<BRK>The authors improve the fine tuning of a data representation allowingresistance and recognition of adversarial or corrupt inputs. The work is well presented, and the main significance is in providing a fastway to partially gain some of the robustness available in much more difficultadversarial training. Their approach limits how much fine tuning is allowed to deviate from theoriginal, robust representation. To maintain robustness during fine tuning, one (expensive) option is toadversarially fine tune the downstream task. The authors propose a quickerapproach: a loss function maintaining similarity between fine tuned andoriginal representation (e.g.low fine tuning distortion of dot productsimilarity). Firstly they address what sets of data augmentations are most effective forimages, using a mechanism that probabilistically switches between augmentationtypes for supervised and unsupervised tasks. Secondly, the more interesting results with similarity loss showed very good (usually best)adversarial performance. Accuracy on clean data was degraded by ~ 10%, still less than thedegradation caused by adversarial training. Similarity loss can be applied to other existingtechniques (like their MoCo baseline). The authors  comments strengthen the argument for the method. I (perhaps liberally) conceptualizetheir approach as one of tethering "close to a rotation" but still don t have a simple understanding ofwhy/when this should be better than tethering "close to original parameters".<BRK>This paper uses a different data augmentation (AugMix) scheme to improve self supervised representation learning. The paper s presentation is clear, but the paper could be more thorough. Since the technique is simple and general, it could easily be broadly applicable to the burgeoning area of self supervised learning. This work counteracts this limitation by also assessing the robustness of self supervised representations, and the technique they propose improves robustness. They improve robustness with data augmentation, a volatile ingredient in self supervised learning. SimCLR reminds us that the choice of image modifications is crucial and must be carefully selected, so it was not obvious a priori that their technique should help. That would demonstrate generality. The paper could show results with another robustness benchmark, such as ImageNet R. This would make a stronger case for robustness. Logit pairing can lead to an overestimate of adversarial robustness. Update: I am happy with the changes and am keeping my score.
Reject. rating score: 4. rating score: 4. rating score: 7. <BRK>Summary: This work focuses on sparse reward robotic manipulation tasks from image and point cloud inputs, given a few demonstrations, and proposes an algorithm that consists of a Q attention module and a confidence aware critic. Through an ablation study, it’s clear that the Q attention agent is important for its ability to reduce the input dimension to the continuous control agent. Weaknesses:  It is not completely clear what advantages the Q attention agent brings over standard attention modules. For instance, an alternative might be the pixel position itself. The choice of baselines also seems quite arbitrary.<BRK>The authors’ main algorithmic improvements with ARM include 1) a Q attention agent that extracts interesting pixel locations with an explicit attention module, 2) a confidence aware critic that leads to increased stability of training, and 3) a data augmentation strategy around expert demonstrations. The motivation for the Q attention agent is not made immediately clear. The authors point out that the keyframe selection method likely needs to become more complex with more sophisticated tasks. This again reduces the motivation and impact of the algorithm, if the hand engineering of the reward function must now be replaced with hand engineering of the keyframe selection method.<BRK>The method makes use of some assumptions regarding what would be the important states and makes use of a selection mechanism to populate an experience buffer. Results show that whereas vanilla implementations of SAC, TD3 and QT Opt with a standard replay buffer can not learn the tasks, the proposed method succeeds. A further ablation study shows the crucialness of the attention driven cropping step in this setting and the importance of the sample selection strategy and the confidence layer of the SAC Q function. Using high level control as the action space (next desired pose as opposed to immediate joint actuations)The method is presented as a general manipulation algorithm. While it does seem to generalize to a rich set of examples in RLBench, it makes use of some properties that are limited to a subset of manipulation tasks. The paper does not provide code to replicate the experiments.
Reject. rating score: 4. rating score: 7. rating score: 8. <BRK>This paper discusses implicit SDF representations encoded by neural networks. The idea of encoding an implicit function in the weights of a network without latent code is not new at all, and in particular common practice in all the NERF literature. The fact that it will lead to better reconstructions than learning a common latent space is obvious. (this also implies that the same quality for a similar budget can be obtained with a mesh)  90 seconds to obtain a representation is still very expensive. It s also unclear how errors would accumulate when re encoding several time a given shape.<BRK>This paper presents a simple (and convincing) idea that neural networks can be employed to "memorize" the surface geometry of 3D objects. **W2** While the paper paints the rosy side of neural implicits, it might benefit from quantifying the "ineffectiveness of weight encoded neural implicit 3D shapes". 2019.[B] Park, Jeong Joon, et al."Deepsdf: Learning continuous signed distance functions for shape representation." A seminal reference is [D].<BRK>The paper proposes a weight encoded neural implicit representation for 3D shapes. The idea is to encode every shape in the network weights of its own designated small MLP network, instead of trying to learn a latent space of shapes. This leads to a really compact shape representation based on signed distance fields that could be interesting for many applications. The approach outperforms latent encoded shape representations such as DeepSDF in terms of compression and accuracy.
Accept (Poster). rating score: 8. rating score: 6. rating score: 5. <BRK>Amari s Natural Gradient has been very successfully applied for policy optimization, e.g.in a recent line of work by Schulman et al.These benefit of using these natural gradients that restrict the change in KL divergence between successive policies was more stable and faster convergence. In this paper the authors build upon recent work on Kernelized Wasserstein NGD by making it more widely applicable and scalable. Moreover they present a good empirical comparison between KL NGD and Wasserstein NGD on a combination of pedagogical toy problems and some standard RL benchmarks from OpenAI gym. Overall this paper will be a good contribution to the conference and I recommend acceptance. 3.On page 3 it is said that "the penalty only accounts for global proximity in behavior .... " in reference to equation (4), but PPO is not implemented with a single value of $\beta$, i.e.the strength of the penalty typically varies as optimization goes on.<BRK>### Weak points:  	Novelty: the work however has low technical novelty where it combines several known results into a new framework. The interpretation of the framework also seems straightforward, e.g., it is of course that updating along the Wasserstein natural gradient would incorporate the local geometry of parameterization and help overcome some ill conditioning issues where KL has. Empirical significance: The empirical results though promising are not strong given that this is mostly an empirical work. In particular, the present work presents the experiments for PG case in only 4 environments which I think insufficient to make a reliable conclusion about its empirical significance. ### My finial recommendation After the discussion and revision, the authors have presented more convincingly and more clearly the empirical significance and applicability of their method. I highly recommend the authors to highlight the lastest discussion in the final paper, especially the ill conditioned argument, as it is highly relevant to the practitioners. I think this paper can be interesting for a moderate number of readers, especially the use of the open sourced Roboschool could also increase its reproduciability. I agree to increas my score to 6.<BRK>The paper introduces methods for reinforcement learning based on a Wasserstein Natural Gradients (WNG), an approach to measuring similarity between policies. The paper is well written and easy to follow (the conclusion section is missing though). Although the method is interesting, I think that the current experimental evaluation has significant flaws: the method does not demonstrate the state of the art performance and significantly improves over the baselines on a small number of tasks. I believe that the paper will significantly benefit from comparisons  with stronger baselines such as PPO. Moreover, the experiments performed in the paper (figure 2) demonstrate that the approach only marginally outperforms the baselines on the harder HalfCheetah and Hopper tasks that raises concerns regarding the generality of the approach. The paper proposed an interesting approach to policy constraints in RL but the experimental evaluation is not sufficient.
Accept (Poster). rating score: 7. rating score: 7. rating score: 6. rating score: 6. <BRK>I thank the authors for incorporating some of the feedback during this discussion. ## Strengths:+ This paper presents a clear reward based mechanism to train the teacher. that should be referred to when explaining curriculum generation in the context of RL. The threshold is gradually increased based on the agent s capabilities, allowing for more and more difficult goals.<BRK>Some of these baselines obtained no score in the fully observed setting on the "easy" task subset, and hence were not run on the hard environments. While one might be able to make some claim of the form "if they can t do the easy task, they can t do the hard," I am worried that these choices, used together, really obfuscate the performance differential. The authors did a considerable amount of evaluation, so I hesitate to simply ask for more, but I do wonder if this approach is particularly suited to a narrow range of environments. As a result, I cannot at this time recommend acceptance, though I am certainly open to being persuaded that my worries are unfounded.<BRK>My major concerns are the novelty of the idea and the generality of the proposed method. This paper proposes a setter solver or teacher student scheme for training goal conditioned agents (student/solver) in a discrete environment MiniGrid. The major idea is to make the goals proposed by the teacher staying at an appropriate/medium level of difficulty, i.e., fetchable for the student but not too easy. During training, when is the student conditioned on the extrinsic goal or the intrinsic goal?<BRK>This paper introduces a teacher agent in reinforcement learning framework. That is, the goal should be achievable by the student, yet the goal should not be too simple that the student stops learning towards harder goals, and eventually achieve the original goal from the environment. How does sample efficiency for AMIGo compared to baselines? The idea is natural and experiments make a lot of sense.
Reject. rating score: 3. rating score: 4. rating score: 4. rating score: 4. <BRK>Detecting anomalies is a notoriously ill defined problem. The notion of anomaly is not a rigorous concept and different algorithms produce different results. The principle claims that when data capacity and computational constraints are removed, an AD algorithm should be invariant to  reparametrization  of the input. My main critique of the paper is that this principal constitutes a completely unreasonable requirement of any AD algorithm, to the point where it is meaningless. As a side remark, this principle may be useful if further constraints are put on f, for instance, if we may want the AD algorithm to be oblivious to unit change in the data, which translates to f which multiplies dimension be a constant.<BRK>For the phenomena the paper references, I am not sure this is a good assumption. The more important question to me is when this principle makes sense at all. To summarize again, my main problem is, I am not convinced that “the result of an anomaly detection method should be invariant to any invertible reparametrization f” and that the failure to do so by density based methods helps understand their failures as pointed out in recent literature. Therefore, methods that assign anomalies purely based on low density values cannot achieve a principle proposed by the authors: “anomaly detection methods should be invariant to any invertible reparametrization f”.<BRK>*Quality*I have a major concern regarding the principle stated on the page 4 that the whole paper discussion is based on. First, I totally disagree that anomaly detection methods should be invariant under any invertible reparametrizations. In practice, quite often the very definition of an anomaly detection problem is tighted to a concrete data representation. This is clearly wrong thing to ask from a classifier and instead people focus on finding useful feature extractors. *Significance*I don’t think that the results of the paper are significant in their current form and might cause a confusion with its statements.<BRK>Specifically it proposes the principle that an outcome represented by a random variable X or it s transformation fX should not result in change of it s classification as outlier versus inlier. The paper identifies inliers by introducing "typicality" as a measure. However is any practical sense, the invariance argument is specious. Alternately if the question is the incidence over area (solar power potential) then its the random variable of land area that is relevant. As the paper states in the final discussion, "defining anomalies might be impossible without prior knowledge"  this may not be a bad thing, and the same material used to argue for a knowledge based approach to anomaly detection that includes specifying a random variable appropriate for the domain would be a valuable, acceptable paper.
Reject. rating score: 5. rating score: 5. rating score: 6. rating score: 7. <BRK>The authors propose Feature Contrastive Learning (FCL) to regularize models to be more sensitive to features that have higher utility, i.e.change the classification loss of the model to a larger extent. The claims of the paper would be stronger with robustness evaluation against the widely studied L p norm and recent invariance adversarial examples that are also mentioned in this paper. Contrastive learning is conducted by training the feature extractor of the classifier to minimize distance between the two hidden states in a particular positive pair, in order to align feature sensitivity with utility.<BRK>Summary & Pros:  This paper introduces contextual feature utility and contextual feature sensitivity to measure and identify high utility features and their associated model sensitivity, and proposes Feature Contrastive Learning to  balance robustnessand sensitivity in deep neural network training. However, I have still some concerns below:  Topic Concerns. The goal of this work is to balance robustness and sensitivity. In fact, I am confused the definition of robustness and sensitity as adversarial robustness contains the concept of sensitity. There is not much related work in the paper , and I don t know how important the direction is.<BRK>Summary of work The authors introduce the concept of contextual sensitivity to describe the importance of the feature, which is defined as the absolute value of the Jacobian of loss with respect to the input. Experiments are conducted on CIFAR10, CIFAR100 dataset, and a new synthetic MNIST dataset.<BRK>### SummaryIn this work, the authors focus on the robustness against only common corruptions and perturbations by defining a contextual feature utility metric. They dubbed this method as Feature Contrastive Learning (FCL). Two metrics were introduced (utility and sensitivity) but only one of them was used in the rest of the work with only a hand wavy explanation of their relationship (e.g., it would be interesting to see how they interplay when training with and without FCL)1. ### RatingI like the simplicity of the idea and its applicability. Since the authors are interested in these simple corruptions, the value of the work is hindered by the absence of ImageNet experiments.
Reject. rating score: 6. rating score: 6. rating score: 6. rating score: 7. <BRK>The experimental results on pruning of small networks (e.g., Figure 3,Appendix Figure 7) are suggestive that geometric issues may be important for understanding the success of lottery ticket pruning. The simple observation (beginning with Appendix Eq 9) that adversarial attacks can also be analyzed as attacks on the first layer weights is interesting, although seems like a second separate topic. This skips an important step in the analysis: is tropical pruning effective on small toy problems? The method reasons about the geometry of the network after it is initially trained but then the paper measures the effect on the network after it is retrained. 3.The paper does not provide enough intuition and explanation behind the unproved leap from Theorem 2 to Eq 1, and then from Eq 1 to Eq 2. The text refers to subfigures of Figure 3 (e.g., a first subfigure showing explicit decision boundaries) that seem to be missing. The work in the appendix bringing in analysis of adversarial examples is interesting but seems to be a second major topic that is not fully developed in the main paper and could be the subject of a different paper. **edit, based on revision and after discussion**. Thanks to the authors for answering questions 2 and 3 in discussion and adding an experiment to the paper validating the tropical pruning method in a small setting.<BRK>This paper shows the decision boundary of a shallow ReLU network is a subset of a tropical hypersurface, and this tropical hypersurface is dual to a convex hull of two zonotopes given by the weights of the network. The paper then considers various learning tasks and real data sets, most notably neural network pruning by using an interesting optimization to preserve the tropical hypersurface. The tropical geometry content seems mostly about language to me. Also Proposition 1 and Corollary 1 in my opinion can be removed from the document. On the other hand, the network pruning application that seeks to preserve the convex hull of the zonotopes seems rather substantial to me, with a nice derivation in the appendix. I would put more emphasis on this myself. Confusing, until some bit below where it is stated $\delta(R(x))$ actually is not subdivided because there is no bias (so it is just a polytope). In particular, is $T(R(x))$ also a superset for the boundary between the linearity regions of $(f_1 + f_2)(x)$? I would suggest the authors emphasize this subsection more.<BRK>This submission proposes the use of tropical geometry to understand the decision boundaries of neural networks. The targeted problem is quite interesting and the proposed method looks to be solid; however, I didn t thoroughly check the correctness of the proposed theorems. In the experiments on tropical pruning, since for VGG16, the proposed method only shows better performance on the CIFAR10 dataset, more experiments are desired to make a strong conclusion. For instance, performing experiments on ResNet. How about the efficiency of the proposed approach? Is it practical to use the proposed method in a deep neural network?<BRK>Summary:  This work studies the decision boundaries of neural networks (NN) with piecewise linear (ReLU) activation functions from a tropical geometry perspective. The authors also allude to the use of tropical geometric perspectives on NN decision boundaries for the generation of adversarial samples, but do not explicitly discuss it in any detail within the main text of the paper. It would however be important, to provide some intuition about how one would study decision boundaries when the network is not bias free, in the main text. In particular, how would the geometry of the dual subdivision $\delta(R({\bf x}))$ change? Strengths: + The paper is insightful and novel. It might be best if the authors focused on their theoretical contributions in this paper, added more text and intuition about the extensions of their current bias free NNs, fleshed out their analyses of the lottery ticket hypothesis and stopped at that. The exposition and experiments done with tropical pruning need more work. Introduction: “For instance, and in an attempt to…” Typo – delete “and”. [1] Zhang L. et al., “Tropical Geometry of Deep Neural Networks”, ICML 2018.
Reject. rating score: 4. rating score: 5. rating score: 6. <BRK>The paper is well organized and it firstly tries to give the proof on normalization for domain adaptation. As my understanding, the proof is not directly related with domain discrepancy. The author gives an geometrical interpretation of the proposed method and it is the first trial to analyze a normalization in domain adaptation in a theoretical manner. Although I know that there are too many relevant works for domain adaptation, this work misses too many works on normalization inspired works for domain adaptation.<BRK>This paper is well written and well organized. It should be also justified to use d in the numerator. However, instead of them, the authors use cosine similarities between Gs to compute A^{st/ts}, and it is not justified theoretically or empirically. Just after Eq.(7), the authors state "since y^s has the highest cosine similarity with y_i^t," but why is it? The existing studies on subspace alignment based domain adaptation (e.g., [R1]) should be referred. [R1] "Unsupervised visual domain adaptation using subspace alignment," ICCV 2013.<BRK>This paper proposes a novel normalization method termed Collaborative Normalization (CoN) to eliminate domain discrepancy for unsupervised domain adaptation. Do you replace all of the BN layers in ResNet50? Pros: + Neat motivation; + Novel methods with clear contributions;+ The writing of this paper is excellent;+ Extensive experiments and good performance. If yes, is it stable to replace CoN from BN in the training process?
Reject. rating score: 4. rating score: 5. rating score: 6. rating score: 6. <BRK>It presents both theoretical and empirical proof to show that well calibrated ensemble member does guarantee calibration in the final ensemble. Accompanied by empirical evidence from CIFAR datasets. **Weakness**  * Novelty may be limited: one central contribution of this paper is to provide a mathematical derivation to confirm the observation made in Rahaman and Thiery (2020) and Wen et al.(2020).Although I appreciate author s work on providing mathematical explanation for recent empirical findings, I m not sure if the submission in its current form  is contributing significant novel theoretical insight beyond the fact that ensemble prediction is less confidence, since max of the mean probability is no greater than mean of the max probabilities. On the other hand, the empirical investigation is conducted on a single vision task (CIFAR 10/ 100). This paper can be made stronger by investigating synthetic situation where the ground truth is known, or extend experiment to also other data modalities (like  Guo et al.(2017)).* Organization: Given the place of the new approach (dynamic temperature scaling) in the experiment, it might be worthwhile to devote some paragraph to introduce the procedure in more detail.<BRK>  In general, my opinion is aligned with AnonReviewer1 the theory and the empirical contribution do not feel sufficient. Summary:The paper study calibration of ensembles of DNNs and its relation to the calibration of individual members of ensembles. The work demonstrates that i) members of an ensemble should not be calibrated, but the final ensemble may require calibration (especially if members of an ensemble are calibrated) ii) provide theoretical results to support the statement iii) propose an adaptive calibration scheme (dynamic temperature scaling) that uses different temperatures based on the confidence of a model. Concerns: 1) The main question of the paper "Should ensemble members be calibrated?" 3) The calibration of ensemble has been proposed in [Ashukha2020, 5 Discussion & Conclusion]. ("The resulting ensemble predictions ..., requiring calibration functions to be optimized for the ensemble prediction, rather than ensemble members.") The SKCE is an unbiased estimate of calibration. The dynamic temperature scaling is not proofed to outperform the basslines.<BRK>They also raise an intriguing connection between calibration of ensemble members and ensemble accuracy, one would not expect a priori that if both are calibrated the ensemble would do worse than the average member. I could see this result being interesting to people who study ensembles as well. #########################################################################Pros:  I think it’s a nice observation that calibrating the members of an ensemble may not yield a calibrated ensemble. It’s easy to come up with toy examples where this is the case, but it’s interesting that this seems to be the case in practice. This seems rather unrealistic. I’d remove the mentions of regions and I’d instead mention the other results (prop 2, 3, 4) in the main paper, Section 4.1. The definition in Equation (1) uses p(x, y). Temperature scaling is performed on logits, not on the actual probabilities.<BRK>Through some theoretical developments, the paper supports that a given ensemble cannot be more confident than the average individual members for regions where the ensemble is well calibrated. Empirical results, on CIFAR 100 and three different deep models, report a comparison of ensemble calibration, where calibration is done over all members in order to achieved a calibrated ensemble decision, over individual calibration of members with no feedback from the ensemble decisions. Pros:  Overall well written paper. Straightforward proposal, simple yet meaningful on several aspects for better understanding of the link between calibration and ensembles. Cons:  The proposal is somewhat trivial, although I do not have knowledge that it has been investigated in detail elsewhere. Evaluation on only one dataset (CIFAR 100) in the main paper, with another dataset for the appendix (CIFAR 10). Results on CIFAR 10 in the appendix are not very compelling. However, it is the first time I see this point well articulated, and the authors have made a good effort to develop theoretically backed explanations to support this.
Accept (Poster). rating score: 9. rating score: 6. rating score: 6. rating score: 6. <BRK>#### Reasons for ScoreThis paper provides a much needed evaluation of probabilistic object detection from a statistical point of view. The paper is well written and a pleasure to read. #### Questions for Rebuttal PeriodCan you motivate the selection of object detectors? Why is were the energy score and DMM selected, were other alternatives considered? Aggregated metrics could also hide details in some classes.<BRK>The main claim of this paper is that, for the bounding box regression, non local based algorithms have superiority in the consistency with the predictive distributions than local based methods. The experiments are conducted on some state of the art detectors and fairly demonstrate them. The analysis is technical sound and considerably well written.<BRK>This paper has potential, but I found it not particularly well written. In fact I found it hard to get its main point since it touches in many aspects of probabilistic object detectors, but none of the points are really clear. There are some other issues that need to be clarified   please see below. 1  It is not clear from the formulation how the paper handles multiple detections2  In Section 3.1, there are inconsistencies in the nomenclature   x is written as boldface and then in italics, but they mean the same thing. 6  The proofs for the scoring functions being proper or not should be more formal.<BRK>Although the authors summary their observations at the end of this paper. 3.I cannot figure out enough differences between NLL, DMM, and ES from the analysis figures in the experimental part. The paper is well formulated. Weaknesses and Questions:I am confused by the main contribution of this paper.
Accept (Oral). rating score: 9. rating score: 9. rating score: 7. rating score: 5. <BRK>This paper examines gradient based attribution methods that have been proposed in the explainability literature from a theoretical perspective motivated by a recent observation in energy based generative models. First, the authors point out a general weakness of gradient based attribution that derives from the fact that input gradients do not provide well defined explanations, since the shift invariance of the softmax output makes them arbitrary. The authors then propose that the reason for the success of gradient based attribution models can be explained by the fact that discriminative models "contain an implicit" class conditional density model (the mentioned recent observation about energy based generative models). The authors then carry out empirical studies that convincingly confirm the prediction of their theoretical ideas. In conclusion, this paper establishes very interesting fundamental theoretical connections between discriminative models, energy based generative models, and gradient based explanations, uses this theoretical framework to explain how gradient based explanation are overcoming the softmax shift invariance problem (also pointed out in this paper), and introduces practical training procedures to take advantage of the gained theoretical insights to generate better explanations, which are also empirically verified in simulations.<BRK>I.e., can the  probability gradients  also be arbitrarily manipulated? To test their hypothesis, the paper re interprets the input gradient as class conditional generative model using the score matching view. The argument in this paper seems obvious in hindsight, but that is exactly why the paper is a significant one. I have several additional questions later in my review, but this work is important and suggests that insights based on input gradients might be spurious. The claim that input gradients can be easily manipulated is not new, but the general insight in this work is new and important.<BRK>In this work the authors explore the link between the explanatory power of input gradients and the alignment between the "implicit density model" of the softmax based deep model and the "ground truth" class conditional data distribution. To improve the paper, the authors could add some comments expanding on the practical impact that this results will have on the work of ML engineers who use input gradients as a tool to improve model accuracy. The paper is full of interesting insights and ideas, such as soft max shift invariance property and trivial input gradient perturbation, connections between score matching and adversarial training, and others. There are no inconsistencies or errors that I can see in the paper to the best of my knowledge.<BRK>The hypothesis stated for the paper is "input gradients are highly structured because this implicit density model is aligned with the ‘ground truth’ class conditional data distribution?" A similar comment can be made about the gradient norm model RE: hyperparameter searches. It is not clear why it needs to be conditioned on x either. I am not convinced by the speculation given in the paper. Finally, the Gradient Visualization experiments are very unclear. The authors weight the entire regularization term with a single lambda  1e 3. This raises a few important questions.
Reject. rating score: 4. rating score: 5. rating score: 5. rating score: 6. <BRK>Pros1.The problem is interesting and important. It is meaningful to study heterogeneous graph pre training. 2.The presentation is overall good. There are a number of studies [1,2,3,4,5] for graph pre training. Despite the difference in unsupervised task design, I did not find exciting thing from this work.<BRK>Cons:(1) This paper is not technical. All techniques are not originally proposed, and the whole model is simply ‘putting everything together’. Overall, this paper is well organized and easy to follow, clearly presenting a well motivated model and how each component handles the information extraction. (5) There is no information on how to balance different loss terms. However, the author does not present how to balance the weights among loss terms.<BRK>I agree that there is contribution to putting these supervision signals together and showing results on drug repurposing for COVID 19. All four supervision signals are based on existing works. However,   K Means perform poorly on many datasets (e.g.graphs where true communities are highly overlapping). how do you decide on the number of communities $K$ and does it affect performance a lot?<BRK>Summary: This paper proposes a universal and unsupervised GNN based representation learning (node embedding pretraining) model named PanRep for heterogeneous graphs, which benefits a variety of downstream tasks such as node classification and link prediction. (2) The heterogeneous information maximization signal seems not clearly explained. (See detailed comments below)Detailed comments: (1) It is strongly recommended in related work to explicitly explain the difference from other graph pretraining models such as [1,2] and the reason why they are excluded from baselines. Strong aspects: (1) The motivation, problem formulation, and model illustration are well explained.
Reject. rating score: 5. rating score: 6. rating score: 7. rating score: 7. <BRK>Reconstructing molecular force fields is a very active and thriving field, hence this paper is highly relevant. ##########################################################################Reasons for score: Overall, I vote for not accepting, but if the authors address most of the comments I don t have a problem if it is accepted. As mentioned before, this problem is of high relevance and is likely to have a considerable impact in the physics and simulations community. Nevertheless, this is a non energy conserving and non rotationaly covariant force field, meaning that for most of the applications of a force field, this will provide unphysical results. ##########################################################################Comments:  The authors state ”However, if successful, accurate and fast ML based models may lead to significant practical impact by accelerating simulations from O(hours days) to O(ms s), which in turn accelerates applications such as catalyst discovery.” And “If successful, ML could be applied to problems such as catalyst discovery which is key to solving many societal…” These statements hint that this has not been done yet, which is a half truth. Hence, phrasing the manuscript like this, it could be misleading for the reader. In short, learning forces is equivalent to learn linearisation of the energy surface which is much more informative. MLP is not defined prior usage. In some parts appeared just as e_{st} and on other as \textbf{e}_{st}. A more interesting comparison would be to shifted tanh function. From my point of view, the main downside of this article is the fact that the ForceNet is neither exactly covariant not energy conserving.<BRK>This has the benefit of not requiring to differentiate an energy model and may be more flexible. The approach is well motivated and the paper is well structured and written. A strong point of the paper is the extensive discussion of model design choices. A major weakness of the model is its lack of rotational covariance. In the presented application to relaxation of molecules on surfaces, this might be acceptable (since rotation only occurs in 2d here and relaxation path usually do not have a lot of structural variance), but for more flexible systems this will certainly lead to problems. I doubt that data augmentation can make up for this and the deviation of force predictions under rotation should be shown. In particular for MD simulations this might be a deal breaker, since the resulting model might not be energy conserving. For these reasons, the proposed approach might be of limited practical use beyond the demonstrated application. To prove otherwise, additional experiments with more flexible systems would be required. Finally, the authors state: "However, practical models for real large scale and complex problems remain out of reach."<BRK>The authors argue that force centric learning could be better in terms of prediction accuracy compared to energy centric learning. As such, they focused on force centric simulation and did several simple, but effective optimizations to make their model more accurate and easier to scale to large size. Experiments validate the effectiveness of ForceNet. Overall, I think it is a nice application paper. It is easy to follow and gives clear reasoning behind their designs. Though many of the design options look simple and may not be strong enough from a machine learning perspective, these designs are tailored towardslarge scale quantum chemistry simulation and would be nice to showcase the potential of machine learning models, especially GNNs, to these application domains. From this perspective, I am happy with its contribution. This would be a big plus for this paper, as it will promote more ML experts to pursue this field.<BRK>When employed for quantum chemistry simulation, wherein DFT forces are replaced with ForceNet computed forces, it converges to similar energy structure as DFT in $10^3 \times$ lesser time. **Quality**The paper is very well written and easy to follow. The description of their model, experimental setup, comparison to existing baselines and their architectural choices are precisely described (as well as evaluated). **Originality**As mentioned above, this work combines best of two world proposed for quantum chemistry simulation in the past   GNN based framework and continuous convolution. From the outset, the architecture seem to me simple but effective extension of SchNet work with modifications in interaction layer such as,1. Different basis and activation functionForceNet achieves this by trading off rotation invariant property while leveraging random rotation data augmentation to achieve rotation covariant. Have you experimented ForceNet with shifted softplus as well as other non smooth activation such as Leaky ReLU etc. Comparison to this was missing under basis function choices.
Reject. rating score: 5. rating score: 5. rating score: 6. rating score: 6. <BRK>Review:This paper proposes Triple Search (TRIPS), a differentiable framework of jointly searching for network architecture, quantization precision, and accelerator parameters. To address the dilemma between exploding training memory and biased search, the proposed framework leverages heterogeneous sampling where soft Gumbel Softmax is used for weight update and hard Gumbel Softmax is used for probabilities \beta. Pros:+ The idea of co searching hardware architecture, neural architecture, and quantization precision is promising. + It s nice to see the ablation study on sequential optimization and joint optimization. Concerns:   The paper is not well organized. The state of the art NAS algorithms tend to first train a shared weight hyper net and then perform searching algorithms such as the evolutionary algorithm. It would better if the paper can show the comparison between TRIPS and the following methods. The results in Table 2 may not be fair.<BRK>This paper introduces Triple Search for jointly optimizing neural network architecture, quantization policy, and hardware architecture. Specifically, it proposes a heterogeneous sampling strategy to tackle the dilemma between exploding memory cost and biased search. Besides, it also integrates a differentiable hardware search engine to support co search in a differentiable manner. Cons: 1.I think this paper is not well motivated. Overall, I think co searching neural network architecture, quantization policy, and hardware architecture is an important task. [2] Single path one shot neural architecture search with uniform sampling.<BRK>##########################################################################Summary: The authors present TRIPS, a Triple Search framework to jointly search for the optimal network, precision and accelerator for a given task with max accuracy and efficiency in a differentiable manner. The authors propose a heterogeneous sampling approach that enables simultaneous update of weights and precision without biasing the precisions options or exploding the memory consumption. Further, they also formulate a novel co search pipeline that enables a differentiable search for optimal accelerator. I recommend that authors attempt to minimize the impact of such features, since they make the baseline ASIC accelerators more general which the proposed solution does not support. ##########################################################################Reasons for score:  The paper presents a thorough description of TRIPS a joint search algorithm that enables scalable search of optimal network, precision and accelerator for maximum accuracy and efficiency. Section 2: Hardware aware NAS subsection, “acceleration efficiency, thus can lead* to sub optimal solutions.”3. Section 4.3: Comparison with sequential optimization subsection: “and hardware side, a natural* design”5. However, there are no results or discussion on transferability of TRIPS models to other applications, consider removing this line. It is somewhat difficult to read, consider reordering the implementation discussion and Section 3.2/3.3. 4.Section 4.2, it’s not clear why the authors selected the hardware limitation of 512DSP units. It would be helpful if the authors added the precisions used for the previous work datapoints as well.<BRK>The nice thing about the paper is that the evaluation is pretty solid, with FPGA implementations and simulations for the ASIC flow (not that those simulators are perfect, but they are at least widely used in the community so using them is probably OK.)* Second paragraph on page 2, can you precisely define what you mean by "path" there? * What exactly is your hardware cost model, L_{cost}? It doesn t seem to be using a standard optimization method, which is perfectly fine, but then theoretically analyzing the proposed method would be important.
Reject. rating score: 3. rating score: 4. rating score: 7. <BRK>The main claim that the authors have identified an important factor for improving certified training remains unsupported by the inability to improve noticeably upon results from prior work. This is a much more significant difference than for any of the comparisons made to other work in this paper. * It is unclear how fast the proposed method is. * I am not sure about the utility of the experiment on tightness and Figure 3. While this paper does provide a new technique with some theoretical justification, unfortunately neither the theoretical justification or experimental results are significant enough to recommend acceptance. The authors argue that because their method has a smoother loss landscape then similar methods, it performs better. This is enough to at best demonstrate a weak correlation, but not enough to demonstrate causation.<BRK>The authors argue that this is because IBP has a smoother loss landscape compared to linear relaxation based methods. But it states that the benefit comes from preferring dead ReLU neurons to unstable ones. With such an optimization, bounds of unstable neurons become tighter, so how can such tighter bounds make the model favor unstable neurons less than CROWN IBP with relatively looser bounds? After more consideration, I think the “More favorable landscape” paragraph is still insufficient to address the second point in my above cons. The author response argues that some “$(p/q)$” have looser or tighter bounds, but these are considered for relaxation *locally*, not the tightness on the final output, while the relaxation optimized in the paper is to tighten the final output. 2.AnonReviewer2 has reminded me that the “Fastened CROWN” work had a similar method about optimizing the lower bound of the linear relaxation in verification, which seems to be very similar to the method proposed in this paper, in terms of the verification part in certified training. Although this paper focuses on certified training and has some different analysis, the major modification on the method is still on the verification part, and thus I agree that a discussion on the comparison with Fastened CROWN should not be missed. 3.It is promising that the proposed method outperformed the modified CROWN IBP ($\beta 1$) and IBP, and there seems to be a significant margin.<BRK>Although linear relaxation based methods have a tighter bound on worst case loss with adversarial perturbations than IBP based method, the authors found in numerical studies that towards the end of training, IBP outperforms linear relaxation based methods. The authors hypothesized that this was because IBP loss landscape was more smooth, which helped optimization. Based on this insight, the authors proposed a favorable landscape method. The authors showed in numerical studies that the sum over the worst case margin for each class is lowest for their method. The paper is clearly written. The insight is interesting, and could help future researchers.
Reject. rating score: 3. rating score: 6. rating score: 6. rating score: 6. <BRK>The justification of the approach is to figure out instances that would allow to learn a model with PBE that would generalize better, compared to learning from synthetic datasets that are randomly generated. The proposal is relatively straightforward, using an adversarial optimization loop to enhance the training set for better (at generalization) PBE approach. The algorithms presentation in Appendices A and B are not very clear and not very useful to grasp all the details.<BRK>The paper proposes an approach to develop synthetic datasets aimed at training DeepCoder alike models. Cons:   Sec.3 could be improved. The language is clear, and overall structure of the paper is OK. (2008).Measuring generalization performance in coevolutionary learning.<BRK>in this sense, this paper title is inaccurate in saying the evolution is evolving how to construct synthetic datasets, as we do not evolve over \phi, but only over I. maybe a more appropriate title would be something along the line of evolving adversarial inputs for synthesis. The user adds a 4th I O as a result. I especially liked the passage in this paper that uses the adversarial I O as test set and measure performance with it.<BRK>* Novelty: Although the idea of adversarial data augmentation to improve generalization across domains has already been explored in the literature (eg.Volpi et.al.), I haven’t seen this applied in the PBE setting. * Clarity: The paper is well written and well motivated. The proposed method is evaluated on two domains (DeepCoder and Karel) and compared with two state of the art methods.
Reject. rating score: 4. rating score: 4. rating score: 5. rating score: 5. <BRK>#########################PAPER SUMMARY#########################This paper proposes the game theoretic model of Bayesian Stackelberg Markov Games (BSMGs), a generalization of Markov games, as a formalism for studying Moving Target Defense (MTD) systems, a type of defender attacker game with applications to cybersecurity. The paper then shows experimental results supporting the BSS Q algorithm s success at finding the Strong Stackelberg Equilibrium of BSMGs. My main concerns about the learning process described in the paper remain. #########################STRONG POINTS#########################  Unifying reinforcement learning with leader follower games is an interesting direction for research. The sensitivity of the experimental results to choice of parameters is not included. The experimental results are not presented with sufficient clarity (see the "Questions for Authors" below). #########################DECISION RECOMMENDATION#########################I recommend rejecting the paper, because I believe that the contributions are not sufficiently broad as to warrant acceptance.<BRK>The paper proposes a new model called Bayesian Stackelberg Markov Games (BSMG) to capture the uncertainty of the attacker s types as well as their strategic behaviors. The authors design Bayesian Strong Stackelberg Q learning that can converge to the optimal movement policy for BSMG. But this contradicts with the description of Algorithm 1 in which the Q value of the attacker s are computed, which implies that the attackers care about the future. However, the reviewer had a hard time to understand the model of BSMGs and how the attackers behave in this model.<BRK>In this paper, the authors model a problem of responding to an attacker in a Stackelberg bayesian setting with an MDP. The authors provide a Q learning like solution to the problem, its convergence to a Stackelberg equilibrium asymptotically, and some experiments to show the performance of the proposed method. Moreover, I have some doubts also on the significance of the provided experiments. The proof of Proposition 1 is key in the results provided by the paper. The experimental evidence you provided does is not statistically significant. This dramatically compromises the strength of the experimental results you provided. I think that a strong assumption of the proposed framework is the knowledge of the attackers  distributions.<BRK>High level summary:This paper introduces a Bayesian Stackelberg Markov Game (BSMG) model that considers a defender’s uncertainty over attackers’ types when implementing defensive strategies. It also proposes to use a Bayesian Strong Stackelberg Q learning method to learn defense policies by first simulating an adversary to obtain feedback of an attack and then computing the Bayesian Strong Stackelberg Equilibrium for the BSMG with a solver. In this way, this work relaxes the assumption that the defender knows attackers’ types in existing game theoretic models for moving target defense. It introduces a Bayesian Strongly Stackellberg Q learning method that converges to the Bayesian Strong Stackelberg Equilibrium of the BSMG. In the experiments, while it is helpful to compare BSS Q with several existing baselines, it is unclear whether the performance of BSS Q is comparable to the Bayesian Stackelberg Game model when the defender has complete information about the attackers’ types. The fonts are too small.
Accept (Poster). rating score: 9. rating score: 7. rating score: 7. rating score: 5. <BRK>* The notes on the interpretability is very interesting and differ the work from other methods. * The paper is very well written. Would be nice to know how does the proposed method compared to it. As a result, NAS becomes more sample efficient, which is empirically verified by extensive study in this work. Post rebuttal comments I thank the authors for their responses.<BRK>W.r.t whether the comparison to other SOTA NAS algorithms is of good quality and fair, I think the input from reviewers with a NAS background would be valuable. It happens that due to the WL propagation scheme, the WL graph kernel consists of interpretable features while computing the similarity. In a variety of experiments, the authors show not only increased performance of detected architectures but also find subgraphs that are found by other algorithms as well. I find the aspects of novelty, interpretability, and quantitative results convincing enough to recommend acceptance.<BRK># Post Rebuttal: I thank the authors for taking the time to address my concerns. The paper is well written and the proposed approached is promising. However a few points need to be clarified, and it would be great if the authors could address them in the rebuttal. # Merits  The paper is clearly written and the approach is well motivated  The proposed method achieves  strong results compared to other Bayesian optimization strategies based on Gaussian process surrogate models.<BRK>The discussion was really fruitful. Unfortunately, I still have a concern about interpretability of the proposed method, which is a central topic in the paper. > Second, the example the reviewer gives is not a case when averaged gradient fails. In the last response, the authors explained the interpretability issue through combination of motifs, but it did not resolve my concern. Another difficulty of the gradient based importance evaluation is that the lack of uncertainty evaluation. Overall, the idea would be reasonable, and the approach would be useful. I couldn t find general procedures for the architecture search, from the main text of the paper.
Reject. rating score: 4. rating score: 5. rating score: 6. rating score: 7. <BRK>The paper proposes a method to infer goal distance maps for use in path planning using transformer networks. These 2D distance maps are evaluated on both planning and combined mapping and planning experiments on synthetic data. While the proposed method seems technically sound and based on the experiments works as intended, it is not clear how the proposed system would be applicable in practice. Additionally, the motivations for the proposed approach over classical planning methods, such as run time and incomplete environments, were not explored. From a machine learning point of view, there is nothing particularly novel about the proposed method which uses well known and existing methods to build the planning framework. The proposed method produces a distance to goal map based on an obstacle representation. While 0.25 cm grid sizes for navigation can be acceptable in large outdoor environments, the 10 degree discretization used would be unsuitable for most manipulation tasks. One of the motivations given by the paper for the development of an end to end learning framework is the computational efficiency of having to only perform a single forward pass over the input. Given the 2D grid structure of the input and outputs, one would expect a comparison to widely used search methods such as A* and R*. As the focus of the paper was on planning, this also caused some confusion on their relation. This is in stark contrast with decade old methods such as A*, RRT*, and trajectory optimization which have such guarantees. However, the question regarding the ability to handle larger state spaces is only touched on in the appendix and it is not clear from that description what the state space was. While I could see improvements and clarifications I could not see the main concern I had, handling of high dimensional problems, being addressed.<BRK>This paper presents Spatial Planning Transformers (SPTs); neural network modules that perform spatial planning over grid like state spaces. The paper also goes on to present the idea that differentiable mapping and differntiable planning modules could be trained end to end, for better performance. This is evaluated against two baselines (value iteration networks (VINs) and generative path planning networks (GPPNs)) on small scale navigation and manipulation tasks. SPTs seek to replace memory based models, such as LSTMs with purely attention based models such as transformers, to potentially capture long range dependencies. I look forward to the author response, and am open to changing my evaluation if the concerns are adequately addressed. **W1** Discrete action spaces: It seems like the proposed approach is tailored to discrete state spaces or maps (and perhaps to discrete actions)? Upfront, it seems dubious, as it is unclear how the transformer blocks might apply to continuous maps. For instance, if one were to move from a 2D (planar) space to 3D, this seems like it will dramatically increase planning complexity. It may also be worth using some of these approaches as baselines for evaluation. The presented method will seem better placed in the context of other recent contributions if more baselines were to be introduced. For instance, are there various combinations of differentiable mapping and planning strategies that can be evaluated? One would also have expected an evaluating against non differentiable mapping + planning (i.e., "classical" robotics) strategies to strengthen this claim. This could benifit from a revision, and I strongly encourage the authors to address some of the issues raised above.<BRK>The method is interesting as it is fully learnt in an end to end fashion. Also the paper extend its findings to out of distribution maps and  the cases where the ground truth map is not known to the agent. The motivations and the architecture are clearly presented, but I think there is a lack of clarity in the way the methods and the results are presented, especially in the most interesting section, the one about the mapper. Also the analysis are limited to reconstructions of the map and some attention weights in the appendix. Interesting new approach with potential applications in both navigations and manipulations task. Cons:1.Although the method is a clear advantage over previous ones it still requires to know the ground truth distance to get the map. 2.Instead of using a paragraph to explain the transformer architecture, which is relatively well know, my suggestion would be clarifying the mapper section with more notation to clearly state the loss. This would already help to most likely increase my score. 3.In section 3.2 the authors claim that “While it is possible to train a separate mapper model to predict maps from observations, this requires map annotations which are expensive to obtain and often inaccurate”. I don’t think this is true: e.g.Gregor K. et al., 2019  >  https://arxiv.org/pdf/1906.09237.pdf present an example of how a map can be learnt. 4.It is not clear how much the model relies on having perfect distances as supervision vs. noisy ones.<BRK>The paper correctly identifies some of the shortcomings of classical planning methods. The idea to use a transformer for planning seems promising and that is backed up by experimental results where the method outperforms previous work. I think the paper has some interesting ideas and the experimental section seems promising, but some assumptions are not stated clearly enough (I will discuss specific points under "Cons"). ### Pros* To the best of my knowledge, this is the first paper to propose combining transformers with neural planning in the style of value iteration networks. * The paper deals with both robotic manipulation and navigation in the same framework. It is nice to see these related problems being tackled jointly. ### Cons* The section on the mapper makes it sound like it maps one observation to an entire map, though in almost all cases, one observation won t be enough to do so. No mechanism for aggregating observations over time is described. The only way I can see this working is if the method gets an exhaustive dictionary of images from many locations and orientations in the environment, such as the 3D experiments by [Lee et al.](https://arxiv.org/pdf/1806.06408.pdf) The experiments section also gives this impression. This does not invalidate the approach but I would like this to be stated more clearly and earlier on in the text. * The end to end module is trained using action distance labels. I don t understand how this is preferable to assuming we have full knowledge of the obstacles, which the paper seems to imply by saying:  > This essentially allows us to train a mapping module without any map level supervision using a pretrained planner and action level supervision. My issue here is that I don t understand how we could get exhaustive shortest distance labels without having access to an occupancy map or a list of obstacles.
Reject. rating score: 3. rating score: 4. rating score: 5. <BRK>This paper proposed a cross modal retrieval augmentation for the multi modal classification task (VQA). My major concern about this paper is the lack of novelty and experiment comparison. The proposed image caption retrieval architecture is not novel at all.<BRK>This paper explores a new direction, to utilize the searched results (image caption pair) to improve downstream multimodal learning tasks. Then they use the pre trained model to search the relevant terms for image or text input, and augment the searched results as the input for the downstream multi modal tasks. However, plenty of cross modal retrieval methods achieve better performance than DXR.<BRK>In this paper, the authors present a method to use unstructured external knowledge sources to improve visual question answering and image caption retrieval. The proposed method can achieve somewhat improvement for visual question answering, but drop the performance for image caption retrieval with a more complex model. However, Table 3 does not give us a throughout comparison.
Reject. rating score: 5. rating score: 6. rating score: 6. <BRK>Summary: The paper proposes a deep reinforcement learning algorithm of advantage actor critic (A2C), which ﬁrstly learns the causal structure of the environment and then leverages the learned causal information to assist policy learning. The causal structure is computed by calculating Average Causal Effect (ACE) between different categories of entities, and the authors assume that an intrinsic reward is given to encourage the agent to interact more with critical entities of the causal graph. Furthermore,  there were unclear points in the implementation (without sharing codes) and the results. Pros:1.The authors designed causality based intrinsic reward to an agent, to encourage it to interact with critical entities for accomplishing the task.<BRK>In this paper, the authors propose a new deep reinforcement learning algorithm that learns the causal graph representation of the environment and leverages it to assist policy learning. I would like also to see a proper definition of the entities and how the learned causal graph of entities impacts the performance of the proposed method. I consider that the requirement of no prior knowledge is an interesting and valuable contribution of this paper. Thus, I consider it is an open research question the application of the proposed method in these scenarios. The categories of the given entity causal graph could be inferred to build the causal relation among categoriesof entities as assumed as input to CARE.<BRK>The authors proposed to learn causal relations to accelerate thelearning process in deep reinforcement learning. The proposed approachfirst learns a graph to represent the causal structure of theenvironment calculating the Average Causal Effect (ACE) between differentcategories. It uses an intrinsic reward to encourage interacting withthe most relevant entities of the causal graph which accelerates thelearning process. For that they obtain data of the agentinteracting with the environment and use an encoder decoder approachto obtain k categories. Then predict the next state based on thecurrent state and an action, where the decoder predicts the next stateof each category. (how manydecoders?) The results should also include the time used to obtain the causal graphs.
Reject. rating score: 3. rating score: 4. rating score: 5. rating score: 7. <BRK>This paper proposes a neighbor search method for negative sampling in extreme classification. Have you studied that in your experiments? ii) You claim you are using an O(1) algorithm independent of negative classes N. But this is simply not true and not reasonable because your K and L may need to increase when N grows in order to keep a constant collision probability. It would be interesting to compare these two carefully in experiments.<BRK>Granted, sample softmax would be of order $O(\log N)$, but in practice this clearly is not a handicap. ~~For an application such as this, the details of LSH’s choices are critical, yet the paper only glosses over them. So it clearly still needs the added parallelism to be competitive.<BRK>If we assume K and L are constant then yes; but theoretically in order to get good result we need to have larger values especially for L. Please elaborate on this. However, most of existing approaches are (argued to be) either inaccurate or computationally costly. + The methodological contribution of the paper is more or less an off the shelf use of LSH for negative sampling.<BRK>In that case, previous work came up with a technique to subsample bad classes for each data point to approximate cross entropy or other used loss. Prior work used uniform samples, but this paper proposes an approach based on locality sensitive hashing. One comment is that the writing is fairly sloppy and could use additional polishing.
Reject. rating score: 3. rating score: 4. rating score: 5. rating score: 6. <BRK>It is unclear why distillation is included, as it does not directly    address the problem of distribution shift in offline RL, outside of    the intuition that it mitigates bad $Q$ estimates. In addition, theypropose to ensemble a distillation policy. Decision BRED is an interesting approach, but the experiments do not demonstratethat it is better than the baseline. As such, my preliminary rating ofthis paper is rejection. This is not clear to me, how does fine tuning connect to modelling    the dataset generating policy? Figure 1b: the agent is trained on the offline dataset and then that    dataset is thrown away for the online agent, whereas the agent in    red gets to keep using it. The sampling scheme isincremental and simple, but seems like a novel approach. Distillation onthe other hand, is well explored in RL (Czarnecki et al.2019) and doesnot seem to contribute much to the problem addressed in the paper. Quality and Clarity The paper is easy to follow and well written. However, I have someissues with the title. The title suggests that your aim is to addressdistribution shift in online RL. Of course, the problem you areaddressing is further online training after offline training. Figure 4: Going from figure 4a to 4b, balanced replay is shown to be    more beneficial for a dataset generated by a random behavior policy. Moreover, the online only curve is very different in Figure 4c    than 4a and 4b, and to my understanding there should not be any    difference. Section 3 in particular highlights the correlation    between distribution shift and the performance of offline RL    algorithms. Sensible baselines are used to compare the proposed    method (BRED), and these are discussed and reasoned in detail.<BRK>The method incorporates a balanced replay scheme for both online and offline samples, and an ensemble distillation to stabilize policy learning. The proposed method is evaluated on benchmark environments. The paper is clearly written and easy to read. However, the problem setting in the paper is not well motivated and the goal is unclear. Moreover, some important experiments details are missing which makes it hard to assess the empirical results. Therefore, I recommend to reject the paper. ##### Supporting arguments and clarification questionsFirst of all, I think it is not motivated why online fine tuning procedure is necessary (see first paragraph in the introduction). It is also unclear to me what is the goal we want the proposed method to achieve  (e.g., what are the evaluation metrics)? Without a clear goal, it is hard to assess the significance or soundness of the proposed method. It might be due to the fact that we are optimizing different objectives. I think the paper should also include a baseline, which is the performance achieved the offline RL agents (i.e., horizontal lines in the learning curve). In Figure 3, it seems like BRED is much more stable compared to other methods in Hopper medium, so I wonder if the reason is that BRED already learns a good policy with offline dataset and it does not change much during the fine tuning procedures.<BRK>But it will be better if the authors can make it explicit. Two mechanisms are introduced: (1) using two replay buffers for offline and online data respectively, and training the agent with data sampled from these two buffers with a certain ratio (the ratio changes in a way that more online data is used in later epochs); (2) learning an ensemble of independent agents in the offline phase, and distilling them into a mean policy to overcome bootstrapping error. Summary: This paper proposes to deal with distribution shift problem between online and offline samples when the agent trained by offline data is fine tuned with online interactions. The paper is overall well written and easy to follow. The main contribution of this paper is combining CQL with two tricks: balanced replay and ensemble distillation. I am curious whether the authors have tried other ways. 4.Although policy ensemble can make the learned Q function more accurate and stable, it is not very clear to me why it tackles distribution shift. Even the distilled policy has a more accurate estimation of the Q values, the estimation is still w.r.t.the offline data distribution. It still suffers from the bootstrapping error when using out of distribution samples. 2.In the experiment, I am wondering how an “ensemble only” method would work and compare to baselines, i.e., use multiple pertained CQL agents, and fine tune the distilled policy. The “Effects of ensemble distillation” paragraph shows a comparison between the proposed ensemble distillation and the ensemble of independent policies. 3.The paper focuses on the distribution shift problem in offline RL. I think this paper is mainly dealing with the state distribution change case.<BRK>This paper considers the problem of policy learning in Markov Decision Process (MDP) from the combination of online and offline samples. The offline samples are generated by a behavior policy in the same MDP model, i.e., the behavior agent and the learning agent share the same state action space. The learning procedure goes as follows. One first trains a MDP policy from the offline data; the online samples are then used to fine tune the learned policy. The authors propose a simple yet effective approach. First, the authors keep separate offline and online replay buffers, and carefully balance the number of samples from each buffer during updates. Then,  multiple actor critic offline RL policies are trained, and a single policy is distilled from these policies using ensemble methods. Experiment results show that the proposed method consistently outperforms state of art algorithms. This paper is clearly written and well organized. I am not sure about the novelty of the proposed method, since it seems to follow the line of carefully reweight online and offline samples. However, the experimental results show a significant improvement over existing methods. Question for the authors:1. In practice, what is a good heuristic for selecting the initial fraction p0 of online samples? How sensitive is the learned policy w.r.t.the initial fraction p0?
Reject. rating score: 3. rating score: 3. rating score: 4. rating score: 6. <BRK>This paper proposes a simple and general purpose evaluation framework for imbalanced data classification that is sensitive to arbitrary skews in class cardinalities and importances. This importance criteria can be regarded as opposite of one of the presented criteria, "Rarity". Overall, the analysis is not convincing enough to verify the benefit of the proposed metric. This is again not the ground for the claim "our new framework is more effective than Balanced Accuracy – ... also in training the models themselves." The novelty and the originality of this paper is not enough that there is only limited benefits to the community, which does not satisfy the ICLR standard.<BRK>This paper presents a weighted balanced accuracy to evaulate the performance of multi class classification. Basically, the performance for a multi class problem can be evaluated by decomposing the original multi class problem into a number of binary ones based on one against rest manner, and then evaulating the performance scores for each of the binary ones using any well known metric for binary classification, and then, aggregating the performance scores. The proposed idea is just a simple natural extension of balanced accuracy with a weighting scheme. It is nothing new.<BRK>Summary:This paper proposes a new evaluation framework for imbalanced data. Specifically, they introduce an additional weighted term in the formulation of balanced accuracy. Pros:  The proposed framework is simple and effective. The proposed framework can be used in many application domains. This term is not clearly explained in the paper. Since the weighted terms e.g., $w_i$ play an important role in the proposed framework, more examples should be given to explain how to choose these parameters in different application domains. Overall, I think the novelty of this work is limited and the experimental results are not very convincing.<BRK>The paper presents a simple addition to the Balanced Accuracy approach   which the authors refer to as ‘importance’. The paper evaluates the new metric   but only agains the Balanced Accuracy metric (which seems quite restrictive). The paper lacks a demonstration that your approach actually does give a more appropriate ordering of the machine learning approaches. 5) Paragraph starting ‘Let us illustrate the problem’   this talks about a dataset in the most abstract sense. 6) The related work on evaluation metrics seems a little short. How do the other proposed approaches compare to your work? 7) “As we discussed with examples in previous sections, in many real world classification problems,”   you only give one example and the ‘real world’ case that this refers to is not provided. 8) Most sentiment analysis approaches are not based on RNNs   can you justify why you used this approach?
Accept (Poster). rating score: 7. rating score: 6. rating score: 6. rating score: 6. <BRK>Figuring out which models to send to which clients; 2. Computing their personalized weighted combinations for each client. Personalized federated learning is an important problem, and I see a lot of value in practical applications of federated learning2. The proposed method is simple and intuitive**Cons**: 1. I see strong claims in the paper such as  we are the first to our knowledge to enable transfer to new specific data domains of interest through personalized FL. 2.Would be helpful to provide a convergence analysis of the proposed algorithm.<BRK>The paper proposes a new FL method that computes in every communication round for each client a personalized model as starting point for the next round of federation. The paper defines the client specific objective as some loss function of the weighted combination of all (or subset) models on a client specific validation set. The paper evaluates the proposed FL algorithm on standard datasets for image classification by comparing to alternative FL methods.<BRK>This paper introduces an approach to personalized federated learning. The nodes construct a weighted average of the received models so that the mix performs well on their local *validation* data. The proposed model is experimentally demonstrated to be competitive or better than previous work on an extensive array of tasks. The writing is generally clear. While there might definitely be scenarios in which this is reasonable, the large amount of communication could be prohibitive in many practical scenarios, and exchanging local models might still raise privacy concerns. Is division by zero never a problem in the proposed weighting normalization scheme?<BRK>This paper proposed FOMO, a personalized FL framework. Comparing to existing methods that conduct local adaptation starting from a single global model, FOMO keeps track of all models, and personalize by computing the weighted version among all models. Although the experiments are not done on FL golden standards such as EMNIST/Shakespear, the experiments and reasonings are well presented and well conveyed. (b) The DP approach adds noise to updated models, which could lead to a significant personalization performance drop, which hasn t been discussed in the paper, and DP is critical to ensure data privacy. I am expecting the authors to address the privacy part with some updated experiment.
Accept (Poster). rating score: 7. rating score: 7. rating score: 6. rating score: 6. <BRK>Rather than using a fixed t >s model to translate target monolingual data in order to augment the training set for the s >t model, the proposed approach first pretrains the t >s model as usual then jointly trains it with the forward s >t model using a meta learning approach: the s >t model is trained on the syntetic backtranslated data and a "meta validation" loss is computed on a paralled dataset, which is used to update the t >s model using REINFORCE. The approach is similar to the DualNMT model by Xia et al, but rather than updating on monolingual data based on LM and reconstruction scores, it uses a reward based on the cross entropy on parallel data. The paper also proposes a way to adapt this method to a multi lingual setting. The authors report small but consistent improvements.<BRK>The paper proposed an interesting approach for back translation. The backward model is updated using gradient of the forward model on meta validation dataset (i.e., parallel data). In general, I find the approach is nice. While the paper compared Meta BT with offline BT (i.e., MLE as mentioned in the paper) and DualNMT, I think these two baselines are not sufficient to verify the claims made by the paper. While MetaBT avoids multiple iterations of iterative BT, the evidence is not provided in the paper in terms of training time for MetaBT. As the author already mentioned that MetaBT has a large memory footprint, thus it’s slower to do one update in MetaBT.<BRK>The paper presents an extension of the back translation method which provides a means of leveraging monolingual data in NMT where the quality of the data generated by the back translation model is controlled through a meta learning regime that trains the BT model jointly with the actual translation model. The method is an interesting application of meta learning to NMT and worth seeing the results. The motivation is the main weakness and a better discussion and comparison to related work would help clarify the applicability of the method. There is on the other hand not any empirical support in either claim that these create a weakness, thus, grounding the motivation for the paper.<BRK>The approach is evaluated on WMT EN DE and EN FR and compared against a simple sampling strategy for backtranslation, and dual learning. strengths:+ the idea of end to end optimization of the backward model to maximally benefit training of the forward model is novel and interesting. Specifically:   what data sets do you use for meta validation? I gather that you use some type (which?) Do you also apply these techniques to the backward models, and if so, is the initial model for meta back translation initialized differently? Which language model was used for the additional rewards? What monolingual data was used, only target languge data or also source language data? If we take "meta" to roughly express some self referentiality (meta learning is learning to learn; a meta analysis is an analysis of analyses), is meta backtranslation the backtranslation of backtranslations?
Reject. rating score: 4. rating score: 5. rating score: 5. rating score: 6. <BRK>In this paper, the authors introduce a class of games called Hidden Convex Concave where a (stricly) convex concave potential is composed with smooth maps. On this class of problems, they show that the continuous gradient dynamics converge to (a neighbordhood of) the minimax solutions of the problem. * The paragraph "Our class of games" and "Our solution concept" quite verbose and hard to understand (maybe adding in equations the definitions of Nash Eq.and Von Neumann solutions would help). Since a goal of the paper is to provide intuition on the dynamics of certain games with respect to certain solution, these should be made very clear for the reader, this is not the case at the moment. This kind of contradicts the statement that the results are "non local" in the abstract. While looking at the 23 pages appendix, I feel that Eq.(1)   the dynamics; Theorem 1   the reparametrization; and Lemma 2   derivation of the non increasing potential, are the most important results (or rather their instantiation for GDA dynamics). * The term "hidden convexity" may be a bit confusing since it may refer to e.g.local strong convexity around the solution. * In Theorem 2, the sense of "stable" here should be defined.<BRK>In particular, recent work by Letcher (2020) and Hsieh et al (2020) suggests that even many recently proposed modifications of simultaneous gradient descent are not guaranteed to converge in the non convex concave case. They furthermore show a notion of "hidden convergence" of the $f_i$, $g_i$ that is particularly relevant to the overparametrized regime. In the last section, the authors relate their findings to GANs. However, in this case the application section and motivation of the paper might need some restructuring. I still find the theoretical findings and method interesting, but I think that the work requires substantial refocusing and the identification of more examples of "hidden strong convexity" before being published at a top tier conference.<BRK>This paper studies min max problems of the form L(F(x),G(y)), where L is a strictly convex concave loss. The authors prove that, under certain assumptions, the GDA dynamics converges to von Neumann solutions. This is an almost immediate conclusion one can draw from studying (Vlatakis Gkaragkounis et al.2019), and hence the contribution does not seem to meet the standard of top ML venues. In addition, the authors studied unconstrained min max problems with hidden structures, while in Section 4 they claimed that GANs can be viewed as **constrained** min max problems. I would like to point out that the presence of constraints is a significant difference as it would completely invalidate all the analysis in this paper. In conclusion, I did not see much novelty of this submission in terms of theory, and there is no practical implication.<BRK>### SummaryWith the aim of improving our understanding of GAN training, the paper studies the behavior of gradient descent ascent (GDA) dynamics in a subclass of the so called _hidden convex concave games_. As an example, the authors prove that when the hidden game is strictly convex concave, with safe initialization GDA dynamics converge to a point that solves the hidden minimax game. The analysis and the presentation are clean and the appendix is extremely well organized. #### Slight overclaim on the problems covered by the paperUp to page 2, one may believe that the paper addresses the general hidden convex concave game as defined by (HCC). #### Ambiguities in how GANs fit the frameworkWhile the authors claim that GAN training is a specific case of HCC, it is not straightforward from the text that this in fact indicates either of the following1.
Reject. rating score: 3. rating score: 3. rating score: 4. rating score: 7. rating score: 8. <BRK>This paper consider the decentralized adaptive algorithms. At the first glance,  I am really happy to that the adaptive methods are used for the decentralized optimization. In line 9 of Algorithm 1, the   denominator is $\sqrt{\hat{v}_{t,i}}$. However, in Algorithms 2 and 3, it is changed as $\sqrt{u_{t,i}}$. In  the proofs, the authors proved the convergence based on $u_{t,i}\geq \epsilon$. The proofs can be quite simple. This paper does not present any insights for the decentralized  adaptive methods. It only depends on $u_{t,i}\geq \epsilon$. The numerical results show that the proposed method is similar as DSGD.<BRK>This paper studied the decentralized adaptive gradient methods and provided convergence guarantees. Experiment on MNIST is conducted to show the effectiveness of the proposed approach. 1.The theoretical result is weak. The linear speedup result is not proved as in (Lian et al.2017), the benefits of adaptive gradient methods are also not illustrated in the bound in Theorem 2 and Theorem 3. As illustrated in Theorem 2 and 3, the learning rate $\alpha$ is set to be less than $\epsilon^{0.5}/16L$. It is unclear what is the bound if the LHS is the standard gradient squared norm as in (Lian et al.2017).It is important to use the same measure as in the previous literature for fair comparison. 3.The experiment is weak. Doing distributed training only on a tiny dataset on MNIST is not sufficient. I would like to see results on larger datasets such as CIFAR and ImageNet.<BRK>They also point out a potential divergent problem of an existing method and investigate the conditions to ensure convergence. Finally, they conduct some experiments to verify the performance of their algorithm. Pros:1.This paper is the first one to use adaptive gradient method to decentralized training paradigm and the authors also provide a completed convergence analysis for their algorithm. The authors also investigate the conditions to make sure these decentralized variants will converge. It seems not to be an obvious conclusion from the reference Chen et al.(2019).Is there more explanation or proof about why AdaGrad and AMSGrad satisfy the condition? And does it mean Adam still diverges even after using the algorithmic approach proposed in this paper? However, the convergence of common adaptive gradient methods such as AdaGrad and Adam is still not clear. Do most of the adaptive gradient methods have the same theoretical guarantees as AMSGrad? 3.\mathbb{E}[\sum_{t 1}^T \lVert ( \hat{V}_{t 2} + \hat{V}_{t 1}) \rVert_{abs}]   o(T) is the key condition to ensure the convergence. But when the above equation is O(\sqrt{T}), the convergence rate is worse than the centralized counterpart. 4.In section 3.4, the experiment is divided into homogeneous and heterogeneous data, which is very confusing. What is the reason for doing this and what will happen if we just deal with the dataset normally? 5.In the homogeneous data experiment, the performance of DADAM and decentralized AMSGrad are similar. What is the reason that the learning rates on different node tend to be similar?<BRK>They first present a counterexample where a simple decentralized scheme, applying Adam, converges to a nonstationary point. (Suggestion: I would maybe add a cleaner, more flushed out version of this proof in the appendix, maybe with illustrations.) The motivation and counterexample in DADAM case are solid, and the following theorems seem to suggest gradient error norm $\to 0$ at rate $O(1/\sqrt{T})$ which is reasonable in nonconvex optimization. The intuition in the adjusted merging scheme is also reasonable, and makes sense that it would work better than vanilla merging schemes. However, I did not have a chance to carefully check the proofs, which are clearly the main contribution of the paper. One thing I would suggest is a more thorough set of numerical experiments. The two examples shown, in fact all the methods converge, and while the proposed method converges faster, it isn t really verifying the paper s main point, which is that the standard distributed methods diverge and the proposed method converges. Showing this on a standard machine learning task would improve motivation.<BRK>The proposed method is novel and is among the first works to consider a decentralized communication graph without a master node. Finally, the author tests their method on a simple CNN and show their superiority compared to DADAM to achieve a close to the centralized performance. However, I have some minor comments to improve the manuscript. 1.The experimental evaluation of the work is quite limited. I understand the space limit but it would have been nice to see more experiments instead of showcasing Algorithm 2 with an extra example in Algorithm 3. It is important to see the convergence behavior of the method (on the training data) with respect to the DGD on various datasets/networks in practice, rather than observing how the testing accuracy behaves. 2.What are the drawbacks of this method? Do you have any quantified evaluation in this respect? I suspect it can be significant especially if the trained model is large and the agents have limited memory/computational resources3. This sentence is not accurate as algorithms like AdaGrad and Adadelta do not use momentum on the past gradients. For example, take AdaGrad with a fixed learning rate of \eta instead of m_t.
Reject. rating score: 3. rating score: 4. rating score: 4. rating score: 7. <BRK>And in Section 7 (Conclusion) the paper claims that it exposes this new risk. In the context that the technique has been introduced, it seems like the [malicious] actor would only be fooling him/her self rather than fooling the model/classifier. 4.Abstract: "Extensive experiments verify the theoretical results and the effectiveness of our proposed methods." 5.Section 6 Discussion: "What we can do at present is using a relatively more robust model as a surrogate of the true friend to improve the robustness of a weak model." The attacker might mislead the  relatively more robust model  as well. Why has that not been suggested?<BRK>This paper considers attacks that flip the label for those data points that are incorrectly classified. It proposes a risk metric to capture this and uses prior attacks for this type of attack. Positive points:The main idea in the paper is, at first look, interesting but then on more thinking I have doubts (see below):Negative points:  It is not very surprising that mis classified data points can be perturbed to be properly classified using exactly the same kind of attack as prior work (not sure why authors call the attack conceptually different). Why call it R_rev? I do not see how the tradeoff arises here. Also, other reviewers have similar issues and also more issues other than what I point out, which seem valid issues to me.<BRK>This paper suggests that we should also pay attention to robustness in misclassification. The authors envision a scenario where a "false friend" examines the output of a network, and attacks the mislabeled examples to make it look like they are labeled correctly. But I didn t find the attack scenario built on top of this observation and the proposed mitigations compelling.<BRK>SummaryThis paper presents a new kind of adversarial attacks, named hypocritical attack. It tricks a model into classifying data correctly with a perturbation. The authors review the adversarial attack and define the new hypocritical examples and risk. This paper suggests another consideration to the ML community. What if we apply the technique to make hypocritical examples in the training data? It would be better to describe the toy example since it is hard to understand the text s experiment in this paper.
Accept (Poster). rating score: 8. rating score: 7. rating score: 6. rating score: 4. <BRK>This paper shows that the class of two player markets have no satisfactory outcome in the usual sense. Players should neither escape to infinite losses nor converge to strict maxima or non critical point. Some concrete examples are analyzed with negative results. This paper is a reminder of researchers: we should carefully model the objective functions of multiple interacting intelligent agents and the interactions between them.<BRK>The authors introduce some problems for which any “reasonable“ method has an undesired behavior. The notion of “reasonable method” is quite general and makes the result of this paper interesting. ### pros:   The framework of “reasonable methods” is quite general. The paper answers an important question of the game optimization community. The paper is easy to read and well written. I do not know if it is true or false (it is likely to be true), but this claim is not obvious (since it is an infimum of continuous functions). Also, the statement in your theorems only considers the reasonable algorithms with the hyperparameters, ensuring that (R2) is valid. An easy way to get rid of this additional assumption may be to remove the mention of hyperparameter (in the sense that an algorithm with hyperparameters that make it locally converge to local maxima is not a reasonable algorithm). How can you reconcile that a method may depend on all the previous iterates and Assumption R1? ### Global convergence in Balduzzi et al.2018I am not sure what global convergence result you are mentioning. Also, in your related work section, you should differentiate Hsieh et al.2020 from the other related work. In the contribution section, $\mathcal A$ is not defined.<BRK>I still find the main message interesting but not critical for any real application. %%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%The paper provides an impossibility type of result for convergence of reasonable dynamics in general multi loss settings. Thus under the definition of a  reasonable  algorithm as an algorithm that avoids strict maxima, the dynamics cannot converge to equilibria in this case and has to therefore cycle. The paper builds on a series of recent papers that show that many reasonable dynamics fail to converge in reasonable multi loss settings (e.g.GD in zero sum games). On the positive side, the results accounts for a large class of optimization/gradient driven dynamics. The main of message of the paper is that multi loss settings are very different from single loss settings and we cannot quickly and easily apply tools from optimization and hope to succeed at least not in all cases.<BRK>1.For Theorem 1, as the reviewer understands it, for an optimization problem whose only critical point is a strict maxima, it only has four outcomes, which are listed in the theorem. The result seems quite intuitive and provides very limited understanding for the problem. Please list other possible outcomes for the general problem and state in such way that the paper finds some impossible outcomes which can be excluded for consideration. 2.Theorem 2 states that a two player game with a specific loss cannot converge to the only strict minimum. Even more unclear to the reviewer, such a result can provide useful value to the community or not. A game with only strict maximum cannot converge to the maximum is not quite interesting to the community (as the reviewer believes). Some minor comments:The paper use the term of “simultaneous” quite a lot in the first part of the paper. This is quite confusing for the readers as this term is not quite acknowledged.
Reject. rating score: 4. rating score: 5. rating score: 5. rating score: 5. <BRK>The author present a novel neural moudalar network  for video grounding tasks, which can provide interpretable intermediate reasoing outcomes and show the model robustness. (in this paper, just fuse the detected information to get the final answer by a response decoder). • The reasoning struture in this papar is simple. The parameter here refer the input of each module, which is different from model parameters.<BRK>The model explicitly resolves entity references (dialog understanding) and detects actions from videos (video understanding) for response generation. Table 6 denotes that the evaluation results decrease with longer video or larger context modeling which is the main focus of the paper. It might be argued that this approach would not generalize. It would help to explicitly mention the neural modules previously defined and the novel modules, eg how the “find” module differs from Kottur et al.2018.Have the authors experimented with the dialog based modules from Kottur et al.2018   eg.<BRK>This paper studies the language grounding aspect of video language problems. It proposes a Neural Module Network (NMN) for explicit reasoning of visually grounded object/action entities and their relationships. Overall, the paper is motivated clearly and is delivered with good clarity. The followings need to be clarified. Please clarify the performance gain and possible reasons.<BRK>Summary:The paper studies the application of neural module network to video grounded language tasks. They propose a method dubbed Visio Linguistic Neural Module Network (VilNMN) to retrieve spatio temporal information in a video through a linguistic based parsed program. (2) The use of the mathematical symbols in the paper is very confusing. Honestly, I am skeptical about the results on the TGIF QA datasets as the gap between VilNMN and the existing methods is very significant. I would be okay to raise the rating if the authors sufficiently address my concerns during the rebuttal phase.
Reject. rating score: 3. rating score: 4. rating score: 6. rating score: 7. <BRK>This paper studies online test for qualitative treatment tests. The authors propose a scalable online algorithm for Type 1 error control. I find the paper under developed that the writing has to be substantially improved, and the presentation, especially in Section 3.2, is not friendly. 1.The authors claim to have an "Online" algorithm. The challenge of online testing is that we are doing testing at each iteration, and it is difficult to adjust all p values. Can the authors give some real motivating applications?<BRK>I am a statistician but I am not an expert in sequential test. I understand the motivation behind the paper, but I would clearly have liked to be able to clearly understand the assumptions used. I would also have to work on a  "real" algorithm for which one is able to control "everything" (in particular, the strategy to choose the .I do not have the impression that this is completely the case here and some aspects need to be clarified. The meaning of Eqs. (6) and (7) are very difficult to grasp   How your results compare to multi armed bandit testing with online FDR control ?<BRK>This paper proposed a powerful online sequential test which can efficiently detect qualitative treatment effects (QTE). Theoretical guarantee on the Type I error is presented. Overall, the paper is well written, with a clear mathematical definition of the problem, solid theoretical result and experiment results. However, there are some questions that I did not fully understand. I think this paper will be more interesting to readers in the industry if a detailed case is presented to bridge the introduction and the content. 2.The author may argue that the experiment on Yahoo! Today Module is an example, but I did not find the point on this experiment. The set of article IDs is very huge so it is not very interesting to compare the effect on two article IDs only.<BRK>  Contributions  This paper proposes a new framework for A/B testing in the frame of randomized online experiments. Moreover, a bootstrap method is provided in order to determine the stopping boundary. This  circumvents the absence of any tractable analytical form for the limiting distribution of this new test statistic. Finally, the method is accompanied with experiments on the finite sample performance of the test procedure with simulated and real data from Yahoo!. Strong points  The paper is well written and all results are well introduced with interpretation in words which makes it easy to follow. My grade can be further strengthened if the authors can address the points which for me need to be clarified, the biggest one being the correction of Figure 3 to fully support the efficiency of the approach. Minor details: For readability, I would add in the title of Figure 1 that A is the treatment applied and Y the associated reward, since they have not been yet introduced at the time of the figure’s reference. Questions to help to clarify  How does it relate with Bandits tests that are also online? The author s addressed my concerns and I guess the ones of the other reviewers too.
Reject. rating score: 3. rating score: 4. rating score: 5. rating score: 5. <BRK>Unfortunately the authors link directly to the code, and the code is not anonymous. I assume this was a typo on the authors  part. The references are very poorly formatted.<BRK>Most importantly, I would have liked to see a comparison between EinStein VI code and what the code would have looked like without EinStein VI. ### Reasons for scoreUnfortunately, there is not enough to go on in this paper, which is why I recommend reject.<BRK>Weaknesses   The only claim in the paper that is well supported is that the authors have extended NumPyro with SVI. I m assuming that this is what the abstract was referencing. Space permitting you could make a note as to why these are there in the guide.<BRK>Followings are a few of my questions and comments:1. Also, the authors give the public code link of their implementation in the paper, which may expose their identity, but I am not sure if this violates anonymous requirement of ICLR submissions.
Reject. rating score: 4. rating score: 6. rating score: 6. rating score: 6. <BRK>The paper proposes the use of playbacks for UDA. The author uses the trained model and an offline 3D object tracker to generate high quality pseudo labels of the target domain. The paper can be understood in general and the writing is easy to follow. The results of the paper are practical. The authors have done experiments on 5 data sets to show the generalization capability of the method and compare the two decent baselines with the proposed method. However, in general, I think the novelty is still limited to the ICLR community.<BRK>The topic of adapting 3d object detectors to new domains is important. I really enjoyed reading the paper. Discussions or experiments on this point would be very helpful in understanding the application domain of the proposed method. The paper is clearly written and the method is well motivated. I am not sure whether a paper with extensive experiments and relatively small technical contribution should be considered as a good paper for ICLR. After reading other reviews and the rebuttal, I opt for acceptance.<BRK>This paper proposed an unsupervised domain adaptation method for 3D lidar based object detection. Thus, if the paper gets accepted, I strongly encourage the author to rewrite the introduction and properly reflect the core contributions. Pros:* The idea is simple and straightforward. The approach is technically sound. Cons:* I have some concerns regarding claimed contributions/novelty. * The online tracker is not on par with the current state of the art. Post rebuttal comments  I carefully read the rebuttal and other reviewer comments.<BRK>Pros:  The proposed method is simple yet effective and has wide uses in real world applications  Solid experiments across 5 benchmarks. This method does not rely on the source domain data and learned trackers. The proposed model will fail to handle this situation. The generation of the pseudo labels depends heavily on the confidence scores obtained from the object detector. How is the threshold of 0.80 chosen? Would other thresholds be more effective? However, the application of 3D detection and the extensive experiments are great and may benefit further research significantly.
Reject. rating score: 4. rating score: 4. rating score: 7. rating score: 7. <BRK>This paper proposes the use of Deep Learning, namely a multi layer perceptron, for approximating the Whittle index in restless bandits. The problem and the background on restless bandits are clearly exposed. The statement of Corollary 1 is not correct. In the general case, the Whittle index is not optimal, it is a heuristic. As the authors suggest the statement of Corollary 1 should be rewritten as a necessary condition for a neural network to be Whittle accurate. The authors write that the algorithmic complexity of the algorithms proposed in recovering bandits is (\binom(N,M))^d. However in the cited paper the authors propose the use of optimistic planning with a given budget B. So the algorithmic complexity of their algorithms is B, which is a parameter. I am still not convinced by their experiments on recovering bandits.<BRK>This paper considers the problem of learning how to control restless bandits. The main contribution of the paper is to propose an algorithm, NeurWIN, that uses a neural network architecture to learn the Whittle indices. As acknowledged by the authors, there are a number of recent work on learning Whittle index (last paragraph of Section 2): I do not understand why these solutions are not implemented. The paper is a bit sloppy and not very precise. For instance:  The authors write several times "Finding Whittle index is typically intractable" without proof or precise reference. I doubt that this statement is true. page 4: is it really the problem statement? Moreover, discretizing at the kB level would lead to about 10^3 states in which case Whittle index would be applicable. I thought that NeurWIN was learning those indices.<BRK>Maybe this was because I was not familiar with the Whittle index  and such kind of policies for restless bandits. But the paper is very didactic, I liked to discover these concepts. The approach is elegant and looks useful from the results. Second, it would have be very useful to consider more baselines in the experiments. At least, it would have been important to consider a policy trained with Reinforce on individual states, and then select the M best outputs at each round,  as it is done for the proposed approach ! At last, authors give conditions for the applicability of their method. What kind of restless bandit settings are excluded due to these two necessary conditions ?<BRK>The paper then uses this observation to form exactly such an environment and train a neural network using REINFORCE to maximize discounted reward for the environment, thus in principle achieving a good approximation of the Whittle index of the arm. The network can then be used to approximate a Whittle index based policy for restless bandit problems. In particular in the deadline scheduling problem where the optimal strategy is known, it can be observed that this policy can match the performance of the optimal policy after several hundred training epochs. I recommend the paper for acceptance. I would encourage the authors to provide confidence bars for their algorithm as the results are presented over 10 independent runs only. It is vital to see the variance in rewards obtained by the strategy.
Accept (Poster). rating score: 9. rating score: 6. rating score: 6. rating score: 5. <BRK>### Recommendation**Accept**The idea is good and experiments are good. There are some concerns about the clarity of the paper but those can be worked on. The paper introduces a novel architecture and demonstrates improved output variation and more controllability, which is an important current issue for TTS research. "progressively adding steps of flow on the last step of flow has learned to attend to text" on >once? "Flowtron succeeds in transferring not only the somber timbre, the low F0 and the long pausesassociated with the narrative style"  > "not only the somber timbre, but also"#### Appendix1. It would be good to smooth out the consistency. If the latter, I would like to see more discussion on that. "We provide results on speech variation" etc.<BRK>The authors present Flowttron, an autoregressive text to speech network that isa merging of the well established Tacotron architecture along with flow basedgenerative models. Moreover, the model is able toproduce various styles of speech depending on how the latent variables aresampled, as well style transfer between seen and unseen speakers. Thedescription of the flow networks is sufficient. 3/4.I would recommend an accept, given the authors can address the major technicalproblems with the paper. The work is interesting, the experiments are sufficient,and the audio samples are convincing.<BRK>This paper describes a specific approach to the hot topic of  auto regressive Flow based speech synthesis. The fundamentals of coupled flow layers are exposed pretty well, but I quickly got lost in the details of the work. I have a number of specific comments to address that. re: 3.5.1 VISUALIZING ASSIGNMENTS and 3.5.2 TRANSLATING DIMENSIONS, same comment as before, where are the results provided? Eq.(7): I can guess, but can you confirm the meaning of the circle with dot operator? Section 3.2: The inference method has not yet been explained. Do the MOS scores improve? Section 3.3.2, "Our different speaker interpolation samples show that Flowtron is able to graduallyand smoothly morph one voice into another." Where is this measured/reported in the paper?<BRK>This paper presents a text to speech synthesis system, called Flowtron which uses a normalizing flow to generate a sequence of mel spectrogram frames. The difference between the proposed Flowtron and the previously prosed flow based methods is, the authors argue as the main contributions, its ability to produce more diverse and expressive speech samples of specific speech attributes by sampling from the latent space. Evaluation is done using the two public datasets, and a number of experiments are performed to show that the proposed method not only achieves a good MOS score in general but also generates speech samples with variation, including the style transfer. The proposed method yields a comparable MOS score to a reference (Tacotron 2) in terms of speech quality. It would be better to embed the samples on the website for easier navigation.
Reject. rating score: 3. rating score: 4. rating score: 4. rating score: 6. <BRK>Is this also the case for a very large number of rollouts? __Strong points / weak points__+ (+) The paper is well written and very easy to follow+ (+) Using MPC on a reward function that was learned by IRL can be reasonable  ( ) Lack of contribution / novelty  ( ) Using the GAIL reward doesn t seem sound. *Using GAIL for IRL:* The paper repeatably refers to GAIL as an IRL approach. However, GAIL does not infer a reward function in accordance with the problem formulation of IRL.<BRK>The authors propose a method to enhance imitation learning by using MPC on the reward function learned by an imitation learning approach. The chosen MPC approach uses the learned policy to generate candidates for a search process that maximizes the reward. To learn the reward function, the authors propose to use GAIL. The writing is generally clear and easy to understand. While this makes the paper relevant, the use of a single existing MPC approach with a learned reward function is a relatively minor contribution. This explains why the agent is able to achieve higher scores on benchmark tasks; however, the discriminator by itself is not generally useable as a reward function like the authors suggest.<BRK>IMITATION LEARNING REVIEW The gist of the paper is to improve IRL with the so called IMPLANT algorithm which is supposed to improve control policies in learning controllers. This paper presents a framework for mitigating the learned dynamics during imitation learning using an augmented cost fuction at test time. * How was \pi in line 11 computed? * Is there an upper bound on the length of the horizon H? * I am concerned that the reward function in equation (4) is just a restatement of the optimality principle and do not see why this is being rebranded as a new algorithm.<BRK>#######################################################################Reasons for score:  Overall, I vote for a weak acceptance. I like the idea of using test time planning for tackling the test time perturbation in imitation learning. My major concern is about the clarity of the paper and some additional environments (see cons below). Specifically, IMPLANT builds on top of GAIL, and learns an additional value estimation during training, and utilizes the estimated reward function, value function for closed loop planning based on model predictive control (MPC). The design is interesting. For the motivation, it would be better to provide more details about it, which seems not very clear to me.
Reject. rating score: 3. rating score: 4. rating score: 5. rating score: 6. rating score: 7. <BRK>Overall, I enjoyed this paper and thought it was quite clear, but it does not feel substantial enough for a conference publication. The main contribution is the detail of how to implement stochasticity in a MuZero architecture. My main concern with the paper can be summarized as: What is this work s impact? What is the key premise that makes it a reasonable line of work? A minor issue also concerns the presentation.<BRK>One major weakness of the paper is there is its lack of novelty/contribution. The core idea of interleaving chance nodes with choice nodes to model stochastic environment in tree search is not brand new. The main contribution of this paper is the integration of such idea into the specific MuZero tree search framework. Although the Atari environment is known to be a deterministic, it becomes stochastic when there is only a limited horizon of past observed frames. I’m wondering whether there should be other more comprehensive ways to do this?<BRK>Compared to MuZero NDMZ also learns a function that determines who is to act (player 1,2 or chance) and a distribution of chance outcomes. * The model is clearly described. * While Nannon 12 5 6 is still a relatively small game there is a significant gap in win rate of NDMZ against optimal player. Is this a good result? Possible improvements* Discussion of results is very brief. How does performance scale with more MCTS iterations?" While scaling up might be nontrivial it would significantly improve the message of the paper.<BRK>Summary:This paper presents NDMZ, an extension of the MuZero algorithm to handle games with chance moves. pages 3 4: I find the addition of player "d" and the no op action unnecessary, and just making the formalization clumsy. I do not see they extending the expressiveness of the formalization in any way, and just add unnecessary complexity. On the other hand, I felt that some of the new formalization is a bit clumsy.<BRK>The new algorithm borrows the idea from non deterministic MCTS and the theory of extensive form games. Comments:This paper is generally well written and clear. The empirical results demonstrate that NDMZ can achieve a similar performance as AlphaGoZero, which is very interesting provided that MuZero does not get access to a perfect game simulator. The reviewer is not familiar with AlphaGoZero s performance in these games and is wondering whether this is expected.
Accept (Poster). rating score: 8. rating score: 8. rating score: 5. rating score: 4. <BRK>This paper introduces the generative modeling of the task of vision and language navigation. To do so, token wise prediction entropy (1 TENT) is introduced. Thus by visualizing the 1 TENT for each timestep we can analyze how and when a model fails or what kind of capabilities are missing in the model. al.and this work. Please move one of them to the appendix   which will give more space to panorama images. Q1: Section1: The assumption that the action probabilities are uniform given the state makes sense. It would be great to see the generalization of the generative approach to outdoor scenes.<BRK>It would help to have more details in the paper rather than relegated to the appendix, maybe at the expense of the long TENT analysis? That the combination of discriminative and generative policies gives best results is a major point in the paper, but the combined policy description is relegated to the appendix which feels weird. Nits:  typo Introduction Chang et al.is \citet but should be \cite  Possible typo after Eq (3), "penalizes all the actions" should this be "penalizes all actions except a_t", or is a_t penalized as well (e.g., global optimum is zero)?<BRK>The paper focuses on learning a navigation policy for a vision and language navigation problem. It is better to use a more concrete terms. works well than Gen. or Disc. is the main statement and main contribution of this paper. However, there is very little discussion on why this is true methodologically (not empirically). The authors say "We use the network architecture of the speaker from (Fried et al., 2018) to implement generative policies which include a language model p(X|at, ht)." It is better to at least describe the model in appendix.<BRK>**Paper Summary**The paper addresses the problem of vision and language navigation (Anderson et al., 2018). **Paper Strengths**  The paper shows that the proposed generative model outperforms the discriminative formulation of the approach. The results have been compared with a number of strong baselines. I do not consider this as a large change compared to the previous work. It would be good to clarify. Quantitative analysis over the entire dataset should be provided. Section 5.3 is about a figure that appears in the appendix.
Reject. rating score: 4. rating score: 6. rating score: 6. rating score: 6. <BRK>The proposed pooling approach itself is rather straight forward, but intuitively makes sense. There is no implementation or demonstration of this approach, even on toy examples, let alone benchmarks on graph datasets. There is also no discussion on the impact of the proposed pooling on the training of the resulting RNP GNN. It is not clear to me that this approach is feasible to apply in practice, beyond a hypothetical thought experiment. There may be more that I missed, but the paper and supplement would benefit from some additional proofreading.<BRK>As it is, this is a borderline paper (leaning on the positive side). The Authors also suggest that the theoretical framework can be translated to a running model with a certain ease and that, in fact, its practical implementation and empircal assessment is on the way. I am not convinced that this paper can have a strong impact without an empirical validation.<BRK>Summary: The goal of the paper is to show that GNN s (without exponential computational complexity) can be constructed with the ability to count subgraphs. Pros:1.Theoretical contributions which provide a strategy to learn expressive representations with the ability to count subgraphs and distinguish graphs based on the same.<BRK>Although subgraph counting is an interesting problem from a theoretical viewpoint, the weakness of this paper is that it is not answered how relevant it is to real world tasks and how successful it is.
Reject. rating score: 5. rating score: 6. rating score: 6. rating score: 7. <BRK>This effect is measured by the authors by trying out the existing Siamese few shot detector on 4 datasets: PASCAL, COCO, Objects365, and LVIS showing that the gap in performance on the seen training and the unseen (novel) testing categories is reduced when the base dataset has more classes (e.g.on LVIS where there are more than 1K classes, this "generalization" gap is shown to be minimal). The authors also quantify empirically the effect of increasing the model size and of prolonging the training schedule on this gap. There is nothing wrong in not proposing a new algorithm and instead   uncovering an important fact that was so far overlooked, but as I noted above this is not the case, the paper highlights a well known fact....<BRK>This paper provides a variety of studies to understand the generalization gap between known and novel classes in one shot object detection. The most notable observation was that it was more important to increase the number of object category than to increase the number of instances per each category in order to reduce the generalization gap. Figure 5 is very important and well presented to support the main claim. 3.Can this claim be applied to any kind of category? (Figure 5A) Did the method suffer from training as the number of categories increased?<BRK>The paper is clearly presented and well organized. However, increasing the number of classes also increasing the number of training samples in your experiments, and using more training data could have higher performance. About the shot numberThe authors focus on only the one shot setting, but the proposed hypothesis could be kept for the N shot setting where N > 2. 2.About the usage of Siamese Faster R CNNThe siamese faster R CNN is a suitable network to validate the hypothesis, but it is somewhat old even with a more powerful backbone network.<BRK>Strengths:  The paper is well written and it’s easy to follow the story. It provides a comprehensive review on related papers on object detection especially one shot detection and their limitations. Although the paper focuses on the data used for training, it gave great insights for understanding the generalization of object detector and provides practical guidelines for future large scale data collection. However, it’s not the case that the more categories the better, there should also be enough diversity in data distribution and granularity in label definition.
Reject. rating score: 4. rating score: 5. rating score: 5. rating score: 5. <BRK>Authors present an approach to partition the state space by generating a diverse set of goals, and to explore the state space effectively by finding policies that can reach to these goals effectively. Surely, goal oriented RL performs reward maximization in some sense, right? It is not justified, and there is no theoretical evidence to demonstrate that this indeed gives us the diversity that you tout it would.<BRK>**Summary:**This paper presents a new algorithm called Regioned Episodic Reinforcement Learning (RERL), which combines ideas from episodic memory, with automatic sub goal creation or “goal oriented” RL. I think the paper could be greatly improved by a little more attention the presentation of the experiments and their analysis in the main text. I agree with the other reviewers about the experiments: they feel rushed, and the design and presentation is not as careful as it could be. The method works by dividing the state space into regions, where a different goal identifies each region. **Strengths:**The paper combines two RL techniques in a novel way to address important issues in deep rl. I think this paper presents a novel and interesting idea that is sufficiently supported by the empirical experiments. **After Author Response and Discussion:** Thanks to the authors for their responses.<BRK>### SummaryThis work proposes a curriculum learning in RL that combines goal oriented RL and episodic RL in order to achieve a sample efficient exploration exploitation trade off. Clarity: The paper could benefit from a clear presentation in order to help properly understand the proposed methods and all its components. Exploitation is leveraged through a region based memory. ### Concerns  \mathcal{X} is sometimes used as a set sometimes as a distribution. The mapping \phi is quite central to you method, but I failed to understand which type of mapping was used in your experiments. ": Does this mean, the anti goal is always in the same region as the visited state ? the one from eq.2 (i.e.using the region based goal and anti goal sampling ) ? This seem to deserve more attention to better evaluate the algorithm. There is a problem with some figures and appendices references.<BRK>The paper proposes a reinforcement learning scheme intended to strike a productive balance between exploration and exploitation by way of decomposing the state space into regions and developing policies for "solving" each region. Although the paper claims to include "extensive experiments", the design and analysis of the method was rather coarse. But, based on the presentation up to this point, I was expecting to see a comparison on an environment that is good for goal oriented methods and one that is good for episodic algorithms. My main frustration was that a lot of the paper was spent presenting the algorithm and then the experiments were presented only very briefly. For an algorithm like this, its utility is measured in its ability to work according to its design. If not, then seeing some assurance that the reason that it is working well is because the problem structure somehow matches the algorithm structure would be very valuable. "tasks to guarantee stability"  > "tasks guarantees stability"? "etc.": Redundant, given "such as"? "suffer from generating appropriate goals"  > "suffer from the difficulty of generating appropriate goals"? Not quite. So, any set of states is a subset of the state space...?
Reject. rating score: 4. rating score: 4. rating score: 4. rating score: 5. <BRK>This paper considers federated learning with straggling and adversarial devices. In experiments, the stragglers are simulated in a simplistic manner, and the gains over FedAsync are not quantified. Post Rebuttal Comments  Thanks to the authors for their response, and for updating the manuscript. However, updated Theorem 1 seems to raise more questions. The paragraph after Theorem 1 does not mention how entropy and loss based filtering methods can achieve small \Omega_{max}, but only says that "if the entropy based filtering method successfully filters out the model poisoned devices, and the loss weights \beta_i(k) of the adversaries are significantly small for data poisoning and backdoor attacks... ... then we have a small error term". Further, even when there are no adversaries, what is the theoretical improvement over FedAsync? Specifically, what is the impact of staleness on convergence. It is important to elaborate on how much the complexity at the server will increase by using entropy based filtering on public data.<BRK>Review: This paper proposes Sself to achieve robustness against adversary when there are straggler. Then the models are aggregated based on their weights and staleness. The`experiments presented in the paper look good. It is not very convincing to me that the filtering threshold is easy to select. What are the numbers of adversaries can be tolerated by this method? Minor comments:  The curves are a bit bumpy. "Robust aggregation for federated learning."<BRK>Summary:The paper studies the problem of asynchronous training  with robustness to adversaries in federated learning. The authors propose an idea based on entropy based filtering to simultaneously filter out adversaries, and a weighted averaging technique to handle staleness in the gradients. > The idea of using a public data set, and metrics from that data set to aide filtering out adversaries is interesting, and possibly worthy of pursuit. > I find Theorem 1 s result a bit confusing. As per the paper, they seem to be allowing an arbitrary number of stragglers (and arbitrary degree of straggling). However, this is not sufficiently captured in the statement of Theorem 2.<BRK>**Paper summary**The paper claims to propose the first algorithm that can handle adversarial machines and stragglers simultaneously in the federated learning setting. To handle stragglers, the paper takes a semi synchronous approach by taking a weighted sum of gradients depending on staleness. Note that to handle the adversaries, the algorithm needs a public dataset at the server, using which it can evaluate the entropy and loss scores of each gradient. The problem of handling stragglers and adversarial machines (including data poisoning adversaries) seems a very relevant problem. 2.The handling of stragglers seems to be theoretically backed (although I have some concerns about the handling of adversarial machines) and Theorem 1 shows that the convergence up to some error is fast. **Score justification**As mentioned in the Concerns section, I am not sure if the theoretical guarantees for the algorithm are correct.
Reject. rating score: 4. rating score: 4. rating score: 6. rating score: 6. <BRK>Specifically, firstly, the paper proposes to utilize an n gram splitter to segment the text into several segments. Using these tasks to perform fine tuning on top of a pre trained masked language modeling shows improvements than directly fine tuning on the pre trained language models. By conducting experiments on the GLUE benchmark with the BERT base and BERT large models, the results are improved. The general idea is simple and easy to understand. Combining span level information has been widely studied in the literature[1] (not cited). 3.As the paper focuses on training span representations, it is necessary to perform experiments on entity based tasks.<BRK>Previous works reveal that span level information can enhance the performance of PrLMs if they are used in pre training. To this end, the paper proposes a method that combines span level information into the representations generated by PrLMs during the fine tuning phase. To combine span level information, the paper first breaks a sentence into various span components. Then, an accumulated representation with enhanced span level information is built based on the sub token level representation provided by PrLMs. The experimental results on the GLUE benchmark show that the proposed method improves the performance of PrLMs. The paper presented a way that combines span level information into the representations generated by PrLMs during the fine tuning phase.<BRK>Summary: The paper introduced a fine tuning approach which adapts the subword level, span level and sentence level to the target tasks. Empirical studies on GLUE benchmark show that the proposed approach consistently improves the performance of BERT. Pros: 1.The idea of leveraging a n gram model for pre trained language model fine tuning is interesting. Cons:My concern is the significancy of the results. Questions:If the model is pre trained with span level information (e.g., spanBERT), will the proposed method outperform normal fine tuning?<BRK>This paper presents an approach to incorporate span information in pre trained language models like BERT during fine tuning. The fine grained representation in a same span within the segmentation is aggregated to a span level representation using a CNN model. The experiments show that the proposed model can achieve similar performance gain as other span based language models which includes span information during pre training. The paper is well written and the proposed method is novel. However, the proposed model introduces complexity in the downstream tasks and may affect training time in the downstream task. The authors claim that the proposed model perform similar to models like SpanBERT but it is not clear if both models are improving on similar aspects. Some fine grained analysis of results with these models might be insightful.
Reject. rating score: 4. rating score: 5. rating score: 6. rating score: 6. <BRK>This paper proposes an advanced masking strategy for CutMix augmentation based on the low pass filter. If the VAE is not optimized well, the analysis will not be convincing enough. **Pros**\+ The mutual information analysis provides us a new perspective to understand different data augmentations. It is good evidence that FMix can sometimes offer benefit to real world applications, but I think more evidence that FMix can really solve problems of previous MSDA in a certain scenario, e.g., the edge bias as pointed by the authors. This paper proposes a CutMix variant where the mask is sampled by a low pass filter.<BRK>In this work, authors provide an analysis of mutual information for MSDA and the develop a new variant of mixup. 2.They develop a new augmentation method and improve the performance of masking MSDA. The proposed measurement is not helpful for designing new methods. Note that the mutual information in mixup is lower than baseline while mixup still outperforms baseline. 3.The experiments on ImageNet is unconvincing. It is closely related to fmix but equipped with a learnable strategy to obtain patches for mixing.<BRK>The paper presents an interesting analysis of CutMix and MixUp data augmentation techniques. The idea to use fourier noise to construct masks for a variant of CutMix is interesting and well motivated. The finding that MixUp provides more compressed representations does not necessarily mean that masking augmentation methods are better than interpolation ones. This seems to contradict the previous adversarial analysis in which MixUp was found to not yield significantly more robustness. I maintain my original score.<BRK>This paper introduces a new mixup method that builds masks by first sampling a grey scale mask from fourier space, which is subsequently transformed into a binary mask. In general, using VAEs for MI estimators depends heavily on the quality of the generator, so these neural estimators might be better suited. The procedure seems fine, but why not compare to the way masks are sampled in context encoders [1]. This seems like an important baseline masking method to compare to.
Reject. rating score: 5. rating score: 5. rating score: 6. <BRK>This paper tries to generate synthetic unobserved spectral imagery from a set of existing spectral channels. For this specific satellite band to band translation task, authors adopt the VAE GAN framework (adding a skip connection between the input and generator), with a new added spectral reconstruction loss. Pros:1.As stated by the authors, this paper introduces a shared spectral reconstruction loss to a VAE GAN architecture for synthetic band generation. Cons:I would say this paper has limited contribution, in terms of the novelty of the method. For me, this paper is an application paper.<BRK>  Summary  This paper proposes a new method for image to image translation on multi spectral imagery. It’s not very clear how authors address the problem of different band resolution. Pros   Obtaining additional data for satellite imagery is notoriously difficult. Therefore, the approach, presented in this paper, is very interesting and important application. Cons   The authors claim three main contributions in the paper: “1) introduce a shared spectral reconstruction loss to a VAE GAN architecture for synthetic band generation; 2) test our methodology on real world scenarios; 3) present and release a test dataset of four hemispheric snapshots from three publicly available geostationary satellites for future research.”1) The novelty of the method seems to be relatively limited and domain specific. The paper introduces an original contribution, which was not published before (to my knowledge). If yes, what other types of satellites/sensors can it be applied to?<BRK>This paper applied the unsupervised VAE GAN model for satellite to satellite translation to generate synthetic spectral bands. Of course, it will bring new solutions for the subsequent application of satellite images. In the experiments, the comparison methods are missing. There are so many notations in the proposed method introduction,  however, the meaning of notations to the satellite images are not clear. I think most notations are from ref[23], but the most contribution of this paper is the application to satellite images. It may be more meaningful to introduce the notations from the satellite images perspective. The experimental dataset is complex, but the introduction is insufficient. I suggest to enhance the introduction of the problem and formulate the mathematical model from satellite to satellite translation clearly.
Reject. rating score: 4. rating score: 7. rating score: 7. <BRK>The goal of this work is to enable existing pre trained transformers (e.g.GPT 2) to operate over long input contexts. Thus the summary vector introduces recurrence where each segment can use information from the previous segment. My main concern with the paper is that unfortunately the evaluation is only limited to perplexity numbers on a couple of datasets. While this is a useful metric for evaluation, this alone is inadequate to demonstrate the quality of the model as a text generating system or as a language model that will be fine tuned for target tasks or to understand how much impact the model will have in these applications. For the model to be considered as a text generation system, there needs to be some human evaluation of the generated outputs. There are a small number of examples in the paper but that is not enough for a quantitative assessment. How will model fare when used in a target task defined over long input contexts? The related work section includes some papers that evaluate on such tasks. The executed experiments show that this idea is likely to work but the gaps in experimentation leave much room for speculation about the potential impact of this approach in end applications.<BRK>Summary:The paper proposed to add a recurrent component to pretrained transformers. The component pools the hidden states of a context window and passes it to the next context window as an additional input to the self attention layer. The component reduces the memory usage at both training and inference time, and enables the Transformer model to work on a longer sequence. The component is evaluated on two language modeling datasets and outperforms baseline models. The paper proposed a nice and simple way to make use of the existing pretrained Transformers with reduced memory usage and extended sequence length. This should benefit practitioners who want to apply these language models on a more diverse set of downstream tasks where the sequence length doesn’t fit the one from the original pretrained model. The results presented in the paper are significant. I believe the difference is more than relative embeddings.<BRK>The paper proposes recurrent connections between two adjacent Transformers, which transfers the previous context to the next step. This is a practically useful technique, improving the performance (perplexity in the experiments), and worth publishing. The authors argue that topical information or so between adjacent windows is propagated. The authors said more complex recurrence modules do not make any significant difference. For example, the authors fixed l_ins to be 2, without an explanation. The Transformer model is also fine tuned with the recurrent connection. So, I was wondering if the fine tuning improves the Transformer model too. In Eq.(1), is there 1/T?
Accept (Poster). rating score: 7. rating score: 7. rating score: 7. rating score: 5. <BRK>(minor) The fact that number of modules (N) and number of active modules allowed at a time (k) are fixed: I guess this can be viewed as a weakness, thought this is shared in RIM as well, and not that relevant to what is being evaluated/investigated in the paper. It might also help elaborate on what is meant by “knowledge”. **Questions to authors**: 1. I also think that the tasks picked for evaluation are suitable to investigate systemic generalization.<BRK>Summary:The authors present an approach that leverages inductive biases for knowledge decomposition and relational reasoning with the aim of generalizing over out of distibution tasks in RL with stationary rewards. Finally, the paper is clearly written and well structured. I think the work could be more ambitious but I believe that the authors have illustrated that this is a potentially useful innovation for RL. The tasks evaluated on are GridWorld and the BabyAI.<BRK>How do the different attention mechanisms interact? Comparing their approach with an LSTM wrapped in a similar "meta learning" layer, they show that their approach outperforms the LSTM, and is generally able to quickly adapt to changes in the environment. The paper overstates the novelty of their approach.<BRK>This paper proposes a meta learning (called) method for the model based on RIMs architecture to realize fast adaptation on new unseen tasks. The base model is the same as RIM, which is composed of multiple independent LSTM modules. An input attention is used to determine which independent modules are activated, taking output state from them. It is better be improved via revision. Does the paper have any results to prove that the proposed method is better than using different learning rates? But is the frequency the same as the fast update or slow update in the method.
Reject. rating score: 4. rating score: 6. rating score: 6. <BRK>Inspired by gradient based NAS of single path formulation, the authors propose a super bit model, a single path method, to decide the optimal number of quantization bits and pruning of a group of filters. BOPs look quite similar for many configurations (including the second row of a fixed number of quantization bits). The gain on BOPs of ABS seems to be marginal. In Figure 2, if BOPs can be replaced with memory footprints, it would be useful to estimate the compression capability of the proposed method.<BRK>The paper is well written (although it has some minor flaws), but in general easy to follow. Both channel pruning and residual quantisation are existing ideas, this might reduce the novelty aspect of this paper. I stumbled with the number representation until finishing reading section 3.1. 3.In Page 6, you mentioned you train the binary gates for weights and activations alternatively and this provides better performance. I am a little confused here.<BRK>Different from other approaches, the proposed method integrates multiple bit configurations (including pruning) into a single architecture, which is named “Super bit”. The architecture uses binary gates to automatically select bit resolution. In addition, the super bit model is differentiable and jointly trainable with parameters.
Reject. rating score: 4. rating score: 4. rating score: 5. rating score: 6. <BRK>This paper proposes to understand the behavior of deep networks for classification tasks by studying the dimensionality of the "class manifolds", i.e., regions in the data space that are mapped to the same one hot output. To measure such dimensionality, the paper proposes a method that is based on intersecting the class manifold with a random affine subspace of varying dimension. The idea is that when there is a intersection then the dimension of the random affine subspace is roughly the codimension of the class manifold. However, this same line of argument does not seem hold for the class manifolds learned by neural networks: if I consider a random input to a network then because the decision boundary is piecewise linear, it is with high probability that you can go towards all directions and maintain the class label. But the ability of deep networks for performing complicated nonlinear mapping, which is the key to its great success, likely makes such low dimensional manifolds to be highly nonlinear. It is said in the paper that it is generated at random, so perhaps that means a i.i.d.Gaussian vector, but presumably the variance of the Gaussian distribution could have an impact on the result as it captures how far the affine subspace is from the origin.<BRK>This paper proposes an empirical method for estimating the dimensionality of a class manifold, defined here as a collection of points for which the last (softmax) layer of a neural network maps them to a membership probability vector associated with a specific class. The authors note that if the sum of the dimensions of the class manifold and the cutting plane exceeds the full spatial dimension, the chance of an intersection of the two is high. Pros: 1) This is an interesting approach to the problem of dimensional modeling. The descriptions are clear and accessible. Cons: 1) As acknowledged by the authors in the caption of Fig 2, the dimensional estimates seem much higher than the typical estimates of intrinsic dimensionality as determined by local estimators (e.g.LID / Levina & Bickel, etc). 2) Following from 1), some of the conclusions reached from the experimental analysis are not fully convincing.<BRK>The authors propose a cutting plane method, inspired by intersection theory from algebraic geometry, to analyse the properties of neural networks. In general, there are no major issues with the paper. Based on these considerations, I recommend for the acceptance of the paper with an initial score of 6. Can you consider to remove it, as the purpose is not clear and it does not seem to introduce any additional information? Consequently, the estimated dimension of the class manifold is correct. This would result in considering a higher dimensional object (in this case a plane) to guarantee the intersection with the parabola. Consequently, we would underestimate the dimension of the class manifold. Therefore, I agree with the authors that the whole analysis is exact when considering hyperplanes. But what are its limitations when moving to the nonlinear regime? If so, do the same findings hold?<BRK>This work aims to study the characteristics of the class manifolds provided by a multiclass classifier. In particular, the main goal is to determine the effective dimensionality of these manifolds for the case of neural networks that outputs normalized probabilities. The theoretical foundation behind the paper seems to be sound, however, as a disclaimer, this research area is not close to the main expertise of this reviewer. As a recommendation, it will be great to test the method using an artificial case with known ground truth about the effective dimensionality of the class subspaces, so it will be possible to directly validate the findings of the cutting plane method. The current analysis focuses on the case that the cutting plane dimension leads to 50% of the target class (d_50%), which is not the only choice, specially considering that a highly relevant goal is to quantify the effect of manifold dimensionality on generalization. In summary, this is an interesting area of research to shed lights on the process used by learning models to transform from input space to class probability space. This work will be of interest to ICLR and I recommend to be accepted as a poster contribution.
Accept (Oral). rating score: 8. rating score: 7. rating score: 7. rating score: 7. <BRK>The main contribution of this paper is an Omega(d^2) vs O(1) separation. In particular, separation between fc nets and convnets trained by Adam, which is a corollary of the hardness result in this paper, is not implied by previous results. The lower bound is proved by using Benedek Itai’s approach and carefully bounding certain covering numbers. This paper is generally well written and provides lots of intuition on why the hardness results hold.<BRK>I think this paper attacks the important problem of sample efficiency of ConvNets. The notion of equivariance between algorithms, including orthogonal and permutation equivariance, is clean and inspiring: it can help us understand and improve various algorithms and architectures, and may also be extended to other settings. Now I think SGD on this new architecture becomes orthogonal equivariant, even if we don t train the first fully connected layer.<BRK>The paper presents an interesting analysis of MLP and convnets, where they show a gap between the number of required training examples to generalize well. This approach, which relies on an older result, provides an intuition as to the success of resnet.<BRK>The paper studies simple distributional settings in which convolutional neural networks give a provable sample complexity advantage over fully connected networks. Did the authors explore a range of architectures and optimization hyperparameters for the fully connected networks in Figure 1?
Reject. rating score: 5. rating score: 6. rating score: 6. <BRK>This paper aims at improving accuracy of multi horizon univariate time series forecasting. The authors propose an encoder decoder attention based architecture for multi horizon quantile forecasting. + The idea of explicitly using forecast error feedback is interesting. Both manuscripts of Fine and Foster (2020 al b) are not published and I could not find them online. The summary of Fine and Foster (2020 a; b) in Section 2.3 is not enough for me to judge the relevance of the results in Fig 2 and 3. #### DecisionI would tend to reject this submission. The Horizon Specific encoding is interesting but does not allow to forecast at inference a horizon that has not been trained on (as parametric method such as DeepAR have). #### Additional feedback  Figure 4 in appendix D should be mentioned in the main text as it is helpful for the reader. Section 3.3: what do you call a *bidirectional* 1 D convolution? (As opposed to unidirectional?) Eq (2): Consider providing the factorised form which is easier to parse. This hinder one of the main selling point of the paper that their model reduces the volatility of the forecast.<BRK>This needs more explanations what are the major difficulties to scale up TFT for the problem. After reading other reviews, I agree that the algorithmic novelty is limited, but the model is well adapted for multi horizon forecasting problem. The results can not be easily reproduced. The experiments on the large scale demand forecasting dataset and other publicly available datasets show that the proposed model outperforms or is comparable with the CNN, RNN based models as well as the transformer based model. The baseline (MQCNN) has no reference. Cons:I found this paper lacks clarity. .I don t think this is true. The purpose of the horizon specific attention in this paper is to merge the encodings of multiple horizon. Since the proposed model architecture is similar to MQRNN [Wen et al.2017], it should be the baseline. 2.The proposed model performs better than MQRNN for public datasets. Adding MQRNN as another baseline and the result without horizon specific attention for the large scale demand forecasting would be helpful (in Table 3 and Figure 2). In my understanding, the major differences are horizon specific attention in the proposed model and a different design of decoder in TFT. I assume TFT cannot be easily applied for large scale demand forecasting.<BRK>(Note: I am not well versed in the forecast modeling literature but I am reasonably so in the use of Transformer models in NLP tasks)The paper proposes MQTransformer, an improvement on MQRNN (Wen 2017) for multi horizon forecast prediction that leverages the Transformer architecture. Their contributions are:1) using learnable positionable embedding from event indicators2) using an attention head for each of the k horizons that need to be forecasted3) applying decoder self attention so that the model can use one horizon prediction to improve a later oneThe technical / modeling contributions aren t very novel or profound, so the ICLR community at large may not find them that inspiring. I marginally support accepting this paper for this reason. * Modeling novelty is limitedSuggestions for improvement:* In equation 2, it s more clear to write as (x_s + r_s)^T W_q^h W_k^h (x_t + r_t), no?
Accept (Spotlight). rating score: 9. rating score: 7. rating score: 6. rating score: 6. <BRK>Pros:The framework used by the authors is clear and easy to understand but also very general. The authors’ mix of empirical results and theoretical analysis makes a convincing case for the accuracy of their observations. The authors go beyond observation and analysis and use their insights to design new approaches to pruning that outperform existing techniques. The paper is well written and well organized. The main limitation is that all experiments were conducted on relatively small datasets (CIFAR).<BRK>Can you provide any results for unstructured pruning? The idea of employing gradient flow is novel for its purpose and seems to be accurately executed. The main concern is that there is a gap between the flow model and the actual optimization method used in this work (SGD with momentum), or more generally standard optimization methods for deep learning. Further questions:  Why do you study structured pruning *only*?<BRK>## Pros  Authors provide simple and useful explanations to various pruning criteria that are based on the Taylor approximation of the loss functions. I recommend authors to run their versions in unstructured setting too. ## Cons   Authors choose to focus on structured pruning since resulting networks are dense and acceleration is straight forward. This is likely to be true in simple settings, however it is not a sufficient condition; especially for networks with batch norm. You can arbitrarily scale neurons if there is a batch norm and you can come up with arbitrary ordering if needed. The work would also benefit from including unstructured pruning experiments.<BRK>This paper proposes a detailed analysis on pruning heuristics, and its applications to early pruning. it s very timely research to guide the audience which heuristic is better. The other concern is the evaluation and the scale of the dataset.
Reject. rating score: 3. rating score: 5. rating score: 7. <BRK>  Summary: A multi agent market simulator using GANs are proposed. Clarity: The paper is not clearly written and not self contained. Originality & Significance    Sec.2 is supposed to describe the proposed algorithm, however, it is hard to see any novel contents. If the novel contents are written in another section, then it should be rearranged so that the readers would easily catch them. Cons: Hard to find out due to unclearly written paper. I recommend the authors check the grammar of the whole text again.<BRK>This paper proposes using a GAN to learn a discriminator and then use the discriminator to tune a non differentiable simulator. The authors use this idea in the context of stock market simulation. Shouldn t there be richer statistics   for example as in [Li. There are so many parameters in the simulator that can be configured (trader arrival rate, params in the Ornstein Uhlenbeck process). Why not calibrate these? The simulator used appears to be rich, but clearly still an approximation of real behavior in the market. The goal of interpretability is nice, but not sure the paper gets there by only tuning few parameters.<BRK>  The authors have presented a new technique to create multi agent market simulation using GANs. There are some problems with the presentation of the article. Sometimes it seems that a sentence is added in section 1.2 to cite an article. The statement does not seem to be right, "Because SIM GAN only uses historical, and not simulated data for training, once the discriminator is trained – it can be used to calibrate any time series model (not necessarily multi agent) that represents a given historical dataset". If this is true, then where does the fake time series data come from? That would be a good test of its (un)bias.
Accept (Oral). rating score: 9. rating score: 7. rating score: 6. <BRK>Pros:  The paper is well written, well structured and clear at all times. Use of genetic programming for the search of a new learning rule for neural networks. Cons:  The main idea behind this type of meta learning is always very interesting to revisit.<BRK>This paper proposes a method of discovering, through evolutionary search, programatically defined RL algorithms.<BRK>##########################################################################Summary:The paper introduces learning of RL algorithms via evolution. I believe the notions introduced in this work could be useful for the community and their results seem to indicate a promising direction.
Accept (Poster). rating score: 8. rating score: 7. rating score: 7. rating score: 7. <BRK>The modified flow introduced in this paper better approximates the practical behavior of SGD as it does not require vanishing learning rates and it allows to use random shuffling in stead of i.i.d sampling. The numerical experiments also provide some insights into tuning hyper parameters such as learning rate and batch size. Specially, I enjoy reading section II.<BRK>This extends a previous (Barrett and Dherin, preprint) analysis of GD using the same framework, but limited to full batch GD. Can you discuss this assumption a bit more? Overall, I think this contributes new interesting insights which are very relevant for studying minibatch GD in deep learning.<BRK>Many of my comments and questions have been resolved and, consequently, I have increase my score and **recommend accepting this paper. Overall, the paper was interesting and pleasant to read. Why wouldn’t we also be interested in the covariance of the iterates?<BRK>Recommendation:I recommend accepting this paper. I find the main result and the analysis technique interesting and novel.
Accept (Poster). rating score: 8. rating score: 7. rating score: 6. rating score: 5. <BRK>The authors proposed a novel method for regression problems with outliers. Pros:1.The motivation of the paper is very clearly stated in the text, and the sketch of the theorems make the paper easy to understand.<BRK>Strengths:  the problem formulation is clean and clearly explained  the method presentation is well written  the techniques in used in the optimization steps, after  the relaxation step, are well motivatedWeaknesses:  it is not clear whether the significance of  Regression without Correspondence, is high. The percentage may vary and it is not clear how a user of the method should find a good initialization regime. Note: calling a method ROBOT will make it very difficult to google<BRK>The paper writing is very clear. Also, the organization and presentation of experiment part are very clear and easy to follow.<BRK>The paper presents a method for robust regression with no correspondences. It is in general expected that AD techniques can match explicit derivative formulas, unless the problem is not well suited to the specific AD technique used. And for sure EM must bring many of the advantages over AM as the paper promises. It s important to make explicit what exactly are the proposed benefits. Although what was the initialization afterall?
Reject. rating score: 5. rating score: 6. rating score: 6. <BRK>This paper proposes to build a defense against different types of norm bounded attacks via gated batch normalization. This work treats different types of adversarial attacks as data from different domains and proposes to use gated batch normalization to improve the robustness across different domains. The idea is interesting. Adaptive attack for the GBN layer is missing. Does the paper use the best attack algorithm for each norm bounded attack? al.Towards the first adversarially robust neural network model on MNIST.<BRK>This paper introduces an algorithm for defending against multiple adversarial attacks (L1, L2, L inf) by learning separate batch norm statistics for each attack type. How many runs were the reported results dervied from? The algorithm is evaluated on a range of datasets (MNIST, CIFAR 10, Tiny ImageNet) and is shown to outperform recent competing approaches on a range of adversarial attacks.<BRK>### SummaryThe authors propose to improve the robustness against adversarial attacks in deep networks via the use of a customized normalization strategy. The experimental section evaluates GBN against competitor approaches, either directly purposed for adversarial robustness (e.g.MSD) and conceptually similar normalization techniques (MN, MBN). CGN achieves convincing performance, and the underlying idea of separating BN statistics seems to improve adversarial robustness in a substantial way. ### WeaknessesWhile the methodology of several gated BN units is well motivated, the technical novelty is somewhat limited. Why was this particular layer chosen?
Reject. rating score: 5. rating score: 5. rating score: 6. rating score: 6. <BRK>Update: In the revised version, the authors have addressed some of my technical concerns, therefore I slightly increased my score. The proposed approach has running time in $O(N \log N)$ instead of $O(N^2)$ for each iteration and is thus faster than previous graph learning methods. Moreover, the presentation of the paper could be improved (see comments above).<BRK>How does the discussion of smooth signals on graphs connect to the proposed approach? Using the authors  interpretation of "convergence" in this case, however, it looks like their method does not improve appreciably on the uNN graph. The paper is well written for the most part, and the method is intuitive and persuasive.<BRK>The performance of the algorithm is justified using developments of worst case efficient algorithms for Laplacian matrices, and experimentally, the algorithm converges quickly when starting with nearest neighbor graphs, and leads to significant increases in accuracy. Strengths:+ the paper puts together a lot of different ideas, many of which have solid theoretical foundations.<BRK>Quality and clarity: The theory in the paper is technically sound. The scalability of the algorithm makes the contributions significant.
Accept (Poster). rating score: 8. rating score: 7. rating score: 4. rating score: 4. <BRK>The paper can be improved by a more comprehensive and detailed analysis of the behaviour of the algorithm, in particular, the options that are found. The approach is based on a two level hierarchy, *scheduler* at the top and *worker* at the bottom. Perhaps such an analysis can be added to the appendix. The paper develops a hierarchical reinforcement learning algorithm and analyzes its behaviour in four robotic manipulation and navigation tasks.<BRK>This paper present HIDIO, a hierarchical RL method that leverages self supervised losses to discover intrinsic options while learning a scheduler to leverage the learned options to optimize the accumulated reward. HIDIO differentiates from prior work through enabling the low level network/worker to discover task agnostic options that can be generalized to future tasks, thus requiring no pre training of skills,  and at the same time makes minimal assumption about the task structure. This paper is in general well written with clarity. The proposed  method is technically sound and empirically outperforms existing methods.<BRK>The paper describes an interesting approach to self supervised option learning for hierarchical reinforcement learning. What do these environments enable that existing ones do not? The key concern that I have with this paper is its organization and presentation. Furthermore, many sections seem to be an aggregation of contributions, related work, and results, which further makes the paper hard to follow. I believe that this does the paper a disservice by decreasing readability. 2) There is extensive work on option discovery [1,2]. as well as learning decompositions for reinforcement learning [3 6]. 3) On page 2, the authors say, "for the purpose of this paper, we consider a hierarchy of two levels".<BRK>#######################################################################Summary:In this paper the authors present a new method for hierarchical reinforcement learning, demonstrated with a 2 layer architecture, in which the higher layer Scheduler policy choose lower level Worker policies at fixed intervals. The lower level Worker policies (options) are trained through an intrinsic entropy minimization. #######################################################################Reasons for score:This paper seems to be based strongly on the work in Sharma et al., 2019b and Eysenbach et al., 2019, and while it makes a number of interesting modifications to those methods there is in sufficient decoupling / ablation of the issues. I think the paper would be notably stronger with additional experimental evidence #######################################################################Pros:1.
Accept (Spotlight). rating score: 8. rating score: 6. rating score: 6. rating score: 6. rating score: 6. <BRK>This paper presents a local attention + relative positional encoding type of network suited for image classification and detection tasks. The first focus of the approach is to make attention scale to (2D) images. Still, there is hope to recover long( er than context size) range attention with multiple layers (as in ConvNets) and this is not studied/discussed.<BRK>This paper presents an efficient method to model long range interaction. Two kinds of lambda functions in lambda layer, i.e., content lambda and position lambda,  allows the model to capture both dense content and long range interaction. 2.Some typos with the paper. 1.This paper is not easy to follow.<BRK>This paper proposes a novel lambda layer to capture long range interactions by transforming available contexts into linear functions, termed lambdas and applying these linear functions to each input separately. 3) Check the typos in the paper.<BRK>This work proposes a lambda layer to capture long range context. >The performance is consistently good on classification, detection and instance segmentation. Though interesting decomposition of query context is introduced by linear functions, the overall design is still follow self attention, the new part is incremental. >Some notations are missed in Table 1.<BRK>Summary1.This paper present a new method, the lambda layer, for capturing long term dependency. This paper says that the lambda layer is a general framework, but the results only include vision tasks. However, all of the experiments are related to the vision tasks. The lambda layer is a simple and effective method.
Reject. rating score: 4. rating score: 4. rating score: 5. rating score: 5. <BRK>The paper suggests two approaches to combine the concepts of robust Markov decision processes (MDPs) with that of constrained MDPs. Finally, numerical experiments are presented on RWRL problems, such as the cart pole and the walker, showing the effect of using the redefined operators. The general problem that the paper studies (namely, robust constrained MDPs) is nice and worth investigation, but the offered combination is straightforward, the theoretical results are weak, and the paper is poorly written (see below). Several notations and concepts are not specified, starting from the state and action spaces of the MDP. In section 2.3.1, in equation (2), it is not clear with respect to what probability distribution the expectation is taken in $V(s)$. The structure of the paper is also a bit chaotic. For example, there is an "Experiments" and also an "Experimental Results" part, both containing results of various experiments. Moreover, Sections 6 and 7 should be subsections of Section 5, etc.<BRK>The standard Reinforcement Learning framework is limited in many ways, and numerous variants have been introduced to deal with aspects such as partial observability, temporal abstraction, safety, domain transfer, etc. The authors propose the formulation of two objectives, that merge the two aspects and include both a worst case evaluation over the ambiguity set and a constraint violation penalty term. The applicability of the proposed approaches is demonstrated on a benchmark of Mujoco tasks, where they are shown to compare favorably to several baselines. It seems so, but it should be written explicitely. 3.The derivation of A.2 seems a bit sloppy, since the last term in line 4 is identified as$ \mathbb{V}$while it does not strictly correspond to the definition 1. I feel like  the authors intended the definitions 1 and 3 to be seen somehow as *operators* rather than *functions*, which could allow to retain the sup/inf in the definition of $\mathcal{T}^\pi_{R3C}$ and $\mathcal{T}^\pi_{RC}$, but it is a mere speculation and certainly not what is written in the paper. In conclusion, this paper comprises a clear motivation, promising insights and encouraging results. But in the present state of vagueness of the theoretical framework, I cannot recommend acceptance.<BRK>Pros:   Constraints and misspecified models (or models that change over time) are real barriers to deploying RL in many applications. Therefore, the authors study an important problem that has garnered a lot of attention. The manuscript offers some theoretical footing and some empirical evidence for the proposed methods. Update after rebuttal:I appreciate the authors  answers and revisions of the manuscript. The theoretical presentation is clearer with the new notation and I appreciate the improved Figure 2. I understand that this approach was considered before, but that does not make it a good idea. The theoretical results seem to be immediate consequences of the work by Tamar et al, 2014. I appreciate that the authors followed my suggestion to evaluate on more than 3 random seeds. Maybe a simpler and faster method could be used to showcase the benefits of the new objective. Overall, the new version of the paper is better, but I think the empirical evaluation and the writing should be further improved. Therefore, comparisons such those presented in Table 2 are not meaningful. Could the authors clarify this? I appreciate that there is a breakdown of performance per task in Figure 2. Minor issues:  All plots are difficult to read.<BRK>In this paper, the author combines robust MDP and constrained MDP for continuous control tasks. 1.First of all, I found such a combination is straightforward. Any real world environment could have such an issue. 3.The key contribution is from Theorem1. This is useful to show the algorithm convergence. However, any guarantee of safety and robustness can not be theoretically given, either during training or inference. Theorem 1 does not imply these guarantees. 4.The Lagrangian method is standard to deal with the constrained problem. Theorem 3 is not necessary. 5.The authors should avoid using “real world” unless real world experiments are performed. I don’t believe simulations using cart pole, walker, and quadruped are neither enough for illustration nor represent “real world”, even though the name of the software the authors use is named after “real world”.
Accept (Oral). rating score: 9. rating score: 9. rating score: 7. rating score: 7. <BRK>**Summary:**This paper provides a theoretical analysis of self training for semi supervised learning, unsupervised domain adaptation, and unsupervised learning. The expansion assumption requires that the neighborhood of small sets have a class conditional distribution that is large. Under this assumption, the authors show population results for an algorithm that performs self training under the objective that enforces input consistency. In particular, the summary of the theoretical results in the main paper is very well done. The results are already summarized (well!) Can you comment on the realism of Assumption 4.1? Is it possible to verify this on toy examples as you have done with the assumptions in Section 3?<BRK>Authors analyse and shed some light to several aspects of using unlabelled data during training. Intuitively such an assumption is quite reasonable. In their view input consistency brings a local stability/generalization and expansion property brings global stability/generalization. The authors supply quite a bit of theoretical novel material to support their intuition and analysis. Every of them is a field by itself. Supporting arguments:a)	I found the assumptions paper quite intuitive and necessary. The novel material in the paper is extensive. For example, can authors present some evidence of expansion property for a chosen deep neural network on a dataset (or multiple datasets) and quantify the expansion property based on some metric.<BRK>The key assumptions are 1) the “expansion” assumption which characterizes the low probability data subset must expand to a neighborhood with large probability; and 2) the neighborhoods of samples from different classes have small overlap. Then the authors established the upper bound of the prediction error on the population when minimizing the self training and input consistency based loss on the population. merits1) In the theoretical aspect, the established results can explain the success of self training training and is of high quality. 2) Moreover, this work is well written and easy to follow.<BRK>Summary of the paper: The paper gives a theoretical justification of self training. It proposes a new notion of "expansion"   the amount of data distribution in the neighbor of an example. Review: Overall, the paper is written nicely and gives a novel perspective on self training. Several questions:	1) It is a bit strange that semi supervised learning requires a stronger assumption on expansion than unsupervised learning. I feel that the paper gives an interesting notion to consider for self training and the statements are rigorous.
Accept (Poster). rating score: 7. rating score: 6. rating score: 5. rating score: 4. <BRK>This paper studies generalization through a novel "bootstrap" framework relating the test loss from training on a finite fixed training set to the loss when training on fresh samples at each iteration. The framework is quite novel and suggests alternative perspectives on understanding empirical phenomena such as the success of overparametrization, data augmentation, and implicit biases. Positives:  The paper is clearly written  The empirical bootstrap phenomenon is very interesting and the experimental results are quite surprising. Cons:  The paper could benefit from some more prominent discussion of settings where the bootstrap error is small vs large. It was briefly mentioned in this sentence in the conclusion: " The bootstrap error is not universally small for all models andlearning tasks: for example, we found the gap was larger at limited sample sizes and without dataaugmentation. " The choice of soft error to compute the bootstrap error seems somewhat ad hoc, especially for the datasets studied where the Bayes optimal risk is presumably quite low. For example, it seems like ImageNet DogBird should be quite easy to separate unambiguously. Some other comments and questions:   It would be interesting to hear the authors thoughts on the feasibility of rigorously analyzing the bootstrap error. Are there any obstacles to analyzing this error that arise in the prior literature?<BRK>Paper Summary The authors propose a bootstrap framework for understanding generalization in deep learning. Post rebuttal UpdatesIn light of the authors new experiments and response, I have increased my score. However, I feel that the presented framework is novel and the experiments consistently demonstrate that the bootstrap error is consistently low under soft error in a number of settings. Hence, the empirical test error is controlled by the online error (i.e.a rapid decrease in the error in the online setting leads to low test error). The authors then provide empirical evidence to demonstrate that same techniques perform well in both over and under parameterized regimes. The bootstrap error shows slight deviations under the MSE, and thus, I do feel that the scope of generality of this work is still limited. The bootstrap framework presented for understanding generalization is novel to the best of my knowledge and provides an interesting connection between optimization in online learning and generalization in offline learning. 2.3.The authors present significant empirical evidence that the bootstrap error (in terms of soft error) is consistently low in image classification settings across a number of architectures. While at first this framework serves as a very appealing alternative to the classical decomposition, the fact that the bootstrap error is low seems heavily reliant on the soft error only found in the classification networks using a softmax on the outputs. In particular, the authors focus on the soft error in their decomposition throughout the work, but in Appendix A and in section 6, it appears that the MSE/cross entropy loss produce a significant bootstrap gap. At first, this may seem innocuous, but if this bootstrap gap is low only for soft error, this would imply that we need to then focus on understanding how quickly the population soft error decreases, which to the best of my knowledge is a very different object of study in optimization than the MSE or cross entropy loss. This experiment should resolve the authors  claim with the gap in Figure 6c being large due to unbounded weights when minimizing the empirical loss. 3.3.(Minor) I have a hard time understanding how strong some of the claims are in the paper. In particular, the authors claim a main implication as "The same techniques [...] are used in practice in both over  and under parameterized regimes." They do state this in briefly on page 6, but it seems to be claimed less strongly in the introduction. In particular, while I find the authors  bootstrap framework appealing in that it could nicely connect generalization and optimization, I am concerned about the results relying heavily on the soft error (especially since the softmax activation is not necessary for deep networks to perform well on test data).<BRK>This paper proposes a bootstrap framework to study the generalization problem of deep learning, by decomposing the traditional test error into an ‘Ideal World’ test error plus the gap between. It then proposes to explain several phenomena in deep learning using the bootstrap framework. Pros:+ The proposed decomposition of test error is new to the field. Cons:  My main concern is that the motivation of the proposed bootstrap framework is unclear. I have a hard time understanding how the proposed framework overcomes the two major obstacles of classical approaches listed in the introduction section. However, the main claim of the paper (claim 1 in Page 3) is a pure experimental claim. The paper will be much stronger, if claim 1 can be characterized theoretically with bounds on the error term (even in a simplified setting). The implications of the proposed framework and results on how to improve the existing deep learning methods are not clear to me. Assuming the bootstrap error is always small, then how can one modify the current deep learning training method to reduce the ‘Ideal World’ test error? I would recommend the authors to lay out these metrics in the main body of the paper to add clarity. Post Discussion Update  I appreciate the authors  efforts for responding my concerns and comments. Although the proposed framework may suggests a potentially interesting future direction for the deep learning research community, it still does not fully convince me its feasibility. In particular, the paper would be much stronger, if the author can dig deeper with the Real World test (soft) error, which I view it as the end goal of a real world classification system, to provide more specific directions on how to make it smaller, or more importantly, how it changes the current deep learning training paradigm.<BRK>This paper defines a metric of generalization gap between the ideal world and real world via  bootstrap approximation , and seeks to use his gap to explain some phenomena. Although I have respect for this paper trying to define some statistical terms in the deep learning society, I have a very strong concern about the novelty, as it appears to be whether trivial or hard to understand. For example, I could relate to the empirical finding that bootstrap error defined in claim 1 is uniformly small, but I personally believe that the  TestSoftError  defined in 3 should dominate the generalization error and the paper circumvents this term to study some term that is way non significant, especially considering how the datasets are built around CIFAR or DOG bird, via simple data augmentation that is frequently used in vision society. The problem being mentioned in the paper, i.e.Most if not all techniques for understanding the generalization gap (e.g.uniform convergence, VC dimension, regularization, stability, margins) remain vacuous , appears to be not tackled at all in this paper, which, in its main effort, to discuss generalization. Section 5 says some practical suggestions to train deep networks, which seem to be not persuasive, especially that the ideal world error might be harder to measure than that in the real world, according to the paper.
Reject. rating score: 4. rating score: 4. rating score: 4. rating score: 5. <BRK>The paper suggests a method for training binary neural networks. The proposed method is to partially train with full precision and then continue with binarized training using the straight through estimator. The method is very simple and there is very limited technical contribution, so in order to be worthy of publication it needs to be supported with compelling experimental results. Unfortunately this is not the case. The main claim is speeding up training by a factor of 1.2/1.6. While this can help the importance of speeding up training (unless by a much larger factor) is quiet limited. The useful speedup BNN present is at inference time, and the time to train a network is of much less importance (at least for this kind of speedup). From Fig.1 I am not convinced that the speedup claims hold. You can see the test accuracy for binary training on 3 of the experiments reaches the Partial Pre training level but it doesn t look like it completely flattened out yet. It looks like if you want take the best model on test, then you don t get any speedupResults on cifar 10 seem quiet poor (both full precision and 1 bit).<BRK>This paper proposed one simple method called partial pre training to speed up the training of binary neural networks (BNN). The partial pre training method is simple and easy to implement;2. The main concern of the proposed method is kind of heuristic and there is a lack of theoretical explanation, whether rigorous or not, why this method works. As a result, it is unconvinced that the partial pre training could universally improve the speed as a general method. 3.There is a lack of explanation of the contradiction result with previous result proposed in Alizadeh et al (2019), which dismissed the approach of pre training. Is there good explanation of such an opposite result? It would be better to show results of different split between full precision training and low precision training. 4.Regarding pre training for BNN, there are some related works from the Bayesian perspective. In Shayer et al.(2018), they used the result of full precision training as the prior for the binary training, which improves the final result, as opposed to Alizadeh et al (2019). The Bayesian perspective provides an explanation of the effectiveness of a good prior. In Meng et al.(2020), they showed that STE could be viewed as Bayesian and obtained good result even with a uniform prior. Given the above results, since the authors demonstrate that partial pre training can increase the speed for STE, does this imply that partial pre training provides a better prior than full pre training? R., Khan.E. Training Binary Neural Networks using the Bayesian Learning Rule. ICML, 2020.<BRK>This paper addresses the problem of slower training speed with low precision training of neural nets. It presents a simple solution: first train with full precision on half of the budgeted trainining time, then train with low pecision in the remaining time. Pro:  The proposed idea is simple and it is nice to see that it worksCons:  I feel there is not enough content (in terms of ideas and experiments) to warrant a full paper. Compared to other ICLR papers, the contributions seem on the low side. The paper mainly compares the proposed method with low precision training, but the results would be stronger if also compared with full precision training followed by quantization. Suggestions:  Experiment with more quantization methods. Currently we do not know whether the proposed method is uniquely suited to the PACT method used, or is a general technique. Even the 12500 second training time is just <4 hours. Table 2: what is it/s. Table 1 shows learning rate schedule for t up to 60k in CIFAR or 450k in ImageNet, but Figure 1 stops way before those points in the x axis. Another clarification point about Fig 1 and 2: for the proposed method, is the full precision training part of the time included in the calculation? I believe so but just want to double check.<BRK>#### CommentsSummary:The authors propose a fast binarized neural network training algorithm that splits the whole training process into the full precision training stage and the binary training stage. The experimental results show some improvement about training speed in terms of iterations and wall clock time. Generally, the paper is well written. Strength: The idea is reasonable and the method is presented clearly. The experimental results indicate that the proposed method can accelerate the convergence of the binary networks on image classification and collaborative filtering. Weakness: The comparison between the proposed method and quantization finetuning method is lacked, which seems like a closely related work. The analysis of the proposed method should be also enhanced. The reason why partial pretraining can improve the training of binary neural networks should be investigated more deeply. Comments:(1) The authors claim that the proposed method allows for faster from scratch training of binarized neural. This seems contradictory to the partial pretraining. (2) The improvement of the proposed method is not significant. The proposed algorithm speeds up the training marginally and cannot improve the final test accuracy. This limits the contribution. Overall, the reviewer doesn’t recommend accepting this manuscript at its form.
Accept (Poster). rating score: 7. rating score: 7. rating score: 7. rating score: 7. <BRK>They then demonstrate how their method can be used to approximate graph representation learning techniques over a variety of datasets, as well as enabling the application of non scalable techniques to large graphs. One issue I took was with the authors  use of the term "tensor:" they claim that their method allows for efficient implementation via standard tensor operations. The authors do not formulate anything particularly new: rather, they unify a variety of algorithms for graph representation learning under a common approach, parameterized by two functions (Accumulate and Bias). However, there is novel value in the analysis of the meta algorithm as an unbiased estimator of loss functions dependent on powers of a transition matrix. Because of this, the scope of analysis in this paper is somewhat limited: the authors should more directly discuss cases where this is not true, beyond the brief mention of node2vec in the appendix. Although there are a few missing pieces, the proposed approach is useful in scaling approaches that were previously not scalable, and thus warrants being accepted into ICLR 2021.<BRK>Pros:1)	The graph traversal via tensor functionals is proposed with stochastic graph traversal algorithm based on tensor optimization that inherits benefits of vectorized computation and libraries. 3)	Authors proved the learning is unbiased, with controllable variance. Cons:1)	As shown in the experiments, the implementation of existing methods under GTTE can be more efficient and less memory requirement.<BRK>The proposed graph traversal can be applied in conjunction with graph neural networks such as GCN, Deepwalk,..etc, and ideas proposed in the paper would interest and influence other researchers in the community. Can the authors provide more detailed comparisons with different baseline methods? The authors discuss the applicability of the proposed method with existing methods. Cons:a.)One limitation is that experiments are not very convincing since there are only a few baseline methods that are compared with the proposed method.<BRK>### SummaryThe authors propose a "meta algorithm" for approximating various graph representation learning schemes: generate batches of random trees with fixed fanout (and possibly biased probabilities of selecting different edges), and use them to accumulate information to approximate operations on the graph. The idea is beautifully simple, and generalizes running independent random walkers, an approach that is used in deriving many related algorithms. The biasing and accumulation functions are user provided, and the authors show how different choices of these functions can be used to approximate a number of graph representation learning schemes. Experiments show the approach is much more scalable than competing approaches (though, to be fair, some of the competition was not targeting scalability). Is there any difference?
Accept (Poster). rating score: 7. rating score: 7. rating score: 7. rating score: 4. <BRK>Summary: The authors use Domain Invariant Categorical Semantics to improve unsupervised domain translation (UDT). They show how this can improve results on Semantic Preserving Unsupervised Domain Translation and Style Heterogeneous Domain Translation by doing experiments on MNIST< >SVHN (features traditionally learned are very different but digit identity could be the same) and Sketches >Reals (distinct styles) respectively. The paper is simple to read and the idea is intuitive.<BRK>The paper addresses the domain translation problem and proposes a novel approach to translate images between domains in an unsupervised manner, by integrating unsupervised learning of domain invariant semantic features between the two domains. In overall, it is a good paper with an original and potentially inspiring idea and a convincing application of conditional GANs, would be an interesting read for many in the conference.<BRK>This paper does not seem to emphasize the technical novelty of this work. The experimental results on Sketches and Reals show that the proposed method yields high quality generations. Section 3.1 consists of unsupervised representation learning, clustering, and unsupervised domain adaptation. Unsupervised domain adaptation is realized by the minimization problem described in P4.<BRK>Summary: This paper proposes to learn the categorical semantic features in an unsupervised manner to enhance the ability of image translation at preserving high level features, and obtains some good results on Semantic Preserving Unsupervised Domain Translation and Style Heterogeneous Domain Translation problems. How does the model solve the two problems according to their differences? The experiment and qualitative evaluation are too limited. Minor issues  What are the essential differences between SPUDT and SHDT problem?
Accept (Poster). rating score: 7. rating score: 6. rating score: 6. rating score: 4. <BRK>Review:This paper handles the problem of missing not at random (MNAR) by extending the MIWAE model to MNAR scenarios. Summary:The problem of MNAR is very important in practical scenarios, specially when handling tabular data. The paper was well written and explained, and although it can be seen as a simple extension to the MIWAE model, nonetheless I consider it to be a relevant and interesting contribution. I was under the impression that this paper was only evaluated on real attributes, mainly based on the use of MSE and RMSE as evaluation metrics.<BRK>The paper introduces a deep latent variable model (DLVM) for missing data problems where the missing mechanism is missing not at random (MNAR) and therefore cannot be ignored. It presents an approach for fitting the model based on importance weighted variational inference and reparameterization trick, and demonstrates the application of the proposed method in simulated and real data sets. The experimental results demonstrate the trade off between data model flexibility and missing model flexibility. Overall, I think the results in the paper should be useful in a number of applications and the paper has enough contributions to merit publication.<BRK>The formalisation of the missingness process is very intuitive. Also the literature referenced provides one of the most comprehensive overviews on the topic in the statistics community, on the deep learning side it seems that there is a lot of research covered on the side of deep learning latent variable models based on variational auto encoders (VAEs), but the complementary work on Generative Adversarial Networks (GANs) appears to be less well covered. What I found a bit limiting is that the not missing at random process was so simple and restricted, and that the missingness ratio was not explored systematically. Especially for a study like this, which makes a presumably important and strong contribution to the field of missing data imputation I would recommend to demonstrate the effectiveness of the proposed approach by a more realistic experimental setting.<BRK>It is important for this paper to highlight what is the novelty of the proposed approach in terms of technical innovation. To learn the parameters of deep latent variable models, the paper adopts importance weighted variational inference techniques. Experiments on a variety of datasets show that the proposed approach is effective by explicitly modeling missing not at random data. P1: The related work is well done.
Reject. rating score: 5. rating score: 6. rating score: 6. rating score: 6. <BRK>The paper proposes to define the weights approximate posterior of a NN with inducing variables. The main benefit of the approach lies in the compactness of the description (Fig4, right). Moreover, if complexity and memory footprint is the main concern, MC dropout sounds like a reasonable alternative to Bayesian NN. Why do the authors do not compare their proposal to a conventional MC dropout approach? Overall, the contribution of the paper appears to be valid but relatively limited in scope compared to what exists in the literature (inducing variables are known, Bayesian NN are known, ensemble methods are known). What is the gain in knowledge brought by the paper? I might have missed a piece of the contribution, but the way the paper is written does not help. It is not self contained (many previous works have to be read to follow the story), and remains relatively opaque to the non expert. It lacks a clear and eye bird picture of the approach (including both training and inference steps), to position and compare it to existing works. The overhead in optimizing (1) or (2), compared to training a deterministic NN, is not discussed. Variables d_in^l is used in Section 2, but only defined in Section 3.<BRK>Compared with BNN and deep ensemble, the proposed approach in thiswork has a storage advantage. Furthermore, this work provides a bettertrade off between accuracy and calibration. # pros1.The approach in this work is quite interesting. The idea of augmenting    weights with auxiliary low dimensional latent variables in a deep neural    network seems natural at first sight, but this approach is novel as far as    my knowledge is concerned. Although VI with local latent variables is an    old technique, this application in deep neural network is novel. 2.The authors also proposed an efficient approach that can sample from the    variational approximation conditioning on the latent variable. Since the    original weight is large, such a sampling is necessary. 3.This paper provides extensive empirical results and sufficient theoretical    results. # cons1.My major concern is all experiments are conducted on ResNet18, and this is    not a typical choice for practical problems. I think experiments on other    larger net such as ResNet56 on CIFAR10 will make this paper more    convincing. It seems better if  q(U)    also depends on input data, however, the authors choose to use a fixed    q(U).<BRK>How does FFG U perform when increasing dim(U) such that the parameter count is the same as FFG W? In addition, they also discuss how more efficient sampling from q(W|U) can be realised via an extension of the Matheron’s rule to the case of matrix random variables. ### Pros  The method provides a novel way to induce parameter efficiency in variational bayesian neural networks  It connects to the sparse GP literature and the trick seems to be general enough, in that it can be applied to any distribution that admits some parametrisation in terms of a Gaussian random variable. The extension of the Matheron’s rule can be of independent interest  Extensive set of experimental tasks, with a nice ablation study for lambda_max and sigma_max### Cons  Comparison against alternative approaches that induce parameter efficiency are missing  Results are mixed and sometimes not very convincing### Recommendation While I find the main idea very interesting and the presentation of it relatively clear, I unfortunately cannot recommend acceptance of this work as is. The main motivation behind improving parameter efficiency can also be performed with other, perhaps much simpler, ways and comparisons against such approaches is missing. Furthermore, the results, at least in their current state, are a bit mixed and thus not very convincing. Using inducing variables in neural networks in not a new concept, as it has been used before in, e.g., [1, 2], but the motivation of using them as a means towards reducing the number of parameters of each distribution is new. The authors explain the main idea behind them in a clear manner. My main point for feedback is for the experimental section. If then memory efficiency is the main target, I believe that some reasonable baselines are missing. Both of these baselines would provide a more complete picture and would be a better signal in understanding whether the inducing weights framework is a better choice overall. Sometimes, both FFG U and Ensemble U underperform compared to FFG W and Ensemble W and it would be good to know if it is due to less free parameters to optimise or due to the model definition itself.<BRK>Because the variational parameter shifts from $W$ to the smaller $U$, the resulting model potentially uses even fewer parameters than a deterministic counterpart. And these approaches all maintain a "mean parameter", thus the memory costs are at least as large as the original network. In comparison, this paper turns to an augmented space using the joint Gaussian distribution, and instead model the variational posterior for the compact *inducing weights*. Given that the inducing points approach has achieved big successes for scalable GPs, I think the proposed method could be impactful for uncertainty modelling in deep learning models. **Experiments**This paper conducts experiments covering both image classification and out of distribution detection. If it was the former, I would reckon that the particles of the same layer would converge to the same place; if it was the latter, it should be made clearer in the paper. **Clarity** In spite of my questions regarding the model details, the paper well presents its main methods. Besides, I think Sec3.3 is worthy of more polishing, since it looks confusing when you start by studying $U_c$ instead of $U_r$ or $U$.
Accept (Oral). rating score: 8. rating score: 8. rating score: 7. rating score: 7. <BRK>In practice this will perhaps have something to do with e.g.the regularity of the PDE solution (the gradient with respect to the solution is similar to that of its neighborhood). I recommend to accept the paper.<BRK>2.Would it be possible to provide some preliminary in appendix on the solution to the section 5 PDE example. Thus I recommend to accept this paper. I only raise these questions up for improvement on the paper; they are not concerns on the quality of the current version. The memory saving here is achieved by trading off gradient variance for activation memory saving.<BRK>The paper developed and proved the proposed method in a general framework (which is nice!). Still, I feel that the explanation for applying this method to neural networks is somewhat lacking. I will temporarily give a reject.<BRK>It will also help to understand the different curves of Fig 5. The application on stochastic optimization of PDE is also very interesting and relevant. I think the contribution of this work is solid and I vote for clear acceptance.
Reject. rating score: 3. rating score: 3. rating score: 4. rating score: 4. <BRK>Rules are mined from the KG using AMIE and recursive backward steps are taken, using the mined rules, to determine if a fact is true.<BRK>The paper proposes a framework (EM RBR) for doing Knowledge Base (KB) completion. Instead of the direct triple score from an embedding based method, EM RBR allows the triple score to be calculated as a composition of the scores of the rules mined from the KB. This is blatant cherry picking.<BRK>The work utilizes relational background knowledge contained in logical rules to conduct multi relational reasoning for knowledge graph (KG) completion. Besides, I found that if many of the test triples can not be matched by rules, the performance improvement would be subtle. 2) FB15k and WN18 are too old and suffer from the test data leakage issue. I think it is very confusing.<BRK>The motivation for choosing this algorithm is not explained in the paper. This makes the algorithm a little hard to understand. This paper can certainly be improved in a number of ways but the contributions are worthwhile and it may inspire some interesting future work. Maybe the statement can be rephrased? [AMIE](http://resources.mpi inf.mpg.de/yago naga/amie/amie.pdf) is used for auto mining rules from the KB.
Accept (Oral). rating score: 9. rating score: 9. rating score: 7. rating score: 7. <BRK>Overall, this is a strong paper and I recommend it for publication. They train over 250.000 agents with different settings and suggest empirical guidelines. I d add that strictly speaking no _iteration_ is necessary, as for instance IMPALA, coming from the A3C line of development, does both asynchronously in parallel, and I suspect so do the authors given their use of SEED RL. I assume this is a task yet more daunting, but no less useful to the overall community of researchers.<BRK>##########################################################################Summary: This paper conducted a large scale empirical study that provides insights and practical recommendations for the training of on policy deep actor critic RL agents##########################################################################Reasons for score:  Overall, I d vote for acceptance to the paper. Pros:	1.Reproducibility is one of the main issues for various RL algorithms. 2.The paper is well written, and the suggestion is useful to me. Sorry, but I didn t go through all the details in the appendix.<BRK>As a final comment, this is a solid work and will be very helpful to the community. Given that it is implemented on the new SEED RL framework, so it would be better if the implementation code can be published. In overall, this study is exhaustive and helpful to both RL researchers and practitioners.<BRK>The paper presents an empirical evaluation of many algorithmic choices made in the implementations of on policy actor critic algorithms in deep reinforcement learning (RL). PROSThe paper is well written and clear. I appreciate the breadth of the choices that the authors consider. CONSI have a doubt about the robustness of the results. The authors decided to use the median over 3 seeds for the evaluation.
Reject. rating score: 5. rating score: 6. rating score: 6. <BRK>FOSAE++ is demonstrated on three environments: * Photo realistic Blocksworld* MNIST 8 puzzle* SokobanIn each environment the input is the set objects obtained using a domain specific segmentation of the image. ## Reason for scoreI m sorry but by expertise in the domain of this paper is way too limited to be able to do a fair assessment. This is not necessarily due to the introduction, but rather to my lack of expertise in the domain. Many of the concepts are alien to me (PDDL, classical planning, lifted action models, object centric representations, STRIPS). I was also surprised that more than 2 pages were dedicated to background and previous work. Also, I m a bit surprised that the authors claim to have achieved "unsupervised end to end neural system that generates acompact discrete state transition model (dynamics / action model) from raw visual observations."<BRK>This work presents FOSAE++, an end to end system capable of producing "lifted" action models provided only bounding box annotations of image pairs before and after an unknown action is executed. Building on recent work in the space, the primary contribution of this work is to generate PDDL action rules. The biggest flaw in the paper is the rather sparse results section. Though I appreciate the author s candor in this regard, at the moment the paper is rather weakened by the lack of inclusion of additional planning experiments, particularly since the 8 Puzzle domain is arguably the easiest domain from which one might generated a lifted action representation, due to the black background surrounding the digits. It provides helpful additional content that, while not necessary for understanding the paper, aid in understanding and implementation.<BRK>This paper addresses the problem of learning dynamics model directly from raw sensory inputs. The authors propose an unsupervised end to end model that can perform high level tasks planning on raw observations. This work extends Asai et al.2020, 2019 etc, and with improved symbol generation and lifted PDDL. The authors follow the experimental setup as seen in prior work, where three artificial environments (blocksworld, MNIST 8 puzzle, and sokoban) are used for planning. The notations used throughout the paper can be a little bit confusing. Experiments are limited to relatively simple environments.
Reject. rating score: 4. rating score: 5. rating score: 6. rating score: 7. <BRK>This work suggests a new discriminative mutual information estimator that relies on a classifier to directly estimate the log density ratio of p(x,y)/p(x)p(y), without variational lower bound. In general, the idea is easy to follow and simple simulations are done to demonstrate its effectiveness. However, I still have some concerns:1. For the latter, there are also a few recent proposals based on a classifier to distinguish p(x,y) from p(x)p(y). It would be better if authors can clarify this point. 2.Authors discussed the theoretical optimum of their estimator when the number of samples approching to infinity. In the simulations, it seems that the number of training samples is also very large (e.g., 160k). What will happen if we replce MINE with DEMI. Can authors provide some explanation or theoretical analysis? 5.It seems to me the work is prepared in a quick time.<BRK>This paper proposed a discriminative estimator for mutual information, to alleviate the shortcomings of the existing estimators such as MINE and SMILE. A classifier was built to decide whether the sample is drawn from the joint distribution or the independent one (product of marginals). The paper was written with clarity and easy to follow. Here are some detailed comments on the technical contribution of this paper:1) There is a closely related piece of work in the literature (see below). They also proposed a discriminative estimator for KL divergence, with mutual information as a special case. It would be nice if the authors could relate to this existing work, and provide experimental comparison to their estimator. "CCMI: Classifier based conditional mutual information estimation." (https://arxiv.org/pdf/1906.01824.pdf)2) From Figure 1 right column, we see that all estimators (including the one proposed) underestimate the mutual information when it is high. Often, estimating the mutual information is not the end goal, but an intermediate step to achieve other goals (see the MINE paper for examples). 4) A minor point: In equation (10) the last part, it should be (1 z) * log( 1   q(...) ) instead of (1 z) * ( 1   log(q(...)) ).<BRK>The main idea is that, instead of learning (generative) distributions of joint and marginals, learning a single likelihood ratio that is discriminative and hence more tractable: a posterior $p(z | x, y)$ trying to distinguish between the joint distribution $p(x, y)$ and the product distribution $p(x)p(y)$. Once the posterior is learned, it can be used to estimate the MI. The method makes sense, and the training procedure is very simple, achieving better estimation than the baselines. provide an interesting insight. ## Weakness:  The method only discusses estimation of mutual information, not maximization of MI for representation learning. The biggest weakness of this paper would be: experiments. The training data used in the experiments is either low dimensional or synthetic, so there remains a question about how well this method will scale to a high dimensional, and challenging deep learning setting. ## Question:  The hyperparameter $\alpha$ is said to be set to 1.0 (Section 3), which does not seem feasible based on the Equation (5) (7). Can the authors clarify on this? Also, I am curious how sensitive DEMI is on the choice of the prior hyperparameter $\alpha$. This would be a good analysis to have for the completeness of the paper. ## Additional comments:  Section organization: I suggest having an introduction as a separate section, with methods being the following section.<BRK>Seems like the most direct way to estimate mutual information using a classifier. I like this work because it is much more straight forward than the prior work such as MINE. It shows sufficient performance on the experiments shown.
Reject. rating score: 5. rating score: 6. rating score: 6. <BRK>The paper proposes conditional transport as a new divergence to measure the difference between two distributions.<BRK>The paper proposes a new transport based divergence between distributions (CT) and a variant for empirical distributions (ACT). $d(x,y)$ seems to be part of the "navigator" $\pi(x | y)$   an energy based conditional probability and $c(x, y)$ is the point to point transport cost.<BRK>Summary:The paper introduces an asymptotic conditional transport divergence to measure the discrepancy between two probability distributions. The evaluation shows the effectiveness of the proposed method on both toy datasets and some popular image generation datasets. But readers might doubt whether the comparison is fair or not.
Reject. rating score: 4. rating score: 6. rating score: 6. rating score: 7. <BRK>This result has been first demonstrated by [1] with MLP network (on the MNIST dataset) and the presented paper extend this finding to CNNs (on CIFAR10, CIFAR100 and SVHN), RNN and LSTM. * The link between neuro plausible learning rule and back propagation is interesting. [1] Whittington, J. C., & Bogacz, R. (2017). An approximation of the error backpropagation algorithm in a predictive coding network with local hebbian synaptic plasticity. Neural computation, 29(5), 1229 1262. [2] Rao, Rajesh PN, and Dana H. Ballard. One cannot claim to approximate arbitrary computational graph if one demonstrates the approximation on a specific loss function (which is known to poorly perform on classification problem). Nature neuroscience 2.1 (1999): 79 87. * Some crucial points would have deserved in depth discussion and are just ignored (see below)* The paper is not well enough motivated: what’s the point of such a local approximation beside the neuro plausibility (faster ?Consume less resources ? This should be included in the paper, as it will strongly strengthen your claim. However the authors also raised the weight transport problem. On my understanding the proposed framework is still suffering from weight transport as the backward connection weights are the transpose of the feedforward one (due to the derivation of the forward operator). The paper would deserve an in depth discussion concerning this point. I have the intuition that this problem arises because the authors are tackling a discriminative problem (i.e.finding a mapping between inputs and labels) using a generative model (PC is a generative generative model as described by [2, 3]).<BRK>### Update after author responses: While the author address some of my comments, I would have still liked to see a more detailed discussion of how the algorithm compares in terms of algorithmic scaling, which I think is relevant because it is a fundamental property of the algorithm, even if it is targeted towards understanding biology. So my score remains the same. Summary:The authors extend recent work on MLPs to show that predictive coding converges asymptotically to exact backprop gradients on arbitrary computation graphs. Overall, I vote for an accept because I think the generalisation is quite useful and interesting for training deep networks with local learning rules. The authors demonstrate that this method works, but haven t demonstrated its computational advantages clearly enough. There are some issues of clarity that I have also outlined below. Strengths:+ The generalisation of the earlier MLP results to arbitrary computational graphs is quite powerful esp. since it can be applied to most deep learning architectures. The experimental evaluation is also extensive. This is related to the previous point, where the utility of the predictive coding network is not demonstrated sufficiently. E.g.Fig.3.Algorithm 1 does t seem to be referenced anywhere.<BRK>##########################################################################Summary: Authors propose that predictive coding gives similar convergence as backprop algorithms by extending the work of (Whittington &Bogacz 2017,  https://www.mrcbndu.ox.ac.uk/sites/default/files/pdf_files/Whittington%20Bogacz%202017_Neural%20Comput.pdf) to arbitrary graphs. This is an important topic both for the application of classical deep nets to neuromorphic hardware, but also for our understanding of computations in biological tissues. This makes the point of the paper more convincing and complementary to similar works. ##########################################################################Concern: A major issue of this paper is to not identify its originality. The extension to « arbitrary graphs » or to CNNs is straightforward in theory, While it is not explicitly stated in  (Whittington & Bogacz, 2017), an unwrapped RNN is by definition a feed forward graph and is therefore a direct application of their work. Concerning novelty, the actual derivations (eg for the LSTM) are original and the simulations clearly support that original contributions. However this material is at the end of the paper or in appendices. Recentered on this original contributions and how this makes a suitable contribution to the community would make the paper acceptable to be accepted at ICLR.<BRK>The paper extends prior work on equivalence between predictive coding and backprop in layered neural networks to arbitrary computation graphs. This is empirically tested first on a simple nonlinear scalar function, and then on a few commonly used architectures (CNNs, RNNs, LSTMs), confirming the theoretical results. The importance of this advance is highlighted by noting that the demonstrated equivalence shows how in principle modern architectures could be implemented in biological neural systems, and that the highly parallel nature of predictive coding could lead to efficient implementations in neuromorphic hardware. The paper is very well written, easy to read, and includes a nice introduction section with a fairly comprehensive overview of backprop, and the problems related to its potential implementations in biological systems. That the exact backprop gradients are computable in a fully local system with Hebbian plasticity for an arbitrary graph is an interesting and promising result. Throughout the text, predictive coding is quoted as biologically plausible. This isn t strictly true as noted already in (Whittington & Bogacz, 2017), as e.g.dedicated error nodes are not known to exist for every cell in the brain. I d suggest calling this "potentially biologically plausible", and including a short discussion on how these plausibility concerns could be addressed. Fig.2 shows the model converging at high inference learning rates for the case of the scalar function. What convergence condition was used? How does convergence speed depend on the size/diameter of the computational graph? This is similar to Fig.9, but asking a slightly different question   i.e.how many iterations are needed to reach convergence as a function of graph size.
Reject. rating score: 3. rating score: 4. rating score: 4. rating score: 5. rating score: 6. <BRK>This paper examines n gram level features encoded within the hidden states of recurrent neural models. The proposed method approximates the hidden states of LSTMs and GRUs with a first order Taylor series, which the authors claim is an adequate approximation with small enough inputs. The authors apply their method to models trained on synthetic sentiment analysis and language modeling datasets. The paper is difficult to understand, and many assumptions are not properly justified. The experiments are also not convincing, as a large portion of the analysis focuses on small synthetic data, and some of them do not have clear takeaways or explanations as to why they were conducted in the first place. how generalizable are these findings to standard initializations used in NLP architectures   similarly, on page 3 the authors assume that the higher order terms of h_{t 1} are "insignificant"; in practice, it is unclear how often this is true. can t the authors actually show quantitatively how good the approximations are? if the higher order terms indeed do not affect the quality of the approximation, that could be justified by some experiments. i m not really convinced by Table 2: the approximations could be quite different from the original model but still yield good downstream accuracy. why are synthetic datasets used at all here? what is the point of training models with the approximations instead of the original GRU/LSTM cell equations? i don t understand the significance of Table 3.<BRK>This paper proposes to linearize GRU and LSTM cells (as error terms should be negligible when inputs are small in magnitude). Putting these linearized, or, really, affine, RNN cells together into a single layer sequence processor, thanks to the affine ness, we can decompose the score that is obtained by taking dot products with a query at each timestep into contribution by immediate unigram features and all subsequences leading to this unigram. The authors evaluate these scores, showing that they do and don t capture phenomena in a synthetic dataset and proceeding to show that when both training and evaluating with this simplified network on SST yields strong results. How sensitive are results and what can we say about statistical significance? The overlaid histograms definitely need to have some transparency or be shown another way right now it is impossible to see what is "happening" in the blue bars as they are hidden by the orange bars. #### Criticism/weaknesses:  It is unclear how the simplified/affine ized architecture relates to vanilla RNNs those, too, can be decomposed like that and one could just as well look at these features. Random words without any sequential coherence? In addition to that, the notion of a "sequence level polarity score" that here complements the token level score is not at all mentioned in Sun & Lu (2020) as far as I can see, so to say that their methodology is used is misleading in more than one way. This paper mentions the linear layer, but not the pooling/attention, so it s unclear if that is a poor paraphrase of Sun & Lu or whether this paper here too diverges from the paper it claims to build on. I think it would ve been very interesting to see whether or not this simplification is legitimate, that is whether the assumptions made in the derivation are justified: train with the original cell formulation, but then evaluate *quantitatively* using the approximate cell to essentially create a table like Table 2 or perhaps even a scatter plot for individual logits to see how much things change or don t change.<BRK>Essentially a linearization of the GRU and LSTM. Given the simplifications, the authors argue that their model captures N gram information for the sequential information. The paper presents results suggesting that this approximation is able to capture much of the same sequential information as the LSTM and GRU on benchmarks such as SST 2, PDB, Wikitext 2, and Wikitext 103. What this paper excels at is a thorough theoretical formulation of the proposed approximation, and a comparison with an approximation without the sequential information. However, the relevance of this paper is not clear to me, and the introduction and related works does little more to explain that relevance than stating: “understanding the essential features captured by GRU/LSTM”. Below I have made a few comments/questions:Abstract:“Sequential data in practice”   unclear what this means“sequence level representations brought in by the gating mechanism”   I dont understand this sentence“essential components”   vaquely defined“Based on such a finding”   rephraseIntroduction:“gradient vanishing or explosion issues”   vanishing or exploding gradients“While such models ...”   this whole sentence is a little vagueRelated work:“With the variants”   its variantsGeneral comment (also for introduction): While you mention many interesting findings in recent years, it is difficult for me to assess how exactly your work differs. Please use the related works to emphasize what you are doing differently than previous work in your field. Why is negation, and a synthetic variant, important to explain the relationship between N grams and LSTMs? Why do you choose the datasets you do? Why is SST 2 and benchmarking against older language models of interest? I don t see my issue of relevance addressed.<BRK>The authors expand the sigmoid function and the hyperbolic tangent function using Taylor series to obtain approximated closed form mathematical expression of hidden representation when using the GRU or the LSTM as the update rules. The approximated hidden representation of the GRU and the LSTM update rules can be separated into two terms, (1) the current token level input feature and (2) the sequence level feature, which is a weighted sum of all previous tokens. As the hidden representation consists of two feature terms, one can take each feature (either token level or sequence level) separately for a downstream task, e.g., evaluate how good when sequence level feature is used for predicting polarity score in sentiment analysis. The tasks that were used in the experiments are sentiment analysis and language modelling. However, it is predictable that sequence level feature should be meaningful. I don t see much of insights by showing that the polarity score from sequence level features indeed align with this prediction. If we can to apply Taylor expansion to simple recurrent neural networks (RNNs), such that we can expand the hidden representation of a standard RNN into two terms: the current token level input feature and sequence level feature, how would the results look like and how can we relate them with what were reported in this paper? It is interesting to see that the approximated versions of GRUs and LSTMs can perform on a par with the original models on language modelling tasks, however, these results don t necessarily improve our understanding on how gated RNNs are capable of learning good representations of n grams.<BRK>This paper provides a reliable interpretation of modern RNN models, through unrolling GRU and LSTM cells. The approximate state representations include a token level term that only depends on the current input token and a sentence level term that depends on all inputs until the current token. The experiment section shows that the approximation shares similar behavior and performance as the original model. Overall, the paper is well written and easy to follow. I believe that this study still provides interesting insights for those who want to develop better recurrent or non recurrent models in the future. My major concern is that the language model experiment didn’t include a stronger baseline method, such as AWD LSTM, which provides a significantly lower ppl compared to these in the paper. It would be interesting to see a more detailed ablation study, that studies the importance of each term in A(x_t).
Accept (Poster). rating score: 7. rating score: 7. rating score: 6. rating score: 5. <BRK>In this paper the authors propose a variant of the extragradient method that i) achieves optimal convergence in the smooth and nonsmooth regimes, ii) allows one to work with singular operators. The same for a solution $x^*$. The proposed algorithm still requires a Bregman Finsler function as an input. For example, no doubts that the rate interpolation is a great property. Larger step? Obviously, it should matter for the convergence in the end. If $V$ was contaminated by noise, does it imply that $V$ is not monotone? Fig.2 confirms that UniProx can provide a fruitful template for adaptive min max optimization methods that attain optimal rates in smooth/non smooth problems." First, as I said above, large constants in the rate and omitted discussion of it. I believe the algorithm was designed not for solving unconstrained minimax problem. First, we should distinguish theoretical results and practical. The condition (C.2) is global on the other hand. The explanations there are also more transparent.<BRK>In this paper, the authors present an adaptive stepsize strategy for extragradient in order to recover both the ergodic 1/sqrt(T) in the nonsmooth case and 1/T in the smooth case (without the knowledge of the Lipchitz constant). The presented method is rather simple but seems effective both in theory and in practice. The paper is well written: the motivation, context, and basic definitions are especially clear. The algorithm is intersting and the analysis seems sound. Would it be possible to do some kind of restarting to mitigate this effect? Could the authors comment on the computability of the proximal mapping (16)? (For instance, instead of the stochastic case which seems a bit off topic).<BRK>This paper propose a novel algorithm that solves the min max problems and games based on the extra gradient (EG) framework. One of the main goal of this paper is to achieve the optimal convergence rate for both smooth/nonsmooth setttings without assuming the Lipchitz continuity/boundedness conditions. This work can potentially help in solving games and reinforcement learning problems. However, the authors are encouraged to provide more experimental results to illustrate the practical impact of relaxing the assumptions. Will the same result hold with constant step size or for example $\gamma_t   1/\sqrt{t}$?<BRK>This work proposed a stepsize for the extragradient/mirror prox method that works both in smooth and non smooth settings. The stepsize is based on the empirical values of gradient/operator differences, and mirror maps are used to allow for non euclidian geometry such as KL divergence. The paper offers us three convergence results: 1 bound for the extragradient update (no mirror map) and 2 bounds for the mirror prox update (convergence of iterates without a rate and convergence of the restricted gap with rates). I think it s fine that the theory is derived for monotone operators, since many algorithms designed for convex objectives often work on nonconvex problems as well. However, unlike the algorithm of Bach and Levy, the one presented here does not allow for stochastic gradients, which is crucial in practice. / Unbounded" property of the stepsize. I also think that this problem can be solved by methods from [1, 2] (depending on if it s smooth or not) Finally, the problem in Example 5.3 is also not motivated in this paper. In Lemma 1 of the main part, the authors use the notation $\gamma_{\infty}$, but in the appendix, they use $\alpha$.
Accept (Oral). rating score: 9. rating score: 8. rating score: 8. rating score: 7. <BRK>The authors introduce a novel method for inferring a simple algebraic expression for an output in terms of an input. For each token of the sequence, a recurrent neural network outputs a probability distribution on possible tokens, from which the token is sampled. The authors introduce a novel reinforcement learning objective that optimizes the quality of the best examples instead of optimizing the average quality. (For example, they disallow redundant expressions like log(exp(x))).<BRK>This is clearly a very relevant task towards constructing explainable AI systems. The proposed approach in this paper is based on the generation of mathematical expression with recurrent neural networks, by exploiting background knowledge about the form of the expressions to impose constraints on the generated examples. The RNN is trained by maximizing a risk seeking policy gradient that aims to increase best case performance. A very solid experimental evaluation is carried out on several benchmarks, comparing the proposed approach with state of the art systems for the same task, including commercial software such as Eureqa and Wolfram. The proposed methodology is shown to perform better than all the competitors. Overall, I consider the paper to be a strong contribution for ICLR.<BRK>SummaryThe paper uses RNN trained by risk seeking RL objective to predict mathematical expression that generated the target dataset. In the second step placeholders for constants in sampled expression are optimized again by gradient optimizer to maximize the reward function. Strong points+ The paper is clearly written and potentially relevant to larger audience. + The proposed method is relatively easy to implement and it leads to good empirical results. My perspective is that all the techniques should find the correct solution in the limit with enough computational resources (under assumption that they can escape local optima).<BRK>Furthermore, instead of optimising the RNN directly with policy gradient, the proposed algorithm also makes use of the risk constrained method that emphasises the risks above a given percentile criteria. The proposed approach has achieved the best performance on several benchmark symbolic regression datasets. #### Pros+ This paper is motivated by an interesting problem. I like the idea of using domain knowledge to introduce hierarchical constraints, which seems to be effective in this task. + The authors have done extensive experiments, the presented algorithm performs well on the benchmark dataset. + This paper has covered a wide range of related work. It looks more incremental rather than a novel contribution. Still, there is no such result in the current submission. #### RecommendationI think this paper proposes a nice approach to tackle the symbolic regression problem.
Reject. rating score: 3. rating score: 4. rating score: 5. <BRK>There are a couple of concerns I have with the paper and some questions I will elaborate on in detail. Therefore, at the current state of the paper I cannot recommend acceptance. Prior work shows that Transformers trained on a (masked) LM objective learn about sentence structure implicitly [1]. In fact, although the authors "hypothesize" that explicit structure can help   which they do not properly ablate in my opinion (see 2 3)  , the results in the paper show no clear advantages on their benchmarks when comparing to BERT. The increased complexity of the method (ie., relying on other trained models) should be warrented by showing a clear advantage in doing so. 2) Fair comparisons: The models in the paper are trained on another pretraining corpus. Furthermore, BERT models use much lower dimensional sentence embeddings which typically hurts performance. A way to make comparison fairer would be to apply a random projection of the 768 features to 4800 features. I also wonder how well a randomly initialized model would perform, that is, how much does the pretraining actually help? [2] shows that it might not be required at all. I am not convinced from the paper that the model learns anything more semantic than that. My personal opinion on introducing explicit syntax into neural nets (note that this will not be part of my decision): Given all the evidence we have so far on pretraining LMs using generic model such as transformers, I just don t see any reason why we should still try to explicitly fit our potentially faulty and biased syntactic frameworks into neural networks when more generic models (such as transformers) can learn structure directly from the data. Our linguistic frameworks can still be super useful for understanding and systematically testing our models, but I think we should refrain from  introducing our potentially limited understanding of language into those models. A random thought: How would pretrained parsers help a model on twitter like text or child speech?<BRK>The proposed approach is a simple extension of the work of Logeswaran & Lee (2018). First, the results on SentEval are not convincing in comparison to other general purpose methods (i.e., BERT or QuickThought) which do not require a parsed tree during training and inference. The results also do not justify the use of additional resources (dependency and constituency parsers) as it complicated the models and is not applicable for many other languages where parsers are not available or their performances are still far from usable. Note that, there are several pre trained models after BERT (e.g., XLNet, XLM R, ALBERT,...) which can be used to obtain sentence embeddings. I think the authors should also compare the results with those models. Third, while the paper claims posed a hypothesis that structure is crucial to build consistent representations, this is not shown in the paper. I think that the paper lacks some analysis/examples  to show in which cases modelling structure of the sentence explicitly is necessary. For example, Hewitt and Manning shows that syntax is embedded in deep models (i.e., BERT).<BRK>Some of my concerns have been solved thanks to the rebuttal. #### Summary  This paper proposes a self supervised (multi view learning) method that constructs sentence embeddings by exploiting different types of sentence encoders, including tree encoders based on constituency and dependency trees. The authors also conducted some qualitative analyses to reveal the inner working of their method. Even though the results presented in the paper is interesting, in my opinion, there should be a stronger contribution that leads to this paper to be unique and informative. Moreover, reliable parsers are only available for a few resource rich languages such as English. Considering this problem, the paper would be much persuasive when the inference speed of each compared model (in addition to its training time) is provided in experiments. Even though (A) is controlled in the presented experiments, (B) is not. In other words, readers cannot infer whether the performacne of a model is due to its architecture or its training data. To make the paper more convincing, I recommend that the authors test the BERT model in the condition where the dimensionality of the BERT embedding is expanded to larger numbers, making it comparable to those of other models.
Accept (Poster). rating score: 7. rating score: 6. rating score: 6. rating score: 6. <BRK>Summary:This paper describes a deep learning approach for predicting Hamiltonian systems. This paper can both handle non separable systems and use a symplectic integrator to enforce conservation. Since the ICLR community is similar to the NeurIPS community, I think that this paper would be of interest. I like that the extension for non separable systems directly builds off an approach for extending integrators to non separable systems (Tao, 2016). Further, the Tao integrator is built into the network training.<BRK>The authors propose a variation of Hamiltonian Neural Networks (HNNs) with a built in symplectic integrator. The authors use Tao’s integrator (2016) at the core of their model (HSSNN) to achieve better conservation of energy in systems with non separable Hamiltonians. Many works have experimented with training with different integrators, and in that sense this is a very incremental contribution, however, Tao’s integrator is a state of the art integrator, and I think the community will benefit from reading about it, and seeing it implemented. I think the paper is a bit borderline, and in its current form a lean on the side of "Weak Reject", but would be happy to raise the score, if the authors can make improvements in the axis mentioned above.<BRK>The paper extends the symplectic family of network architectures towards modeling nonseparable Hamiltonian dynamic systems. More specifically, the paper implements a  symplectic integration schema (from Tao (2016)) for solving arbitrary nonseparable (and separable) Hamiltonian systems within a symplectic neural network architecture. Overall, the paper is well structured and well written, however there are still some inconsistencies that need to be addressed and clarified. How and why was this value chosen?<BRK>The work proposes a novel method for solving non separable Hamiltonian systems, using Tao s approach in which two copies of the phase space are tied together by an additional Hamiltonian. I found the paper difficult to follow. This should be added to the discussion, with a theoretical and/or empirical analysis.
Accept (Poster). rating score: 8. rating score: 7. rating score: 6. rating score: 5. <BRK>The paper under review studies the question of whether gradient descent can solve the problem of calibrating a deep neural network for separating two submanifolds of the sphere. The contribution is restricted to a simple set up and addresses the question in the finite sample regime.<BRK>The paper studies the conditions for a deep fully connected network to separate low dimensional data classes. The authors show that if these conditions are met, with high probability a randomly initialized network converges to a classifier that separates the two class manifolds. The separation Delta between the two manifolds seems to be a critical parameter in the proposed performance bounds. 2.Although I like very much the analysis proposed by the authors, some of the main assumptions regarding the studied setting may be too restrictive in practice: a. The proposed bounds apply only when the separation parameter Delta is positive. b.The assumption that the data samples lie exactly on a Riemannian manifold also seems to be restrictive.<BRK>As a model the authors use a deep fully connected neural network and train it to separate the submanifolds, representing different classes. They assume that sub manifolds belong to the unit sphere. The proved result seems to be imporant for understanding the generalization ability of deep neural networks. Also, I would expect more comments on which tools, used for proving the main results, are completely new and how they can be used to establish similar results for other network architectures. Also, I would expect some comments how in principle the restriction that the submanifolds belong to the unit sphere can be removed. Some more discussion how the proposed setup is related to the setup, considered in the paper of Goldt (2019), are needed. Although, I can not verify the details of the proof.<BRK>The paper analyzes deep networks in terms of multiple manifold problems. However, it needs easier explanations for a wider audience. The paper talks about one dimensional cases, and I was wondering if the multiple manifold problem in one dimension is enough. If I understand correctly, this one dimension manifold is supposed to represent one class in the last layer. Actually, some of them are used without the description.)
Reject. rating score: 4. rating score: 5. rating score: 6. rating score: 6. <BRK>Especially as someone who is more familiar with the fair representation learning literature, I get confused when the authors refer to a sensitive attribute, or private part of the data, in this setting – it seems to be a different notion than I am used to and it is never clearly defined. Summary: This paper gives a method in the class of learning representations which have some information censored. Strong points:  	The problem as I understand it is interesting and the work is well situated in the privacy literature. Experiments on CIFAR and CelebA demonstrate some good behaviours from this method, mostly beating baselines 	Experiments are mostly good, decently thoroughWeak points and Clarifications: 	The main weak point of this paper is the exposition and clarity – I find that the problem setup is not explained particularly well.<BRK>### SummaryThis paper presented a method for learning private representations. This method is based on adversarial representation learning and the main technique contribution comes from focal entropy. The experimental results show that focal entropy can improve the accuracy of the target predictor without increasing adversarial accuracy. Some technique details are not presented clearly. I have listed some of them here:   a. However, in the following description, this loss is related to $\theta_{tar}$(Equation (6))### Comments1. The optimization objective involves many individual terms. The experiments should further provide results to show how the trade off parameters $\beta$ controls privacy leakage and target accuracy. 2.This work is done in an adversarial training manner, can leakage reduction be achieved in a differentially private training manner, i.e., training the encoder using dp sgd?<BRK>## Summary  The paper tackles the problem of adversarial representation learning i.e., to learn a low dimensional representation of the image that encodes target relevant non sensitive information, but not (often correlated) sensitive information. Objective**  There are a few things unclear to me in the objective (Eq.1 6) and would appreciate if the authors clarified them. (c) I am also confused with the sanitization term (Eq.6).In particular of why the sensitive attribute classifier $\tilde{S}$ is a function of the target attribute classifier $\tilde{\theta}_{tar}$? After all, this appears as a secondary objective opposed to the primary objective of learning a representation $z$ which minimizes information leakage. **2.Writing   Sec.3**  My issues in understanding can be partly attributed to the writing in Sec.3, which I found difficult to follow for a few reasons. (b) While the architecture in Fig.1 is designed to accompany the text, I find it somewhat incomplete and unclear.<BRK>########################################################################## Summary: The paper studies how to learn private representations that only captures the non sensitive attributes of the dataset. ########################################################################## Reasons for score: I like the paper largely. It provides a nice practical method for sanitization, although it is hard to give theoretical privacy guarantees of VAEs. My main concern is the experimental results are only on two datasets with one task/sensitive attribute setting. As an empirical/methodology paper, I would expect more empirical results. It s not clear if focal entropy should be used in both $\tilde{T}$ and $\tilde{S}$. I m interested in seeing the results, but it is not required for this submission.
Accept (Spotlight). rating score: 7. rating score: 7. rating score: 7. <BRK>Summary:In this paper, the authors aim to build a robust model against word substitution attacks. The experimental results show that the proposed model is indeed more robust than other baselines.<BRK>It would be helpful if the authors could address the questions raised above. *Strength of the paper*:  1. The idea proposed in the paper is quite straightforward yet effective. Using a convex hull could satisfy the three aspects as stated by the authors   Inclusiveness, Exclusiveness, and Optimization.<BRK>The authors answered my questions so I am increasing my score to 7. (2) In training does the model have access to the set of all possible substitutions S(u) or not? This is a very exciting topic and I am glad to see more work in this space.
Reject. rating score: 5. rating score: 5. rating score: 5. <BRK>The problem definition, data collection, and model design are reasonable. This paper provides a new direction for moving machine learning forward in the clinical domain. 2.Collecting data from MIMIC III is a reasonable choice for creating a multi modal multi task dataset.<BRK>There are two objectives for this work:1. A proposed multi modal multi task AI modelFor the first objective, the data supports 6 clinical classification tasks (which were proposed before in a earlier MIMICIII benchmark) but also supports 4 different modalities: physiological time series (the standard modality that used in the prior benchmark), clinical notes, baseline data, and waveform (although not being used in the method/experiments). Why the authors benchmarking MIMIC III and not the recent versions? The authors have mentioned "We also propose an evaluation framework to benchmark models on this dataset.". Does MIMIC has images?<BRK>################################ Summary:This paper discusses the Multi Modal Multi Task MIMIC III (M3) dataset and benchmark, which extends previous efforts in this space to provide a benchmark on the MIMIC III dataset. This paves the way to provide additional multi modal work in this area. ################################ Strengths:  Unifying existing work.
Accept (Poster). rating score: 8. rating score: 6. rating score: 6. rating score: 3. <BRK>This paper studies the effects of random action + observations in reinforcement learning, and proposed to use partial trajectory resampling to improve off policy algorithms such as SAC. I find that this section is generally not easy to read given the packed symbols. In addition to the equations, it would be better to add a figure illustrating the recursive sub sampling process. (2) The paper only augmented SAC.<BRK>Summary:The paper introduces an algorithm for the case where actions have delayed effects in RL, and specifically in the case where the delay is random. Page 3   `One solution is to perform on policy multi step rollouts’ – I don’t see this as a “solution”. The approach is also strong because it deals with random delays which are not as well studied in the literature (though an older related work was missed there, see below). SAC, as I understand it, is an off policy algorithm so it should be able to handle off policy samples. And the delayed observations are the result of the actual actions taken, independently of the policy that produced them. This solution should be compared to in some way.<BRK>Post rebuttal: The updates clearly explain the resampling procedure of this paper, and strengthen the theoretical part of this paper. In which case would the on policy sample with truncated trajectory (i.e., the value function computed by Eq.(8) where the length of the trajectory is $n$) out perform off policy sample with full trajectory (i.e.SAC without resampling)? The paper considers reinforcement learning with delays, motivated by real world control problems. Algorithm proposed by the paper uses importance sampling to create on policy samples of augmented observations, and is empirically shown to out perform base line SAC algorithm. Does it imply that $w_i+\alpha_i$ is a constant?<BRK>This work tackles an existing phenomenon that is often ignored in real world control problems   stochastic lengthed delays. Is this due to the action and observation delay values? How can it be part of a transition probability while according to Def. For example, one paragraph before Sec.3.1 and the first one discuss an off policy partial trajectory resampling method using very vague arguments, and the relation to what was presented up to that point in the paper is loose. The literature review is very scarce. Additional multiple recent citations using SOTA algorithms for constant delay are also missing. IEEE transactions on automatic control, 48(4), 568 574.
Reject. rating score: 3. rating score: 4. rating score: 4. rating score: 4. <BRK>This paper studies the correlation between the flatness of the converged local minimum and the margin. It would be good if the authors can address thesis concerns in the rebuttal session. The experiments are described in detail and seem correct. However, I was worried that (1) the reported results are not new; and (2) the authors argue existing results are misleading but did not give enough establishment to support their argument.<BRK>Thus, they do not fully substantiate the claim. I think the paper presents an interesting take on generalization by relating flatness and output margins. I would argue that it might be even more fruitful to also investigate input margins, or margins in some representation (i.e., some layer of the network).<BRK>Would be nice to note this in the paper. Having said that, I agree that the relationship between the trace of the Hessian of cross entropy loss to its margin is strictly speaking, novel. Depending on how concurrent the authors think that work is, the authors may want to consider citing it. ### Overall opinionWhile understanding connections between flatness and generalization is an important direction, unfortunately, I feel that this paper does not provide significantly new insights into these terms.<BRK>The empirical study in the paper reveals that both in linear and nonlinear settings, there exists an intimate connection between the trace of Hessian and the margin. I think this is a direct consequence of the positive homogeneous property of classifiers. For the theory part, there seem to be some typos.
Reject. rating score: 3. rating score: 4. rating score: 5. <BRK>The approach is a modification of Bengio et al.The main new idea is that the objective function can be tweaked by replacing a KL divergence term with a term involving domain shift induced generalization errors. The paper covers an interesting subject, and the idea to directly use domain generalization error learn causal structure is exciting. However, the paper is clearly not yet baked. The main ideas of the paper are unclear, as are the validity of the core insights. The paper will require extensive revision and formalization before it s ready for public consumption. These give a flavor of my issues with the paper, and will hopefully provide some direction for the authors. However, I stress that these issues are only examples. Presumably, the actual aim is to compare the likelihoods of two distinct models corresponding to A >B and B< A. But this is false in general; a very flexible model will simply memorize the training data. 3. in general, the paper suffers from a confusion in the notation between population parameters and finite sample estimators. (update: I read the appendix, and the intended statement is simply, "If P(A|B) Q(A|B), but P(B|A)! Q(B|A) then 0   KL(P(A|B),Q(A|B)) < KL(P(B|A),Q(B|A))"5.<BRK>The paper presents a novel approach for determining the causal direction between two random variables $A$ and $B$. While previous work has proposed to use this insight to determine the causal direction by comparing the adaptation speed, this paper proposes to directly measure and compare the generalization performance. The paper builds on this logic proposed by Bengio et al.(2020) and proposes a more efficient measure for the generalization performance of a model. * The presented proofs seem correct and are presented in a clear way. In the current version, it is not possible to follow the experimental results without consulting the paper by Bengio et al.(2020).It would be very helpful to add a description of the data and models used, as well as the measures compared, e.g.what is $\sigma(\gamma)$? * Consulting the paper by Bengio et al.(2020), it seems that the results in Figure 2 can only be interpreted as correct if the model also predicts the correct causal direction. * Regarding noiseless dynamics, the paper presents an experiment in the appendix showing how performance degrades when increasing the standard deviation of additive noise.<BRK>The paper proposes an efficient approach to learn disentanglement representation causal mechanism, based on a generalization loss between a training data set and transfer data sets. Empirical studies show it achieves better sample and computation efficiency than a previous work (Bengio et al., 2020). There are no comparison with other methods. In (Mooij et al., 2016) datasets, representation learning are not needed; for other datasets when representation learning is needed, existing criterions of these baseline methods could replace the generalization loss. With these results, it would be better to judge the effectiveness of generalization loss. Other Thoughts:  Besides the marginal distribution shift, P(A|B) and P(B|A) may both change significantly, even one of them is the correct causal direction, as the underlying distribution may shift. Can authors comment how their approach could handle such situations? The statement "this work in causal representation learning will be helpful for more advanced artificial intelligence" is rather vague and pompous.
Accept (Poster). rating score: 8. rating score: 7. rating score: 7. rating score: 6. <BRK>Paper summary:The paper provides a reformulation of the fundamental operations in Euclidean space that are used in neural networks for the Poincaré ball model of hyperbolic space (and thus hyperbolic space generally). The paper was well written, and the authors performed regular sanity checks, such as re deriving the Euclidean case for c  > 0. Turning its focus to attention models, the paper proves a theorem regarding the equivalence of various hyperbolic midpoints proposed in previous work.<BRK>However, is this also the case for the other types of gyromidpoints? The authors consider in particular the multinomial logistic regression problem. Thanks to this reformulation, the authors propose a new generalization of the fully connected layers of neural networks to hyperbolic geometry. In general, I think that the paper is hard to read. In particular:  The split operation is not motivated until the multi head attention part (i.e.2 subsections after it was defined). Do they use the same optimizer between HNNs and the proposed approach?<BRK>It proposes new ways to reparametrize hyperbolic multinomial logistic regression (MLR) layers to reduce the number of parameters, to generalise fully connected layers  as well as split and concat operations to be more flexible and less computationally expensive. Further they expand hyperbolic neural networks to convolutional layer and attention mechanisms. _Strengths_:   Paper is well written and easy to understand. I also appreciate a good overview over hyperbolic geometry and consistent notation. The work proposed extends existing architectures with convolutional layers and attention mechanism which is prevalent in many deep architectures nowadays and thus an important contribution. As this was a good paper to begin with, I am keeping my score of recommending an accept.<BRK>These building blocks include a tighter parameterization of the basic fully connected layer, and the concatenate/split operations which are then used to define a version of hyperbolic multi head attention. Overall, this paper does not present fundamentally new ideas in the development of non Euclidean geometry for deep learning, but offers solid improvements in various aspects of constructing architecture components for hyperbolic neural networks and is a useful contribution.
Accept (Poster). rating score: 8. rating score: 7. rating score: 6. <BRK>I think this is a strong and interesting paper. It is well written and properly explains its ideas. I am especially impressed by the figures which are clean and helpful in understanding the ideas of the paper and performance of the method. This is just not true. I would like to see experiments comparing to other voxel based methods in the paper, and also shown here. At the very least other voxel based methods should be referenced, and their absence from the experimental comparison justified.<BRK>The paper proposes a generative method for 3D objects (voxels representation). The paper shows results on shape completion and generation, obtaining fairly good results. CONS  Input/Output: the method is restricted to work in the voxel setting and it is not easily extendible on point clouds. This limits the quality of the generated objects. Presentation: some concepts (like the Cellular Automata or infusion chains) are only marginally discussed; I would like to see a better introduction to them since they are not so common concepts for Computer Graphics community. PRE REBUTTAL RATINGI think the paper could be strengthened by further analysis, experiments, and presentations fixing. I agree with other reviewers that the rebuttal is satisfactory, and raised the score accordingly. Thanks to the authors for their effort.<BRK>This paper proposes an interesting formulation of 3D object point cloud generation   sampling from the transition kernel of a Markov chain. Given an input partial shape, the method iteratively updates the 3D shape by growing point clouds in a local region, and finally outputs the complete 3D object. Besides, the paper is well written and easy to follow. It would be better to show the quantitative results using the same set of hyper parameters for all classes. My intuition is that this method can complete 3D objects (for example, only given the bottom of the lamp), because it is trained from a large amount of data, so it is easy to "remember" the data distribution of each class. I will re evaluate the score based on the author’s feedback, .
Reject. rating score: 3. rating score: 5. rating score: 7. rating score: 7. <BRK>Strengths:i) The motivation, organization and the overall writing of the paper are clear. Weakness:i) The problem of verifying properties of NN based controllers is studied in the literature [1,2,3,4,5,6,7,8,9,10,11,12,…], which is not discussed in detail in the paper (i.e., there is a paragraph present in the supplementary material). Moreover, there are no experimental comparisons made to any of these previous works. ii) The tested experimental domains are not the best representatives of the control problems motivated in the paper. As such, the resulting NN architectures are very small (as seen in the supplementary materials).<BRK>There are many prior works in this area from outside the machine learning community, and I m not totally convinced that this paper provides a substantial contribution over those works. ## Strengths   The problem setup is clear and well motivated   The methodology seems solid## Weaknesses   The work is framed in the context of model based RL, but since the model (including the estimate of the modeling error bound $d$) is assumed known, it would seem like this should be framed more squarely within the optimal control literature. Within this literature, there are many methods related to Safe MPC (e.g.[1], [2])  that it would seem are attempting to achieve the same objective as what is stated here. As a result, assumptions need to made such as monotonicity (work of Del Vecchio e.g.[3] [4]).The experiments that you show are very simplistic also (low dimensional and very simple/convex geometry). but I find this assumption that you are given the modeling error bound diminishes the contribution of the work since everything downstream in your algorithm will be impacted by this value, which is hard to get in practice.<BRK>update after rebuttal: the answers were convincing, and the paper improved. As of now, I see it as marginally above the acceptance threshold. Strengths  A novel method for verifying the safety of NN based controllers  A framework to incorporate inherent modeling errorsWeaknesses  A better comparison to related work is in order  The experiments are not very extensive, and it is not clear how to reproduce them. Questions for Authors  Please compare your work to the literature listed below. What is novel, how does it relate? How would your method perform on state of the art benchmark sets like from OpenAI or Deepmind? I understand you have a continuous state space (as given by the neural network), but do the experiments/examples require such a complicated model? [3] Eriksson, Hannes, and Christos Dimitrakakis. In any case, it should be an overapproximation.<BRK># Comments to the AuthorsSafety verification is a crucial topic for the acceptance and deployment of learned methods in control systems. Coming from the experimental evaluation, there are a few questions regarding the applicability of the method:a) How would it handle dynamic environments, e.g.moving obstacles? As it is, only the deterministic trajectory of the agent appears to be evaluated. c) In Figure 3 (right) it appears as if some green starting states are in an unsafe area. Is this a visual artifact or does the method not guarantee that all unsafe starting states are discovered (which was my impression before)? Regarding the presentation of the paper, I understand that the authors are restricted by the conference format, but I would encourage to revise the selection of material for the main paper and the appendices.
Reject. rating score: 4. rating score: 4. rating score: 5. rating score: 7. <BRK>This paper proposes to use contrastive learning to learn representations from cardiac signals (ECGs). The model incorporates ECG domain knowledge, patient specific, and relationships between multiple leads (channels), in the learning process. Major: However, some of the domain knowledge based intuitions are unclear (might even be wrong). It seems more reasonable to experiment on a larger unlabelled dataset, such as the MIMIC III waveform database (https://physionet.org/content/mimic3wdb/1.0/).<BRK>The authors use contrastive learning by exploiting the fact that a single patient can generate multiple ECG signals, and there are multiple views (i.e.leads) for the same ECG signals. Pros:  The proposed approach is easy to follow, makes much sense, and is well motivated based on the domain specific knowledge of ECG signals. Self supervised representation learning from electroencephalography signals. There are already several papers proposing self supervised learning methods for time series data such as audio signals, ECGs and EEGs [1, 2, 3, 4, 5].<BRK>This paper proposes a contrastive learning method for cardiac signals. The problem it targets is an impactful problem in the medical domain. The paper has a good topic, good methods, and good experiments. However, it still needs some effort to be a good paper. 3.not sufficient description of the task. This is far from sufficient.<BRK>The paper describes a method for unsupervised learning of patient representations from ECG data. The method is simple and a relatively minor extension of contrastive learning to time series. The paper doesn t currently  contain a description of the actual input and the network architecture. Equations 1 4 need to come with some explanation.
Accept (Spotlight). rating score: 8. rating score: 8. rating score: 7. rating score: 6. <BRK>Summary of Submission:This submission points out that 0 padding in convolutional networks induces areas where responses for object detections are significantly decreased. However, I do strongly believe that many people are unaware of the impact and importance of these effects and therefore I believe publication of this content is important. This submission contains a thorough analysis, points out the problem with 0 padding and shows how various alternatives show significant improvement. The text is well written and easy to follow. There has been work [39] which also showed that aliasing in CNNs causes these effects which does not seem to be discussed in this manuscript. Explanation of Rating:This submission describes a problem with 0 padding that significantly impacts an important problem and shows how to mitigate the situation. Therefore I recommend accepting this submission.<BRK>A condition is presented for when this does / does not occur. The paper is well written. Proposed fixes are simple and produce a very significant improvement in performance on imagenet classification and object detection, so are likely to be adopted by practitioners. The paper states:"It is evident that the 1 pixel border variations in the second map are caused by the padding mecha nism in use. This mechanism pads the output of the previous layer with a 1 pixel 0 valued border inorder to maintain the size of the feature map after applying a 3x3 convolutional kernel.<BRK>It is a common belief that choices like padding are minor technical details, which, even when sloppy, the network will just somehow learn to tolerate. This is to some extent true and it is remarkable how well CNN s work even when apparently crippled by questionable design. The paper is clearly written and easily understandable. Overall, while the findings of the paper are arguably fairly simple and elementary compared to many works in ICLR, bringing attention to this sort of effects is valuable and I did learn something from reading the paper, so I would lean slightly towards accepting it. _Update after rebuttal_: The authors have been very forthcoming with further analyses in the rebuttals. I believe the findings deserve to be published and that bringing attention to these issues is a good thing, and accordingly I have bumped my rating up a bit.<BRK>Summary: This paper does empirical analysis on an object detector, esp. By using different padding schemes for conv filters, the authors find the padding scheme is the root for such failures. Update after rebuttal:The author responses and manuscript revisions have addressed most of my concerns. Now I lean towards acceptance. This paper is not written professionally. The following sections are more like experimental notes than a technical paper. More use cases and application scenarios should be investigated. Other comments:I agree that padding could lead to biases when doing downsampling using stride > 1, as illustrated in Fig.6.However, this seems to be easily fixed by doing random image flipping as one of the data augmentations? It would even this bias.
Reject. rating score: 3. rating score: 4. rating score: 5. rating score: 6. <BRK>I have additional concerns about the significance of the proposed method over the baselines which I describe below. The proposed results seem very close to the (untuned) baselines, and so it would be useful to understand how statistically significant they are.<BRK>Summary:This paper did an empirical study on the learning rate (LR) schedule for deep neural networks (DNNs) training. The authors conduct extensive empirical tests to support their claim and the experimental design is reasonable. The empirical results can be explained by other hypotheses as well. The novelty is low. The proposed LR schedule is a slightly modified version of the existing LR schedule.<BRK>Extensive experiments validate the proposed learning rate schedule. The observation of this paper looks interesting, and authors have conducted lots of experiments to validate the effects of proposed learning rate schedules. However, the novelty of this paper seems limited.<BRK>To this end, the paper proposes and evaluates a learning rate schedule that consists of two stages (knee schedule). The proposed learning schedule leads to SOTA results on IWSLT’14 (DE EN) and WMT’14 (DE EN) datasets.
Accept (Spotlight). rating score: 7. rating score: 7. rating score: 7. rating score: 7. <BRK>The dynamics distance function idea is useful and it can be useful for many existing tasks or visual foresight frameworks. From the experiment, this seems to be a simple idea but works quite well.<BRK>I would suggest the authors to be more conservative with this particular claim. The cost function is learned from task agnostic data by randomly generating goal states from the state space and including these goals as input variables in a Q learning approach. The authors indeed provide some evidence in the Experiment section (Table I) in this vein by comparing the results of their approach with that of the policy learned in the distance learning method.<BRK>Figure 2 is nice as a high level overview but the crucial part is definitely the distance function and I felt that wasn t well explained. Fig.6 is good. The work compares to sensible baselines and achieves competitive results. This problem can be solved with dense reward (i.e.distance to tool, then distance tool object) but in this case, this would be incredibly tricky.<BRK>#### Post rebuttal comments:The author response has well addressed most of my concerns on technical details and experiments. The learned distance function is used with the dynamics model for planning from start to goal images using model predictive control (CEM). Please detail why and which theory.
Accept (Poster). rating score: 7. rating score: 6. rating score: 6. rating score: 4. <BRK># SummaryThis paper studies the effect of model over parametrization in GANs. While there is a lot of work on this in the supervised learning setting of classification/regression there is not much in the GAN framework where the minimax objective function complicates such an analysis. This paper considers two types of training of the GAN model, one with the simultaneous gradient descent ascent and one where the discriminator is trained to optimality for every generator update. It provides global convergence results under both algorithms in the case of a generator network with one hidden layer that is large enough and a linear discriminator. # Contributions: Given the importance of over parametrization of neural networks for better performance and low generalization error this work is a step toward a theoretical understanding of this phenomena in the GAN setting. Even though the global convergence results are provided in a simple setting, the proof techniques using dynamical systems and control theory might be useful for extending these results to a more general framework. The comparison between convergence rates of the two training algorithms is also interesting. The experimental evidence seems compelling, although it would be nice to consider examples other than simple gaussians for a more comprehensive empirical analysis. # Comments: 1. It would be nice to compare the two types algorithms considered in terms of empirical evidence. For example a combination of the two plots in Figure 2 would be nice in order to get a sense of the effects of the two theorems 2.1 and 2.2 empirically. 2.Just out of curiosity can we say anything about the training of the discriminator for a finite number of steps and how that affects convergence? Sort of as an interpolation between the two algorithms. While the experiments on the real data are compelling it would be nice to have empirical evaluations where we can compute the true generalization error or know the ground truth which is not simply Gaussian. 4.It would also be nice if the authors could provide a summary of the proof of the main theorems either in the main paper or at least at the beginning of the appendix jus to provide a rough sketch of what pieces are required to show such a result.<BRK>This paper studies how over parameterization plays a role in GAN training. Theoretically, it shows that a GAN with over parameterized 1 layer neural network generator and a linear discriminator can converge to global saddle point via stochastic optimisation. It also provides empirical results to support its findings. ## Pros  The paper is easy to read. ## Cons / Questions  I m not sure if the term over parameterization is well defined in the paper. But here we only see a descending curve (for performance) within the regime of parameter space the paper studies. To be more specific, is $k 256$, which is only 2x or 4x of common choices of feature numbers, considered as over parameterized? The paper says, "One of the key factors that has contributed to the successful training of GANs is model over parameterization". But in fact, a lot of works have been showing that a careful balance of generator and discriminator capacity is in fact important, which is also agreed by the authors in the end of Section 3. Does similar result hold for MLPs? Testing it on MNIST may be useful. Can you explain how step sizes and other parameters are set while varying the capacity of GANs? The study on generalization gap seems to be incomplete. In particular, we also need to check the GANs simply memorizes the data while increasing the capacity, right? It s unclear if a dropped FID in both train and test (i.e.Figure 5) can rule out this issue.<BRK>Summary:While much work has been devoted to understanding the role of the discriminator in GAN training, comparably little is known about the role of the generator. In this work, the author address this important question, more exactly the role that generator overparametrization plays in GAN training. Theoretically, they show that RELU networks with a single hidden layer converge to the global minimum under simultaneous gradient descent with high probabiliy, provided that the dimension of the hidden layer is large enough. My main concern is that I find the presentation somewhat lacking at this point. In particular, as detailed below, I would appreciate a discussion of overparametrization vs model complexity. Finally, I believe that some additional guidance in interpreting the results of the theorem would be helpful. In particular, the comparison of the setting of Theorems 2.1 and 2.1 should be more prominent since the difference between these two result is what really captures the relationship to GANs, as opposed to arbitrary RELU models. In particular, how is it different from model complexity. It is not clear to me why Figure 1 should concern "overparametrization" as opposed to model complexity? Similarly, a key element the lower bound on $k$ in the theorem seems to be that ensures that minimization objective has optimal value of zero, that is the "data" can be reproduced exactly. This again seems to me to rather be about model complexity than about overparametrization of the model. Looking at the proof it seems that the concentration argument leading to a well conditioned Jacobian does not depend on model being perfectly able to represent the "training data". "The networks are optimized using Gradient Descent/Ascent (GDA) to reach a saddle point of the min max optimization problem." One of the key factors that has contributed to the successful training of GANs is model overparameterization. By increasing the complexity of discriminator and generator networks, both indepth and width, recent papers show that GANs can achieve photo realistic image and video synthesis (Brock et al., 2019; Clark et al., 2019; Karras et al., 2019)." This seems a bit misleading since at least the Karras et al.work suggests much more specific modifications than just increasing the complexity. "In particular, it has been empirically observed (as we also demonstratein this paper) that when the generator/discriminator contain a large number of parameters (i.e.are sufficiently over parameterized) GDA does indeed find (near) globally optimal solutions. In this section we wish to demystify this phenomenon from a theoretical perspective." minor points:  "very simpler"  > much simpler After reading the author s response and the other reviews I still lean slightly towards acceptance and have therefore left my rating unchanged. While not being an expert on the subject, I find the work interesting. In case the paper gets rejected, I recommend to the authors to the feedback provided by the referees to clarify the narrative of the paper.<BRK>In this paper, the authors proposed to analyze the over parameterization in GANs optimized by the alternative gradient descent method. Specifically, considering a GAN with an over parameterized G and a linear D, the authors proposed the theorem 2.1 to provide a theoretical convergence rate in GAN’s training. However, this paper is problematic. The details are as follows:1. The gap between the paper’s title and theoretical claims. Thus, the generalization bound between training error and the test error is the main concern in this topic. However, in this paper, the authors mainly focus on the convergence rate during the training. I think this topic is more correlated to the non convex optimization problem, rather than the over parameterization problem. First, what is the data distribution? In the whole Sec 2, there is no detailed explanation about the data distribution, which is one of the most important parts of the analysis. Though we assume that the data is Gaussian, simply minimizing the distance between the mean of data and the ones of G is not enough: the variance in the data is not taken into consideration. In Theorem 2.1, why $V$ is not optimized in Eqn. In Theorem 2.3, why $f$ is a general mapping? In GANs, it should be a parametric mapping from z to x. However, it is still restricted to a linear discriminator. Clarity needs to be improved. In Fig.4, with smaller k, G’s capacity is not sufficient to capture the data distribution. Further, the paper does not provide any bound on the gap between training error and test error. [*2] also uses control theory to understand GAN’s training and [*3] adopts control theory to improve the training dynamics of deep models. [*1] Farnia, Farzan, and Asuman Ozdaglar. arXiv preprint arXiv:2002.09124 (2020). [*3] An, Wangpeng, et al."A PID controller approach for stochastic optimization of deep networks."
Accept (Poster). rating score: 6. rating score: 6. rating score: 6. rating score: 5. <BRK>The paper highlights a problem in existing goal reaching RL agents, in that they do not explicitly allow for trading off speed (how fast you reach the goal) and reliability (how often you reach the goal). How does C learning relate and compare to this existing technique? I can imagine a number of other approaches which may perform better, worse, or about the same. The experiments appear to have been performed carefully, with a helpful demonstration of the trade off enabled by the proposed algorithm.<BRK>### SummaryThe paper proposes learning horizon aware goal conditioned policies by integrating horizon dependency in value based methods. This allows trading off speed and reliability. ### Main contributions  The C learning is a different approach to integrate temporal abstractions in value based method. It is acknowledged that this requires "some knowledge of the environment, for example a metric over states".<BRK># Update after the rebuttalThank you for including the comparison to TDMs   the results are now much more convincing. I think this should be one of the main results in the paper (which should be presented in the main text rather than in the appendix). I increased the score assuming that the authors will reflect my final suggestions. # Summary This paper proposes cumulative accessibility functions that estimate horizon aware value function for goal reaching RL problems. The TD model has almost the same Bellman equation as Equation (3) and it can also trade off reliability and speed at test time.<BRK>The paper proposes C learning, which is an essentially a horizon aware Q learning. The authors evaluated the performance of the proposed method on multiple hand crafted tasks which allow for multiple way of reaching to the goal with a varying hard time limit. 2) Comparison with existing goal reaching methods. As mentioned by the authors, the reachability problem can be formulated as in an existing RL framework and therefore the proposed method can also be compared with existing RL methods. For example Fig 7 has incomplete curves, the metrics are not clearly defined and it is not clear how the baselines have been evaluated. I should mention that the website includes more realistic tasks and addresses this problem to some sense but the paper in its current form lacks.
Accept (Spotlight). rating score: 8. rating score: 7. rating score: 6. <BRK>The authors focus on the aspect of optimizing the architecture parameters to overcome the criticism of weight sharing methods. The author s method relies on the mirror descent supporting their methods with a theoretical guarantee for the fast convergence. Furthermore, the GAEA can easily be applied to existing NAS methods which I believe making this work more valuable. Overall, I recommend clear acceptance. This paper will provide new insights/perspective to NAS algorithms which adopts weight sharing methods.<BRK>DARTS is a Neural Architecture Search algorithm which aims to find the most accurate network architecture within a human defined search space. Providing numbers for the authors  reproduction of PC DARTS would strengthen these results. * Strong empirical results for some of the NASBench 201 experiments. * Paper appears to provide solid baselines for most experiments. This needs to be better explained. * Unclear presentation of theoretical results. (Part of the problem might   as the authors suggest   be due to limited headroom.) On the plus side: the authors appear to provide direct comparisons between DARTS based NAS algorithms with and without their proposed GAEA modifications (although this requires some confirmation of details which are currently missing from the paper).<BRK>Pros:1.This paper gives a proof of finite time convergence, which is the first paper working on this. This is a new perspective of NAS methods. 3.The experiment results show the efficiency and effectiveness of the proposed method. Cons:1.Using EG to update architecture parameters can only accelerate convergence, which has nothing to do with improving the NAS performance.
Reject. rating score: 4. rating score: 4. rating score: 5. rating score: 5. <BRK>The paper proposes new regularizers for obtaining adversarially robust models, inspired by the "manifold assumption" that data lies on a low dimensional manifold. This makes lean towards rejection, and I encourage the authors to better motivate the proposed regularizers and further compare to previous works in theory and in practice. why were these particular baselines chosen? ** update after rebuttal **Thank you for the detailed clarifications. I still find that the recommended method used in practice, which only encourages robustness to local random perturbations, is too disconnected from the motivation of manifold regularization, and will thus keep my score.<BRK>The author introduced a regularization based robust training method that can perform well on CIFAR 10. 2.The comparison in the experiment section contains many different aspects of robust training. 2.The biggest concern comes from the perturbation part as well. So I doubt the local perturbation based estimation will not recover the actual tangent vector, etc., of the manifold. 3.The benefit of this method is not well addressed. Some more discussion on the benefit as well as the compromising will be appreciated. The (3) (6) can be combined and the most important equation (section 5.4) contains no numbering. I would recommend the author to number those important equations. 3.The equation in section 5.4, if expanded, it would be a norm on $||\cdot||_2$ as well as $||\cdot||_H$, which is also the robustness argument this paper is made.<BRK>*** Post response comments ***Thanks for the detailed responses and clarifications, especially regarding the manifold used. The proposed method is evaluated on CIFAR 10 under $\ell_2$ and $\ell_{\infty}$ ball attacks and shown to be state of the art in terms of verifiable robustness to $\ell_{\infty}$ attacks at $\epsilon   8/255$. I also agree with the other reviewers that the overall method as implemented seems a bit disconnected from the core idea of manifold regularization, and rather appears to be a variant of stability training (Zheng et al.2016).As such, I will be keeping my original rating; nonetheless, I want to again thank the authors for the interesting and vigorous discussion.<BRK>Then, the authors propose to neglect the correlation terms between different samples and to assume that the affinity matrix is diagonal   they however employ some "ghost" samples for computing the affinity. Indeed, the natural setting of the stability is along the manifold, where any points outside the manifold is an outlier: this leads to a different notion of manifold. In my current understanding of the paper, the data manifold isn t really used and the method remains local, and employs solely the Euclidean metric, which sounds paradoxical given the promises of the paper: if two data points are in the same neighborhood, this information can *not* be used by the current algorithm. This is not introduced. The notion of regularity which is used here doesn t introduce more complex smoothness that what is used traditionally in the litterature of adversarial examples for neural network: it s almost purely an additive perturbation that relies on an euclidean distance. Indeed, if the data were sampled from the same low dimensional manifold, then this assumption would be too strong.
Reject. rating score: 4. rating score: 5. rating score: 6. rating score: 6. <BRK>The paper studies the embodied agent problem. The paper provides a self supervised approach to learn environment level representations for embodied agents. The problem studied in the paper is an important problem. The approach of adding signals is reasonable. The motivation of using self supervised learning seems to be clear but not very strong for this problem. What is the policy for navigation tasks and what is the setup?<BRK>**Summary**The paper proposes a self supervised approach for learning environment level representations for embodied agents. The idea is that agents collect images and their corresponding poses during a walk through phase. Using contrastive learning, the model is trained to distinguish the features of an unseen zone from the rest of the zones. This should be clarified in the rebuttal. The rebuttal does not provide that. Several previous embodied representation learning works are outperformed by simple image level baselines.<BRK>This paper presents a self supervised method to learn environment level representations for embodied agents. The representations are learned using a zone prediction task. The authors have done an incredible job of reviewing related work comprehensively and discussing the proposed approach in the context of prior work. Weaknesses:  I believe the main weakness of the method is the requirement of depth and pose as input during the self supervised pretraining part. This is counter intuitive, the representations should be tested on a different task such as object goal or image goal navigation.<BRK>#############################################################################################Summary:This paper proposes Environment Predictive Coding which leverages predictable information from video trajectories of egocentric movements to learn the environment features in a self supervised manner. Learning the predictive features from the environment is an interesting idea for using self supervised signals from the video trajectories. The approach assumes training data of video trajectories from other agents in the environment. What is the task or reward these agents use? #############################################################################################Recommendation and Explanation:I recommend acceptance since the proposed self supervised task is interesting and could be useful for the embodiment navigation community.
Reject. rating score: 3. rating score: 5. rating score: 5. <BRK>Summary:This paper studies the detection and recovery problem in spiked tensor models in the form T   \beta v0^\otimes k + Z, where v0 is the underlying spike signal and Z is a Gaussian noise. The authors claim that: 1) they "build tractable algorithms with polynomial complexity", "a detection algorithm linear in time"; 2) the algorithms are very suitable for parallel architectures; 3) an improvement of the state of the art for the symmetric tensor PCA experimentally. Minor comments:  Page 2 Notations: typeface of v is not consistent. I am not able to follow the proofs in this paper due to missing definitions of terms and notations. Also some claims are not proved. Cons:   The readability of this paper severely suffers from its writing. This becomes worse considering the fact that this paper studies tensor problems   many tensor related terms have multiple definitions (e.g., eigenvalues, ranks). Is this the same as the first O(n)? End of Page 3: how is \mathcal{G} related to trace invariants formally? Section 2.2: this is not clear. What are the matrices here? The authors claim "polynomial complexity" at the beginning of the paper, but it is never proved.<BRK>The problem consists of recovering a (single spike/multiple orthogonal spikes) tensor corrupted by a Gaussian noise tensor. The authors proposed a new algorithm which allows recovering a signal for a sufficiently small signal to noise ratio. The paper takes an interesting question about tensor PCA and proposes a promising approach to solve it based on the trace invariants. For me, the problem is encouraging, while I would appreciate a discussion about possible machine learning/AI applications (learning latent variable models?anything else?) Unfortunately, I am not working in this area and probably not familiar with recent results ##########################################################################Cons:  1. Applications for ML/AI/Language processing are not very clear for me, and I would appreciate a discussion on this in the paper. I would highly appreciate having more experiments on real data (if any) and a detailed comparison of the methods in terms of accuracy/memory/time.<BRK>The paper presents a pair of interesting algorithms using trace invariants to detect the signal in the signal plus noise tensor PCA framework. The algorithms function by considering cutting an edge in the graph representation of the trace invariant, yielding a matrix whose leading eigenvector provides a (up to a rotation) estimate of the signal vector $v$. This algorithm appears to be very interesting and works well in a series of simulations. Unfortunately, the presentation of the paper makes it very difficult to assess the importance of the contribution. What is the variance of a graph (as in Theorem 4)? The proof sketch of this theorem is very hard to follow. If $\alpha>\sigma(I^{(N)}(T))$ then a spike is detected? Both algorithms are only presented for 3 way tensors, but the Theoretical claims are for higher order tensors? This is a finite $\beta_{det}$ result, with a claim only holding in the limit. In any event, I am confused about the summands. I do not see why the all $Z$ sum would have a $\beta$, while the cross terms would not. In the experiments, important details are left out.
Reject. rating score: 5. rating score: 6. rating score: 6. rating score: 6. <BRK>A noticeable feature of the paper is that it investigates emergent communication in the case of non uniform distribution of intents and costly communication (i.e.agents are penalized for effort). *** Clarity ***The paper is, in general, clearly written and is a pleasure to read. Figure 4 is deeply confusing, however. *** Quality ***The quality of the experimental support is, unfortunately, the weakest part of the contribution. The authors introduce a number of exciting ideas, all of which make the communication learning setting more realistic. Specifically, the authors are studying communication in an embodied setting, Zipfian intent distribution, and the idea of effort based action cost. Torque curriculum makes the results very specific to the considered setting. Moreover, the agent population considered in these experiments is 10 (mentioned in the appendix). With such a small population, it is not surprising that the observer can simply memorize the individual patterns, and is not pressured to infer the underlying structure. It seems very natural, therefore, to increase the number of agents in the simulation. Moreover, some of the limitations I describe in the "technical soundness" section could have been controlled for in a simpler setting. Overall, all things considered, it s not clear what are the insights gained from the embodied approach. For example, for any action with a certain energy level, there is a mirror action (with all joint torques multiplied by  1) with exactly the same energy consumption. This raises a question of why then we see good performance in 2 action scenario. Rare intents are sampled less frequently, therefore the model has less time to optimize its actions for that intent. Lastly, the architecture choice and problem setting formalization also raise certain concerns. *** Conclusion ***The paper addresses a highly relevant problem and proposes a number of interesting ideas. As authors mention, using an embodied setting with multidimentional continuous action and observation spaces makes the problem extremely challenging. In particular, the choice of a feedforward architecture seems extremely limiting, when combined with the state description that the authors used. The state space for the policy (sender) agent only includes the current joint configuration, without the history. Alternatively, it is possible to "boost" the embodiment part of the paper, e.g.by pre training the agents on some other tasks, thus making certain actions more natural, etc. *** Update after rebuttal ***I have read the rebuttal and I deeply appreciate the detailed response by the authors. I must clarify that I fully agree that studying how embodiment affects cognition/behavior is an extremely important and exciting area of research. I think that additional experiments that the authors introduced help to strengthen this point, although more experiments could still be beneficial (e.g.systematically varying the population size), as well as a more thorough theoretical discussion. The main point is truly interesting, however, which makes the paper borderline. Overall, I believe that the paper is extremely promising and I would love to see an expanded version published. I feel very torn about the decision, but at the moment, I believe that the paper is still below the acceptance threshold, although only marginally. I am happy to adjust the score up, and I regret that I can not switch it to an "accept" recommendation.<BRK>##########################################################################Summary:The paper deals with agents that communicate non verbally via actuating their joints in a 3D environment. Furthermore, the authors find that the current training approaches are brittle, and they propose and evaluate approaches to address this challenge. ##########################################################################Reasons:Overall, I vote for accepting. The authors develop and evaluate an algorithm that allows for emergent communication which generalizes to novel partners. ##########################################################################Pros:* Novel algorithm to perform emergent communication via joint actuations* Algorithm allows for generalization to novel partners* Suggestions to improve training stability##########################################################################Cons:* It would be desirable to train the agents N times with different initializations and report mean and std of the performance* More experiments and evaluation results with various joint numbers, intents, potentially different communication strategies would be helpful ##########################################################################Misc:The text in Figure 1 is too small.<BRK>This paper studies how gesture based (non verbal) communication can emerge in embodied multi agent population. In their problem setting, there is a set of agents, and an observer. Given an intent sampled from a Zipf distribution, the agents generate energy efficient trajectories, and the observer predicts the intent given these trajectories. The authors evaluate zero shot coordination performance by evaluating a third party observer trained and tested on two separate subsets of agents from the population respectively. Pros: 1.The idea of using energy cost and combining energy cost with a non uniform distribution for emergent communication is new. Cons:1.The paper is sometimes hard to follow with missing details. 3.The assumption of a monotonic ordering for intent distribution is strong. The assumption that energy cost is available during training/testing is also strong. However, is energy cost the only option here? I understand energy cost may be a better option, but the authors should discuss and compare with other options. Finally, even if the learned policy achieved zero shot coordination with a third party observer, it is not human interpretable since humans cannot directly observe energy cost. It is better to have a more detailed figures explaining your problem setting and motivation. The assumption that the intent is from a monotonic distribution seems a little bit strong to me. e.g.a subset of intents has equal probabilities to be selected. So the number of steps per trajectory is fixed? Why?In 5.1 where are the "unseen observer agents in the population" come from? My understanding is that there is one observer and a set of agents during the training. Algorithm 1 in A.2 is not clear. For example, Line 2 says "Let A< population of agents". However, "A" never showed up again in the remaining of the algorithm; The term discriminator (line 3), receiver (line 5), and observer (line 15) are used interchangeably in Algorithm 1.<BRK>Different from most papers on emergent communication. this paper uses the motion of three joint agents. The agents meet partners that they have never seen in the training phase, presenting the challenge of the universal protocol. To make a universal protocol possible, the authors study intents sampled from Zipf distribution and energy regulation. The authors conducted experiments on tasks with 2, 5, 10 intents. To achieve better than chance accuracy on tasks with 10 intents, a torque curriculum is needed. Pros:1.The setting of ZS coordination with non verbal emergent communication is novel and interesting. The results also support the intuition. Cons:What is the aim of using motion as a communication protocol? In human communication, actions are often used as an iconic gesture to describe intents that are not well described by words. In other words, a grounded world should be used to study ZS coordination. This paper seems to study a problem that is not realistic.
Accept (Poster). rating score: 7. rating score: 7. rating score: 5. <BRK>I believe the paper should be accepted and would be a nice contribution to the current research. The paper proposes a novel approach for post hoc calibration of outputs of the neural networks to estimate uncertainty of its prediction.<BRK>The phrasing does even consider any other possibility, and some of the algorithms seem to be quite specific to this setup, requiring significant changes in the “real world” case where test classes are not equally distributed. This paper proposes an information maximization binning scheme for calibration.<BRK>The paper is well written and the authors provide enough motivation and intuition of why maximizing the mutual information between labels and quantized logits would help multi class calibration. There are some concerns and issues that I think needs to be addressed.
Reject. rating score: 4. rating score: 4. rating score: 6. rating score: 6. <BRK>In theoretical analysis, the authors conclude with an Gaussian assumption on the data distribution. Gaussian condition on dataset is very unrealistic, and its theoretical analysis should only be considered as dealing with a toy model. Authors should remove this Gaussian assumption, or at least give strong reasons in plain natural language why Gaussianity can represent natural datasets in the adversarial study.<BRK>The paper proposes a two stage defense method to improve the adversarial robustness over different perturbation types. 2.The proposed idea is interesting. It clearly caused the gradient making problem in the adversarial attack later on to test the robustness. The gradient is blocked before reaching into the binary classifier so that the adversarial attack fails, which I think it is not truly improving the model s robustness. 3.Also, the assumption that different norm adversarial examples could be clearly separable might be wrong.<BRK>This paper proposes an ensemble approach to deal with multiple perturbation types. The underlying idea is to train a robust classifier for each perturbation type (i.e., l1, l2, and l inf) and choose a model to predict based on the decision of a perturbation classifier which is trained to distinguish perturbation types. You did mention this work, but not compare it in experiments. 4.I recommend testing Projector against the attack over a uniform average of Mp in addition to what is doing.<BRK>Summary:This paper proposed a pipeline method which first classify the attack type and then choose the proper predictor for that type. It will be good if it can be extended. The proof part is clear and natural. Cons:(1) The method and analysis only apply to the Lp attack type.
Reject. rating score: 3. rating score: 3. rating score: 4. rating score: 5. <BRK>The authors extend a hierarchical clustering algorithm from prior work for this purpose. The proposed TreeMAML algorithm is conceptually simple. Dissecting Learned to look at the tree structure it proposes would help diagnose this issue. The comparison between TreeMAML and MAML might not be very fair. #### Recommendation  I currently recommend a clear reject (3). Just in the first paragraph: inappropriate capitalization of multi task and meta learning; "The field of of".<BRK>The submission proposes a meta learning algorithm attuned to the hierarchical structure of a dataset of tasks. ##### Weaknesses:1) **Significance**: Since the evidence provided in favor of the proposed algorithm is in the form of an empirical evaluation on a synthetically generated dataset, the present impact of the algorithm is limited. In particular, there is no evidence that (i) the algorithm works for larger and/or more complex datasets; and (ii) that natural datasets of interest to the community exhibit a hierarchical structure analogous to the synthetic datasets presented in the submission.<BRK>This paper proposed a learning algorithm of meta learning: TreeMAML, to share information across tasks in meta learning models. The paper compared the results of TreeMAML with MAML and the Baseline on SinusRegression Task and Linear Regression Task. And the model structure presented in figure 1 is similar to the model framework in Yao’s paper.<BRK>##########################################################################Summary:The paper studies a simple modification of MAML to address the problem of meta learning hierarchicaltask distributions. The authors refer to [2] in the section 2 of the paper and mention that it is not task agnostic. ##########################################################################Reasons for score:  Overall, I vote for rejecting this paper. Lack of extensive experiments:a) The paper does not include any experiment or discussion comparing the proposed method with HSML[2] or other exist method.
Reject. rating score: 4. rating score: 6. rating score: 6. rating score: 7. <BRK>Assessment:Whilst I think the submission tackles an important problem that could be of interest to the ICLR community, the novelty and experimental evaluation are limited and thus I do not recommend acceptance. Below are some of my concerns/questions and I appreciate the response from the authors. 1.The key contribution is the paper is the combination of the prediction interval loss and the prediction loss. Each of these losses are not new, for example: the prediction interval loss has been considered by Pearce et al (2018). 3.The contribution of the paper includes the output head architecture and the loss function, so I’d not call this a deep neural network.<BRK>+ The paper is clear and well written. + The choice of how to produce the point prediction is well motivated and backed up by ablation experiments. It seems like the value should not be identical between the different experiment arms. It would have been nice to see an evaluation under dataset shift, similar to the "Flight Delays" experiment in https://arxiv.org/pdf/2007.05864.pdfOverall, this seems like a simple and valuable technique that addresses an important problem, but its applicability is somewhat limited by the fact that it produces bounds rather than a distribution.<BRK>The authors propose a method for predicting a prediction interval. The are several things I like about this paper:  The paper is written quite clear   Uncertainty calibration is an important topic. From this quantiles and thus prediction intervals can be derived on top of an "exact value prediction" which would be the mean. it is thus similar to a Softmax in classification or  a neural network that predicts both mean and variance (as in  (Lakshminarayanan et al., 2017)). Indeed as Section 4 show,  the model does not contain any parametric uncertainty. Overall,  the improvements appear  to be only marginal improvements.<BRK>**Quality and Clarity**While the overall message of the paper is clear, the explanation of the method in Section 4 is a bit hard to follow. **Originality and Significance**The authors seek to jointly address the problem of making accurate predictions and generating tight prediction intervals by designing a new loss function that combines these two goals. It does not seem to have been used in any of the experiments which is fine since a single model seems to be doing well. 4.What is the difference between the MOI variant of PIVEN and the QD baseline? Regarding Query 3, I appreciate the addition of the Deep Ensemble results though I find that the text of Section 5.5 has not been changed to reflect the same.
Reject. rating score: 5. rating score: 6. rating score: 7. rating score: 7. <BRK>**Summary**The paper introduces two simple modules, SelfNorm (SN) and CrossNorm (CN), that are highly modular and can theoretically be attached to different parts of the CNNs to control the balance between style and content cues for their recognition. The authors argue that SN then learns to emphasize _important styles_ and suppresses less important ones, with the underlying assumption that the first and second order statistics are often sufficient and necessary representations for style. This leads to diversified virtual styles in the training data, effectively factoring out the model s dependence on style cues for recognition, according to the authors. And I suspect if the chaos is intended, for downplaying the discouraging side of the results. No.**First of all, we need a decent, if not mathematical, definition of the four terms above. Under this terminology, I find it hard to agree that texture is not content. **It is difficult to agree that SN and CN, which are argued to control style and content for the benefit of the recognition task at hand, are really working as speculated. 3.**Introduce order in the experimental reports. Please aim to improve the paper in the rebuttals and paper revisions. For example, why is ImageNet performance for SN and CN not compared against augmentation baselines considered in the CIFAR experiments in Tab1? What are the individual performances of CN and SN for CIFAR on those 4 architectures in Tab1? What is the effect of location for SN in a CNN (equivalent analysis for CN is presented in Tab4)?<BRK>**Summary**This paper proposes two novel norms: Selfnorm and Crossnorm for model robustness. Their method shows state of the art robustness performance on both fully and semi supervised settings, and classification and segmentation tasks. The proposed method is domain agnostic and can be applied to different settings and tasks. **Weaknesses**  SelfNorm is a learned normalization, thus I think the statement "they take opposite actions ... at separate stages testing v.s. Should SelfNorm be used in both testing and training? In the experiments, both SelfNorm and CrossNorm units are placed in a ResNet block. It s not clear in the paper. It is better to keep the original formula (as shown in Figure 1(a)) in Eq.2 as well so that it is easy for readers to understand. The related residual channel attention (CA) [a] should be discussed in this paper. The authors are suggested to give some discussion on the difference between the CA mechanism and proposed Norms. CA is also a unit that is inserted in the ResNet block so that it s interesting to have an experimental comparison as well.<BRK>1.SummaryThe paper presents two new methods to improve corruption robustness and domain generalization: SelfNorm, a way to adapt style information during inference, and CrossNorm, a simple data augmentation technique diversifying image style in feature space. This is great as both tasks can be seen as two sides of the same metal and it is great to see more methods testing on both. distract rather than add to the story. The ablation study was hard to follow and tbh I think it could have been shortened presenting only the most important results in the paper and the rest in the appendix. The method is very simple and I think that is a major strength. It should thus also be possible to present it in a more concise form. 4.Recommendation As it is right now I think the paper has to be rejected because the write up is just too chaotic and vague. I do however like the method a lot and I would vote for accept if it was rewritten substantially to focus more on an understandable presentation of the method than abstract concepts and colorful terms. 6.Additional feedback   Figures 1 and 2 are a bit hard to understand.<BRK>This paper investigates the CNN model robustness against problems, e.g., texture sensitivity and bias. In particular, this paper proposes to recalibrate style using SelfNorm motivated by the fact that attention help emphasize essential styles and suppress trivial ones and reduce texture bias using CrossNorm by swapping feature maps within one instance. The intuition of the proposed method is clearly presented and the paper is well structured. The idea is somewhat novel for me. The statement cannot clearly illustrates the process in training and testing. I suppose both SelfNorm and CrossNorm would be used during training to get a trade off point and only SelfNorm is used during testing. It is encourage to add an algorithm flowchart to indicate the process. The authors explains SelfNorm recalibrate feature style while  CrossNorm performs style augmentation. The experiment results of SNCN are not significant compared with AugMix though their combination achieves new state of the art.
Accept (Poster). rating score: 9. rating score: 7. rating score: 5. <BRK>This paper addresses probabilistic data driven model calibration, i.e.aligning predicted target probabilities with actual ones. I honestly do not have much critic to address to this work which seems to have reached a level of maturity perfectly adapted for publication in ICLR. It is this re definition of the problem that allow them to encompass prior arts as special cases. This problem has been extensively studied for classification tasks but solutions for regression tasks have limitations as illustrated by Fig.1.<BRK>Summary:The authors present an approach for testing calibration in conditional probability estimation models. They develop an MMD kernel estimator and expand on practical choices of kernels that are computationally tractable. Review:This is an excellently written paper. The intro and first few chapters are a joy to read and really explain the problem well. However, I think the authors did a good job explaining the challenges of this extension. The resulting estimators are now applicable to many more problems than the existing work. My main issue comes with the lack of empirical studies. It leads me to believe that maybe this is not that useful of a method, since the authors did not have anywhere that they could apply it to and derive meaningful insights or uses. Overall, I like the paper.<BRK>I would have liked to see more convincing experimental evidence of the marginal benefit of this approach beyond common calibration metrics. It is not clear, from my limited background knowledge in the related work, why it should be helpful to introduce a dummy variable and then integrate it out. The novelty of this work seems to lie mostly in its applicability beyond these types problems: the authors write, "A key contribution of this paper is a new framework that is applicable to multivariate regression, as well as... discrete ordinal or more complex (e.g., graph structured) [output]," so I venture a guess that the intended audience for this work is relatively small. I think some reference to kernels or RKHSs should be made in the title or the abstract.
Reject. rating score: 5. rating score: 5. rating score: 7. rating score: 7. <BRK>To supplement this claim, the authors evaluate GNNs with RNI and higher order GNNs over a carefully constructed synthetic dataset and show that RNI (even if only a fraction of the nodes are randomly initialized) are as expressive as higher order GNNs. Overall, I find the topic and the findings of the paper quite interesting. There is also room for improvement in the presentation and writing. To my understanding, this is the only place where the randomness of the initialization is used. In the partial random node initialization (GCN x% RNI), are you initializing some nodes randomly and other deterministically?<BRK>This paper studies the power of message passing neural networks (MPNNs) with random node initialization (RNI). The results on these two datasets show the merit of adding RNI to MPNNs. In fact, if [1] didn t exist, I would have given a strong acceptance to this paper. All mentioned about the connection between this work and [1] is “Indeed, RNI has enabled MPNNs to distinguish instances that 1 WL cannot distinguish, and is proven to enable better approximation of a class of combinatorial problems (Sato et al., 2020). However, the effect of RNI on the expressive power of GNNs has not yet been comprehensively studied, and its impact on the inductive capacity and learning ability of GNNs remains unclear.”, which does not seem satisfactory.<BRK>In a way, the universality comes from the network not taking into account the node features due to the full randomness but more of a statistical behavior and the fact that the nodes are completely distinguishable. **Post Rebuttal**I thank the authors for the quick replies and updates to the paper. The partial RNI is not well explained in the paper, and it was not clear whether the partial applies to the feature dimensions or the nodes. I keep my positive score.<BRK>The paper study the effects of adding random features (RF) to graph neural networks (GNN). Next, a novel dataset is defined that is aimed at evaluating the performance of models that have high expressive power. Finally, several experiments show that adding RF performs well on the proposed dataset. I think the paper is well written and well organized. If this is true, then I think it is important to point that out in the main text.
Accept (Poster). rating score: 7. rating score: 7. rating score: 7. rating score: 6. <BRK>The solution proposed is the combination of an RNN (LSTM) and Fast Weighted Memory (FWM). Math derivations and experimental results seem sound. I m inclined to accepting this paper.<BRK>This looks like an interesting paper with an original proposal. [2] Ba, et al, NeurIPS 2016, Using fast weights to attend to the recent past, arXiv:1610.06258. Dense associative memory for pattern recognition, arXiv:1606.01164. The main problem that I am having is with the proposed network, specifically equation 3. Could the authors please clarify this?<BRK>This paper presents a new method called Fast Weights Memory (FWM) to add an associative memory to an LSTM. The authors propose the new task "catbAbI", a variation of the existing task "bAbI". * good results meta reinforcement learning for POMDPs compared to LSTMs.<BRK>That said, I believe this is a promising line of research and encourage the authors to try to address the issues raised. That said the overall form of the model doesn t seem fundamentally different from what is proposed by Ba et al.(2016) who also used fast weights as a way to attend over past hidden states in combination with a "slow" weighted RNN trained via gradient descent optimization albeit some of the details differ. The authors evaluate the model over two separate datasets.
Reject. rating score: 4. rating score: 5. rating score: 5. rating score: 6. <BRK>The proposed method also lacks in depth technical/theoretical analysis; thus the paper novelty is limited. This paper proposes a deep neural network for Premature Ventricular Contraction (PVC) differentiation and segmentation from electrocardiogram (ECG) signals. Quantitative experiments show better performance than baselines on differentiation tasks. 3) For the evaluation of segmentation, only several cases of qualitative evaluations are not convincing.<BRK>This paper presents a method for segmentation and classification of ECG data applied to the task to segmenting and detecting Premature Ventricular Contractions (PVC). A couple of question I d like to hear the authors responses to:1) Why did they not do any experiments on these public datasets? However, I would argue that this is primarily an application paper. I d like to know the author s response to that and if Table 1 does show these results perhaps linking the rows to the previous approaches might be helpful?<BRK>Such choice will incur a positive bias with regards to the actual expected generalization error. The work is light on theory and the contribution mostly resides on the empirical improvement. However, the evidence for this improvement is not rock solid, as it is shown on a single dataset, which has a rather small sample size. Also, I fear that hyper parameters are not set fully independent of the final error measure.<BRK>The paper proposes a framework for the classification of arrhythmias in electrocardiogram (ECG) data. Main concern is that the proposed approach seems rather ad hoc: The combination of segmentation (or attention) and classification in a joint fashion seems hardly new and while the results obtained are good, there is no systematic evaluation how the method compares to other state of the art ECG classification methods. As a result, it is quite difficult to exactly assess what the authors have done or what they mean. • “We do not use the output of the segmenter L as the attention map directly but instead perform a pooling with large kernel size first” – Why is this done? What does “large kernel” mean?
Reject. rating score: 4. rating score: 4. rating score: 4. rating score: 7. <BRK>From this analysis, I feel that we might want to expect SimCLR training to be somewhat working but not fully done. Could the author explain it? I wonder whether the authors could give more visualization on the learned representation. I keep the score after reading the rebuttal / author comments. I thank the authors to conduct a lot of additional experiments. Since the authors also agree that "two nearby states can have very different representations", it doesn t make sense to treat nearby states as positive pairs (and use SimCLR) to learn the representation from the beginning, which contradicts the purpose of the entire paper. If the authors cannot show other benefits of this representation learning (other than the performance boost on a 1 2 tasks), then the performance gain could be just due to other reasons that are not known. The additional experiments (e.g., Appendix F) only compares Random with RIDE SimCLR, and I wonder what s the performance of RIDE? Overall the authors need to rethink the proposed approach in order to give a consistent story of what is a good representation for RL exploration.<BRK>In particular, the proposed method is a direct extension of an existing method [RIDE; 1], paired with advances in contrastive unsupervised representation learning [SimCLR; 2]. Unifying count based exploration and intrinsic motivation. Moreover, the authors the draw connections between their proposed method, RIDE SimCLR, and EC [3]. I find this formulation interesting and useful. This is trivial to calculate in MiniGrid (as it s given by default) but it is not accessible in more complicated settings (e.g., what about a procedurally generated robot arm manipulation task?). If authors use simulator state visitation/episodic count that means that they access privileged information and hence all the RIDE results from the original paper should be revisited since the comparison to the baselines (ICM, RND, Count) is unfair. Most notably, if in the NoisyTV task they use simulator state count then this is indifferent for RIDE and hence RIDE SimCLR. In International Conference on Learning Representations,2020.<BRK>Using procedurally generated enviuronments from MiniGrid, they go on to show superior performance of EC SimCLR over RIDE, RIDE SimCLR, and RIDE MinAgg, while they claim RIDE and RIDE SimCLR to have comparable performance. I see two key weaknesses:1) The set of environments and baselines is fairly narrow. it would be much more convincing if the authors compared their algorithm to a broader list of baselines on an environment for which RIDE did not show such favorable performance, given the similarity of their approach to that of RIDE s. I do not agree with the statement: "We note that MiniGrid provides a sufficiently challenging suite of tasks for RL agents despite its apparent simplicity, as ICM and RND fail to learn any effective policies forsome tasks due to the difficulty posed by procedurally generated environments." If there were clearer utilities demonstrated, it would be more convincing, but right now I find this emphasis confusing. I recommend rejection, though I am certainly open to changing my mind, especially if the case can be made that the benchmarks are exhaustive enough, or that it s unreasonable to expect more benchmarking within a single publication. I think I know what it is, but it is not "RIDE SimCLR with A   min" given that RIDE SimCLR has its own specific A.<BRK>The presentation was clear and organized, with the new method getting both better performance and some improvements in interpretability. However, I was unimpressed with the characterization of the RIDE method upon which this method was based. They document and explain their methods clearly, including hyperparameters. In contrast, are there any tasks RIDE would do better at than your approach, or does the "distance" focused metric work in a strictly superior way? I would have been more impressed by the paper if additional tasks such as this would have been included. It would be nice for the author s to suggest a contrast with RIDE SimCLR: it seems to me that it naturally does not provide a lot of intrinsic reward for actions but rather through movement away from where it has been, so it is natural that there is more reward for moving into new rooms rather than specifically on opening the door.
Accept (Oral). rating score: 9. rating score: 7. rating score: 7. <BRK># Pros and Cons The work addresses a key problem in reinforcement learning, which is learning effective policies from unlabeled visual images. I am confused. The paper also argues for the correctness of their approach using a contraction proof and a theoretical argument for generalization to new problem domains. Definition 2, the bisimulation metric contains a max … so this is worst case discrepancy between the futures between state space actions and empirical actions.<BRK>In Figure 4 the proposed approach is outperformed by contrastive learning in the default setting. The approach to use the state bisimulation metric to supervise observation representation is intuitive and clearly motivated. recommendation and reasoningThe paper is well written and presents a clear approach to a well motivated problem with strong evaluation results. I recommend acceptance.<BRK>**Summary**The paper focuses on how learning state representations that encode information relevant to the task can improve reinforcement learning from pixels. **Strengths**  The paper is generally well written and easy to follow for the most part. I will highlight minor comments / weaknesses which, I think if addressed, would definitely make the paper stronger.
Reject. rating score: 5. rating score: 5. rating score: 6. rating score: 7. <BRK>This paper suggests an interesting approach to knowledge distillation, which uses architectural properties rather than the loss function to encourage knowledge transfer between a teacher and a student. 2.Table 1: what is the "Baseline"? The method results in improvements in student performance compared to training the student without the teacher. The main positives I see in this paper are:+ modest improvements over baselines in the one stage KD setting+ provides a different perspective on KD compared to recent works that are based on loss functionsAlthough the results look promising, I think the paper is not yet ready for publication. The main issues I see are:  no comparisons against network pruning and compression methods, which are arguably more relevant than KD baselines  little insight or analysis of why the method works  concerns with the experiments, detailed belowThis paper positions itself as a KD method, and argues that a novelty is in doing KD via architectural tricks rather than via a loss function. My third concern is with the experiments. Second, many of the prior KD methods perform best when their objective is combined with the original Hinton KD objective (see Table 7 of Tian et al.2020), but this comparison is not provided in the current paper. These two concerns mean that I’m not sure the proposed method is really outperforming competitive baselines from prior work. Two stage approaches are useful when you are _given_ a big model, which maybe you do not have the resources or data to train, and want to compress it or adapt it, e.g.for mobile deployment. There are interesting tradeoffs between these two paradigms and one is not better than the other. I also think the paper overemphasizes how simple the method is. Abstract: “temporally” —> “temporarily”?<BRK>I think the idea is somewhat novel for the Explicit Connection Distillation, especially for cross network layer to layer gradients propagation. This paper proposes a new strategy of Knowledge distillation called  Explicit Connection Distillation (ECD), and the ECD achieving knowledge transfer via cross network layer to layer gradients propagation without considering conventional knowledge distillation losses. Experiments on popular image classification tasks show the proposed method is effective. ##########################################################################Cons: 1) The first concern is about motivation. (2): The drawback of one stage KD methods is a little bit overclaim,  Both ONE and DML can be applied to a variety of architecture. In my opinion, the teacher design of  ECD  follows a similar strategy with ONE and its variants, which is the teacher is wider than the student. Is the Dynamic additive convolution component used to in student network, does this will influence the comparison of  Table2, Does ONE and DML also use that? 3) Why the automatically generated teachers of ECD is much lower than other methods in Table2 in term of performance and results in higher student performance. Is there any explanation here, like such as [1][2]? 5) Some recently SOTA work is missed [3][4], although I know the performance of this paper is outperformed.<BRK>Given the architecture of a student network, the teacher network is constructed by replacing each convolutional layer with a dynamic additive convolutional layer. In this way, the teacher network is guaranteed to be more capable than the student network. Then, the teacher and student models are trained together, minimizing their own training losses. 2.By adding an additional distillation loss on the logits (the ECD* method), it achieved competitive distillation results. Cons:1.The authors claim that the gradient flow (from the teacher model to the student model) helps improving the student model. This is my greatest concern. The proposed method still need to train a teacher model. On the other hand, if the proposed method became popular, researchers may add auxiliary losses on top of the proposed method, and that would not seem like a drawback for the proposed method. See responses in this thread for reasons.<BRK>Summary:The paper proposes new KD framework, i.e., Explicit Connection Distillation (ECD), which unlike existing methods, designs teacher network that is well aligned with the student architecture and trains both the networks simultaneously using explicit dense feature connections. The proposed method is evaluated on CIFAR 100 and ImageNet datasets. So, the method overcomes the problem of selecting teacher network or alternatives of distillation losses for the task at hand. By design, the generated teacher network has features aligned with the student network at every layer. So, this is not a major limitation in existing methods. In the proposed ECD framework, both the teacher and student networks are trained simultaneously, so number of trainable parameters (teacher parameters + student parameters) would be large. Selecting an optimal value of kernel number ‘n’ is a concern. Table 4 shows shallow layers migrate more information than higher layers and dense connections are preferred on shallow layers only to get optimal performance. Queries for authors:  Any restriction or range of values that alpha can take? Is the performance of the teacher reported in Table 1, obtained through auxiliary teacher involving feature connections with the student network? While training using the proposed ECD, how to decide number of epochs for training (based on either teacher or student performance on validation data)? General Remarks:While the creation of auxiliary teacher directly from the student network removes its dependencies from pre trained teacher but dependency on several design choices like the number of dynamic additive convolutions for the first module and appropriate places for adding connection paths in the second module for explicit flow of layer to layer gradients remain.
Reject. rating score: 3. rating score: 3. rating score: 4. rating score: 7. <BRK>This paper proposes to use a deterministic classifier to replace the samplingprocess in randomized smoothing based certifiably robust models. So the entire procedure is not certified anymore. On the positive side, the proposed method may work as a good empirical defense,since the smoothed classifier can be learned quickly and can be more robustthan the original classifier.<BRK>The relation between randomized smoothing and PDE seems to be an interesting direction to explore.<BRK>But as far as I know, there are only pretrained models on MNIST and CIFAR 10 in their repository, could the authors provide an official link to this baseline? However,  there are mainstream certified training methods that are deterministic during inference (with a single query) [1][2][3][4], but are not discussed or empirically compared in this paper.<BRK>This paper address this problem. This paper points out an interesting direction for certifying robustness, the method is simple and effective. The paper is clearly written. 4.The speed for randomized smoothing certification is a major concern for the community.
Reject. rating score: 3. rating score: 4. rating score: 4. rating score: 4. <BRK>2.The second major issue of this paper is that the quality of the evaluation is quite weak. 2.The paper is well written and easy to understand. Cons:1.The biggest issue of this paper is the lack of novelty.<BRK>I maintain my score to rejection of this paper. Pros  The paper is generally well written and easy to follow.<BRK>However, the paper has important shortcomings   it has reduced novelty, there are important missing details, and the empirical evaluation is weak. Some of the other unjustified claims include "this model being the first application of VQVAE to video data" as discussed.<BRK>Although, this paper claims that the model is able to predict higher resolution video outputs, the quality of the prediction is not clearly evaluated in the paper. The multi level discrete latent variables help to predict higher resolution videos. *Limited evaluation and comparisons. 3.It is not easy to judge the output quality by just looking at the prediction in Figure 4.
Reject. rating score: 5. rating score: 6. rating score: 6. rating score: 6. <BRK>Maybe the analysis part in 4.1 is more suitable for the related work. Maybe, more understanding could be gained if there was an additional "tiny" model that would show that when going beyond a certain size, the performance degrade. Finally, in the light of many changes in the paper and the original request of all the reviewers to have the writing in the manuscript improved, I think the paper would benefit from another round of reviews. Besides, the contributions of this paper are not very clearly stated. The paper is quite hard to understand, it immediately jumps into details of the method in Section 3 without providing sufficient overview and intuition into what the method tries to achieve and what the issues with previous works are. Why is it possible to take average or max reward values of the reward map?<BRK>The proposed method is natural and makes use of a few existing works. 2.The visual reward maps show that the proposed method learns a more semantically meaningful reward map than VIN. There is little analysis of the learned reward maps beyond a few plots for visual comparisons. It would be interesting to analyze the differences between them and, as a bonus, provides a rule of thumb on which one to use. Please clarify a few terms in the paper: a) Embedded MDP in section 3.1 near the bottom of page 3b) What is the conditional VAE? Could the authors clarify whether it means Behavioral cloning (as in supervised learning with state action pairs from demonstrations) or VIN?<BRK>This paper addresses the problem of imitation learning. I think this paper is quite interesting overall, and the idea seemsworth pursuing. I know that the general approach to imitation learningof first learning a cost function from the demonstrations and thenapplying RL to optimize it has a very rich history, but I have notseen prior work that learns the cost function in the manner proposedin this paper. I do have some questions which I hope the authors can address in the rebuttal. 4.I am curious how important the use of both forward and inversemodels is in the reward learning module. The authors provide somejustification why both are needed, but it would be good to supportthis with an ablation experiment.<BRK>The paper demonstrates a novel usage of VIN like planning module for inverse RL and the result is quite compelling. Although the writing can and should be improved to help reader to understand what is going on, including multiple grammatical errors like " we introduce a variational solution to optimize the both submodules jointly". Since the policy is able to learn much better with the learned reward function, I will imagine it learns to take action that is completely different from the demonstration.
Reject. rating score: 4. rating score: 5. rating score: 6. rating score: 7. <BRK>The framework is interesting and the problem studied is important. However, this paper suffers from some major deficiencies. It is not clear how one can set the g^{max} in practice. Thus, this algorithm has limited applicability in practice. From the convergence result in Theorem 7, it seems like there is no improvement. Further, g^{max} needs to be set in a very specific way that is dependent on D and d, which is again not known in practice. So whether we can attain the convergence rate is unclear. 4) The authors acknowledge in their experiments that the results are sensitive to the scale of the discrete Gaussian noise. Hence, the privacy communication cost convergence rate tradeoff is not clearly delineated. How much is "a little bit"?<BRK>While it is claimed that the discrete Gaussian distribution is better than the binomial distribution, no in depth comparison is made. An advantage is that the communication cost can be reduced by stochastic quantization of the values to be transmitted. It is unclear whether the same stochastic quantization technique can be used for the classic Gaussian distribution. In conclusion, while the idea is interesting, I have some concerns with the presentation and the insufficiently motivated novelty (and the related lack of more in depth comparison with existing work). * The paper points to a few shortcomings in the literature, but exaggerates their importance. Firstly, with Binomial noise, the output of a learning algorithm would have different supports when any client participates or withdraws from learning", it may be true this is the case when literally considering the cpSGD paper, but such problem typically can be overcome easily.<BRK>This paper proposed a privacy preserving federated learning algorithm with efficient communication. Moreover, they apply the discrete Gaussian mechanism with Renyi DP analysis for privacy guarantee. The RDP analysis for discrete Gaussian mechanism is nice, which allows a tighter composition with analytical moments accountants. Note that constant matters in differential privacy. (1) In Figure 1, three methods use different quantization. Moreover, it would be much better to add a non private baseline without quantization. If not, how to set the noise scale for cpSGD and D2P FED? Is it possible to align both the privacy budget and communication cost?<BRK>##########################################################################Summary: The paper proposes the discrete Gaussian based differentially private federated learning algorithm to achieve both differential privacy and communication efficiency in federated learning. In particular, it adds discrete Gaussian noise into client updates and uses secure aggregation to prevent the server from observing the individual updates. The algorithm satisfies RDP and has lower communication cost compared to the previous method cpSGD. ##########################################################################Reasons for score:   I like this work. 1.The problem is critical in federated learning. The D2P Fed algorithm has nice performance on the trade off among privacy, utility, and communication cost. To employ secure aggregate, it s natural to use a mechanism with discrete support (or discretize that). But there is much more beyond a simple combination of the discrete Gaussian mechanism and secure aggregation. It is also confusing in Algorithm 1. It should be clearly defined.
Reject. rating score: 4. rating score: 6. rating score: 6. rating score: 6. <BRK>This paper proposed EMaQ, an approximation of Bellman operator, and thus yields a variant of deep Q learning algorithms in both online and offline settings. A key fact is that the proposed method can be directly got from BCQ by settings the perturbation parameter to zero. In general, the findings of this paper seem not surprising at all given BCQ and BEAR paper. 2.There is some ablation study on the choice of generative model for $\mu(a|s)$, the number of action to sample, etc. (2) The new algorithm is more closed the tabular settings. So does that yield stronger theoretical guarantees in function approximation settings? Why there is or is not a very different behavior of the algorithm? 3.A general problem of this paper is over claiming. The former two are also model free, and those numbers are all reported in the D4RL whitepaper. (2) "EMaQ provides a quite intuitive and surprising measure of the complexity for offline RL problems."<BRK>The paper proposes a simple offline RL algorithm based on Q learning that instead of computing the exact maximum over actions, takes the sample maximum when sampling from an learned estimate of the behavior policy. Experimental results are quite promising (competitive with previous algorithms in both offline and online settings). The authors demonstrate that utilizing autoregressive models significantly improves over simple VAEs when used to model the behavior policy in offline learning (both for the proposed method as well as prior work), highlighting the need to place more focus on how we model the behavior policy in offline RL. However, with how important the choice of model used to model the behavior policy appears to be, I am unsure as to how significant the new proposed backup operator is. While I feel the paper clearly highlights the importance of how we model the behavior $\mu(a\vert s)$, I don t see particularly strong evidence, either theoretically or empirically, why one should prefer using the EMaQ backup. If we simply match the Q values in the two MDPs, they appear to be equally complex from this measure, despite the fact that the bandit MDP is far simpler to solve.<BRK>This paper analyzes off policy algorithms using a sampling based maximization of the Q function over a behavior policy for both Bellman targets and for policy execution, introducing the “Expected Max Q learning” operator EMaQ for analysis. The paper then proposes to use more expressive autoregressive models (MADE) for learning the behavior policy from replay buffer data (either online or offline), which turns out to be very important, especially on harder tasks in the D4RL benchmark. The results are quite strong, matching or exceeding BEAR on D4RL while at the same time matching SAC for online learning. The highlight of the paper is results on both offline and online RL.<BRK>The paper establishes theoretical analysis of the proposed operator in convergence, sub optimality, etc. A practical algorithm is proposed based on previous techniques (an ensemble of Q functions) and a different generative model using an autoregressive architecture. Experiments demonstrate the effectiveness of EMaQ. Strong points: The paper studies the important problem of how to mitigate distribution shift in offline RL, and proposes a simple but effective algorithm. The performance of EMaQ strongly relates to how good \mu is learned. What if \mu is close to a deterministic policy (e.g., \mu is not learned well and is close to a deterministic policy)? How to guarantee this diversity? How are the computation time and learning curve (besides the final performance shown in the paper) of EMaQ compared with other methods?
Reject. rating score: 4. rating score: 4. rating score: 4. rating score: 4. <BRK>Experiments are conducted to demonstrate the effectiveness of the proposed algorithm compared with Bayesian optimization based methods. The paper paves a way for combining multi objective evolutionary computation and deep generative modelling, which could be a potential benefit to other areas in machine learning. **Presentation**The lack of details and clarity in the method section makes the paper hard to understand. Is regularizing the generative model the sole purpose of the property predictor? Combining deep generative modeling with evolutionary algorithms is a very interesting idea in general.<BRK>Summary: The paper proposes to tackle multi objective optimization of molecular properties by combining a genetic algorithm with a fragment based generative model of SMILES strings. Experiments show the model can produce a rich Pareto front of samples, outperforming bayesian optimization ran in the latent space of the same generative model. While combining evolutionary algorithms with deep molecular generative models is a very interesting research direction, the current draft uses a very limited set of metrics and baselines, making it unclear if the proposed method is comparable to the state of the art. Technical novelty of the paper is rather limited (combining several previously published ideas), although the particular combination does seem novel. Both modifications are rather standard, which raises expectations with respect to the experimental evaluation of the method.<BRK>The authors combine deep generative models and multi objective evolutionary computation for computer aided molecular design. In particular, they employ a variational autoencoder (FragVAE, as molecule modeler) and a multilayer perceptron (as property predictor). Evolutionary operations explore the latent space of the generative model to produce novel competitive molecules. The paper is interesting and tackle a very relevant problem. Why only multi objetive Bayesian optimization methods are included in the final comparison?<BRK>  quality/ clarity   needs work. not enough discussion of the more technically novel content, like the details of the multi objective optimization method. originality / significance   Draws on some interesting ideas from the evolutionary computation community on multi objective optimization. The Methods section does not clearly delineate what is prior work and what is a novel contribution. Overall, is my assessment correct that your work combines an existing encoder decoder model (FragVAE) with some methods from the evolution literature for multi objective optimization? Is the primary novelty to adapt these evolution methods to work in latent space? However, the paper s exposition and experiments are too focused on details of the FragVAE model and do not analyze the details of the multi objective optimization approach enough.
Reject. rating score: 4. rating score: 6. rating score: 6. rating score: 7. <BRK>The main empirical claims are that (1) class sensitivity is negatively correlated with robustness to corruptions (2) class sensitivity is positively correlated with robustness to adversarial perturbations. For example the differences in the class selectivity curves in Figure 2 appear marginal. ##########################################################################Questions during rebuttal period: Please argue why the differences between class selective curves in Figure 2 are significant. The authors use the word "causal" several times in the paper, which appears to me dubious. I can find no justification for a claim of causality here, since the results are correlative. Can the authors clarify this? The authors only examine one dimensionality estimation method: the number of PCA components required to capture 95% of the data variance. However I lament the author s choice to use "worst case perturbation" and "average case perturbation" to refer to "adversarial attack"and "corruption". The literature on adversarial attacks is quite large at this point, and "adversarial" is the de facto terminology. I still have doubts over the claims in this paper.<BRK>This work investigates two classes of perturbation robustness, average case perturbations which are considered to be naturally occurring in image data, and worst case perturbations that are perturbations generated by an adversary. The authors find that decreasing class selectivity will increase robustness to average case perturbations while reducing robustness to worst case perturbations. While the experiments are thorough in showing a relationship between class selectivity and robustness, many of the observations do not map convincingly to the conclusions. This conclusion is not sufficiently supported by measuring robustness as a function of class selectivity. It also seems odd to hypothesize two different conclusions for opposite outcomes. The same issue of drawing a conclusion regarding the worst case perturbation analysis in section 4.2. Where do images from datasets like ObjectNet (https://objectnet.dev/) or Natural Adversarial Examples (https://arxiv.org/abs/1907.07174) fall in the average case to worst case dimension? As it currently stands, this paper appears to be a collection of observations in need of a clear conclusion. Other notes: It would be illustrative to have some visualization of the average case perturbations. $SI_u$ in Eq 3. should also be a function of $l$<BRK>As even the authors stated in the paper “We hesitate to generalize too broadly about our findings, as they are limited to CNNs trained on image classification tasks. pure empirical results and not enough theoretical justificationI understand that this is the first paper trying to connect average case robustness and worst case robustness with class selectivity but I still would like to see a bit more discussion on the theoretical side. the impact of the observations need more discussions. However, I think it needs a bit more discussion on how other researchers/practitioners can benefit from these findings. For example, how should one leverage such observations to detect adversarial / naturally perturbed error prone input , improve robustness, or find the optimal tradeoff between different measures (e.g.natural accuracy, average case robustness, worst case robustness. What’s the difference between “distributed semantic representations” and “sparse semantic representations“ in this sentence? ################################################Suggestions:The paper can be improved by addressing at least one of the three cons I mentioned. ################################################Post Rebuttal:Thanks the authors for their detailed responses! The authors responses addressed most of my concerns. the authors showed that their results generalize to Tiny ImageNet and ResNet50 with additional experiments. 2. the authors added some discussions on the theoretical side. Besides, I also appreciate the addition of experiments on AugMix and PGD which imply the bidirectional causality of  class selectivity and perturbation robustness.<BRK>The intriguing finding is that robustness to adversarial images is positively correlated with class selectivity. 3.If the conclusions in this paper holds, I think this is one step further from the well know accuracy robustness tradeoff [5,6]: not only do we have tradeoff between clean accuracy and adversarial accuracy [5,6], but also a (possibly inherent) tradeoff between average  and worst case adversarial accuracy. Cons:The general idea to use class selectivity as an indicator for model robustness in this paper is very interesting. But I do think more experiments and discussions are needed to explain these phenomenons, especially the different behaviors of average and worst case corruptions. If the conclusions in this paper holds, we would expect adversarially trained models to have much better class selectivity than normally trained models. 2.Similarly, there are some methods (e.g., AugMix [4]) improving model robustness to average case corruptions. 3.In my point of view, one possible explanation for the different behaviors between average case and worst case corruptions could be that normal adversarial images (generated by PGD, FGSM as in your paper) are out of distribution samples [3], while average case corruptions are likely to be on distribution. So I m a bit curious about the behaviors of the "on–manifold adversarial examples" defined in [3]: do they behave more like normal adversarial images (e.g., causing a larger increase in early layer dimensionality) or more like average case corruptions (causing a smaller increase in early layer dimensionality )? Update:Thanks the authors for their response. All my concerns are addressed and I decide to increase my score from 6 to 7.
Accept (Poster). rating score: 7. rating score: 6. rating score: 6. <BRK>Summary:This paper addresses the visual question answering in a multi turn or conversational setting. However, I seriously concern about the step (1) of the proposed method (page 1). (2) Secondly, a path generator is trained to predict a path from the current dialogue turn to past dialogue turns that provide additional and relevant cues to answer the current question.<BRK>This paper proposes creating a semantic graph connecting the multiple turns in a dialogue and subsequently learning reasoning paths in that graph to find the most relevant nodes for answering a given question in a dialogue context. * The proposed method is novel. This questions the generality of the proposed approach.<BRK>The paper studies the problem of video grounded multi turn QA and adopts reasoning paths to exploit dialogue information. A nice feature of the method is that the generated reasoning path can serve as extra explanations for the answer.
Reject. rating score: 5. rating score: 6. rating score: 6. rating score: 6. <BRK>Second, considering channel level dependencies in attention has limited novelty. Thanks!Second, I appreciate the extra experimental results and visualizations.<BRK>There are some unclear statements in the paper. Is each rank 1 tensor associated with the special regions of different objects?<BRK>However, it is not clear to me how this can faciliate the inference in Eq.9.<BRK>All the results demonstrate the outperformance.
Accept (Poster). rating score: 7. rating score: 6. rating score: 6. rating score: 6. <BRK>The paper is well written, the proposed method is clear, and the experiments are sufficient. This is not entirely true. The work of Parisi et al learns a parametrization producing infinitely many solutions at the same time using a specific loss, while Yang et al train a neural network which takes preferences over the objectives as input, and generalizes over them. Despite being tested only on MORL, these algorithms can easily be extended to MOO. In particular, the work of Yang et al is very reminiscent of your algorithm, since both include the preference vector in the network input. The evaluation is sufficient, but I would suggest to move the evaluation of evolutionary algorithms to the main section.<BRK>The high level idea is to learn the entire Pareto front simultaneously using a single hyper network, which receives as input the desired preference vector and returns a Pareto optimal solution whose loss vector is in the desired direction. 2.The paper is well written and a substantial number of experiments are conducted, promising good results of the proposed method. My major concern is that there is a similar work [1] that shares the same spirit with the work. 2) Page 6, ‘We therefor’ should be ‘We therefore’. [1] X. Lin, Z. Yang, Q. Zhang, and S. Kwong, “Controllable Pareto multi task learning,” arXiv preprint arXiv: 2010.06313, 2020.<BRK>___________________________________________________Summary: The paper proposes using a hypernetwork to learn the entire Pareto front of a multi objective optimization. Strengths: + It is very interesting and useful that the paper tries to learn the entire Pareto front directly. + The paper proposes using hypernetworks to learn the Pareto front for multi task learning. PHN is a solving method for MOO problems. This is more important than the experiments on the multi task learning.<BRK>The paper proposes a method for multi objective optimization. The key idea is to learn the entire Pareto front at once by training a hypernetwork that takes preference vector as an inputs and outputs network parameters, which corresponds to a point on the Pareto set with the desired trade off specified by the preference vector. The method improves HV from the baselines, in several multi task learning problems, including image classification, regression and, mixed classification and regression. +) The main contribution of this work is to learn a continuous function that maps a preference vector to network parameters that corresponds to the desired trade off. I think this method can be useful when understanding the relations among tasks by significantly reducing the training time to obtain the Pareto front. )
Reject. rating score: 4. rating score: 4. rating score: 5. rating score: 8. <BRK>The paper studies a pre training approach to reinforcement learning. As a first contribution, the paper proposes two strategies to use a pre trained policy for discovering an efficient task dependent policy: i) the action set is expanded such that the policy can choose to follow the pre trained policy and ii) exploration may be done by following the pre trained policy on t timesteps, t being randomly chosen. First, the pre trained policies are evaluated based on how much reward they are able to collect, and compared to other unsupervised approaches. Second, the final policy is compared to epsilon greedy and epsilon z greedy approaches. In the two cases, the proposed approach outperform the baselines. Comments: First of all, the paper clearly lacks of details, and it is difficult to be sure about what is really done, and what is the final algorithms. As far as I understand, instead of proposing a really new approach, the paper is more stacking two approaches (i.e NGU and R2D2, just changing a little bit the action space) and it does not really provide any justification about what is done. I would advise the authors to provide more details on this aspect of their algorithm. Concerning the coverage approach, I am not convinced that the paper allows us to draw any conclusion on the interest of using such a criterion during the pre training phase. Indeed, the authors are mainly evaluating their approach on Atari games, on which there is a clear relationship between the length of the episodes and the final score achieved by an agent. The article is thus lacking evaluation on other types of environments, and the performance obtained by the model on Atari games is mainly due to the use of the NGU model which has been developed more specifically for this type of environment. To conclude, I think that the paper is failing to provide evidence that the coverage approach is a good approach to unsupervised pre training of policies. Moreover, I have the feeling that the coverage criterion may be good for particular types of environments (like Atari games), but not for some others, making the proposed approach very specific to particular problems. Combined with the lack of novelty, and the lack of details, I recommend to reject the paper.<BRK>They then employ the NGU (Badia 2020) They also propose a method based on coverage pre training for transfer and provide empirical evidence in support of their method for transfer in RL on the Atari suite. #### Strengths:  The paper has a good experimentation section, including empirical analysis such as ablation studies and effect or pre training that is insightful. The authors fail to differentiate what do they do in their method that fixes the limitations of the previous work. It is not clear what this work is proposing other than using NGU in the transfer setting directly. In their setup, the agents are allowed as many interactions with the environment as needed. However, if there is a change in dynamics then there can be scenarios where the pre training procedure doesn t result in any benefits. The procedure mentioned in Section 3 is already being employed in many different previous works, and it is not clear what is the setting being considered in this work. **Missing references**: The authors make statements and then fail to give the appropriate references for the same. For instance, the authors quote that "RL techniques have not yet seen the advent of a two stage setting where task agnostic pre training is followed by efficient transfer to downstream tasks", however, there are multiple works that are based on essentially this approach ([4, 5]). **Reproducibility**: There is no mention of code release and makes me skeptical of their results, especially when this an empirical results driven work.<BRK>This work proposes a methodology to use a pre training phase (latent learning) where the agent tries to maximize the coverage of the state space to then bootstrap task solving where the agent focuses on the task defined reward. There is no clear explanation (algorithm ?figure?)of the process, how the pre trained phase and the next phase interact, nor which information is transferred (behaviour   policy ?). If I understood correctly, the contribution of the work is limited to training NGU without external reward, then using this pretraining to initialize the agent in an exploitation phase. The properties of model based RL are quite interesting in terms of finding a more optimal policy compared to learning a reactive policy. They might be arguments in favour of keeping a model free approach but they are not presented. The results are in accordance with the claim of the authors that their approach, favouring exploration, improves the final performance. Reward itself is a coarse metric, is suitable for the agent performance, but it is a weak claim to say that the differences in performance are due to better coverage. In conclusion, the paper lacks a proper structure to explain the method, make it reproducible by readers; it is unclear what the contribution is, and it seems quite limited.<BRK>This paper presents many strong pieces of evidence that this approach can be used to tackle challenging RL problems including hard exploration and multi task learning. It is interesting that the significant gain of pretraining come only when both exploitation and exploration method are used jointly. * Figure 4 provides a useful intuition that more pretraining is beneficial for transfer to hard exploration task. I managed to find the detail in Appendix A, but the pointer does not exist in the main text. I am convinced that NGU is a good pre training objective but, it is not clear whether a more general claim for "coverage" is supported as well. Comment / Questions to author* "but little research has been conducted towards leveraging the acquired knowledge once the agent is exposed to extrinsic reward": I m not sure whether I agree with this description. The only difference with this work is that the previous work did not study a setting with a clear separation between "pre training" and "transfer". * Is there a difference between "ez greedy with expended action set A+ (using pre trained policy)" vs "the proposed transfer method (exploitation + exploration)"? How authors would compare CPT vs joint training? I believe this observation is likely to catalyze future research of the related approaches, and the proposed method itself may be used for different domains to improve the capability of RL in general.
Reject. rating score: 3. rating score: 4. rating score: 4. rating score: 6. <BRK>The empirical numerical analysis on three benchmark datasets (cora, citeseer, pubmed) show that most of the necessary information is contained in the low frequency domain. I think the paper has a pretty good start point: understanding how the spectrum of adjacency matrix will affect the behavior of GCN.<BRK>The paper shows by experimentation, that the performance of GCNs mainly depend on low frequencies (lower end of the spectrum/eigen pairs). Novelty in the study is limited. Hence, the novelty of the paper is limited. Results on more datasets with different spectral distributions would make the study more conclusive. 3.The paper is poorly written, and needs to be checked for language consistency throughout.<BRK>It is better to plot the spectrum of Equation 7 to verify you argument. It is a kind of feature engineering. OriginalityThe paper provides new insights about low pass filters and high pass filters on graphs.<BRK>The authors demonstrate that GCNs mainly rely more on low frequencies rather than high frequencies which is contrary to what is observed in signal processing. For this, the authors use band pass filters which allow only a portion of the spectrum to be utilized by the GCN model. It would be great if authors could provide more intuition behind it.
Reject. rating score: 5. rating score: 5. rating score: 6. rating score: 6. <BRK>This paper proposes distributed SVGD, which maintains N particles both on the server and on the client. The former increases the communication cost and the latter increases the burden of client. Non iid dataset partition is needed to evaluate thoroughly the performance of DSVGD for federated learning.<BRK>This paper proposes a Bayesian optimization algorithm in the context of federated learning. The whole framework is built on top of generalized Bayesian learning. I think it might be better to decompose the proposed algorithms into several components, e.g., the server part and the agent part, and packing important updating steps as a procedure. For example, the convergence analysis for (U )SVGD, even for the most simplified case, and compares with the existing work. 6.A quick question: the proposed algorithm and PVI only selects ONE agent per communication round.<BRK>This work takes the PVI approach and adapts it to the SVGD framework. The implementation of this method for federated learning of large deep neural networks cast doubts in the feasibility of the approach due to the high overhead of sending/receiving multiple set of weights. But I have strong doubts that this approach can provide this guarantee once the "updating one agent at a time" constraint is lifted. So, the transmission of several set of weights can lead to very significant communication costs/delays. I think they have really addressed my concerns.<BRK>However, unlike PVI, the proposed method aims to replace the parametric representation of posterior with a non parametric particle representation developed by the prior SVGD work of (Liu & Wang, 2016). post rebuttal feedback  The authors have addressed most of my concerns. This necessitate the development of a distributed particle aggregation algorithm in Section 4, which is the key contribution of this work. This development is also motivated by two practical desiderata of federated learning: (a) a good trade off between communication load (per iteration) and no.
Reject. rating score: 3. rating score: 4. rating score: 4. rating score: 5. <BRK>The authors propose the use of the Hellinger distance instead of KL divergence to constrain new policies to remain close to the behavior policy. It seems with the datasets and envs evaluated, policy performance actually *drops* as policy optimization is conducted, so it is not clear to me that these evaluations actually provide meaningful information towards which methods perform better in scenarious where we would want to use offline RL. Overall, the contribution does not seem significant enough to warrant publication without strong experimental results, which this paper lacks.<BRK>Reasons for score: Though it has some advantages, I vote to reject this paper. + The loss function for policy can be derived by theory Cons   Changing KL distance to Hellinger divergence has low novelty. Indeed, HD is symmetric and satisfies the axioms of distance. This expectation is also shown in the results. If the proposed method learns successfully while the others fail to learn, it is a meaningful result, but it is not, otherwise. I think that the authors should evaluate performance using better samples to prove that the proposed method outperforms others.<BRK>##########################################################################Summary: The paper provides a new metric   Hellinger distance to be combined with trust region ideas in policy optimization. The major difference from prior work is the change of this distance metric. Though the authors motivated an  improved  version of the trust region lower bound, by using the fact that the Hellinger distance is upper bounded by KL   I think such an improvement in the lower bound is a bit trivial and does not provide new perspectives on the old results. 2.This new lower bound also might not provide additional benefits in practice   because in practice such lower bounds are generally too conservative.<BRK>This paper proposes an algorithm for off policy reinforcement learning using the Hellinger distance between the sampling policy and optimized policy as a constraint. The motivation for the proposed method is explained in the preliminaries section. The writing of the paper needs work. By current policy, what the authors mean is the policy that is being optimized. The sampling policy is the policy that was run offline. Clarifying these terms would help. Similarly, I did not follow "return for the new policy is improved comparing to KL". In paragraph 3: "With the use of Lagrangian, have been derived" needs proofreading.
Reject. rating score: 4. rating score: 4. rating score: 5. rating score: 5. <BRK>This paper studies how to do distributed training for GNNs. However, I have some concerns as follows:1. However, it is a recursive procedure. It is not clear how this method work for deep GNNs. The experiments show how this parameter affect the accuracy. Thus, it is critical to show the percent of immediate neighbors in the overlap<BRK>The paper presents a subgraph approximation method to reduce communication in distributed GCN training. The authors need to add some explanation and experimental evidence for this design. It is not convincing that the method is generally applicable to GCN training.<BRK>It is currently unclear why / whether the proposed subgraph approximation can / cannot be applied to such GNNs for distributed training. Overall, the motivations are clear and the biggest strengths of the paper. ## Comments1) The authors are encouraged to position their work with / compare against the following contemporaneous distributed GNNs in the camera ready / a future version.<BRK>This paper addresses the problem of training GNNs in a distributed environment (e.g., with multiple machines communicated through a network). It is not entirely clear what "approximated graph" means. Specifically, the augmented data are nodes that do not belong to the current partition but are within multi hop neighborhoods of the nodes in this partition. The authors are encouraged to conduct investigations.
Accept (Poster). rating score: 7. rating score: 5. rating score: 4. <BRK>Summary:This paper presents a satisfying solution to the open problem of how to train all tasks at approximately the same rate in multi task learning. Experiments on common MTL benchmarks show the new method compares quite favorably to previous approaches. The paper addresses an important open problem in multi task learning. The theoretical derivations are well motivated based on fairness desiderata. The resulting methods are effective and simple to implement. The theoretical and conceptual comparisons to existing methods make it clear how the paper unifies and extends the existing literature. Weak points:The paper could benefit from a discussion of the limitations of using fairness as a proxy for MTL performance. The motivation is framed as addressing the problem of some tasks being sacrificed/undertrained. However, there is a common and closely related issue of some tasks overfitting before others are satisfactorily trained. Does the approach in the paper address this more general question? For example, in the paper the comment “NYUv2 is a rather small dataset, so uniform scaling can also obtain reasonable results” emphasizes that underfitting is the focus, but if the dataset is so small, could uniform scaling lead to overfitting that IMTL could address? This should be made clear in the paper and in the released code. There is no evidence from the introduction that MTL was around before 2018. In “Results on Cityscapes”: “Surprisingly, we find our IMTL can beat the single task baseline where each task is trained on a separate model.” I don’t think this is surprising. However, it is somewhat surprising that none of the other MTL methods exceed the single task baseline on all three tasks; this highlights the fact that they are missing something.<BRK>This paper introduces an impartial multi task learning approach to balance gradient and loss of multiple tasks. Since biased learning can degrade learning efficiency and this work tries to make a balance in a principled way. Theoretical or empirical analyses on this make the proposed method stronger. The proposed method gives comparable performance compared to other strong competitors but the gap is not remarkable. I wonder if this performance gap can deserve the attention of practitioners. It would be great for more rigorous analyses to show qualitative results and how this approach differentiates it from others. The proposed method needs to show its potential further. In addition, if this approach applies to multiple datasets (as multiple tasks, for example, visual decathlon challenge), the proposed approach may not work well due to the different characteristics when we train them using a hard parameter sharing approach.<BRK>Additional constraint by making all loss weightings sum to one is used. The paper compares the effectiveness of the proposed IMTLs with their counterparts on Cityscapes, NYUv2, and CelebA and claims state of the art performance. First of all, the authors start out by claiming multi task learning architecture improvement will lead to high inference cost (Sec 2 first paragraph), so they decided to investigate loss weighting balancing. In fact, these works also show that better task grouping in network design can already alleviate the need to use sophisticated loss balancing tricks. Although multi task network design and loss balancing can be orthogonal efforts, it is important to have reasonable motivation and fair discussion. However, the proposed IMTL method obtained 91.12% accuracy on CelebA where method in [R2] obtained 91.62% accuracy. Therefore, it is incorrect to claim SOTA in the submission. How much additional computational overhead is needed compared to other straightforward gradient balancing approaches? The authors claim the proposed IMTL L does not require any distribution assumption, resulting in the obtained loss weighting $e^s$ with a regularization term $ s$. The proposed formulation is actually quite similar to the original uncertainty weighting approach (Kendall et al.v1 eq.11) with the Gaussian distribution assumption. The only difference is the 0.5 scale for the first term. This similarity also reflected in the experimental results that in Table 1 we can see IMTL L performs quite similarly to uncertainty weighting (Kendall et al). It would be great to use the space to further analysis the difference between the proposed IMTL L and the uncertainty weighting approach (Kendall et al).
Reject. rating score: 1. rating score: 4. rating score: 5. rating score: 5. rating score: 7. <BRK>Hence, the adaptive attack employed here are not good enought. I highly suggest the authors study [1] to get familiar with how to engineer strong adaptive attacks.<BRK>Results: To defend against adversarial attacks, this work experimentally analyzes the feature distribution of traditionally  trained CNNs for gaining more knowledge about adversarial examples.<BRK>This paper proposes to leverage the manifold aware training to learn compact representation. It would also be necessary to provide analysis for the properties of the learned representation.<BRK>Perhaps not visually, but the factthat objects close in the input space get eventually separated in thelatent space across the layers of the CNN is quite known. ## Additional CommentsIn Section 3.2, the authors propose two auxiliary loss functions tofurther improve the robustness of MAT.<BRK>Summary:This paper tackles the problem of training models that are robust to adversarial inputs.
Reject. rating score: 4. rating score: 4. rating score: 4. rating score: 4. <BRK>This paper proposes to extend successful features by learning the second moments of cumulants in addition to the cumulants. The paper is clearly written and extending successor features to non linear rewards is an interesting problem. In detail:The paper is effectively an instantiation of the successor feature that adds structure to the cumulants. Thus, the claim that this is a “novel formulation of successor features with a non linear reward” seem inaccurate since the original successor feature formulation already handles the case where the reward is a non linear function of the state. However, I do not plan on changing my score as my main concerns have not be addressed. In particular:> the framework does not so much provide a “novel formulation of successor features” but rather presents a specific instantiation...<BRK>One limitation is the assumption that there is some feature space where the rewards of all tasks can be linear in this space. They construct a method for learning features as an autoencoder of the state and experiment on some simple tasks such as reaching to different locations comparing this ``second order  method against successor features. Strengths:  This does seem like the natural ``second order  version of successor features. The interpretations of $\Lambda$ are interesting and ideas around how to use it for exploration are intriguing. Weakness:The primary weakness of this work is the experimental section. Firstly, whether using successor features (SF) or 2nd order successor features (SF^2), $\psi^\pi$ and $\Lambda^\pi$ are a function of a particular policy. Finally, it would be helpful when introducing a conceptual idea as here to have a simple version where only that idea is needed. A more general weakness is that, while this paper improves the expressiveness of SF by allowing a 2nd order relationship between features and reward, it is not clear if this is the key limitation of successor features.<BRK>The derivation is relatively clear and appears to be correct, with the nice result being that the additional term needed to account for the non linearity can still be estimated via a Bellman equation. One fact that the authors fail to address is that the linearity between the features and rewards doesn t limit the expressiveness of value functions computed by SF, since the features can be an arbitrarily non linear function of the observations. Now, it might be the case that the ability to generalize to novel tasks is greater when making the rewards be non linear in the features, but the author should make this distinction. Indeed, this ties into my larger issue when the paper: the Axes task (and maybe the Reacher task depending on the dynamics) is very simple, to the point that it is very unclear why the linear SF formulation does worse than not only the quadratic case, but than a random agent. The exploration angle is quite interesting, but currently its hard to understand the motivation.<BRK>This paper introduces a quadratic reformulation of successor features (SF), in which rewards are given by $r \phi^\top \mathbf{o} + \phi^\top A \phi$ instead of the usual $r   \phi^\top w$. This generalization leads to learning a second order term $\Lambda   \mathbb{E}[\sum_{t} \gamma^t \phi^\top \phi]$ to augment the expected featurization $\psi   \mathbb{E}[\sum_{t} \gamma^t \phi]$. This is explained by the fact that these environments require a nonlinear model:> *Finding a solution to the environmental reward structure is difficult as the reward is anon linear function of the features; in this case, the agent’s coordinates and the current goal location. *However, even in the case of SF, the reward should not depend linearly on the observations themselves, but on a featurization of the observations. Section 1.1 *The object* —> *the objective***Summary:**Though this is an interesting generalization of SF, it does not seem quite ready for publication.
Accept (Poster). rating score: 8. rating score: 7. rating score: 6. rating score: 5. <BRK>Building on the work of Chang et al.(2019),  the authors provide a global convergence result for the hidden representation of a family of recurrent neural networks using standard techniques from the Lyapunov analysis of dynamical systems. Numerical evaluation on a variety of benchmarks shows that the proposed algorithm yields systematic improvement over other RNN approaches. specify the functions \sigma_min and \sigma_max used in Theorem 1  specify the meaning of the one arg function f(h^*) as opposed to the 2 arg f(h,t) appearing in Definition 1.<BRK>Considering a continuous time RNN with Lipschitz continuous nonlinearity, the authors formulate sufficient conditions on the parameter matrices for the network to be globally stable, in the sense of a globally attracting fixed point. Finally, they highlight improved stability of their RNN against parameter and input perturbations. Another drawback in my mind is that enforcing global fixed point dynamics is quite restrictive. It also remains a bit unclear to me how it’s ensured in practice that the matrices obey to the required conditions in the training process.<BRK>The authors proposed a new continuous time RNNs which appears to be extremely similar to CT RNN (Funahashi et al.1993).They then constraint the network representation to account for learning long term dependencies. Positive: The analysis of the stability of the model and other properties is rigor enough and to the best of my knowledge sound and correct. With a few changes to the hyperparameters from the tuned one reported in the table the performance of the proposed model dropped significantly!<BRK>Your stability will not hold for sparse in time data points, because the Euler step would become too big and this is effectively an RNN that cannot handle different sampling time while preserving stability. It has been shown that ResNet works much better than their predecessor,  Highway networks, because of the direct skip connection and better gradient flow. If you add input signals to the NODE, which I guess is what you mean by "NODE RNN", how do you train it with the adjoint method?
Reject. rating score: 4. rating score: 4. rating score: 6. <BRK>To summarize, *it is possible to construct models where the ELBO has a reasonable value, but the smoothed objective behaves catastrophically*. However, I still cannot recommend acceptance at this point, because of a newly discovered issue in the theoretical analysis:The analysis in Section 3 does not take into account the impact of smoothing on the ``downstream  nonlinear layers. It is shown that the smoothing procedure reduces variance of the ELBO, alleviates posterior collapse, and improves on model likelihood on CIFAR 10 under a fixed number of parameter budget.<BRK>1.SummaryThis paper studies the training of deep hierarchical VAEs and focuses on the problem of posterior collapse. In particular, it is discussed that variance may be an issue. This is a major drawback for Hermite VAEs and the complexity of the algorithm is not discussed, nor it is studied empirically. Given 5 MC samples, I interpret that HVAE is 5 times more expensive than other approaches   please clarify this point. * Empirical study of the variance The problem of the variance is discussed in the paper but left apart in the experimental section. I would expect the authors to measure the variance (and/or SNR) of the gradients for the HVAE objective, the VAE, for advanced estimators such as STL and DReG. At least using the right amount of freebits should improve the results (the number of freebits is not reported). In particular, the cost induced by the additional MC samples is not discussed and methods are hence not compared on the same basis. Do the VAE models use multiple MC samples as well? Why using only 2 layers for the VAE models?<BRK>It has clear theoretical inspiration and had solid analysis on variance reduction. Clarify: The paper s presentation is clear. Although motived by advanced tools, but the application of the method in vanilla version of hierarchical VAE (in term of implementation) seems very simple. Thus, the method looks easy to adopt. It seems that it is only applicable to the hierarchical version as the operator is applied in  p(z_i|z_i+1) and if it is one level, it lost the point due to single Normal prior. 3) Experiments only compared to Ladder VAE in number of dying unit but not in term of ELBO for performance. Although there is a chance due to minior setting differences, I doubt the method s performance can match LVAE. 4) in term of performance of ELBO, most of the time, it does not match simple KL annealing either.
Reject. rating score: 4. rating score: 5. rating score: 6. rating score: 7. <BRK>SUMMARY#######The present paper proposes a way to robustify Optimal Transport (OT) with respect to outliers. Assuming that one of the distributions on which OT is computed is $\epsilon$ corrupted (the second distribution being a parametrized distribution one wants to make close to the first one), authors propose to solve Kantorovich s problem for all distributions that are within an $\epsilon$ TV distance from distribution 1. I have also read other reviews and replies. However, my stance on the paper did not really change as I find the contribution insufficient for acceptance. Experiments are proposed, both on robust mean estimation for simulated data, and outlier detection on MNIST. In particular:a) other works with a different approach (see point on related works below) have derived consistency and convergence results for their estimator in the presence of outliers, does something similar hold for ROBOT? I find it a bit disappointing not to have more theoretical insights. I agree however that the threshold makes perfect sense here, and that ROBOT is "always" better than vanilla OT in the proposed experiments. p.1 "there are no methods in the literature for achieving outlier robustness with MKE": the following two references might be relevanta) Staerman, Guillaume, et al."When OT meets MoM: Robust estimation of Wasserstein Distance." In that case, how large can be the impact of bounded outliers? Or the contradiction you get in the end might refute this assumption, rather than the fact that $\Pi_2^*$ is optimal.<BRK>This paper proposes a modification of optimal transport to make it more robust with respect to outliers. The authors show the equivalence of this OT model and a TV regularization of OT (with a cost which coincides up to truncation). The model is already presented in the EMD literature by Peele and Werman (as mentioned in the paper). Formulation 2.1 is essentially similar to that in « Generalized Wasserstein distance and its application to transport equations with source », by Piccoli and Rossi (Eq.3).The model shares also important similarities with partial optimal transport. Theoretical contribution: The equivalence between the two models is, to the best of my knowledge, new. Experiments: there are two setups in which the model is used. First one on a mixture of gaussian distributions, a toy experiment comparing standard OT with truncated OT and shows as expected better performance in estimating the mean of the first « clean » distribution. Valuable improvements for this work would focus on the experiment section. Comparing with other methods such as unbalanced OT and comparing with other methods of outlier detection.<BRK>Empirically, the authors evaluate their proposed approach on a toy example of robust mean estimation and outlier detection for data collection. However, in my opinion, the problem (robustness to outliers) for OT is closely related to partial OT (and/or unbalanced OT) where one only optimal the partial alignment for probability measures (or relaxing the marginal constraints during optimization by divergence). Is it possible to just simply use a threshold to detect "outliers" as in applications in 4.2? As in Algorithm 1 (and Figure 1), it seems that we can simply discard the constants $\Pi_{11}$ and $\Pi_{21}$ to reduce $\Pi$ into a matrix (n+m) x mSome of my other concerns are as follow:  + Although the new formulations are interesting, the authors should compare their approach with the partial OT and/or unbalanced OT which addresses the same concern for OT problem. + The assumption about a "clean" distribution for 1 of the 2 input ones for ROBOT is quite strong in applications. In this sense, I think that the unbalanced OT (as in [3]) may be more advantageous. Optimal entropy transport problems and a new hellinger–kantorovich distance between positive measures.<BRK>My evaluation:* The main methodological contribution, claimed in the paper, is the equivalence between Formulation 1, which is an optimal transport (OT) problem regularized with the TV norm, and Formulation 2, which is a pure OT problem, with a truncated cost. * Even if the methodological contribution is not a real one for experts in a specific area of optimization, in the context of robust methods for data science, the equivalence is interesting to present. You should shorten the discussion about the equivalence and put it in the Appendix altogether. * The application and the experiments show the relevance of the approach and its efficiency. It is robust to outliers: changing one s_n to a very large or very small value does not change the median. But I agree that the truncated l1 cost is even more robust.
Reject. rating score: 4. rating score: 4. rating score: 7. rating score: 7. <BRK>The authors derive an upper bound on the regret of the algorithm that is obtained by carefully choosing the epoch lengths. The algorithm is clearly defined and the results are easy to understand. The problem of learning in non stationary environment is quite challenging and hence the result is appealing. What is the main originality of the paper? Some numerical experiments could be useful.<BRK>Similar constructions have appeared in earlier studies of dynamic regret, to name a few, online convex optimization (https://arxiv.org/pdf/1810.10815.pdf), linear bandits (https://arxiv.org/pdf/1810.03024.pdf), etc. I think the setting is not quite novel, similar settings have appeared in several earlier papers, including Ortner et al.(2020) and Cheung et al.(2020).The method proposed uses a simple restarting strategy on top of an existing algorithm with known stationary regret, for example, the Q learning methods (Jin et al., 2018; Zhang et al., 2020). The idea of restarting strategy for nonstationary stochastic optimization or decision making (even MDP, see reference below) is not new, which clearly diminishes the novelty of this paper. A simple approach for non stationary linear bandits. [NOTICE: this paper is published at UAI 2019 venue, while in the submission authors mistakenly cite as UAI 2020.<BRK>The paper studies efficient model free reinforcement learning in non stationary Markov Decision process. They propose an algorithm with efficient dynamic regret bounds. Besides, they also propose matching lower bounds. pros:  The theoretical results are solid since they achieve minimax regret bounds. I guess this is the reason why this idea works for non stationary environment. Maybe more discussion is needed in the paper. I am glad to see that the new theorem (Theorem 2) has successfully tackle the problem about the prior knowledge of the variation budget.<BRK>Moreover, the authors provide a rigorous analysis of RestartQ UCB and establish a near optimal regret upper bound as well as the first lower bound on the dynamic regret in non stationary RL. The paper is a novel and rigorous theoretical contribution to non stationary RL. It is not totally clear to me why stages are necessary for the analysis.
Reject. rating score: 3. rating score: 5. rating score: 5. rating score: 6. <BRK>This paper proposes a method to compress large scale neural language models (e.g., BERT) without losing too much downstream performance. Experiments across the GLUE benchmark confirm that the method does not significantly harm downstream performance, and it also seems to result in inference speedups. However, the biggest flaw with this paper is that its timing experiments are reported on a single CPU thread, while the majority of practical scenarios rely on GPUs. As such, I cannot recommend the paper s acceptance. i feel like touting the method as "generic" is a little misleading; the proposed method here is complementary to prior work on compression / distillation and should be presented as such. i would have liked to see more discussion of this, although the paper does evaluate over many different tasks. a HUGE concern with the timing experiments is that they were done on a single thread of a CPU, not a GPU.<BRK>Summary:This paper studies a technique to increase the inference speed and decrease model sizes of pretrained NLP models such as BERT. These empirical insights lead the authors to propose an approach based on data aware low rank compression of pretrained weight matrices and can be applied to fully connected and self attention layers. Their approach is able to improve both model size and inference speedwith limited loss of accuracy on the Glue datasetStrengths:1. This is good and the results are also promising, but there are some issues with evaluation (see below). Figure 3 tradeoff between performance and speedup is not very useful unless the plots are also made for existing baselines and compared with the author s proposed approach. 2.The only empirical comparison is with TinyBERT but it seems like there are many recent papers studying similar compression algorithms on BERT:https://arxiv.org/abs/1909.11687https://arxiv.org/abs/2001.04246https://arxiv.org/abs/2002.02925https://arxiv.org/abs/2002.08307that I think should be mentioned and possibly compared to as well. Some analysis of this would be good, and further comparing the tradeoffs between performance, pre processing time, and inference time/space. However, my main concerns regarding novelty and experimental comparisons still stand   I think it is insufficient to say that your method is  complementary to most if not all existing methods, so we could easily combine the proposed method to the others to further accelerate  without showing this or comparing to current approaches. The other reviewers have also brought up important concerns regarding experimental details which I concur with.<BRK>The goal of this paper is to accelerate large scale NLP models. In addition, they show that their method can be combined with distillation methods. Strength  This paper is well organized. They show the experimental results of BERT models as well as LSTM models. It would not be a fair comparison to compare only the inference time. The accuracy loss seems to be non negligible. U and V are column orthogonal matrices (not orthogonal matrix) so that UU^T and VV^T are not equal to identity matrices. They sub sample 10% of the training data to perform low rank approximation. I would like to see the performance change with respect to the sampling ratio.<BRK>This work introduces a low rank based compression method, called DRONE, to accelerate the inference of large NLP models. DRONE also proposes a way to extend the low rank decomposition to dot product attention. Weakness:  The evaluation is inadequate as the baseline does not seem to represent real inference scenarios. Important references are missing, making it not clear the advantage of this work as compared with existing approaches that also compress Transformer networks. First, it is not clear why a large batch size (e.g., 100) is chosen for inference. Is large batch size chosen because DRONE cannot obtain much speedup with small batch sizes? It would be better to add discussions on the choice of batch sizes and show speedup ratios using batch sizes that are close to real settings. Second, the evaluation is done "with single thread on CPU", which does not represent real scenario either and makes the whole evalution less convincing. Question:The empirical observation that weight matrices in BERT models are not low rank is interesting.
Reject. rating score: 3. rating score: 5. rating score: 5. rating score: 5. <BRK>The proposed encoding does not seem novel, and the authors appear to be unaware of significant prior work in this space. Maybe these experiments suggest a possible explanation, but there could be many other alternate explanations. Additionally, the paper has multiple issues with clarity and style that make it hard to understand, and also has some claims that seem unsupported by the evidence.<BRK>##########################################################################Summary: The paper provides a interesting direction in representing source code. My major concern is about the novelty of the paper. The claim of the title is about a generic method for code representation learning, however, the experiments are only conducted on code summarization task, which is not convincing enough to support such claim. Appendix should not be used in this way. 4.Paper writing and organization needs to be improved.<BRK>The paper is well written and easy to follow. %%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%% Weakness of the paper: See detailed review below. Why authors did not test that? However, there are some AST edges that are red in 3c. Is that a mistake or it is intended?<BRK>This paper is well motivated but the organization and presentation remain to be improved. For example, the motivations are revealed to the reader in Sec.3.2.I suggest the authors re organize and rephrase this paper for better presentation. 2.Fig.3: What is the meaning of showing (a)?
Accept (Poster). rating score: 8. rating score: 7. rating score: 4. <BRK>Even though compared to prior work, this work does not assume access to ground truth action labels from the user provided to the interface, it does assume access to ground truth backspace actions. However, the implicit human feedback is assumed to be backspaces typed on a keyboard. It only becomes clear to me by Section 3 what is meant by “handwriting as an input” and how it can be an assistive input modality. The online learning of such an interface to improve its assistive performance and adapt to the user over time is framed as a contextual bandit problem. A reward prediction network is trained to predict the use of backspaces (implicit feedback) by the user. This reward prediction network combined with the default interface policy using Baye’s theorem is used to update the policy of the typing interface. The experimental results with two user studies reveal that the presented method performs better than a non adaptive default interface, stimulates user co adaptation to the interface, and offline learning accelerates online learning. ########################Pros:  The paper is well written and easy to follow. This should be highlighted in the introduction as well. Offline pretraining of the reward model reduces the burden on online user interactions needed for the interface to be improved. The presented technique is applicable to any form of user inputs and the authors test their approach with two different forms of inputs (eye gaze and handwritten characters). The questions are well framed and very clear though! It was unclear to me while reading the paper if the implicit feedback (backspaces) are provided via a keyboard or are also predicted with the user’s input (such as with gaze tracking). Practically, modeling the problem as an MDP would be more realistic versus contextual bandits. From the experimental results, it seems this is the case but it is not explicitly stated. If this is so, is the order in which users attempt to use the two interfaces (default and X2T) counterbalanced?<BRK>SUMMARY:The authors propose a simple algorithm for using online learning from implicit human feedback to improve systems that operate in the contextual bandit setting. The main idea is to capture the presence/absence of corrective actions and use this information to infer a reward signal that the system can use to make decisions later. STRENGTHS:	(S1) The authors present a simple yet seemingly powerful algorithm for building adaptive interfaces from implicit user feedback. (S2) The proposed algorithm can easily leverage existing interfaces and therefore provides an interesting path to improving all kinds of existing systems. In particular, it s not clear what data was used to compute the reported prediction accuracy. It would seem that this is computed over the prediction *for that time step*, but then more detail is needed on the datasets in order to determine whether or not the task somehow got easier for XT2 over time instead of it actually learning. There seems to be very little intuitive reasoning behind where red vs. green appears   has XT2 really learned such a complicated and sensitive decision surface? If yes, why should we "trust" this decision surface? In analyzing 2c closely, it seems like a nearest neighbor classifier would have done just as well. It seems to me that there could be cases in which the user simply accepts an incorrect action from the system without providing the feedback. That said, the current paper is lacking in the description of the experimental results and justifications for some of the assumptions. Should it not be "X2T" since one would replace the "to" with "2" ("x 2 text"  > "X2T")? I m not taking any issue with the current name if this is how the authors meant it, rather just wondering if this is a typo.<BRK>The paper leverages user feedback to improve the predictive model to perform desired actions. The paper conducts user studies that gauge the impact of the proposed interface in contrast to a non adaptive baseline interface. The problem is well motivated and the paper is well written. Some questions/comments:[1] What the paper refers to as “implicit” feedback, i.e., in the context of the paper, the backspace command is actually explicit. This can, for e.g., be used for training an appropriate reward regressor. This feedback however is incomplete, because, we tend to observe the feedback only for the actions presented by the interface. It is unclear how one can utilize a standard yes/no feedback using a backspace key to effectively learn a highly accurate policy that presents us with the next action (for example, a word) given the context. The paper however doesn’t present a discussion surrounding these challenges, which for me is a major shortcoming of this paper. [3] Naturally, some of the assumptions considered with regards to the use of backspaces is pretty strong. The feedback model assumed by the paper is essentially noiseless in that the user always does the right thing with regards to presenting feedback on good/bad actions.
Reject. rating score: 3. rating score: 5. rating score: 8. <BRK>Summary of the paper:This paper explores the problem of constructing invariant representations to certain new environments. Based on the authors  own definition, they consider a few example tasks on random graphs. in some canonical order  what is the canonical order? Overall, it is unclear what contribution this paper has. graphs are simple, meaning all pairs of vertices have at most one edge  This is a wrong definition of simple graphs? Erdos Renyi example (part 1)   The problem setting is described by an example. Figure 1: (a) The DAG of the structural causal model (SCM) of our graph extrapolation tasks where hashed (white) vertices  Hashed (white) vertices do not exist in Figure 1 (a)? While I know what a SCM is, this presents yet another example of poor writing. No motivation or theoretical guarantee is given, neither were we given evidence how it may compare to other invariant/causal models such as IRM or domain adaptation techniques. Consider a permutation invariant graph representation Γ : ∪∞ n 1Ω n×n → R d   How is this even possible? Well, this is apparently wrong? Isn t the paper studying invariant models? $\textbf{Unconvincing experiments}$The proposed model is only evaluated on two toy datasets. This paper suffers from the lack of logic and mathematical rigor; it is full of jargons that are unexplained and undefined. For example, after reading the entire paper, one still can t find a definition of GNN or GNN+, which constitute the main part of the proposed model. Evidence?Well, Arjovsky et al., 2019; Scholkopf, 2019 can be applied to graphs too? A large amount of baselines on invariant models, causal models, domain adaptation techniques are missing.<BRK>In more detail, this paper introduces a model, a so called structural causal model, for graphs where the graph creation process is modeled as a random variable that depends on different independent factors:  environment $E$, which is used to model the graph size $n$; graph property $W$, which is used for example to model the probability of edge existence; and random seed $Z_X$. The ground truth labels of the graph $Y$ are functions of two factors $W$, the graph property, and $Z_Y$ random seed. Then the authors move to define counterfactual coupling of $G^{\text{obs}}$ and $Y$. My best guess was that there is a new random variable $G^{\text{cf}}$ defined with a new environment $\tilde{E}$, and the counterfactual coupling is their joint probability function. The definition of E invariant should be defined properly; e.g., what does "can be sampled" mean? I could not understand the conclusions in this theorem. I found this section also hard to follow. The paper concludes with experiments using these feature maps to demonstrate extrapolation properties. However, this paper suffers from what I find as a bad exposition. I needed to guess many details and I am not sure i got it right. I think that to make this paper useful for the community a rather complete rewrite should be done.<BRK>The paper explores the problem of extrapolation in graph classification tasks and by leveraging Lovasz’s graph limit theory, provides graph representations and related theoretical guarantees on graph size extrapolation in the context of unattributed graphs. Specifically, it is shown that the graph representations characterized by induced homomorphism densities are size invariant under certain conditions. Overall, I find the contributions of the paper solid and of interest to many researchers. Elaborating on a few points will help improve the readability:1. 2.Are the conditions of Theorem 1 violated by any large class of graph models, such as MRFs?
Reject. rating score: 5. rating score: 6. rating score: 7. rating score: 9. <BRK>The authors proposes a new class of divergence between probability distributions that generalizes both the Jensen Shanon divergence and the Maximum Mean Discrepancy: the H divergences. Thus the $H$ entropy requires choosing a set of possible actions $A$ and a loss function $l(X,a)$. On the sample quality task, the authors show that the proposed divergence is also correlated with human judgment for images. However it remains unclear if this flexibility cannot already be achieved using existing methods. The authors suggest that the additional freedom in choosing the set of actions allows to design divergences that are tailored to a particular problem. The proposed estimator: In practice, the optimization is likely to be non convex, especially when the set of actions is chosen to be a parametric family of probability distributions. How does this affects the estimation of the H divergence? It doesn t seem to be an issue in the experiments, but wouldn t this be a disadvantage compared to other tests that have a closed form expression for the estimator?<BRK>This paper proposes a new class of divergences defined by a decision theoretic perspective, in the sense that two probability distributions are different if the optimal decision loss ishigher on the mixture distribution than on each individual distribution. This new class generalizes the popular Jensen Shannon divergence and Maximum mean discrepancy and allows to define new divergences that only take into account differences that lead to different choice of optimal actions. The idea of generalizing divergences using a decision theoretic perspective has already been proposed in [Grunwald&Dawid 2004] which is cited for the H entropy but not for the generalized relative divergence, which should be mentioned. I would have liked to see this idea exploited in the two sample test. 5) After definition 2, it is claimed that $\phi$  is the most general class function…, but there is no proof.<BRK>Summary: The distance or divergence between two probability distributions is essential for machine learning. This paper introduces a new class of divergence functions based on optimal decision loss function. They first introduce a class of entropy functional, namely the loss function depending on the action and state. Using it, they further construct a divergence based on the mixture of probability densities. I still have some questions about the proposed approach. I may increase the rating if the author can address them. It would be better for the authors to discuss the relation of this proposed divergence with Wasserstein type divergences functions.<BRK>The paper shows how the empirical estimator has practical use for two sample tests and measuring the corruption of a sample. The proposed H divergence is "useful" when the two distributions are close to each other, but as the authors acknowledge in the future work, it is an open question whether it could be "useful" in other cases. I am not extremely on top of the most recent literature on measuring differences between probability distributions, so there may be literature that is not being reviewed and ignored, but from an "outsiders" perspective this seems to be a significant contribution to the area.
Accept (Poster). rating score: 7. rating score: 7. rating score: 6. rating score: 5. <BRK>Accelerating convergence of replica exchange stochastic gradient mcmc via variance reductionSummary:The paper presents a variance reduction technique to achieve more efficient swaps in replica exchange stochastic gradient Langevin dynamics MCMC. The paper provides detailed analysis of the method as well as empirical evaluation on some standard deep learning tasks. Overall I would say that the paper is well written and and it is fairly easy to follow the presentation and details in the derivations. 2.The topic is very timely and the method appears to be very useful. As an attractive method for minibatched Bayesian inference, stochastic gradient Langevin Dynamics samplers are of high interest, but tuning the algorithm can be somewhat finicky in my experience. Replica exchange is sometimes extremely useful, and finding good defaults for these types of methods is important. 2.The novelty / originality is limited: A well known type of variance reduction applied in a new way/context where it makes perfect sense though.<BRK>The authors propose a variant of the Replica Exchange Stochastic Gradient Langevin Dynamics (reSGLD) for non log concave sampling by using a variant reduction technique on the estimation of the swapping rate. Assuming that the log density is a finite sum. In the algorithm, what is the role of the thining factor T? For the synthetic experiments, can you explain this choice for different values of F (1 for VR reSGLD and 100 for reSGLD)? I can believe that variance reduction lkeads to a different bias/variance trade off, but it would be good to explain whythese values of F were used. Thm 1: Operator E is not defined in the main text (only in the appendix).<BRK># SummaryThe paper presents VR reSGLD, a method to accelerate replica exchange stochastic gradient Langevin diffusion (reSGLD), which has been proposed recently to tackle non convex learning problems. Should this be $\sigma^2$? The idea of the paper is to use control variates to reduce the variance of energy estimators and thereby improve the swapping rate (which should lead to an accelerated convergence). Unlike previous modifications of SGLD, the variance reduction proposed in the paper aims at improving the energy estimators rather than the gradient estimators. Page 6: Your comment after Theorem 3 ("This theorem implies ..."). Why does Theorem 3 imply a much larger swapping rate? Page 6: You set $\tau^{(1)} 10$ "to avoid peaky modes", but it would be interesting to see the performance of VR reSGLD for a sharply peaked posterior (which is more realistic when thinking about high dimensional parameter spaces and a large number of data...)  Page 7: Why do you anneal $\tau^{(1)}$ in the CIFAR experiment? Numerical experiments illustrate the performance gain achieved by VR reSGLD. What are the $\mathcal{E}$s? So the combination of reSGLD and VR seems like an obvious idea. 3.The improvement presented on imaging data (Table 1) seems to be rather marginal. require?Can you provide some intuition on how to set these parameters? Have you considered using more than two temperatures? Page 3: The sentence starting with "The underlying Dirichlet form ..." is a bit obscure. It seems that the denominator should be $\text{Var}(B|\hat\beta^{(h)})$ rather than $\text{Var}(B|\beta^{(h)})$. The update rule for $\tilde \sigma_k^2$ is unclear to me.<BRK>##  Summary of the paperThis paper extends the replica stochastic gradient MCMC by incorporating the new variance reduction technique. ## Strong and weak points of the paper### Strong points  Although the applied variance reduction technique is not new, the analysis of the swapping rate in Lemma 2 seems novel compared to the past replica exchange MCMC work. Provided an asymptotic analysis for the variance reduction in Theorem 4. It is very similar to the standard control variates methods, which had been extensively applied into MCMCs and other machine learning tasks. Correctness: I did not check all the proof in detail. Novelty: The idea seems not novel but the swapping rate analysis seems new and interesting. But how can we tune this $m$? Does this variance reduction work even I use $m 1$? Does this intuition wrong? Q) In Sec5.2, as for Fig.3 c and d, the effect of variance reduction becomes significant after the sufficient epochs and the author conjectured that this is because that the learning rate is decreased. I think that this suggests that the proposed variance reduction is significantly affected by the step size which changes during the exploration stage in SG MCMCs. And thus I thought that using the constant $m$ during the exploration stage might not be a good idea.
Reject. rating score: 4. rating score: 5. rating score: 7. <BRK>+ The visual analysis in sec 4.3 is quite interesting. The proposed technique is helpful in reducing the attention on texture in these examples. Meanwhile, I have several concerns:  technical novelty: the main contribution of the paper is proposing anisotropic diffusion as a way of data augmentation, which is somewhat trivial. experimental analysis: I am happy to see that the proposed technique achieved improvements across different settings. I am a bit skeptical about the proposed technique in a more general setup, such as learning from synthetic data. Besides, I think more baselines need to be added for transfer learning setting, such as techniques used in domain adaptation.<BRK>In this paper, the authors propose suppressing texture (as low level cue) as it seems to provide shortcuts that prevent the network from learning higher level representations. The method achieves state of the art on Sketch Imagenet (> +10%)Pros:  identifies the low level texture as a source of shortcuts which prevent learning, especially in unsupervised or self supervised setting. Table2: needs clarification, at least on DTD   is it using transfer learning on the right dataset? would be great to have one extra line with state of the art for each of the datasets, e.g.in the form: 70.21 [citation]; when reporting numbers on DTD, it is fine to use only the first split (see SimCLR), as long as the evaluation protocol is specified in the dataset description paragraph. The idea seems simple, it is well validated through experiments, but needs some improvements in clarifying the evaluation on transfer to DTD dataset.<BRK>I maintain my review of this paper. # Main IdeaThe main idea is that many networks learn shortcuts based on texture. In addition, shape information about the underlying natural objects is well preserved. Which means that the original texture information is still present in at least a part of the dataset. This is not a problem in the original fully supervised setting where the authors used SIN for supervised image classification. The performance is not out of this world, but it outperforms training on native image net and on SIN.
Reject. rating score: 4. rating score: 4. rating score: 4. rating score: 6. <BRK>More explanation on the result is required. The paper is well written and easy to follow. Weaknesses of the paper:   Novelty: The novelty is moderate but not strong enough.<BRK>The issue is not specific to this paper   the field of neural network compression can benefit from better reproducability and more robust evaluation, and particularly methods that operate post training can evaluate on a shared repository of pre trained models and report accuracy loss. I have no issue with a simple method if it s well motivated and works well, but:2.<BRK>This paper proposed three different techniques to improve the quality of the post training quantization (PT) results. Although the overall paper is nicely written, the contents might need more novelty for being accepted in ICLR. Here are a few concerns related to it. Even if we believe that synthesizing the test data is necessary, it is not clear why "Retro Synthesis Data" is well suited for the post training quantization. But unfortunately, the performance gain in the experimental results seems to be marginal. To sum, this is a well written paper but it might need more novelty for being accepted in ICLR.<BRK>I have a few comments, some concerns and some suggestions I think can be used to improve this work. I wonder if that is really necessary though. In 7th International Conference on Learning Representations, ICLR 2019. I think the authors should contrast such work with theirs.
Accept (Poster). rating score: 8. rating score: 7. rating score: 7. rating score: 5. <BRK>However a drawback of these models  (over say, LSTMs) is their relatively slower inference speeds. This is especially a problem in RL settings where Actors are typically run on CPUs (not GPUs) and send trajectories to a central learner. The results are promising and the the writing very clear. A few points below:* One question I have is whether the authors ran an experiment replacing the LSTMs in the actors simply with smaller transformer models? Are the authors able to report numbers for these? * While the results are conclusive, the work could be improved by running on larger (or 3 dimensional) environments as considered in the original Gated Transformer paper this builds on. I would recommend accepting this paper.<BRK>##########################################################################Summary:The paper proposes a solution to actor latency constrained settings in RL by using policy distillation to compress a large “learner model” towards a more tractable “actor model”. In particular, it proposes to exploit the superior sample efficiency of transformer models while utilising an LSTM based actor during execution. ##########################################################################Reasons for score:I m voting for accepting the paper. I like the idea of policy distillation between transformer based learner and LSTM based actor in the distributed RL framework. The paper is written with great clarity and is easy to follow. I don t have major concerns regarding this work but hope that the authors can address my minor concerns in the rebuttal period. However, I d be interested to know when do ALDs and GTrXL (which achieves the same result in eventually) meet on the diagram.<BRK>The paper proposes an original idea of applying distillation in the RL setting to solve a problem above. Implicitly (because there is no discussion of this), the paper raises an important question of a trade off between sample efficiency and computational efficiency in RL. ### Questions to the authors  The paper assumes that it s better to lose in sample efficiency while computing faster. The same hold for IMPALA. Experiments      "For each environment, we run the actor and learner model architectures used in ALD as baselines, where each of these models are independently trained using standard RL". should be on the next line, I believe.<BRK>However, they are computationally expensive. Consequently, the authors propose to train transformers on the learner, supported by hardware acceleration, but also train a smaller LSTM agent which can be efficiently executed on the actors. I believe the idea and execution of the paper (in particular the method part) is good. In other words: The current experiments clearly show why and when ALD can provide an advantage. If those environments don t require as much memory, there is no reason to expect that ALD can outperform the baseline, which would be ok for me, as long as it doesn t underperform it.
Accept (Poster). rating score: 9. rating score: 7. rating score: 6. rating score: 6. <BRK>1/ Summary of the paperThis paper proposes a novel spatio temporal graph scattering transform (ST GST) as an intermediate representation of time varying signals on graphs, which can then be fed into a simple ML model. Further, the authors provide theoretical guarantees of the stability of the ST GST representation with respect to input and spatial graph variations, based on assumptions on the pre defined filters used in the transform. This paper is well motivated and well written.<BRK>The authors extend scattering transform to the spatio temporal domain. Pros:1.The authors propose a novel spatio temporal graph scattering transform. We know that scattering transform cannot do as well as CNN on larger dataset like Imagenet.<BRK>I will add it tomorrow<BRK>The authors propose wavelets for both separable and joint spatio temporal graphs. And then the authors design a spatio temporal graph scattering transform (ST GST), which is a non trainable counterpart of spatio temporal graph convolutional networks and a nonlinear version of spatiotemporal graph wavelets. Finally, the proposed SF GST is conducted by experiments, and the results show that it appears to be effective. However, The authors did not give the explanation of the motivation about why did the STG should be scattered by wavelets.
Reject. rating score: 4. rating score: 4. rating score: 6. <BRK>This submission proposes a training strategy that leverages background/noise data to learn robust representations. Experimental results show that the use of this strategy generally leads to improved performance. What about NCE and all the follow up work? There is prior work in this area. However, your related work section suggests something entirely different. Clarity: The clarity in this submission is lacking. Originality: It is hard to judge originality of this submission given the way it is presented. Which theorems are your contribution? Cons: One sided presentation with links to existing works missing.<BRK>This paper proposes a training method for classification, with the goal of training with less data. The proposal is to train an auxiliary classifier at the same time. The auxiliary classifier and the main classifier share the early layers. The auxiliary classifier idea is intuitive by itself, and it seems that Section 3 can be removed without affecting the core content of this paper. The theoretical content is detached from the proposed method and hence not important. Minor points:  Figure 3 is not a fair comparison. There are three angles to look at the proposed method:1. One example is given in Figure 5. Unfortunately this paper does not include such experiments. Table 8 includes reduced training set experiments on CIFAR, however 3/5 of the training set is still way too big, and the remaining 2/5 are not utilized as unlabeled in domain data.<BRK>To improve multi class classification problem using DNNs, authors propose to do multi task learning of solving another auxiliary tasks which tells if data points are just noise/distractors or not. It is common to have some data which is non representative of good training samples but they can be used (under DBT) to make first few layers (and thus all) of network more robust. Can this idea not be done simply by having one more classification unit which detects if x is in domain or noise? That way, whole network can be trained for this. I think it is standard to experiment with that. 3.It is not clear if I use a regularization technique like MixUp or label smoothing, will I still see improvement? I am not much aware of related work BTW.
Reject. rating score: 3. rating score: 4. rating score: 5. <BRK>As in the major point, no clearly elucidate use case. The big pitfall of this manuscript is that the authors have not demonstrated the scientific utility of their method. Significant exploration of the results are shown. Instead, the calcium traces on individual neurons are being generated, which is a derived product from calcium imaging data.<BRK>In this paper, the authors construct a GAN network for generating artificial calcium imaging activity from large neural populations. The DG generated data is very hard to judge, since neither the artificial or generated data are shown, just summary statistics in Fig.1.* Clarity: I would have liked to see a clearer explanation of what goes into the network and what comes out. * What was the role of the Wasserstein objective function? What was their outcome? * Table 1: How was the DG model for the experimental data estimated?<BRK>Further, the authors have not answered question 1 in major comments. The accuracy of the approach, robustness of generated signals from the model are evaluated. Weaknesses:  This paper lacks contribution, and the novelty of the work is limited in terms of methods.
Reject. rating score: 4. rating score: 5. rating score: 6. rating score: 7. <BRK>The paper proposes a robust training algorithm for graph neural networks against label noise. The proposed method contains two parts. Firstly, it leverages label propagation (LP) trained on the clean nodes to assign pseudo labels on train nodes with noisy labels. The final graph neural network is trained with clean nodes, high confidence train nodes, and uncertain train nodes with learned labels. The authors conduct experiments on four graph datasets with manual injected noise and one real world noisy dataset to validate the proposed method. The paper studies an important problem of graph neural networks with noisy label. I have several concerns for the paper. (1)	The idea of using LP (or another algorithm different from GNN) to create pseudo labels for uncertainty nodes is not a new idea (e.g., in [1]). Besides, in the experiment part there is no tuned \lambda compared (random is not good, a wrong \lambda can hurt the performance). Because the authors have a strong assumption that both noisy and clean labels exist, it would be better if the authors can validate such assumption with some real world data. The reported numbers show the improvement of proposed method is rather limited, I would suggest run the methods more times and report mean performance with variance. Besides, since GNNs are good for learning on graphs with few labeled nodes (such as the gcn paper actually only use several labeled nodes per class), I would suggest adding some additional experiments for the size of D_clean.<BRK>This paper presents a label propagation based meta learning algorithm to address label noise. Label propagation helps re label pseudo labels of noisy data and meta learning achieves aggregations. The method is evaluated on several node classification datasets and a custom version of Clothing1M image classification dataset. Pros:  The combination of graph neural network and meta learning is interesting and novelCons:  The paper is not well written. The usage of meta learning is not novel, which from a previous method   L2R, but it is never clearly mentioned. Experiments might not be that convincing. The method is only tested on several graph datasets with synthetic noises, which were uncommon in previous papers. However, most compared methods are only evaluated on common image datasets. The comparison may not be very fair. The proposed method is marginally better than several other methods. Although the method tries to tackle a real world dataset Clothing1M, the scalability issue of this algorithm makes it difficult to work so it is only tested on a custom toy version. So the results are not generally useful to compare to other methods which have tested on full Clothing1M.<BRK>*quality*This paper is well organized. However, the contributions are not clear. *clarity*It is not difficult to understand the framework of the proposed algorithm. *originality*In this paper, the proposed algorithm focuses on the label noise existing in graph node classification tasks. The contributions are somewhat limited, since this is not the first work focusing on this problem. Besides, the main motivation is also not new for semi supervised label propagation. *significance*Although the proposed algorithm is limited in its novelty, I feel that the significance of this paper will inspire researchers focusing on this domain. The topic is practical in realistic machine learning problems, and the proposed method is helpful to reduce the human annotation efforts. 3.In Eq.(8), the authors calculate the final predictions by aggregating the original labels and pseudo labels, in order to fully exploit the abundant information contained in the left training nodes D_left. As a consequence, the results reported in the paper may not be convincing enough. It would be better to follow the public split for fair comparison. 5.The proposed model outperforms the comparison methods when the level of flip noise is greater than 0, but cannot perform as good as L2RW when the level equals 0. I suggest analyzing the influence of the scale of clean labels on different datasets.<BRK>This paper presents the one technique using label propagation with meta learning. The experimental results look promising in two conditions of label noises. The paper is presented clearly and easy to read. The paper present the idea and experiments clearly. I checked a few literature and believe this work is original. Pros:1. clear presentation2. method is simple but seem very effective3. the experimental results outperformed the state of the arts in both synthetic label noise and real noise scenarios. Cons:1. the contributions can be challenged. And the third one is evaluation result, cannot be categorized as a contribution.
Reject. rating score: 3. rating score: 5. rating score: 6. rating score: 6. <BRK>The paper is very interesting, and would be quite valuable for the senior members of the community to read and be familiar with. However, it does not seem to have anything to do with the ICLR topics. Hence, in this reviewer s view, it should either be privately disseminated to area chairs, or published as some form of appendix, but should not be part of the conference. In addition, there are several biases identified in the paper. But since the AC are not identified as biased, and the papers are anonymous, it is not clear what is the mechanism suggested by the authors of how these biases manifest themselves. Do women?Or is there a suggestion that anonymity does not genuinely exist, and most reviewers have good knowledge on who are the authors of the papers they review?<BRK>Quality:The authors put a lot of effort in collecting and cleaning the data, which is not easy. I still have some questions but I think it s of good value for the community to see and discuss the paper in the conference. Clarity:The paper is well written and easy to follow. Remember ACs have access to the review notes and the paper themselves too. Significance:The analysis and conclusion is of good value for the ICLR community. Global % could be lower. But to be clear I still think there s great value for it to be accepted given the topic, and if the authors can answer the questions in the comments. I think the reviewers have no knowledge of author s gender, given the double blind review process? I know you might be able to dig it through by searching arxiv ,but doubt anyone would do that to see the gender. It s better to explain the size of data clearly, is it 2560 papers or 5569 papers? Given the whole simulation set up, I think the key factor is the variation of reviewer score and AC s decision noise.<BRK>The goal of the study is to answer questions like how is the reproducibility or what is the bias in the review process. In general the paper makes some timely attempts for the conference review process. Here are some of my concerns:  In Sec 3.1, the authors basically assume that papers with similar mean review scores have similar variability in scores. In Sec 5.1, the authors only consider the average reviewer score, and an indicator variable for whether a paper came from one of the top 10 most highly ranked institutions as two factors for ACs to make the decision. I think the ACs decision process is too simplified. The authors may want to clearly explain if only these factors contribute to the final decision (other confounding issues matter?), or if the relationship could be captured by a LR model. The authors should report some statistics for how accurate this process is.<BRK>This paper focuses on understanding and analyzing the reviewing process for a large conference such as ICLR and understand the reproducibility of the review process through Monte Carlo simulations. How were the gender labels produced? Was it done through a manual process? With regard to gender bias analysis, one missing type of analysis in terms of reviewers  scores is to take into consideration the arxiv/ resubmission effect. This would provide better insights. 2.The timing of this study is important and some of the findings in this paper raise concerns about the overall review process. Also, a big thanks to the authors of this work for providing the entire codebase to reproduce their results3. or is that an assumption? 4.Figure 4 is really interesting in terms of showing the impact of making submissions available earlier. This raises concerns about how making papers available online is biasing the reviewers in terms of the scores provided. 5.I have concerns over the way interrater reliability is calculated in section 4. The reviewers are randomized and this may not lead to the right number of samples per review and may affect the interrater reliability. Also, the assumptions mentioned by the authors seems to be wrong for this process. "then" should be corrected to "the" in the last line of the first paragraph of section 2.
Reject. rating score: 4. rating score: 4. rating score: 5. rating score: 6. <BRK>This paper is a review of model based approaches of integrating causal inference to reinforcement learning (RL) in different environments (application areas). The paper provides a novel perspective on applications of causality modelling and the presented environments could be useful to practitioners in the respective fields of physics and chemistry. To sum up, my reservations towards the degree of novelty in this paper have not changed (I agree with the authors  summary of their contributions, but disagree as to whether proposing a new benchmark constitutes novelty at ICLR) and I recommend a rejection of the paper in its current form.<BRK>This paper investigates the importance of incorporating structure and modularity in MBRL. It also investigates the influences of varying complexity of graphs. Based on the above reasons, I do not think this paper is ready to publish. The goal of learning causal representation is ambiguous, and it is absolutely a good research topic. However, I fail to see an obvious contribution of the current version.<BRK>  SummaryThis paper proposes a benchmark that aims to systematically evaluate models  ability in learning representations of high level variables as well as causal structures among them. The construction of the benchmark allows building causal graphs with varying complexity, such as the size of the graph, the sparsity of the graph, and the length of cause effect chains. I would appreciate it if the authors can elaborate on the connection between the design of the environment and "chemistry." It would be better if the authors can discuss the necessity of causal induction in these tasks, and how is the ability to perform causal inference correlate with the metrics used in the benchmark. While I like the goal of this paper, I think a set of more realistic environments could greatly improve the significance and potential impact of this paper.<BRK>Summary:This paper proposes a suite of RL benchmarks to facilitate inducing causal relationships from visual observations. They show that structural inductive biases are beneficial for causal relationship learning and model based RL by testing a variety of representation learning algorithms on this benchmark. ##########################################################################pros: + The motivation is clear and interesting. You seem to work in the same direction and they have proposed more complicated and interesting tasks.
Accept (Poster). rating score: 8. rating score: 7. rating score: 7. rating score: 6. <BRK>Strengths:1.The idea of utilizing Hessian without knowing its values is interesting to me. But in Fig.1, the plot is the normalized magnitude stddev. On CIFAR 10 and ImageNet, they evaluate the proposed methods with popular deep neural networks, reporting encouraging performances.<BRK>Significance:The paper focuses on deep neural network pruning, which is of interest to both the academia and industry. This paper explores how the basic L2 regularization can be exploited in a growing fashion for better deep network pruning. [Ref2] Learning sparse neural networks through l0 regularization.<BRK>The paper proposes a new pruning scenario using regularization to better prune the network. By perturbing the penalty term to the converged network, the algorithm can get the Hessian information to score the neurons but uses less time than calculating the Hessian. It uses the L2 regularization and studies how the coefficient \lambda of the regularization term can influence the weight change to derive the neuron s importance in the neuron network. It can better support your statement that the pruning schedule is important.<BRK>In particular they focus on growing regularisation during training instead of using a high regularisation parameter at the beginning. +ve  The paper is easy to follow and the authors make it quite clear about the problems related to filter pruning that they are tackling in this work. Concerns  The core idea of growing the regularisation parameter is not quite novel. Comparing with iterative pruning methods like “Importance estimation for neural network pruning” by Molchanov et.al.
Reject. rating score: 2. rating score: 2. rating score: 3. rating score: 3. <BRK>This paper introduces a number of data preprocessing options for numeric features provided by an open source library automunge. The paper uses one section between normalization and binning to explain a notion  family tree primitives  which is used in the composition of multiple transformations. It is acknowledged that "the metrics for normalized data were slightly better than raw on average, it is not clear if they were sufficiently statistically significant to draw firm conclusions. " When approaching big data scale input with infinitecomputational resources there may be less of a case to be made for much beyond basic normalizedinput. " The library is released as a python package which is easy to access. In the experiments there are certain cases where the preprocessing helps. Cons: 1.The experiment results do not convincingly demonstrate the utility of the proposed library. It is even better than using z score or retain to normalize the data. Then it raises the question whether the setting of using small sample of higgs data matters. That is not a contribution of this paper. To conclude, it is not surprising that one can find some scenarios where some transformation improves the accuracy, but the paper has not presented a convincing scenario where the proposed library is useful. 2.The explanation of  family tree primitives  confuses me. refer to.3.It remains a question how a user should select transformations from the numerous offered options.<BRK>This paper reviews an existing Python feature engineering library, Automunge. It is a pity that the paper does not compare to any other feature engineering library, although there are very many available. The experiments section reports two experiments on one dataset; it reports AUC and accuracy metrics under different feature pre processing: this reporting is hardly relevant and does not demonstrate any interesting property of Automunge. For instance, it would be interesting to see whether Automunge is particularly flexible, clear, or convenient when one wants to implement chains of transformations. Tables 3 and 5, and none of the figures are mentioned in the text, let alone commented upon. The family tree structure which seems to be the object of Table 3 is not explained, and I could not make sense of it, though it seems to be important. In table 4, I could not understand what the columns refer to; equally, the transformations mentioned there are left undefined: for instance, in case, as I assume, "Number of standard deviations from the mean" is $x \mapsto round(|(x_i   µ) / \sigma) |)$ (or maybe the absolute value needs to be replaced by round brackets?this is left ambiguous), it should be properly defined. Table 6 refers to categoric noise injections, but the paper was meant to be about numeric input variables, so I m not clear what it is doing here. Throughout the text, syntax issues make the text hard to understand. The paper is so hard to understand that it does not allow one to evaluate possible upsides of Automunge on its own merit. After quite some hesitation, I have to assign this paper a severe rating due to the conjunction of several severe shortcomings.<BRK>In general the paper discusses standard normalizers rather than introducing novel techniques, and the experiments are too limited to evaluate the effectiveness of the approaches. I did not understand the family tree primitives, unfortunately. What do upstream and downstream mean in this context? What is a “generation”? The experiments also do not show that the transformations described in the paper offer benefits for neural models. In fact, on the one data set and one neural architecture evaluated, using the best normalizations results in essentially the same performance as using raw unnormalized data. Even if the experimental results had been positive, the limited scope of the investigation would make it impossible to draw general conclusions. Minor:classical “or quantum” computation may require non negative feature values? Why mention quantum here? Tables 8  > Table 8The author refers to Automunge as “our” library at one point, breaking anonymity.<BRK>The paper also describes how to use the said library and the various options (including some new forms of normalization) available in the library. Experimental evaluation on Higgs Boson interaction dataset is provided, following the experimentation in Baldi, Sadowski and Whiteson 2014. The primary novelty of the library is to ease the burden of tabular data pre processing. 2.The paper provides evaluation on one dataset (Higgs Boson interactions) to support the claim that the library is of value for many machine learning tasks. The setup of the experiment is slightly different from the original paper (Baldi 2014): i.e.learning rate schedule is different, but network setup is the same. Looking at the results in the original paper (Table 3 of Supp material in Baldi 2014), the results presented in this paper are in the same range as the original paper (but also within the variance of results in the original paper), so it s difficult to extrapolate how significant the improvement due to retain normalization (this paper) is, and how it might generalize to other machine learning tasks, or even other datasets. From the usage examples and reviewing the code, it s clear that the library makes little use of encapsulation of data structures, leading to unwieldy usage. All documentation is provided as a single Readme, rather than as Sphinx documentation or API documentation. There don t appear to be any tests or continuous integration included. Overall comments:For what is mainly a description of an automated data pre processing library, the usage of the library is not sufficiently automatic (i.e.setting up the options amounts to feature engineering, which the paper claims to circumvent). The new normalization technique presented in the paper does not yet demonstrate a clear improvement over current options.
Reject. rating score: 5. rating score: 6. rating score: 6. <BRK>## ReviewGiven as set of pre specified policies, this paper proposes a Bayesian method to estimate the posterior distribution of their average values, by estimating posterior distributions of their discounted stationary distribution ratios. The authors take some time to reveal what is their estimand   the discounted stationary distribution ratios. + The paper proposes a method for evaluating non linear functionals of policy values, such as ranking scores over their values. However, it is not clear from the method description in Section 3.2 how one is able to estimate this joint distribution.<BRK>I am less convinced by the motivation of off policy model selection, since if we want to find some policy to deploy, does off policy learning address the problem? Can you comment how you address the stochasticity in the reward? This idea is not new (see DualDice) however utilize them to learn the distribution of density ratio is novel. The authors did a pretty good job in addressing this problem, though some problems remain and needs improvement.<BRK>Specifically, the authors estimate the posteriors over the correction ratios for state action pairs, which optimize a combined metric of a chance constraint from collected data and KL from the prior. Computationally, the authors demonstrate the advantages of their approach by having better performances in both coverage and power for policy evaluation and better downstream ranking with respect to different metrics for policy selection. I think the paper is well written and has made a good investigation of their approach. Finally, solving eq.14 needs estimation of some expectation over the state action visitation, while the data itself is not independent, how would this influence the results?
Reject. rating score: 4. rating score: 5. rating score: 7. rating score: 7. <BRK>They show modest gains with respect to existing techniques and they lack any information on run to run variance. Pros:   First work analyzing this specific pretraining for point cloudsCons:  Weak novelty  Limited experimental reliabilityOverall, the novelty of the paper seems rather weak. The only novel contribution is the idea of point cloud completion as a pretraining task. This idea is rather simple and similar techniques are well known and used in other fields such as NLP.<BRK>The idea of this paper is simple but fascinating. Actually there are many studies concerning the task of point cloud completion, but using it as the initialization approach to improve the other tasks is quiet novel. Considering the simplicity of this idea, similar initialization strategy can be formulated, such as pre training network on segmentation task and apply them on classification task. 3.Although the experimental results look solid, the reviewer still concerns if the proposed OcCo initialization can achieve the SOTA results or close enough to the current SOTA. So if the OcCo initialization can still succeed in the PointCNN which has better ability of learning point cloud features? In reviewer’s opinion, a method to generate partial point cloud from single view is essentially not a technical contribution for this paper.<BRK>This paper proposes a better pre trained prior for a variety of downstream applications in point cloud analysis. In downstream applications, the obtained encoder will be used as the initial weights in the network training. That provides strong support for validating the effectiveness of the proposed approach. 2.I also like the result that the initialization is only pre trained on the occlusions generated from the ModelNet40 but still work in another dataset. 3.The paper is well written and presented. Specifically, I would like more analysis of why such a pre training method can adapt to different datasets? I would keep my positive rating.<BRK>The paper considers the problem of training networks for point cloud processing through a point cloud completion task. In the current form, I do not think the paper is ready for publication as the paper, in my opinion, overclaims its contributions, misses relevant work (see also below), and misses crucial details necessary to understand the experimental results. I believe that these issues can be addressed, but I would base my final recommendation based on the authors  feedback. Here, the simplicity of the OcCo task coupled with its performance is clearly a major strength of the paper. ### After rebuttal phase ###The comments by the authors and the revised version of the paper successfully address my concerns.
Reject. rating score: 4. rating score: 5. rating score: 5. rating score: 7. <BRK>  UPDATE FOLLOWING DISCUSSION  The authors were responsive and suggested a series of updates to address the concerns raised here. Overall, though, my stance did not change: I’d like to encourage the authors to pursue their idea, but I don’t think the paper is ready for acceptance yet. (significance, effect size, stability)  END OF UPDATE  Summarizing the dynamics observed via high dimensional recordings of neuronal activity is an important problem in neuroscience. This paper addresses the problem of inferring the latent dynamics directly from calcium activity recordings by jointly optimizing for both steps with a sequential ladder variational autoencoding architecture. However, I do not support the acceptance of this submission due to the following major concerns:    Limited methodological novelty: The approach seems to be a direct application of a particular sequential hierarchical autoencoder to this particular calcium imaging problem. There is limited methodological discussion. Focusing on a single domain: The experiments cover a narrow domain for an application focused paper. (e.g, lag window size in OASIS, recurrent network size in LFADS)       b) The OASIS paper mentions different working modes, where only some of the possible parameters are optimized. How was OASIS run in this manuscript? The order of a dynamical system has an established definition in the literature already.<BRK>Quality: Developing appropriate methods to handle the calcium imaging data is an important question as calcium imaging becomes popular. Although some of the results are promising, unfortunately the work is rather incremental. Also, the improvement over simple baseline model is small. Originality: The method for handling the calcium traces is novel, although it is a relatively straightforward modification of LFADS. Significance:  As there is only small improvement over simpler, alternative methods, I feel the significance is relatively low. What “data” were referred to here? 3.The method shows some improvement over a few alternative methods when fitting the data in certain regimes. I worry that, in most scientific applications, the small improvement of CaLFADS over Gaussian LFADS and OASIS LFADS is unlikely to matter. *The paper by Hernandez et al (2020) is also related to this paper. They proposed methods to model the neural dynamics based on the ephys and calcium data. 3.It is unclear whether OASIS LFADS is worse than CaLFADS  is simply due that OASIS LFADS method did not carefully model the noise characteristics of the deconvovled spikes from calcium. While whether this additional ingredient helped with the performance of CaLFADS is unclear, I wonder if the authors had similarly incorporated this zero inflated gamma model to the OASIS LFADS method to make it a more fair comparison. The manuscript is now improved, and I am raising my score to 5.<BRK>I am familiar with the LFADS paper, as well as much of the literature on calcium imaging for spike inference, and latent dynamical systems more generally. The VAE/etc. Similarly, the encoding model was described informally in section 2, and with Figure 1, and Table S1 and Figure S1 helped, but I really wished there were equations. I ve looked through them, and the statistics carefully. If parsing the lower vs higher order dependencies is a key desiderata of this method, it would have been nice to explicitly see the approach doing that in the results. I would think not. This is particularly important for fast dynamics, because the post spike effect can be much more informative than the pre spike information. So, I would think that a more fair approach would be to use a batch, rather than online, comparison.<BRK>The empirical results presented on both synthetic and real data are also great as the authors demonstrate the ability to disentangle the neural factor from the calcium dynamics. As I mentioned previously, this work is a non trivial extension to not only LFADS but **also** to hierarchical VAEs. First, the authors should have at least provided a brief introduction to variational ladder autoencoders. For instance, in the original LFADS paper all the variables were a deterministic transformation of the latent variables (the initial condition and the inferred inputs); this allows the system to be completely characterized by these two groups of latent variables. From reading the two paragraphs in section 2, I thought this was also the case for CaLFADS but once I looked at the algorithm table in the appendix I found that $g_{1,t}\sim \mathcal{N}(\mu_{g_{1,t}}, \sigma^2_{g_{1,t}})$! Thus, it would be interesting to see how the model works when only a partial length of the trajectory is given. Sadly, more work needs to be done on the structure of the manuscript, specifically on details of the proposed approach. I will be more than happy to raise my score if the authors do this!
Reject. rating score: 3. rating score: 4. rating score: 4. <BRK>This work proposes an AutoML framework for multivariate irregularly sampled time series. Besides, hyperparameters and model’s configuration is optimized by using AutoML (including Bayesian optimization). The model is evaluated on the well known time series datasets UCR and UAE. Comments:***motivation***  this works combines different techniques including self supervised learning and hyperparameter optimization. But I cannot clearly find what’s their main contributions in this work since all of these techniques used in this work seems not to be new. I encourage authors to clarify their contributions formally. Besides, as I know, both of the datasets UCR and UAE are not standard datasets for irregularly sampled time series. This may be problematic since it should be missing data, but not true irregularly sampled time series. There may be two different topics in the time series community. ***representation learning*** since representation learning aims to learn some good representative features that can be easily transferred to other downstream tasks. But in this work, it seems that a single feature is learned from a segment of time series. Besides, since in the objective function, there are clustering loss and anomaly detection loss, I think we cannot say it is an unsupervised representation learning approach. To demonstrate that the proposed framework can produce a good time series representation, other downstream tasks such as classification or prediction need to be considered.<BRK>The results seem promising; however, the novelty and motivations are not that satisfied. Here, it would be better if you use the dataset which already has missing components. 9.Fair comparison  It is unclear how the authors optimize the hyper parameters of other methods. If the proposed method is well optimized (among various hyper parameter sets) but the baselines do not, it is not a fair comparison. 10.Computational load  AutoML takes much more computational load. It would be good to quantitatively analyze the computational complexity of the proposed method compared with other methods. The authors should provide various "time series" clustering and anomaly detection algorithms as the benchmarks. Note that K mean, GMM type of things are not designed for time series. I think this can be an important baseline to be compared with. Also, what is the proposed component for dealing with irregularly sampled data? And why does it guarantee suboptimal performance? If you have some motivations, please provide it in the revised manuscript. The below are the reasons for this. There are too many equations but only one sentence in the caption. Many selections in this paper have no underlying motivations even though there are many "blocks" in the proposed method. However, I cannot find the "quantitative" analyses in the revised manuscript and rebuttals for the computational complexity. AutoML is definitely not the novel part because the authors utilize Thomson sampling and Bayesian optimization. However, in the experiments, the authors provide the results on mostly univariate time series dataset.<BRK>This paper proposes an autonomous representation learning framework for multivariate time series with irregular sampling rates. 1) An AutoML solution for hyperparameters optimization under Bayesian framework is proposed to automatically seek optimal network structures and parameters. 2) Variational autoencoders based on generative approach and attention mechanism is employed to learn the semantic representation of time series with limitation of irregular sampling. However, there are some concerns as follows:(1)	The motivation for involving GMM model is not clearly discussed. The proposed sample energy function to denote the level of anomaly is not properly motivated as well. Because of irregular sampling, one computational difficulty comes from the observation that times can be different for each time series in a minibatch. First, there is no sensitivity analysis of the hyperparameters employed in the proposed framework. This paper lacks experiments on irregularly sampled time series data from real scenarios. Consequently, the performance evaluation on PhysioNet or similar datasets should be included in this paper.
Reject. rating score: 3. rating score: 5. rating score: 5. rating score: 6. <BRK>1: The scaling factors can be removed with batch normalization used. 3:  The binary weight networks can be further compressed. Moreover, the authors provide some empirical visualizations and results to demonstrate its analysis. **Pros:**  The observation that the signs of weights with large norms are determined and fixed at the early training stage is interesting. The idea that weights with large norms are stable and sensitive on sign changes can be utilized for improving the training of BWNs, maybe BNNs as well. I can only list some of them. 1): The contributions of the paper include the observation that binary kernels in the learned convolutional layers tend to be centered on a limited number of fixed structural patterns. Some theoretical explanations are needed. 3):  In Figure 4’s caption, what does “the certain appears in one certain Conv layer” mean? 4):  In Sec.4.1, the authors propose to apply QBN on model compression. However, it is not clear to me what the points the paper intends to express. Specifically, the authors claim that “we can reduce the number of parameters to represent a binary kernel by changing the small magnitude weights’ signs”. 3:  Extensive technical details are missing, which makes the algorithm difficult to understand. 1):   The authors should at least explain how to generate the power of two number of binary kernels in details in Sec.3.1.2):   During training, the paper generates $k$ bit (i.e., k is the bitwidth) number of binary kernels. However, during testing, the learnt binary weights should be fixed ($k 1$) and why the “Quant Bit” in Table 1 can be larger than 1? This makes me very confusing. 4:  This paper only binarizes weights while I am wondering whether simultaneously binarizing both weights and activations can have the similar observations. 5:  There is no comparisons with current state of the arts. The 3% loss on Cifar 10 is significant. 6:  There is no conclusions and future works discussions. 7:  It is widely known that the scaling factors can be absorbed into BN layers. In terms of this, it should not be a contribution of this paper.<BRK>This paper provides an empirical study of binary weight networks (BWNs), where they find that 1 the commonly adopted scaling factor is not critical 2 there exists a subnetwork that stabiles early in training 3 the 3x3 filters in VGG and ResNets demonstrate a sparse distribution. They combine all the observations and propose a novel quantization algorithm that achieves more aggressive compression than standard BWNs. The identification of a persistent subnetwork and the analysis on the sparse distribution of kernels are particularly interesting. + The proposed quantization algorithm is interesting, which has a potential of squeezing more redundancy out of standard BWNscons:  If I understand correctly, in the proposed algorithm the kernel distribution is only drawn from the last conv layer of the full precision network, which is then shared across all layers when retraining the BWN. This seems a strong assumption and needs to be justified. What s the reason to believe that the selected frequent kernels are shared across different layers? What s the reasoning of using the threshold when computing the distance to the frequency binary kernels? The experimental results seem to be really hard to interpret for me, and this is perhaps the weakest point of the this paper. In particular, Table 1 needs to have proper baselines. This includes the full precision, standard BWN accuracies, as well as controls which allow one to draw comparisons between the proposed algorithm and basic binarization by equating certain quantities. I suggest the authors work on the suggested improvements which will make this a much stronger contribution. However, I still have concerns about Table 1 (and Table 2). For example, I have a really hard time interpreting the significance of achieving a 3.2x CR with a loss of 3% (92.3   89.2 from VGG 7) in acc with the proposed method (although the paper argues that it s a "bearable" loss). Considering that this is the main experiment supporting the efficacy of the proposed quantization algorithm, I think the paper needs more controlled experiments to demonstrate the practical usefulness of the proposed algorithm. As a result I m keeping my original score and hope the authors can work on the improvements for the next version.<BRK>Summary:The authors show empirically that weight signs are more important than weight magnitudes. They also analyze the optimization process and the structure of optimized binary networks to note that i) there are clusters of weights whose weight remain almost unchanged during the optimization ii) optimized convolutional networks have simple sub structures  They exploit the latter to propose a new quantization method that increases the compression of binary networks. strengths:The idea of studying how binary weights update during the optimization is interesting and may help understand the optimization process of binary but also real valued networks. weaknesses:The meaning of the given simple proof is not clear. The paper would have much more impact if, after showing that BN can absorb the redundant factors, all real valued parameters were dropped. In some sense, the proof seems to show that the obtained result about the little influence of scaling is somehow expected. Regrading the sign flipping, observing that "flipping weights with large full precision magnitude will cause a significant performance drop compared to those weights close to zero" seems also something that one can expect. Finally, the presence of clusters is not really explained and shown in a very accessible way. Are there other results except from Figure 4 (whose caption sounds a bit hermetic: "The X axis indicates the index of a 3 x 3 binary weight kernel while Y axis indicates the frequency that the certain appears in one certain Conv layer")questions:   what happens if all scaling factors are kept fixed? is alpha usually shared by all weights in the network or is layer specific?<BRK>Overview:The Authors show that scaling factors with hand crafted or learnable methods are not so important when training Binary Weight Networks (BWNs), while the change of weight signs is crucial. They make two observations: The weight signs of the primary binary sub networks are determined and fixed at the early training stage. Binary kernels in the convolutional layers of final models tend to be centered on a limited number of fixed structural patterns. Based on these observations, they propose a new method called binary kernel quantization to further compress BWNs. They propose a binary kernel quantization to quantize the binary kernel, which effectively reduces the number of all possible kernels, and can further compress BWNs to 2 5 times. 2.The Authors observe that the weight with the large norm, fixing their signs in the early training stage. And they compared this phenomenon with the lottery ticket hypothesis and propose the primary binary sub networks. 3.The paper is well written and well motivated. It contains extensive and systematic related work study. I like it. The binary kernel quantization method is only applied to 3x3 kernels in this paper. 2.As shown in table1, why authors report the best accuracy among different seed? I think authors should report the averaged results with different random seeds instead of the best result. I think the author has addressed my first concern. For the second one, I would prefer to see the errorbar. I think the findings are interesting [share similar thoughts as R4 s], while the experiments part need to be improved.
Reject. rating score: 4. rating score: 5. rating score: 6. <BRK>This paper proposes a non uniform sampling method for stochastic gradient minibatches for SG MCMC. The idea to non uniformly sample the random stochastic gradient index to match SGULD is clever. Although the particular non uniform random sampling scheme is novel, it is not clear how the two approximations (MH and $x$) in Algorithm 1 affect the error of the sampler in practice. Although Theorems 1 4 may be interesting for the non uniform sampling scheme for stochastic gradient indices (Eq 4), in practice, Algorithm 1 is different due to the two approximations. In particular, I am concerned about the approximation for $x$ (especially since the definition of $x$ seems circular). Although it may appear that the approximation $r_{k+1}   r_k$ works well for your experiments, this may not be the case in general.<BRK>Then the authors proposed coupling these randomnesses so that the final variance is reduced. ## Strong and weak points of the paper### Strong points  Provided a new connection between importance sampling and stochastic gradient. Provided an asymptotic analysis for the variance reduction in Theorem 4, which seems novel variance analysis. ### Weak points  I am not sure the approximations introduced in Sec 4.3 are valid theoretically and numerically. Novelty: The idea seems very interesting and important in the community. Is this correct ?<BRK>I have some concerns regarding the practical implementation of the proposed scheme in general. Overall, I appreciated the discussion about the choice of the hyper parameters. Overall there are good merits to the paper, however there are some aspects on which I would like to have some further clarifications from the authors. The paper is overall well written with a proper formulation and justification of the problem it addresses.
Reject. rating score: 4. rating score: 6. rating score: 7. rating score: 7. <BRK>This submission has significant overlap with the following reference: Jonas Rothfuss, Vincent Fortuin, and Andreas Krause. PACOH: Bayes Optimal Meta Learning withPAC Guarantees. arXiv, 2020. Assumably, the submission is from the same authors as theoretical analyses and experiment setups are almost identical. The analysis of PAC Bayesian bounds led to the conclusion that Gibbs posterior can help minimize the bounds in the meta learning settings. My other concerns are as follows: 1. The authors may need to make sure about the main contributions of this submission with most of theoretical results already presented in the referred paper. 3.The experiments may need to be done more consistently. For example, it is not clear why figure 1 only compares PACOHNN with a vanilla BNN using a simply prior instead of, for example, comparing with MLAP or PACOH with GPs? For classification examples, the authors did not compare with neural processes as claimed at the beginning of section 6.1. How many training samples were used for the five tasks to get the reported results? In A.1, "to proof the theorem" should be "to prove the theorem..."; in step 1 and 2 of the proof, X_k was used to denote the data point and then task, respectively, which is indeed confusing. 5.The authors may want to add complexity analysis of PACOH NN. as stated since the number of tasks does factor in based on Algorithm 1 in Appendix B.<BRK>The paper addresses the problem of learning data driven priors for Bayesian neural networks. To be able to overcome these issues, the authors propose a meta learning framework based on PAC Bayesian theory, in which they optimize a PAC bound called PACOH in the space of possible posterior distributions of BNN weights. Unlike the previous approaches in the literature, their approach doesn’t rely on nested optimization schemes, instead they directly minimize PAC bound via a variational algorithm called PACOH NN which is based on SVGD and reparameterization trick. Experimental results both on synthetic and real world data seem fine, and there seems to be no fallacy in empirical evaluations. Empirical results from various experimental setups are sufficient to show that their framework improves both predictive accuracy and the uncertainty estimates compared to several popular meta learning approaches with additional computational advantages. On the other hand, paper’s contribution to the work of Rothfuss et al.seems limited.<BRK>### Summary:One of the main issues that BNNs have to face is the choice of good informative priors in order to provide precise information about the uncertainty of predictions. This is done by employing the closed form solution for the PAC Bayesian meta learning problem. The meta learner here learns a hyper posterior over the priors of the parameters of the BNN by using the closed form expression PACOH  (PAC Optimal Hyper Posterior). This is applied in the context of NNs, where the priors are to be defined over the NN parameters. Extensive experiments are carried out to show the performance both in regression and classification datasets, as well as its scalability. In all of these regards, the system seems to be competitive, improving on the results of previous state of the art methods and producing promising results in real world problems. ##### Pros:* The method does not employ nested optimization problems, therefore avoiding all the issues related to these approaches altogether. Thanks to the fact that it is agnostic to the inference method in use, it presents itself as a very general purposed approach, able to improve the predictive qualities of previous methods. How does choosing these distributions affect the final results? There have been recent works (e.g.Functional Variational BNNs) regarding artifacts that arise when using large BNNs which make the system not able to properly learn the data. Could these problems be solved as well with this approach since the final prior is constructed using the data?<BRK>The paper proposes a method for learning BNN priors based on optimising a PAC Bayes bound for meta learning, which they call PACOH NN. This would feel more in keeping with the Bayesian flavour, and might give the ability to selectively “freeze” (softly) any weights that are stable across all tasks. You cite the Bayesian MAML paper by Yin et al.However, in the experiments, you only use the regular MAML and first order version, both of which give point estimates. However, since none of the baselines are meta learners, we can’t be sure here if the difference is due to the effect of transfer learning or the superior uncertainty quantification. Another meta learner should be included. Figure S1 in the appendix is really helpful! If possible, I would prefer to see this in the main paperMinor comments/Typos:  P1 variation  > variational  P2 in the BNN description, the regression model have a noise parameter but the classification model does not. For the sub Gamma case, you reference Boucheron et al , which is a book. Please make the reference more specific  Tasks are defined as $\tau_i :  (\mathcal{D}_i, S_i)$, but it would seem more accurate to define them as $\tau_i :  (\mathcal{D}_i, m_i)$.
Accept (Oral). rating score: 9. rating score: 8. rating score: 7. <BRK>The paper goes on to show that this method, termed APHYNITY, produces significant gains in predictive accuracy over purely learned methods, purely physics driven methods, and other forms of combining physical models and learned models. When it is complete there may be some small advantage due to the neural residual accounting for some discretization error; regardless there is no harm, and the learned neural residual is very small. It seems to me that the authors do a good job explaining and relating to prior work on learning physical systems. I thought this paper was clear and well written.<BRK>### Summary of my understandingThe authors propose a method of function fitting for differential equations. They premise that a model $F$ is the additive combination of $F_p$ and $F_a$, which denote physics and augmenting parts, respectively. The functional form of the physics part, $F_p$, is given in accordance with prior knowledge, whereas the augmenting part, $F_a$, is modeled by neural nets. The related work can be more detailed given the recent active studies on physics + ML but seems sufficient from the viewpoint of ODE/PDE fitting.<BRK>However, since I was having a hard time knowing which sets & spaces were important in this paper, I was having a hard time understanding which conditions were important. However, it contains the proofs and backs up many of the nice claims in the paper. To my knowledge, other work on hybrid models do not have theory showing this. I think that the related work section should acknowledge that people have been using neural networks for hybrids with physical models since the 1990s. The theory is about the decomposition existing & being unique, not about this method being able to find that decomposition. I hope that another reviewer can.
Reject. rating score: 5. rating score: 5. rating score: 7. <BRK>### Strengths  The proposed idea is very simple and easy to understand and implement. For instance in Fig.2 for WMT how many "frozen" layers are used for the reservoir transformers? Namely, how much faster are the reservoir transformers for a given number of layers? Therefore, the experimental section should be thorough and should lead to clear conclusions. Due to the the newly proposed metric as well as the lack of comparison between training time and achieved performance lead me to propose rejection. ### Post rebuttal updateI would like to thank the authors for their additions to the paper. Having said that, I believe that the extra experiments and numbers do not paint a clearer picture. Namely, even with all those numbers I still cannot judge which reservoir layer will be better, in what sense it will be better (accuracy or efficiency) and why is it going to be better.<BRK>It s not clear what frozen layers mean for LayerDrop and it need to be clarified with more details. I didn t find clear comparison with stronger baselines on WMT in the revision. The main novelty of this paper is only the Reservoir exploration of Transformer structure. Cons:1.The reservoir operation was explored on other structures.<BRK>The paper explores the concept of randomization in Transformer architectures. PROs:  I find the paper very clear and well written & readable. The experiments offer a nice perspective on the advantages of the proposed modelCONs:  I find inappropriate the use of the term "reservoir" in this work. The experiments show that it is possible to avoid training in some of the layers of a Transformer. EDIT:I would like to thank the authors for the nice work during the review process.
Accept (Oral). rating score: 9. rating score: 6. rating score: 5. <BRK>This problem is well motivated   estimating dose response is a challenging and practically important problem. The paper is extremely well written. It explained complex (poor written) ideas in the semiparametric literature clearly. The theory, as far as I can tell, is sound. It improves the existing results in targeted regularization and can be adapted to analyze one step TMLE. The experiment shows that the model outperforms existing benchmarks on the task it set out to do. My main suggestion to improve the paper is to use some datasets that actually have continuous treatment in evaluating the method. In particular, I would be interested in seeing an application of the methods on real world datasets. I believe the paper at its current state is sufficient for acceptance. I want to thank the authors for writing such an elegant paper. I really enjoyed reading it.<BRK>This paper is to develop a varying coefficient neural network to estimate average dose response curve (ADRF). Although this paper has several interesting results, the paper is full of many typos and small errors. The current paper needs substantial improvement. Here are some detailed comments. 1.The introduction section is not well written since the logic does not flow very smooth. 2.The motivation for （1）can be improved. You miss period in (1). 3.In Theorem 2, please not use (1) (6) to itemize the assumptions, since you use them to denote the equations. 4.In the proof of all theorems, there are some obvious mistakes inside.<BRK>Solid theoretical results are provided to confirm the doubly robustness of the treatment estimator and outcome estimator. This paper is well written, however, I still have some concerns about the contributions. Some existing deep learning methods resort to balanced learning or reweighting. For the continuous treatment, how the proposed method addresses the confounder factors? 2.Another big concern is the varing coefficient neural network. Why the VCnet designed to be dependent of treatment information? The dependence is not theoretically discussed in this paper. 3.The experimental results are not convincing for me. Only two baselines are compared with the proposed method. But the design of VCnet needs the numerical results to confirm its effectiveness for ADRF.
Reject. rating score: 4. rating score: 5. rating score: 6. rating score: 6. <BRK>The writing of the paper needs improvements. You wrote “In the continual learning setting, we assume classes in Y_B are uniformly distributed in every new batch.” Is Y_B fixed for each task or each batch? This means that your algorithm cannot work in the scenario where a task has one class only. Most of existing techniques can handle this case although they use 2 or more classes in a task in their experiments. It is only suitable for Task IL, which is an easier problem to solve. The experiments are not well described, and baselines are old.<BRK> before revision Review: Motivated by the effectiveness and naturalness of energy based models, this paper proposes to use energy based learning framework for continual learning. + The underlying theory of the proposed method is developed by other papers, the only contribution of this paper is to apply the EBM to the continual learning, which is quite straightforward. Training EBMs with assisting networks can be found in [2] and [3]. I agree on that citing all those EBM application papers is not necessary. Thus, the whole contribution is quite marginal.<BRK>Summary: This work shows that energy based models (EBMs)  are a promising model class for continual learning problems. According to the experiments, EBMs outperform the baseline methods by several continual learning benchmarks. In the paper, the authors show that EBMs achieve a significant improvement on four standard CL benchmarks. It shows benefit of energy based model for continual learning. In this work, a difference architecture is usually in EBMS. The architecture is different from the baseline. The experiment setting detail is unclear. The training details of the proposed method and baseline are unclear. It seems the architecture of EBMs is different.<BRK>This paper explores the usage of EBMs in continual learning for classification. Although the application of EBMs in continual learning is novel, the general idea is a special case of the usage of EBMs for structured prediction, which has been widely studied. I believe the works in using EBMs for structured prediction must be cited here as they are closely related. The authors also explored the effect of ML training on interference with past data and showed that using single sample ML approximation can significantly alleviate the catastrophic forgetting problem. I believe that this is an interesting observation.
Reject. rating score: 3. rating score: 4. rating score: 6. rating score: 7. <BRK>It is not nontrivial that the problem (1) leads to a tree. It would be necessary to have some proposition that ensures the desired properties (e.g., solution represents a tree) of the solution or cites such previously known ones. The obtained solutions in (3) (7) are not discrete and how to round up continuous solutions to discrete ones are not shown. As a summary, the current writing lacks important information such as the correctness of the solutions, which significantly reduces the contribution of the paper, even if the experimental results look better than previous work.<BRK>The proposed method is interesting and addresses the tree learning algorithm from different perspective. .It is nice that the method is quite generic and can be applied to both (semi)supervised/unsupervised (e.g.clustering) problems. First of all, I found this paper is hard to follow and understand. Providing a pseudocode of the overall algorithm would be beneficial. What is the use of f(.)? I know that it is a linear function or MLP. It is not well described in the paper. .Experiments lack sufficient evaluations (for regression and classification). Most importantly, I strongly suggest authors to include more advanced baselines (e.g.[1,2,3]) for both classification and regression.<BRK>## SummaryThis paper presents a new approach to learn decision trees via sparse relaxation. The paper includes experiments on publically available datasets, showing how the decision trees produced via sparse relaxation to decision trees produced using other tree induction approaches. ## Pros  Interesting new technique for a canonical problem. Proposed method produces trees that appear to perform well on classification, regression, and clustering problems. ## RatingI have awarded the paper a 6, though I am open to increasing my score if the authors can address some of the questions I include below. In effect, the main argument for the proposed sparse relaxation technique is that it produces trees that perform well on five datasets. I believe that my concerns should be easy to address – either by reporting the optimality gap of the trees in the experimental results, or by including comparisons with methods that are guaranteed to find optimal trees. Reporting the optimality gap would provide some evidence to evaluate the integrity of the decisions you made in algorithm decision. It could also showcase the value of sparse relaxation as a feasibility heuristic for the MIP based approach. This is not the case. I recognize that there has to be a limit to the number of experiments that one can perform.<BRK>The trick is to use gradient based methods for a new sparse relaxation of MPI. I wonder if cvxpy is mostly implemented in Python, given that the authors implemented their method in C++, I would expect a better performance just from that. The paper overall is of good quality. However, authors show strong empirical evidence that their method is faster and competitive to existing results.
Reject. rating score: 3. rating score: 6. rating score: 7. rating score: 7. <BRK>The NASOA first employ an offline NAS to select a group of models and then pick up the most suitable model from this group via an online schedule generator. Weaknesses:The proposed method may be useful in some fine tuning scenarios, however will have low overall impact. From the reviewer s view, lack of novel and interesting part.<BRK>The first step of the approach is an offline NAS to form a pretrained training efficient model zoo, which is followed by an online scheduler which selects the most suitable model and generates a personalized training regime with respect to the target task. In addition, the proposed joint/block macro level search space is novel too. The practical benefits of the proposed approach are obvious for AutoML systems, and the shared ET NAS model zoo will help other researchers working on this topic. It’s not clear to me how the models are organized into groups in Table 2. I understand that it’s due to the lack of space, but it is a little bit frustrating to have no information in the paper about this point, especially about the modifications compared to the original algorithm.<BRK>In this paper, a joint Neural Architecture Search and Online Adaption (NASOA) framework is proposed to achieve a faster task oriented fine tuning upon the request of users. In particular, two main contributions are made in this paper: (1) A fine tuning pipeline that seamlessly combines the training efficient NAS and online adaption algorithm is introduced, which can effectively generate a personalized fine tuning schedule of each desired task via an adaptive model for accumulating experience from the past tasks. To the best of my knowledge, the skip connection is effective to avoid the gradient vanishing problem in a very deep neural network, therefore the skip connections at the lower layers are very important in the network design.<BRK>This paper is the first to address the problem of searching better backbone architectures for downstream tasks and online hyper parameter selection. The whole system proposed in this algorithms offers an end to end solution for producing a well trained architecture for a specific downstream task within a fixed training budget. Through extensive evaluation experiments, the effectiveness is of this system is demonstrated. The searched architectures are not  directly optimized for down stream task performance. 2.This system cannot address the problem with consistently changing down stream task, and the cost of training the scheduler for a new downstream task is a little bit huge because of the large pretraining model zoo. Overall, I recommend this paper to be accepted due to its engineering efforts, several interesting conclusion drawn from empirical study and great impact in industries.
Reject. rating score: 3. rating score: 5. rating score: 6. rating score: 6. <BRK>This paper studies gederated Generative Adversarial Networks (federated GANs). In particular, the authors propose a new method, UA GAN, which is claimed to be better than earlier approaches. I have several concerns. First, the writing can be improved significantly. This makes it hard to understand what are the results and to verify their correctness. Moreover, the theorems refer to "optimal discriminators", however there is no guarantee that one can find such an optimal discriminator or that one can even verify whether a discriminator is optimal. (In fact, the paper doesn t define the term "optimal" precisely)   Even if it would be possible to learn an optimal discriminator, then there is no bound on the amount of time this would take (as that depends on various parameters of the learning problem and the learning algorithm). The paper contains more vague statements, e.g., Remark 1 claims the method preserves privacy, but doesn t define what information is kept private. In conclusion, it is hard for the average reader to understand the paper due to a lack of precision, and the paper insufficiently specifies definitions, assumptions made, and precise formulations of results. Later, Equation (2) suggests that z can be drawn from \mathcal{N}(0,1) and hence is a real number. With the help of Eq (2) some readers may be able to guess that D_j must have values in the open interval (0,1).<BRK>This paper proposes federated GAN with multiple private discriminators. The authors also analyze the optimality of the proposed federated GAN. The review has several concerns on the current submission:1. Is minimax problem（5)  the total loss of UA GAN that is optimized by Algorithm 1? Hence，why D_i defined in（2）is the solution of （5）when G is fixed？2. When and how does the Nash equilibrium of minimax problem （5）holds？ 3. Lack of experiments for federated unconditional GAN to verify the theory.<BRK>The paper provides theoretical analysis on its proposed method and conducts experiments on toy datasets and mixtures of real world datasets to simulate a federated learning setup. Pros:  The problem of training GANs in a federated learning setup is an important problem to tackle as it naturally provides a secure way to train a generative model. It would be nice to also have some experiments in the experimental section that shows the results of this setup i.e with smaller dataset size for some of the discriminators  The experimental results will also be more convincing if results of training the UA GAN on the mixture of all three real world datasets (Font, MNIST and Fashion MNIST) are shownOverall, the proposed method of training GANs in Federated Learning setup shows fairly convincing results. With some additional experimental results I believe this will be a good submission for ICLR. I thank the authors for the additional experiments which have marginally satisfied my initial concerns; ideally, more setup can be experimented.<BRK>This paper proposes an algorithm for training GANs in federated learning setups. Toward this goal, the paper proposes Universal Aggregation GAN (UA GAN), in which at every iteration the server communicates a batch of samples produced by the generator to the local nodes and then the local nodes optimize their discriminator using their observed real and received fake samples. I look forward to the authors  response to my comments. In such scenarios, the generalization error of training one discriminator per local node can be very large, since every discriminator is trained using a limited number of real data points. Currently, there are no theoretical guarantees or numerical analysis on the generalization properties of UA GAN. I think analyzing the generalization behavior, either theoretically or numerically, is needed to address this comment, especially becuase the paper mostly focuses on the theoretical properties of UA GAN. The paper s numerical expeirments only consider setups with non identical distributions across nodes. I recommend providing some numerical results for larger federated learning settings with smaller training sets. These numbers sound closer to a real federated learning problem than the ones used in the paper s experiments. In the current version, it remains unclear how many gradient steps are used to optimize the parameters at every iteration and how the performance of the trained GAN is affected by the number of gradient steps applied for optimizing the discriminator parameters at every communication round.
Reject. rating score: 3. rating score: 4. rating score: 4. rating score: 5. <BRK>##########################################################################Summary: This paper presents an end to end deep retrieval method for recommendation. ##########################################################################Pros:  Novelty: this paper present a method that, as far as I know, is novel. Comparison to tree based methods: the paper presents an interesting comparison with tree based approaches ##########################################################################Cons:   Lack of validation: first and foremost, the work presented in this paper lacks validation experiments. We would also expect more metrics, and in particular, the right recall values as is commonly done in the field. In particular, other papers use recall@20 and recall@50 (as can be seen in paperswithcode.com) instead of recall@10. Missing state of the art: the paper misses on significant portions of state of the art regarding the evaluation. The paper claims to address "large scale" recommender systems (at several places in the paper) but does not address this aspect. Lack of clarity: The clarity of the paper could be greatly improved by putting the description of the algorithm in a single place.<BRK>The paper presents a method for "end to end" learning for retrieving top k items in recommendation system setup. Also, it is not clear how the proposed method addresses data scarcity, which according to the paper happens only in tree based methods, and not in the proposed method as there are no leaves. Also, what seems to be missing is why such an architecture of using stacked multi layer perceptrons should lead to better performance especially in positive data scarcity situations where most of the users  like  or  buy  only few items. The experimental comparison looks unclear and incomplete. Also, it would also be good to see the code and be able to reproduce the results. 2020, where both the papers have been accepted in ICML conference of respective years.<BRK>The paper seeks to propose an end to end learnable retrieval model for recommendation, to replace existing two step based approaches (learn embeddings first, then do MIPS search). The underlying model structure is motivated by KD code (Chen et al.2018), where each item is encoded as a D dimensional discrete vector (with a cardinality K in each dim). Then a conditional, probabilistic framework is proposed to learn the model with a multi path extension for further improvement. However, I have some concerns in motivation, methods, and experiments. The significant feature of the DR model (from the claim in the abstract) to encode all candidates in a discrete latent space. For example VQ VAE[1] also learns a discrete space. Another more related example is HashRec[2], which (end to end) learns binary codes for users and items for efficient hash table retrieval. The experiments didn t show the superiority of the proposed method. I didn t understand the motivation of using the multi path extension.<BRK>The proposed method is claimed to perform on par with the brute force method, under sub linear computational complexity. This is an ongoing research discussion on this domain. That being said, it seems the user embedding is highly important in this model. A user can be modeled in a various ways, e.g., as a sequence of items consumed, or using some meta data. We would like to see more discussion on this. (b) The baselines used in the experiment are not representing the current SOTA. The authors used only one k for P@k, R@k, and F1@k, arbitrarily chosen for each dataset. Overall, the paper is well written.
Reject. rating score: 5. rating score: 5. rating score: 6. <BRK>**Summary:** This paper proposes to use evolutionary computation to search for optimal activation functions tailored to a given architecture. The proposed PANGEA method practically re implements a partial Genetic Programming setup applied to searching for neural network activation functions, with interesting (though minor) results. The literature review covers most basis, though with some crucial misses (GP and Random Guessing). The core of the paper is well put together, but it requires some more work. **Comments:**  Working with 1 dataset and 3 variations of 1 architecture is too small a baseline to prove the point. While I personally believe in the presented results, further experimentation is necessary. The paper exposure is very pleasant, but it effectively hides the relatively small size of the presented work. Figures 4 and 5 are unnecessarily over sized and need to be shrunk. The claim that the evolved activation function is tailored to the architecture is central to the work, but it is not proven. It should be shown with cross evaluation of activation functions and architectures or removed from the paper. The method needs then to be evaluated as a whole, not the single produced results, as the performance eventually depends on the number of reruns (tending to infinity).<BRK>In this paper, the authors focus on the task of discovering activation functions via evolutionary methods. The paper is well written, easy to follow and technically sound. Some of those activation functions are, e.g., APL [1], PAU [2], Mixtures of activations [3], and SReLU [4]. However, your approach has an associated computational cost that is very high, and therefore it is very important to distill what makes it work and raise the following questions:  Is it possible that the improvements come from the parametrization?. What is the behavior when you do exploration but not allowing parameters?. The general form can be easily achieved by a parametric version $\alpha SoftPlus(\beta x)$, which appears often in table 2. Since the SoftPlus is a soft version of the ReLU, one could also find interesting $\alpha ReLU (\beta x))$. Indeed, your search algorithm is useful, but the results seem to point in the direction of parametric learnable activation functions, therefore a comparison to that family of activations is very important. Learning activation functions to improve deep neural networks. Deep learning with s shaped rectified linear activation units.<BRK>In particular, a mutation is added which adds trainable parameters to the activation function. The discovered activation functions are compared on three different architectures to several state of the art activation functions. However, this idea is not new. It would be interesting to know whether the authors used this function or the parametric version proposed by the original authors defined as x*\sigma(\beta * x). The authors claimed that CIFAR 100 is more difficult than CIFAR 10. The generalization is only considered on ResNets. Concluding, the paper is well written and seems to provide a solid empirical evaluation. I think the authors could be more explicit about the novelties presented in this work and why they matter. This might be very similar to earlier ones but the authors convinced me that it is a useful contribution. For this reason, I increased my score.
Reject. rating score: 5. rating score: 5. rating score: 6. rating score: 6. <BRK>#### Paper summary:In this work, a meta active learning approach is proposed to learn the hidden dynamics of control systems, where safety is also a major concern. The performance of the proposed approach is verified from real datasets of deep brain stimulation by outperforming existing baselines for a significant gap in both accuracy and computational complexity. #### Disadvantages:  From the perspective of machine learning, the proposed approach is a combination of existing techniques, thus the technical novelty is limited. The paper is more about solving a specific robotics problem, which may not lie in the major focus of the ICLR conference.<BRK>While it is understandable that the framework is trained on simulated data, is there any justification with respect to the quality of the data simulator used in these two problems? The framework makes use of ideas from active learning, meta learning, and reinforcement learning. 3.While the paper takes safety during the learning of the dynamics as a critical issue, is there any justification with respect to the percentage of safety achieved by the proposed method?<BRK>Summary: This paper presents a meta active learning approach to obtain an LSTM based embedding of a dynamic system and use a chance constrained (probabilistic safe) optimization to find optimal control configuration via applying mixed inter linear programming (MILP) on the learned embeddings of the dynamic system. Weakness:   The algorithm relies on the assumption that the safety region must be hyper ellipsoid with a known radius and the model error must come from a Gaussian distribution with known mean and variance. This makes the algorithm hard to apply for problems when the safety region is unknown or changing over time, which happens often in most real world problems.<BRK>The authors proposed a meta learning approach for active learning in the context of robot control. What I like about this work is that there re several illustrations that make it clear which problem setup it is or what the authors meant by safety. However, I also have some concerns about this paper that needs to be addressed. In the experiments, there are 5 methods chosen to be compared against. But the experiments only show the results of everything combined without detailed analysis of the performance of each functionality while keeping others fixed.
Reject. rating score: 2. rating score: 4. rating score: 4. rating score: 5. <BRK>This paper introduces DYnamic multi Agent Relational Inference (DYRI), which is an extension of Neural Relational Inference (NRI) for dynamic relations. This extension shows improved performance on the real world basketball trajectory dataset. The article is fairly well structured, apart from the literature review, which is somehow missed very important and very related works. My main concern with this paper is a novelty. The main argument is to increase the expressive power of static NRI for dynamic relations, which I think is a very valid motivation for this paper; however, this idea has been already published as Dynamic Neural Relational Inference (dNRI; Graber & Schwing, CVPR 2020). Based on my understanding, the model and inference of DYRI are the same as dNRI. The authors have to clearly discuss their difference with dNRI if they still think those are different. Also, Dynamic Neural Relational Inference for Forecasting Trajectories has been proposed in CVPR 2020.<BRK>This paper presents a method for dynamic relational inference for multi agent trajectory prediction. The method extends the neural relational inference (NRI) (Kipf et al., 2018) by changing the static relations between agents to dynamic relations. The paper conducts experiments on physics simulations and basketball trajectories to show the superiority of the proposed method against different variants of NRI. **Pros:**  ++ The paper is well motivated and clearly written. [1] Dynamic Neural Relational Inference. Graber et al.CVPR 2020. **Rating Justification:**  A very similar idea to this paper has been proposed in prior work [1], which significantly reduces the novelty of this paper.<BRK>This paper builds on Kipf et al.(2018)’s Neural Relational Inference. In particular, this work introduces a latent variable model which treats the interactions (i.e.relations) between different agents as dynamic and time varying. An agent’s future state is conditioned on its history of states as well as its interaction variables with other agents. The authors ascribe the observed loss in performance to “the extra uncertainty introduced by estimating more latent variables.” I’m not totally convinced. Why not model relations as continuous latents (setting aside the fact that NRI used discrete variables)? Figure 2 shows the hidden interaction nodes z^{ij}_t are not conditioned on any other nodes.<BRK>The authors propose a novel Relational Inference system that learns to predict the graph structure underlying the data as well as the updated state of the system. Dynamically infer the graph structure underlying a trajectory, from data and use it to predict the next step2. A specific method to accomplish the first contribution. Moreover, while NRI is a strong baseline, other similar methods have been proposed to tackle relational inference e.g.Graph Attention Networks, Transformers and even straight Graph Networks. Finally, predicting the graph structure lends itself to really interesting analysis, and the dynamic aspects of this are completely unexplored. The datasets reported in this work include up to 10 interacting objects. Since the dynamic inference of the connectivity structure is especially apt to larger scale experiments, it would be really great to see results on more complicated datasets that include computational cost considerations when compared to static graph baselines, although this might be outside the scope of this initial paper. Thank you very much for sharing these cool ideas.
Accept (Poster). rating score: 7. rating score: 7. rating score: 4. rating score: 3. <BRK>This paper gives a new perspective on generalization, motivated by the success of self supervised learning, especially on noisy data. The main point of the paper is that the memorization gap is smaller for self supervised, simple algorithms compared to full supervised algorithms.<BRK>The present paper aims to understand the generalization capability of self supervised learning algorithms that fine tune a simple linear classifier to the labels. In Fact I, why do we need to take the max with zero? In Fact I, it would be helpful to comment on the effect of changing eta. Overall, the paper is clearly presented, innovative, and has interesting empirical and theoretical results.<BRK>The paper analyzes the generalization gap for self supervised learning. Therefore removing these assumptions does not seem to be a real contribution here. It will be more interesting to me if the paper focuses more on the RRM decomposition and gives some further insights into how to use these terms in the future to improve the generalization performance of certain algorithms or tasks.<BRK>The bound proposed by authors is not a generalization bound, thus it is meaningless to talk about the bound is vacuous or not. There can be some misunderstanding on some of the points in the paper, but overall, with the current presentation, I think this paper is not ready for acceptance. Instead, the authors only show the empirical estimation on the robustness gap and  rationality gap. I would like to say, such inaccurate claim makes me feel uncomfortable.
Accept (Poster). rating score: 9. rating score: 7. rating score: 6. rating score: 5. <BRK>This paper alleviates two problems in recent multi hop QA models. 4.Questions      In Table 4, the Decomp method is based on DPR (dense passage retriever). The other problem is computational efficiency. MDR generates such query vectors by comparing the given question and previously retrieved documents. In the section "Question Decomposition for Retrieval," the authors conclude that question decomposition is unnecessary in the context of dense retrieval with a strong pretrained encoder. However, Table 4 shows that question decomposition with a simple open domain QA model has a similar performance to MDR.<BRK>Summary; The paper proposes a simple, clever, and as far as I can tell novel, combination of dense retrieval techniques and pseudo relevance feedback for multi hop (complex) open domain QA. This is an foundational line of work going back at least to the research of Rocchio in the 1970s. Cons:  The paper could be organized better in its final version. Pros:  The paper shows that the idea is effective in terms of performance, yielding state of the art results on two multi hop datasets, HotpotQA and multi evidence FEVER.<BRK>Summary:This paper proposes multi hop dense retrieval for open domain multi hop question answering. It extends previous dense passage retrieval into the corresponding multi hop version by using retrieved passages to latently reformulate the query representation after each retrieval pass. In the end, it can significantly improve the performance on HotpotQA and multi evidence FEVER dataset. The analyses are very comprehensive and extensive from almost every relevant perspective. This is the first work in this direction. If so, why the improvement of multi hop version DPR is so significant?<BRK>This paper extends the recently proposed dense retrieval methods to the multi hop open domain questions, so as to handle complex multi hop queries. The authors conduct extensive experiments on two multi hop datasets, HotpotQA and multi evidence FEVER, and evaluation results demonstrate that the proposed model achieves impressive results on both the knowledge retrieval task and multi hop QA. My only concern is about the novelty of the paper. Besides, compared with [1], this paper seems to just replace the RNN based encoder in the knowledge retriever with the BERT based encoder.
Accept (Poster). rating score: 7. rating score: 7. rating score: 7. rating score: 7. <BRK>Two techniques are introduced for this purpose: skip layer channel wise excitation (SLE) modules, and regularization of the discriminator via a self supervised auxiliary task. However, as is stated in the paper, the StyleGAN2 model has twice as many parameters.<BRK>Summary:This paper proposes a lightweight GAN architecture which is tuned for learning generative models in the case where one has access to only a relatively small datasets, as well as a simple autoencoding modification for GAN discriminators to help prevent overfitting and mode collapse. Results are presented on a range of benchmark and new datasets, using standard metrics to compare performance against existing models. As the authors’ architectural changes are not this simple, one would expect that, for an equivalent FLOP budget and training budget, the new architecture would outperform a “StyleGAN slim,” but it would be good to have quantitative evidence on this front. My take:This is a decent paper, with reasonable (albeit not perfect) presentation clarity and passably significant results.<BRK>In this paper, the authors introduce a new framework for unconditional image generation. The introduce a skip layer excitation module (SLE) that allows gradient flow between activations of different spatial size. This seems to be missing in this model. Different fonts are used. What’s the resolution of the crop? 5) Similarly, comparisons with StyleGAN2 have different training time and batch sizes. A figure showing the reconstruction will be good.<BRK>Overfitting is certainly one of the main challenges in a few shot synthesis setting. Strengths1) The paper proposes an approach for a very challenging and important task of training GANs with the small amount of training data. SS discriminator:The employed auto encoding approach is a typical method for self supervised learning. Minor1) Why also not to employ the SLE module in the discriminator? I note that the paper studies an important problem, achieves good results, and advances GANs extending their application areas. Post rebuttal:I believe the authors have done sufficient work in their revision to address my concerns. The comparison is done using only one metric   FID.
Reject. rating score: 3. rating score: 5. rating score: 5. rating score: 7. <BRK>The tasks are useful when choosing and evaluating different optimizers (e.g.ADAM) for learning tasks. Because of this I recommend the work be rejected. However, my main concern with this paper is that, while TaskSet may be a useful tool for facilitating future research, it is not clear to me that it itself represents an advancement of novel research, which I think should be the bar for acceptance to a major conference.<BRK>Ideally a third dimension of tasks should also be added to the figures so that we know that this meta learning approach generalizes over a variety of tasks. I also think that the comparison between the proposed meta learning approach, vs. the regular hyperparameter search on a given dataset should be made clearer. Right now it is limited to figure 3, and in my opinion the details on how the random search is carried out is not clear enough. The paper is not very difficult to follow, but I am not super convinced of an actual practical use cases.<BRK>Pros:+ Creating a dataset of tasks for learning optimizers is a interesting and useful goal. A large number of tasks is proposed. The experiments are interesting overall and show some insights about the performance of the learned list with increasing number of tasks. I think that this could be better realized in the paper. How are new methods to be benchmarked through this dataset? How are existing datasets for this missing important aspects? The focus on the dataset of tasks is poorly realized in the paper, which is devoted in great part to how an ordered list of 1000 hyperparameter configurations for a learned optimizer performs well in comparison with other random search baselines. While this was well executed overall, it is not initially the focus of the paper.<BRK>The paper presents a suite of deep learning focused optimization problems that would facilitate the development of learned optimizers. In my opinion, this is very valuable. For example, it was not clear to me until I made multiple passes that the application presented in the paper is about meta learning hyper parameter initializations for deep learning optimizers by levaraging the suite to generate the meta learning data set with various task executions. Moreover, given that the main advantage, to the best of my understanding, of the proposed TaskSet is in learning optimizers, the choice of tasks in the paper lack proper qualitative or quantitative justification. In that case, it is not clearly discussed (at least in the main paper) why these set of tasks are selected and why/how they lead to better learned optimizers. However, the lack of proper justification for the choices (beyond just covering a laundry list of deep learning architectures, data sets and applications) to the best of my understanding leaves me partially unsatisfied.
Reject. rating score: 3. rating score: 5. rating score: 6. rating score: 8. <BRK>In this paper, an extension to deep kernel learning is proposed. Experiments are conducted on synthetic data and UCI datasets, with comparison to DKL with linear kernel and deep ensembles. The framework follows closely from DKL with a linear kernel. Also, I am confused why the “concatenation” of features is referred to as “ensemble” in the paper. In terms of experiments, it is weird that for DKL linear kernel is used instead of the original spectral mixture kernel with random Fourier features. However, the fact that a multi output learner is not equivalent to M different single output learners makes it hard to justify the proposed architecture theoretically from the universal approximation theorem.<BRK>This paper proposes a variant of the Deep Kernel Learning model (DKL) [1] where multiple independent networks are trained for the features instead of a single network. The model is compared against Deep Ensemble (DE) and DKL on a synthetic dataset and the UCI dataset. The main weaknesses of the paper is that by using a linear kernel as the base kernel, it is not clear how the proposed model is different from a Bayesian Neural Network with an extra linear layer. Furthermore, the experiments do not necessarily showcase the strengths of the proposed model, namely, the universal approximation property and the regularization that an ensemble provides. Deep kernel learning.<BRK>This paper proposes a special case of deep kernel learning (DKL) using a linear base kernel, where the inputs to the kernel are the outputs of neural nets with identical architectures (which could be thought of as learners that get ensembled); the resulting approach is called deep ensemble kernel learning (DEKL). The paper provides a universal kernel approximating result for DEKL (Theorem 1), explains how to train DEKL efficiently for large datasets using variational inference, and demonstrates that DEKL can outperform deep ensembles and DKL on various datasets. In terms of significance, I find this paper to constitute more of an incremental advance.<BRK>The authors introduce a deep ensemble kernel learning approach as a linear based learning combination, from a deep learning scheme, to approximate kernel functions under a Bayesian (GP) framework. Namely, a universal kernel approximation strategy is proposed from eigen based decomposition and deep learning based function composition. Then, a variational inference strategy is used to solve the optimization from kernel based mappings. Results demonstrate the benefits of the proposal. Your approach can be scalable from the stochastic variational variance; however, the studied databases are small in terms of the number of samples so that the well known GP algorithms could be compared.
Reject. rating score: 2. rating score: 2. rating score: 3. rating score: 6. <BRK>Furthermore I am not convinced of the correctness of the proof since there appears to be an assumption that $\gamma$ is sufficiently larger for $\ell(\theta; x)   \gamma c(x,x_0)$ to be strongly concave. The theoretical contributions of the work, however, are largely already present, or easily deduced from results that are either already published or available online in a pre published format. I welcome the authors and other reviewers to draw my attention to any contributions in the paper I may have overlooked by mistake.<BRK>See, for example, Kuhn et al.(2019) (https://arxiv.org/abs/1908.08729). The topic is definitely important and the authors did a good job of explaining their framework. Besides, there are numerous careless flaws in the paper. 1.Theorem 1 appears to be a standard result in distributional robust optimization and it is unfortunate that the authors did not recognize it.<BRK>This paper proposes smoothing the classifier in the distributional robust learning framework by adding random noise to the input. The smoothed distributional robust framework is used to gain robustness against adversarial perturbations in settings where the classifier is originally non smooth and then smoothed via the additive noise. The theorem needs to be revised since the current version is incorrect. Because of the above issues, I don t recommend this paper for acceptance in its current form. The paper should be revised to resolve these major issues.<BRK>##########################################################################Summary: The authors propose to use the worst case population loss (with bounded Wasserstein distance) over noisy inputs as a robustness metric. The smoothnessof the loss function ensures the problem easy to optimize even for non smoothneural networks (since these are smoothed by expectation over noisy inputs). al s work. isn t the smoothness simply coming from the expected loss over distributions "close" to the input distribution? ##########################################################################Questions during rebuttal period:  Please address and clarify the cons above  #########################################################################Typos: "The smoothness of the loss function ensures the problem easy to optimize even for non smoothneural networks." grammarThe authors use "P" in the introduction before defining it.
Reject. rating score: 2. rating score: 5. rating score: 6. rating score: 6. <BRK>The paper proposes ClusterFormer to address the problem of quadratic compute requirements of the attention mechanism in a Transformer model. To this end this paper proposes to combine local attention to promote local consistency and proposes KMeans clustering to gather global information for every token. The paper establishes strong results on the long form question answering task of Natural Questions in an extractive setup, with it getting the leaderboard position ahead of ETC large. While the idea in the paper is natural and the results on NQ are strong, unfortunately the idea in the paper is not new and has already been introduced in the work "Efficient Content based Sparse Attention with Routing Transformers" [1, 2] which the authors fail to cite or credit. Therefore, I recommend rejection.<BRK>The paper describes a method to handle long documents for question answering. Instead, they propose an approach that clusters individual vectors, and allows communication (attention) among the locations in the same cluster. I am not sure about the intuition behind this   why would communicate between similar vectors more efficient than dissimilar or randomly chosen vectors? I am not sure the results are strong enough to support that clustering is better than random assignments. Please make it clear. 3) would this work on a really lengthy QA dataset such as narrativeQA?<BRK>Or a fraction of it? The approach yields state of the art, and top of leaderboard, results on Natural Questions (long answers). Cons  One of the arguments for the paper is that it is not clear if related methods, like Reformer, can generalize to long sequences. However, in the evaluated implementation (Table 2) LSH is not that much worse than k means. The authors should discuss this work. Num tokens in context? Why using cluster layers at only 2 fixed depths?<BRK>This paper presents a model that is reduces the complexity by grouping related tokens into clusters, such that self attention is applied only within each cluster. The approach appears to work quite well on question answering datasets for which the approach achieves state of the art results on three datasets. The paper is well written and the presentation is very clear. It would also improve the presentation to compare the efficiency of the proposed approach to other methods at varying input length sizes. Similarly, it would be interesting to show the performance of the proposed approach compared to baselines for varying maximum sequence lengths. This would be particularly important to analyze how the model picks up information across long sequences (i.e., showing that clusters are not made up of tokens from the same sliding window). ICML 2019. http://proceedings.mlr.press/v97/lee19d/lee19d.pdf**Questions for the authors:**• Please see questions in the details of k means section.
Accept (Poster). rating score: 7. rating score: 6. rating score: 6. rating score: 6. <BRK>The authors propose to learn richer audio visual representations for weakly supervised action localization. First, the authors propose a multi stage cross attention mechanism to collaboratively fuse audio and visual features. Extensive experiments on two publicly available video datasets (AVE and ActivityNet1.2) show that the proposed method appears to be effective. The authors did not give the explanation of the built network in more details.<BRK>They introduce a multi stage cross attention approach to fuse audio visual features and a consistency loss to enforce temporal continuity. StrengthsThe approach is well motivated and the technical contribution is clear. The paper is well written and the contributions are evaluated against a number of recent SOTA approaches on two datasets for action localization. Weaknesses/ConcernsI would like to see a discussion on the drop in performance in the 3 stage model compared to the 2 stage and 1 stage models in Table 1. This drop is not intuitive so the authors should provide some analysis here. Are these results with the 2 stage model?<BRK>This paper introduces a method for predicting the class of an action in a video and localizing the range. The focus is on using both video and audio modalities. They use a cross modal "multi stage" attention mechanism, a background classifier, and losses designed to encourage temporal consistency in the output. Reason for Score:I would accept this paper. Table 2 shows ablations for the different parts of the loss. Cons:I is not clear multi stage attention is necessary   one stage seems to do well. Did the authors try different hyperparameters (eg.units/dimensions of single attention block) for one stage of attention.<BRK>The authors propose to mix the independent embeddings of audio and visual data by a set of cross attention layers to the task of audio visual event localization. Pros of the paper:Extensive experimental results. Regarding the experiments, on one hand they perform ablation studies that confirm the contribution of some of the proposed losses and the cross attention mechanism for building the audio visual representations, as well as the use of an open max classifier instead of a soft max. Cons of the paper:Novelty of some contributions unclear.
Reject. rating score: 4. rating score: 4. rating score: 5. rating score: 5. rating score: 6. <BRK>The paper shows that on the maximum of a set task, the visual object comparison task, and the IQ test, the proposed module, when combined with deep models, outperforms existing works in extrapolation. The paper empirically investigates how the proposed module can improve relational reasoning in ood extrapolation tasks, which is considered an important aspect for deep neural networks. I m confused what the authors are trying to argue in this point. However, the work does not demonstrate the representation learned has this property. Especially in the IQ test problem? Can it be shown that the representation is good for interpolation as well? It looks like the low dim representation is not well learned.<BRK>The paper presents two neural network design ideas: low dimensional projection and arithmetic comparator. By integrating these two ideas into CNNs, the model can solve a set of tasks that require recognizing 1 dimensional properties of objects (such as the size, the color, etc), and making a comparison of these properties. The authors show that their method has strong out of distribution generalization: such as generalization to larger objects. The presentation of the paper is relatively clear. 4.I suggest the authors discuss more the results in Table 2: what are the failure cases of the model? 5.Table 3.I suggest the authors supply the performance on standard train test splits for RPM as well. This will give us an idea of how the model performs on "i.i.d." 6.I have some concerns about the applicability of this proposed model.<BRK>"Projecting into a lower dimensional space, so that irrelevant dimensions are projected out, will help in extrapolation" is an interesting hypothesis that should be tested. However, I was not convinced that the experiments gave convincing evidence for the hypothesis. It is standard in science to test a hypothesis by keeping as many of the extraneous variables as invariant as possible. However, in this case a different architecture (which projects onto a lower dimensional space) was used for each problem, and there was an impressive improvement. The results of finding the maximum (Table 1) are dramatic. Is it really just the lower dimensional embedding?<BRK>Review: This paper addresses an inductive bias for relational reasoning tasks to improve generalization performance on out of distribution scenarios (so called extrapolation), which the value ranges (continuous variables) or value itself (discrete variables) of the training/test dataset do not overlap. This research deals with one of important topics in AI/ML research. As Section of related work in this paper also mentions these points, it is understandable to figure out the position of this work. I like it. Concerns:  The proposed idea is not so concrete to apply as practical solutions. I think it seems close to a guideline. It seems that the distribution difference between training datasets and the out of distribution ones influences explicitly on their performance. It seems difficult to reproduce the results. Overall presentation is not so clear.<BRK>This paper showcases three studies on relational reasoning where objects need to be compared on a certain attribute (like size). The experiments show large improvements in generalization performance over previous work. The authors attempt to formulate a general method from their experiments. My biggest criticism is that the method needs to be described more clearly. So strictly speaking, your definition of (M, \epsilon, …) learnability is not measuring o.o.d performance; rather it seems to be measuring performance on a “superset” of the training sample. This is clear for experiment 2 and 3. In conclusion, I do consider this work substantial and important. I would be open to revising my score if the presentation is sharpened.
Reject. rating score: 3. rating score: 4. rating score: 5. rating score: 5. rating score: 8. <BRK>This paper introduces a new online convex optimization algorithm that operates in via the reduction to online linear optimization in which the regret is bounded by $\sum_{t 1}^T \langle g_t, x_t   u\rangle$ where $g_t$ is the gradient of the t^th loss at $x_t$. The analysis is restricted to diagonal $H_t$, for which we can break the regret into a sum of $d$ 1 dimensional problems, so it suffices to do the analysis in the scalar case. A regret bound is provided for this algorithm that achieves worst case $\sqrt{T}$ regret, but in cases in which the gradients are small, the regret is much better. By employing this strategy on a per coordinate basis one can obtain an adagrad esque regret bound. However, I am a bit concerned about the quality of the theoretical results obtained. In particular, it is extremely unclear to me that the main regret bound in Theorem 4.1 actually offers any advantage whatsoever over AdaGrad. The authors then observe that in this setting, AdaGrad will obtain $O(\sqrt{\log(T)})$ regret. I suspect the learning rates for adagrad are not tuned properly, but perhaps I am missing something. I have been unable to conceive of any sequence of gradients in which theorem 4.1 actually outperforms AdaGrad, and it is very easy to find sequences in which it does much worse (e.g.simply reverse the sequence of gradients in the provided example). Looking at the analysis, I suspect that this is a fundamental issue with the approach of bounding bregman divergences based on the diameter of the domain. As for the empirical results, these seem a bit more promising so perhaps there is some improved analysis that could be made. If $\alpha_t$ is set via some other schedule, and momentum is used as well, then it becomes unclear if the $\alpha$ tuning and the momentum is not what is providing the gain over AdaGrad rather than other differences.<BRK>They provide a theoretical analysis of the new method showing that it would potentially improve the regret bound of current algorithms. Finally, the proposed method is empirically matched with or superior to other popular algorithms on different tasks. Overall, the method is novel and backed up by some empirical results. The theories require a bounded domain, yet it is not justified. "*The AMX algorithm is at least as fast as AdaGrad and AMSGrad under the same assumptions*": The regret of AdaGrad is ($\frac{D^2}{\alpha} + \alpha)\sum_{i 1}^d ||g_{1:T,i}||$ under the same setting. Indeed,  the bound in Theorem 4.2 is worse than AdaGrad by a log term. *": There is no citation for this statement nor evidence showing that it is actually happening in the real training process. There is only a specific example to show $\tau$ can be 1, but I m still not convinced.<BRK>##########################################################################Summary: The paper provides a new family of adaptive optimization algorithms by designing the proximal function of the adaptive algorithms to minimize marginal regret bound. The paper shows that the regret bound is better than existing algorithms in a sense. The paper also presents simulation study on a variety of domains that compare the proposed algorithms with other commonly used algorithms. However, I have a few concerns outlined in the section below. The paper explains in details how the experiments are done, including relevant parameters and training procedure. However, I am not convinced that this can only be done for AMX, but not for other adaptive algorithms like Adam or AMSGrad. 2.The main theoretical insights is that AMX can converge faster than other adaptive algorithms. However, in many experiments AMX did not converge faster than Adam or AMSGrad (e.g.Figure 2). In this sense, there is a misalignment between theory and practice.<BRK>The paper proposes a variant of AdaGrad which optimizes greedily the increase of the regret per trial. More precisely, it optimizes diagonal matrices which adaptively determines the regularizer of the composite mirror descent updates. The experimental results shows that the convergence speed is comparable to other state of the arts, but often it achieves better test performances. The authors pointed out the proposed algorithm achieves potentially better regret bound than typical \sqrt{T} and this is the first algorithm to do so. I am afraid that that is not correct. In the online convex optimization literature, there are bunch of researches on learning on "easy data", which aims to obtain better worst case regret bounds by taking advantage of “easyness” in the data. It is not clear to me why the proposed method achieves better test performances in the experiments. The authors should try to explain why. A reasonable explanation might increase the technical contribution.<BRK>The paper proposes a new motivation for designing the proximal function of adaptive algorithms. The claims are substantiated empirically for a wide variety of deep learning tasks as well as empirically. I have no complaints about the comprehensive empirical evaluations and the quality of the theoretical results (if correct). I did not have time to go through all the proofs in the supplementary materials. I find that the intuition and motivation for the proposed class of methods is generally clear. It seems, however, that this method (as described in Section 3) is a bit ad hoc as the authors are trying to examine a *one step increment* of the regret. Would a non diagonal and more dense proximal term be beneficial? Also, it would be good to avoid the use of the O notation here to show the dependence of the regret (or at least discuss it) on the other parameters of the problem (e.g., the diameter of the problem).
Accept (Poster). rating score: 8. rating score: 6. rating score: 6. rating score: 5. <BRK>The paper investigates robustness of Capsule Networks (CapsNets) under adversarial attacks and makes several interesting observations around the behaviour of CapsNets under attack regime which have been used later to design a new attack against these types of networks (vote attack). Authors analyze the effectiveness of this attack from different perspectives (i.e.transferability of adversarial examples, the adversarial robustness on affine transformed inputs). The paper is also in general  well written and easy to follow. It would be great if authors could also release the code to reproduce the results. Relation to prior works: I am curious to see if the proposed attack can still be effective for Self Routing Capsule Networks (Hanh et al.) and the other family of CapsNets in which the different routing mechanism has been used.<BRK>I think the approach proposed by the authors to attack CapsNets and show how they are affected by adversarial examples is solid. It is backed by a comprehensive experimental design and empirical evidence, demonstrating that attacking the voting process of routing is better than attacking output capsules directly. Comments:  I am not sure that this statement is accurate "Since the routing process is the main difference between CapsNets and CNNs,...."....Capsules are built under different premises one of which is the routing process. I understand that in this paper authors evaluate a certain property but how do you know that ResNet 18 does not have an effect on this? Improvements:  Generalise this approach to other proposed CapsNets architectures, beyond Dynamic Routing, such as VarCaps (De Sousa et al.AAAI 2020, StarCaps (Ahmed et al.)<BRK>Therefore, they propose a new attack which attacks a CapsNet running only 1 routing iteration. The attacks are shown to be more effective than targeting the 3 iteration capsnets directly. However, the point of Qin et al was to some degree that although adversaries exist that Qin et al method will not detect, such adversaries have semantical resemblance to the target class and they are not undetected to human eye. They find that attackers make the votes orthogonal to the target class parameters and therefore, remove votes from contributing. But given gradient obfuscating as a reason for capsnet robustness their attack is intuitive (rather than attacking the 3 iterations attack a 1 iteration version). The study by itself is interesting and can lead to further robustness of CapsuleNetworks. One caveat of the vote attack is that it seems it is most effective where there is only one Capsule layer. It seems that adding more and more capsule layers would make the model further robust to adversarial attacks. Hinton et al 2018 specifically had 3 capsule layers. Pros: The paper is relatively well written. Also the experimental setup covers several attacks. There is novelty both in the idea and in the voting study. Also, given that Michels et al 2019 has shown CapsNets are not robust to all attacks and already other attacks exist that CapsNets fail on, the relative interest domain of this paper is limited to researchers working on Capsule Networks. As author s mention it is not trivial to apply their attack to other capsule architectures either.<BRK>This paper presents a new adversarial attack targeting the votes derived from the primary capsules of a capsule network. This paper has some novel contributions to the literature but can be improved in a few areas. By creating an attack that specifically targets the votes of the primary capsules this paper is able to increase the success rate of adversarial attacks against capsule networks. This paper would be improved by addressing those 3 main issues. Efficiency of our Vote Attack: It is clear that the vote attack is more time efficient than other attacks, but there is no clear motivation for this improvement. In my original review i mentioned that by not attacking the output of the capsules after the routing procedure, this attack was simply optimizing for representations extracted from a standard neural network. In this way this work is similar to the representation attacks first presented by [1] which showed the success of representation attacks on standard neural networks. 4.)The additional defense mechanisms presented in Deflecting Adversarial Attacks:The authors are right to point out the scope of the paper, and it is perhaps unreasonable to expect this paper to address these defence mechanisms, but their inclusion would greatly strengthen the paper.
Accept (Poster). rating score: 9. rating score: 6. rating score: 6. rating score: 5. <BRK>This paper is in defense of simple semi supervised learning (SSL) with pseudo labeling (PL): authors demonstrate with experiments on 4 vision datasets (CIFAR 10, CIFAR 100, Pascal VOC and UCF 101) that pseudo labeling can perform on par with consistency regularization methods. The main contribution of the paper is the usage of prediction uncertainty selection in addition to the confidence based selection which provides high accuracy of PL used in the further training. For Table 1 would be good to have a clarification on baselines: which are consistency regularization based, which are PL. Hope, this will push the study of simple SSL approach, pseudo labeling, with the new competitive results not only in vision but in other domains too.<BRK>By taking into account the uncertainty of models and only using pseudo labels from high confidence instances with low uncertainty this work presents a model that significant improves on other PL strategies and is competitive with consistency based regularization strategies that comprise the current state of the art. This would extremely compute intensive for large scale semi supervised learning tasks, which is a motivating use case for semi supervised learning. This paper posits that this miscalibration may lead to inferior results in confidence based pseudo labeling approaches. This is important for extending approach beyond domains where specific types of augmentation have been extensively studied.<BRK># SummaryThis paper proposes uncertainty aware pseudo labelling for semi supervised learning, extending previously known methods by negative labels. The method should be positioned against others using confidence filtering and the role of calibration needs to be studied further. Method is independent of uncertainty estimate and data augmentation. Combining uncertainty estimates and confidence estimates, as well as adding negative learning are interesting approaches. ## ConsConfidence filtering for pseudo labels has already been suggested, see e.g.suggestions given in detailed comments. The authors could enhance the ablations studies with and without calibration and  importantly  adjusted thresholds, to clarify that point. "Learning with UPS" have you treated pseudo labels and original labels equally when training $f_{\theta,1}$, if so, how about fine tuning on the original labels? From the paper it becomes clear, you did use data augmentation, not sure why you argue so strongly against it. I am not sure if calibration plays a role here. Calibration only moves the distribution in shape, by setting a confidence threshold $\tau$ the threshold would be changed, but a suitable threshold could be found before and after calibration.<BRK>Strength:The paper notices the problem in pseudo labeling methods of semi supervised learning, which is the erroneous prediction of pseudo labeling. Pseudo labeling is an easy to implement method for semi supervised learning does not require constraints needed by consistency regularization methods, so improving pseudo labeling methods can promote the practical use of semi supervised learning. The paper is well written and easy to follow. The experimental results on datasets of different domains such as image, video show that the proposed method outperforms previous semi supervised learning methods. Constraint:The paper proposes a new method to predict pseudo labels of unlabeled data by confident threshold. The authors fails to discuss the difference of the usage of positive and negative labels between their method and (Kim et al., 2019).
Reject. rating score: 4. rating score: 5. rating score: 5. <BRK>Compared to previous best model it uses LSTM instead of MLP to capture forward dynamics of the system, importance sampling and order invariant encoding of other agents to improve sample efficiency of learning. Strong points* Order invariant encoding uses inherent structure of the problem that improves sample efficiency compared to previous approaches. * The paper is sometimes difficult to follow. * Importance sampling that is claimed to be important ingredient isn t described well enough neither in the main text nor in the appendix. In other domains only qualitative results are presented. This view is further supported by Fig 4 where empirical time complexity also isn t constant as is suggested by Table 1. I would like to see comparison to previous methods applicable to these domains if there are any (numerical solvers?). If there aren t any the paper should explain why this is the case. Is it true that current system is Han & Hu with LSTM instead of MLP + importance sampling + invariant layer? * "memory complexity of LSTM with respect to time is O(1)"   this is true for inference time, at train time O(T) activations still have to be stored. The text can be more clear on that.<BRK>So, it will be great if the authors can provide this information. are clear. Is it making the deep FBSDE framework scalable? by introducing the invariant layer? 2) Comparisons with Han& Hu (2019) is presented only in Table 2. If  so, what is the difference between the 10 trajectories and the deep FBSDE solution? Is it that the method also works on a different domain? Quality:1) The main contribution seems to be scalability. This is done for 10 agents. Yet, we do not know what happens as we increase the number of agents. Significance:Scalability is the main take away of this paper. Figure  5 and Figure 4 show that the proposed method is better than prior methods, yet the performance improvement is not significant. However, the savings could be in the time/space complexity, which the paper does not elaborate.<BRK>The authors present numerical examples on linear quadratic symetric stochastic games with 1000 agents, as well as solutions to partially observed 2 agents racing problem using extended Kalman filter. In the car racing example, it does not seem clear how the proximity to a Nash equilibrium is quantified. It is also surprising to focus on a 2 agents example, while the paper seems to intend to emphasize the scalability of the approach. It is a priori not clear how the presented BSDE representation differs from a related recent paper by Han, Hu and Long (Convergence of Deep Fictitious Play for Stochastic Differential Games). This should be discussed. It seems that this paper only focuses on games with unique Nash equilibrium, although the use of RL algorithm for games seems to become intricate whenever several Nash equilibrium shop up into the picture. Recommandation; Given the strengths of the paper in terms of FBSDE representation of the solution, together with the weaknesses presented above, I recommend to reject the paper, but am looking forward to get clarifications on the points raised above. By the way, how does the solution changes as the number of agents increases?
Reject. rating score: 4. rating score: 5. rating score: 6. rating score: 6. <BRK>This is typically achieved by a gradient descent type algorithm such as federated averaging. The paper argues that federated learning via gradient updates has issues and proposes to use a voting based method for training machine learning models using unlabeled global data. For a fair comparison in experiments, one should consider variations of federated averaging with the global public data. PATE FL and Private kNN FL are largely similar to the original PATE and Private kNN algorithms.<BRK>Summary:The paper proposes two approaches (i.e., PATE FL and Private kNN FL) to train a differentially private global model in a federated setting based on [1] and [2]. (2) I found that the authors use ImageNet pretrained models in the experiments. This setting is weird. In Private kNN FL, instead of training a teacher model, the prediction depends on the labels of the nearest neighbors. The theoretical analysis seems can be easily derived from the existing theorems with a modified sensitivity. In Private kNN FL, a data independent feature extractor is used. (3) The experiments for agent level DP evaluation can be further improved. The authors can try non i.i.d.partition [3,4], which is a key feature in federated learning.<BRK>Objective of the paper:The objective of the paper is to provide new federated learning algorithms based on aggregating labels from a voting scheme, instead of aggregating the gradients directly, to achieve more efficient differentially private algorithms. Strong Points:1)  It looks like the methods get strong results. Weak Points:1)   The paper is not written as well as I would like. Expressions like "each data". I don t understand how the "votes" work, in that I don t know what are the "labels" being voted on, or why the label with maximum vote is a reasonable approach.<BRK>It assumes that the server has an unlabelled dataset on which learning has to be performed. However, in the experimental section, PATE FL is compared to a baseline agent level method, and Private KNN FL approach is compared with instance level methods. 2/ Strong and weak points  The paper makes a very good distinction of the number of agents and the DP guarantees it implies in both settings, with the notion of instance level (i.e.at the individual sample level) vs agent level (i.e.client) DP.
Reject. rating score: 2. rating score: 3. rating score: 4. rating score: 5. <BRK>3.The citation format is in chaos, check the usage of \citep{} and \citet{}. Three versions of multi vocabulary pretraining methods are also studied.<BRK>Although it is not necessary to beat all the state of the art models, the comparison with other models should be given.<BRK>Considering the contributions of this paper, it is more suitable as a technical report than a paper to be published.<BRK>The baseline method is vanilla MLM which was developed for English. The paper proposes several interesting and novel ways to improve the pretrained model in Chinese.
Accept (Poster). rating score: 7. rating score: 7. rating score: 6. rating score: 3. <BRK>SUMMARYThe paper introduces a new, plan based reward shaping approach to RL called ASEQ RS. It is a generalization of potential based reward shaping, one that doesn t have the same optimality guarantees but empirically facilitates faster policy learning. HIGH LEVEL ASSESSMENTThe paper s approach looks interesting, especially empirically. However, I have a number of concerns about the paper s theory: much of it seems to have errors. If the authors demonstrate a way to fix them without detracting much from the generality of the results, I ll increase the score, because the approach looks useful in practice. This can be quite an issue for Cor. 4.1.1 and Theorem 4.2, which are based on 4.1, and also in practice, because in non ergodic MDPs with a discount factor the former version of the assumption can easily fail to hold, potenially breaking the paper s main results. Theorem 4.1 loses its meaning if B   emptyset, in a similar way as Def 4.3. So what are the implications of Theorem 4.2 for choosing gamma?<BRK>As reward shaping is a meta algorithm that can be applied to almost any RL method, it would also be useful to evaluate ASEQ shaping on multiple RL algorithms, not simply DDPG, to show that the benefits of the more flexible class of shaping rewards are not limited to a single class of algorithms. The work does have its limitations, both in terms of the class of problems to which ASEQ shaping can be applied, and in terms of the depth of the empirical evaluations, but it will be of value to the community nonetheless. While this is the most direct comparison between the two approaches, it is possible that using potential based shaping with a simpler potential function, such as the distance between the box and the goal, would lead to better performance for potential based shaping.<BRK>The paper also proposes a method to compute a shaping function from behavior demonstration. Negative points  The proposed approach seems to be restricted to a very narrow class of MDPs. Its premise is that the "classical" reward shaping may be too restrictive, in that it requires the optimal policies before and after shaping should be the same. The paper instead proposes to alleviate this requirement, as long as the resulting policies lead to "similar" long term behavior. In this respect, it could be relevant to at least mention this line of work in the paper. This is a relatively straightforward conclusion, and the whole shaping strategy proposed by the paper follows from this simple observation. The paper concludes with an experimental evaluation section, where the proposed approach is compared against potential based reward shaping and shown to outperform the latter. However, the potential based shaping functions used for comparison are introduced in an ad hoc manner, and it is not clear to me why these shaping functions are good baselines for comparison. Finally, on a small note, I feel that the paper could benefit from cleaning up/streamlining the notation and language. Definitions 4.1 4.3 make use of heavy notation but translate a very simple notion: that an MDP M is "asymptotically equivalent" to an MDP M  if the optimal policy of the former leads the agent to a subset of the goal states of the latter.<BRK>In this paper the authors introduce an asymptotic equivalent (ASEQ) reward shaping reinforcement learning algorithm, which, they argue, is a relaxation of potential based reward shaping (PB). The authors present results suggesting that the proposed ASEQ method is significantly better than PB. The authors state that this new method is a superset of PB (or a “subclass of reward shaping broader than PB”) which allows for a policy to be determined by shaping rewards on all intermediate states in a trajectory. There is unconvincing details on how this is done (eg.the authors state that it is ‘more direct’). The authors do not compare this method with other learning from demonstration methods (e.g.behavioural cloning or Generative Adversarial Imitation Learning)Definition 4.1 is unclear and \mathcal{G} is undefined. The authors make claims that their method is asymptotically equivalent, that is in the long run, behaviour in a modified MDP will have the same result in a modified MDP. I would urge the authors to clarify this point. This is a likely outcome, given that they are providing the method with additional information. Finally, I do not think that this work is a good fit for ICLR given that it is more related to methods in reward shaping for reinforcement learning as opposed to the topics of more general greater interest to the audience at ICLR.
Accept (Poster). rating score: 7. rating score: 6. rating score: 6. rating score: 5. rating score: 4. <BRK>An new approach for NAS using convolutional models on speech is presented. Hence, the presented findings and models are only useful to ASR research focusing on this particular domain. Overall, I think the paper is a very good first step towards more NAS benchmark studies for speech, hence I vote for acceptance despite its limitation in terms of domain. Pros:  The study is clearly novel, As far as I can tell this is the first NAS paper on speech.<BRK>This paper introduces a NAS benchmark dataset for ASR. This is an interesting and pioneer work in the ASR domain. The NAS Bench ASR is built upon TIMIT, a relatively small corpus for speech phonetic recognition.<BRK>This paper studies neural architecture search for automatic speech recognition. The macro architecture might play a critical role here. It is quite discouraging that the models, discovered by NAS after spending so much compute, are not competitive to baseline models reported in other papers. The weakness of the paper is the absolute PERs on TIMIT and WERs LibriSpeech.<BRK>This paper follows this trend and provides a NAS benchmark for ASR by using TIMIT. The paper says that it is different behavior compared with image classification benchmarks. Weaknesses:1) The problem is too specific to ASR.<BRK>The authors contribute to the NAS literature by presenting a framework that works decently well on small ASR tasks, specifically TIMIT. They also show that there is some correlation between training for TIMIT and tasks that have more data, such as librispeech.
Reject. rating score: 4. rating score: 5. rating score: 6. rating score: 7. <BRK>I would expect that high dimensional preference learning algorithms like GAIL and AIRL would also be able to do this, and thus are important to compare against. This is most easily seen from the gradient expression in [4], where the MaxEnt IRL gradient in high dimensional environments is written as$$\frac{\delta r_{\theta}}{\delta \theta} [\mathbb{E} [ \mu ]   \mu_{D_{\tau}}].$$This is the negative of what you might expect because they are defining a loss function to be minimized while you are defining a log likelihood to be maximized. So it becomes even more important to show why this particular structure is an improvement over e.g.the GAN like structure in AIRL. Quality: As explained above, I have serious concerns about the experimental section. Originality: To my knowledge no other paper has inferred constraints based on the maximum entropy assumption in high dimensional environments. Actually, I see, the network is constrained by the sigmoid to output values in [0, 1].<BRK>Description:This paper describes a new approach for inverse constraint learning, whereby an RL agent infers constraints (on state/action pairs) from expert trajectories, and uses this in combination for a given reward function to optimize an agent’s behavior. My main concerns with the work are: (1) the proposed setting itself, and (2) some missing baselines in the experiments. Regarding the setting, I don’t really buy the motivation. A few references don’t say “where” the work was published. But I don’t see the support for this (experimental, theoretical or from the literature) either in people or in AI. Furthermore, it is not obvious to me that this is a better strategy than straight forward IRL. The references need to be cleaned up.<BRK>for a value function to be maximized. The paper also claims, "our approach [...] does not suffer from the curse of dimensionality". In the related work is reasonably complete. The paper assumes that the entire nominal MDP (the MDP without constraints) is given, along with a set of demonstration trajectories, and the agent tries to learn *only* the constraints. This means that once the constraints are learned, there is no more learning going on, only optimization. Thus, a more appropriate title for the paper would be something like "constraint learning via maxent inverse RL". Note that this effect can be drastic: if the expert is suboptimal, then its trajectories don t necessarily imply constraints, which means that the proposed methods might learn fictitious constraints that may make other environments where they are applied unsolvable.<BRK>The authors provide an algorithm to recover these constraints from expert demonstrations. * Comparison with prior art is lacking. * Discussion of related work is sparse and can be more detailed. Based on the above mentioned strengths, I vote for accepting. My concerns (further detailed below) potentially can be addressed during the rebuttal phase. "Bayesian nonparametric inverse reinforcement learning." 2.(page 2) The rationale behind the objective (Equation 7) of the prior art and the proposed approach is identical. 2018.Learning features (which can be in the form of logical constraints) for IRL:    * Choi, Jaedeug, and Kee Eung Kim. Please consider rephrasing to say that this matching occurs at the minima (where the gradient is zero).
Accept (Poster). rating score: 7. rating score: 7. rating score: 7. rating score: 5. <BRK>Furthermore, it s not clear that the chosen baselines are the best meta learning methods out there in terms of adversarial attacks to compare to, which makes the claim of "our methods are much more robust to adversarial attacks than existing approaches" groundless. They propose two energy functionals (first one based on maximizing the norm of the parameters and the second one making the adaptation in closed form, based on the NTK) that approximate the MAML s infeasible learning objective. 2.Authors claim their method is robust to adversarial attacks.<BRK>The authors propose two meta learning algorithms in the reproducing kernel Hilbert space (RKHS) induced by the recently proposed Neural Tangent Kernels (NTK). Currently, it comes off as rather broad and disproportionate to the work and existing work. In the case of MetaRKHS II, the NTK gradient flow based adaptation in Eq(6) forms the inner loop— just that it is a more efficient inner update than the MAML. Meta RKHS IIHere, the authors propose an adaptation function based on the NTK and the gradient flow. Figure I:  The axes and the legends are not readable.<BRK>SummaryIn this paper, the authors view MAML from the lens of Reproducing Hilbert Kernel Hilbert Spaces (RKHS) by applying tools from the theory of Neural Tangent Kernels (NTKs). Based on these insights, they develop two meta learning algorithms that avoid gradient based inner loop adaptation. Their algorithms are theoretically grounded and exhibit improved empirical performance. StrengthsThis paper is generally well written and proceeds to develop insights into gradient based few shot adaptation on first principles from NTK theory.<BRK>Specifically, it proposes two meta learning algorithms where the hypothesis class (i.e.the mapping function set) is defined in RKHS induced by NTK. Overall, this paper is well written and organized. 2.This work is based on recent solid theoretical results (i.e.NTK) from the deep learning theory. 3.The proposed methods have promising performance empirically.
Reject. rating score: 4. rating score: 5. rating score: 6. rating score: 6. <BRK>I would like to thank the authors for the interesting work they proposed. But I encourage the authors to pursue this direction and seek out ways to relax this condition. Theirs is one attempt to address this. Boldface capital T is a matrix but also a function that takes inputs. 5) Assumptions are revealed sequentially one by one and too late in the paper.<BRK>This paper proposes a VAE based model for learning latent causal factors given data from multiple domains. I have three concerns for the current version of the paper. Some experiments, like examining the vulnerability and performance of the system under the condition that the latent factors are controlled or intervened, for the claimed “invariant causal model” are better to be included to convince readers.<BRK># Summary of the review  The authors make a strong case regarding their approach to causally robust supervised learning. Their contributions are thorough and carefully situated in the relevant subfields. How successful would the authors expect such an approach to be? In their generative model, the authors distinguish two types of latent variables. 5 without having introduced it before. Though being dense, the paper is easy to follow (save for occasional typos, see below).<BRK>### SummaryIn this manuscript, the authors introduce a method for supervised learning in an out of distribution (o.o.d.) (3) Additionally, assumptions analogous to those in e.g.Theorem 1 of Khemakhem et al.2020.Provided all those assumptions apply, the authors are able to adapt results in that line of work to their proposed model and prove similar identifiability results for their new setting. All in all, I would also recommend having the manuscript proof read for English style and grammar issues.
Reject. rating score: 4. rating score: 4. rating score: 4. rating score: 5. <BRK>Method is simple, clear, and well written. The accuracies are pretty close to each other and the numbers are from a single run so they are not statistically convincing. Sec 4.3 multi task learning & appendix D, Section 4.4. same as above. Table 2.It says results are also from Places365 and sun397, but they are not shown in the table  Section 4.3. This paper proposes an interesting way of composing neural modules for a number of applications, but the experimental results do not sufficiently demonstrate the advantage of using the proposed approach.<BRK>The first issue relates to the first two set of experiments, (single task and multi task) for which only vague descriptions are provided: experimental setting, datasets, network architectures (number of layers, number of templates)…Most of these important details are either absent from the paper or described only in the supplementary material. Authors evaluate their approach on a very large set of problems and datasets, and provide promising performance. RECOMMENDATIONIn summary, the idea of learning modular, compositional architectures, with a set of pre trained templates that can be reused in different problem settings is quite exciting. The experiment studying similarities across network signature with respect to the problem setting is particularly interesting.<BRK>Summary: This paper considers “modular multi task learning” where parameters in each layer/task are generated as a (layer/task specific) linear (mixture) combination of a common pool of parameters. However the novelty is over claimed/prior literature missed, and the experiments are not good enough to make up for this limited novelty with empirical insights discovered or SotA performance reached. Strengths: + Overall the idea is reasonable, and the results make sense.<BRK>This paper presents a method of composing stacked neural network blocks by linearly combining module "template" weights. I think it could be developed further:  Are the templates that are changed more at the first layers or at all layers? In multitask learning experiments, the mixtures naturally learn to share common components between tasks, while learning task specific components where needed. Overall I find the approach and the investigation of multitask training very interesting. This one point of comparison, but I m not convinced it is the strongest baseline   one could also train all model weights, for example.
Reject. rating score: 5. rating score: 6. rating score: 7. rating score: 7. <BRK>In this paper, two unsupervised agents are utilized at cross model by using the dual nature of the unsupervised machine translation model, in which forward translation of agent_1 is combined with the backward translation of agent_2, more synthetic translation pairs are obtained to train a new supervised machine translation model. Furthermore, CBD is compared with the ensemble method and achieves better performance. The proposed method is quite simple yet effective, but it is also a kind of data enhancement. In addition to these contributions, the paper also has some shortcomings1. The evidence in this paper can not support the claim that the current performance bottleneck of UMT is due to the lack of diversity: the performance upper limit of UMT is still due to the lack of clear supervision signal, which limits the further performance growth. 4.The non golden language sequence as a translation target is called pseudo NMT (PNMT). In general, the CBD method in this paper is a simple and effective data enhancement method to improve the performance of the model.<BRK>This paper introduces a new component to the unsupervised machine translation framework called cross model back translated distillation. The proposed approach is applicable to the other unsupervised methods. Experimental results in several translation tasks show that the proposed approach improves the translation accuracy of the standard unsupervised machine translation models, outperforming the cross lingual masked language model. The analyses are interesting to understand the proposed approach. Can you use parallel data instead and conduct the same analyses so that you can use BLEU score as an evaluation metric? Is it possible to apply the proposed approach to supervised NMT training, by creating BT data from monolingual data? Will the performance be improved better and better? at p.6.What does this "x y" mean?<BRK>This is similar to the approach in [1], but uses an additional stage of back translation with a different model. The authors add this additional stage of training to unsupervised NMT models using different pipelines (PB unsupervised MT, Neural Unsupervised MT, XLM) and show that their approach improves all of these approaches by 1.5 2 Bleu on WMT En Fr, De En and En Ro. It would be great to report results in the original training conditions (this is not a major limitation however, since the proposed approach seems to improve over baselines trained with more data). 3.Did the authors try any experiments with unsupervised models utilizing parallel data in unrelated languages, similar to [4,5] or in real low resource settings [6]? These are more practical conditions for unsupervised MT in true low resource languages. Recommendation: Overall, this is a good paper and I would recommend acceptance.<BRK>This paper describes a method to enhance unsupervised machine translation through data augmentation. The idea is pretty straight forward, if not altogether intuitive, you begin by training two bidirectional (i.e.: they can translate source to target and target to source) unsupervised MT systems A and B. The tested scenarios always have A and B be identical architectures trained with different initializations. They then train supervised MT on both x,y and z,y. The paper reads more like a journal paper that has undergone several stages of review than a conference paper. The authors really leave no stone unturned. The biggest knock against this paper is the relatively small data scenario. It would be greatly strengthened by a full data experiment for even just one or two of the language pairs.
Reject. rating score: 3. rating score: 5. rating score: 6. rating score: 6. <BRK>The paper argues that there are some challenges in learning text representations with vanilla contrastive learning techniques, due to the metric aware property of  KL divergence as well as the difficulties in selecting good negative examples. It then presents two remedies: employ a Wasserstein constraint to the critic function and a simple active sampling strategy for negative examples. I think the paper is not strong enough to get into ICLR. Firstly, the Wasserstein constraint has been proposed to improve general representation learning in Ozair et al.(2020).The novelty of this work is very limited compared to Ozair et al.(2020).The active sampling contribution is a simple heuristic. Secondly, the authors only compare the proposed method with the non CL baseline. Finally, the improvement margins are also pretty small.<BRK>The paper aims to improve contrastive learning for text representation by tackling two challenges: (1) The conventional mutual information (MI) based objective can results in unstable training due to the sensitivity of KL to changes of representation; (2) contrastive learning could require a large number of negative samples, which is inefficient. For (1), the paper replaces the KL loss with Wasserstain distance; while for (2), the paper selects top K most difficult negative samples (defined as the nearest neiborhors to the current representations). Experiments on benchmarks for text classification (GLUE), retrieval, and language modeling show the proposed method improve over standard supervised/semi supervised learning. 2) I m a bit surprised that the experiments do not include any comparison with previous contrastive learning methods, but only compared with models without contrastive learning. It s necessary to include more comparsions to show how much improvement each of the two proposed techniques (Wasserstein distance, hard negative samples) reaches. ".Yet those results are not included in the suppementary.<BRK> Summary This paper proposes an approach to improve (supervised and unsupervised) representation learning for text using constrastive learning. The resulting contrastive learning objective can be combined with standard supervised and unsupervised objectives to improve downstream tasks. Strengths   Sound improvements on top of existing finetuning methods for GLUE  (though I have questions about the semi supervised setup; see questions). Weaknesses   My biggest gripe is that the paper is presented as a "representation learning" work, yet much of the focus is on performance on underlying tasks. It seems like this objective can be combined with self supervision (e.g.masked LM objective) to aid in better BERT like models (which would have been a very interesting result), but this seems to have been left unexplored. For example: on GLUE, which layers of BERT are finetuned more (or less) with this method; on image text retrieval, what are some mistakes made by the standard approach that this method is able to fix? Does the semi supervised experiments utilize the full unlabeled data?<BRK>This paper presents a new contrastive learning methods based on intermediate textual feature pairs, which stabilizes training and overcomes vanishing contrasting signals by imposing Wasserstein constraints on the critic via spectral regularization, and leverages a fast and dynamic selection of high quality negative samples. My major concern is about the clarity of the paper and some detail information of model and experimental design. It seems not very easy to reproduce corresponding codes and experiments only based on the current paper content. For instance: 	The concrete u and v in each experiment should be more clearly explained, especially for u. For instance, Hjelm et al.2019a and 2019b are pointing to the same paper, and Lagler et al.2013 seems to point to a wrong GPT2 paper which is not in the NLP domain.
Accept (Poster). rating score: 7. rating score: 7. rating score: 6. rating score: 6. <BRK>This paper proposes a new deterministic pruning strategy that employs continuous Heaviside function and crispness loss to identify a sparse network out of an existing dense network. Some minor issues are shown below. 2.The proposed algorithm is a deterministic algorithm and may fail in complex network pruning problems. 3.It is recommended to use different labels for different algorithms in the figures.<BRK>SummaryThis paper presents a new method for structure pruning called ChipNet. Is the proposed method applicable to networks for other tasks such as detection and segmentation? There is a submitted paper (to this ICLR) that includes continuous relaxation of discrete network structure optimization for network growing (not pruning). StrengthThe motivation for this paper is clear and quite interesting.<BRK>This submission proposes a way to prune neural networks using a continuous penalty function. ICLR 2020Within this general class of approaches, though, the specifics of the penalty function and optimization in this paper are novel. Writing overall is clear.<BRK>##########################################################################Summary:The paper provides a budget aware regularizer method to train the network to be pruned. ##########################################################################Reasons for score:Overall, I vote for marginal acceptance. However, I have several concerns about the papers (see cons) and authors need to consider these concerns in the rebuttal period. Therefore, I concern that the other readers are confused about the performance of the proposed method over the other pruning methods. (2) Details on the forward steps are omitted. (3) Justification of the usage of two functions (logistic and heaviside function) is not enough. Two separate functions for continuous approximations are quite confusing. (4) This might be the most critical one.
Accept (Poster). rating score: 8. rating score: 6. rating score: 6. rating score: 6. <BRK>The authors propose a novel unsupervised encoding scheme for time series. Utilizing a statistical test for non stationarity, the authors derive a Temporal Neighborhood Coding (TNC) scheme and combine it with ideas from Positive Unlabeled (PU) learning to learn informative hidden representations of time series windows. The presented work is very well motivated and described.<BRK>SummaryThe paper proposes an unsupervised representation (embedding) learning method for time series. 2.The way how the statistical test (Augmented Dickey Fuller) is incorporated for window detecting is not clear. The authors should provide some explanation to this.<BRK>The authors defines the notion of temporal neighborhood to segment time series. The model is trained using a discriminative loss that pushes away (in the feature space) windows distant in time while caring for the possible periodicity [Elkan & Noto 2018] Overall the paper is well written and easy to follow. The idea is very interesting and I see several potential consequences and use cases. I consider that HMM is part of unsupervised representation learning. I don t see how the training steers the encoder to satisfy this property.<BRK>5.The paper is well structured and written in a fairly clear and comprehensive manner. Paper Summary:This paper proposes a self supervised encoder discriminator based framework for embedding the multivariate time series into a compact fixed dimensional representation. I would encourage the authors to clearly justify the choice of different encoder architectures in the two cases. I would like to ask the authors to provide some guidelines on how the window size is (or can be) selected?
Reject. rating score: 3. rating score: 4. rating score: 4. rating score: 4. <BRK>Pros  * The paper explores normalization for Graph Neural Networks, which is a relevant topic. Reasons for score  Because of the lack of motivation and explanation of the proposed normalization methods I vote for rejection. Questions  * I would like the authors to explain the motivation for their proposed normalization techniques beyond the empirical results. (2020).Effective training strategies for deep graph neural networks.<BRK>On the positive side, the paper is very well written and organised. Extremely clear while remaining concise and focused on the point. On the negative side, the first issue is that of limited novelty. The paper should include some sort of summarised statistics, like the average rank of each approach across the different tasks. This way one can also run some tests of statistical significance to tell which ranks are statistically different. I have one final concern as regards the definition of adjacency wise normalization.<BRK>Thus, a significance test on the same for all results would be helpful in understanding the usefulness of the proposed variants. A GNN baseline with existing Layer norm and Batch norm combined is missing. ——Concerns:Major:	  The paper lacks a clear model wise analysis of the usefulness of different normalization techniques	  Though the paper mentions that the importance of normalization techniques varies with tasks, there seems to be no consistent model wise improvement under each task.<BRK>Combined with other two normalization strategies, a combination framework is proposed for weight learning of four normalizations. The experimental results  demonstrate the performance improvements of the proposed framework. 5.Significant tests are suggested to verify the gained improvements. More insights are needed.
Reject. rating score: 5. rating score: 6. rating score: 6. rating score: 7. <BRK>A weakness of the paper is that its primary theoretical advantage over DSS (according to my limited understanding) is that it is applicable to distributions rather than just point clouds, but the experiments only consider point clouds (presumably to be able to compare to DSS). The proposed method is compared empirically to DSS, which achieves the same types of invariance but is restricted to point sets rather than discrete or continuous probability distributions. The Dataset2Vec results in Table 1 are from (Jomaa et al., 2019) but Appendix D.2 states that the publicly available implementation of Dataset2Vec was used. On the first task, the paper compares to models built using Dataset2Vec embeddings as well as DSS.<BRK>The experiments are done on two tasks. The patch identification (out of distribution test) clearly show the invariance to feature and dataset size. E.g., "by (Qi et al., 2017; Zaheer et al., 2017)" should be textual instead of parenthetical. Pros:+ The paper is well written and easy to follow (there are some minor errors or descriptions that need to be improved, but they are not major issues).<BRK>Particularly, the interaction functional \phi is similar to a kernel function. Overall, the paper seems to be well motivated. It is also technically sound and the presentation of the idea is clear. I have some comments that are detailed below.<BRK>This paper proposes a novel set/distribution representation architecture DIDA, which leverages pairwise embedding of the set’s elements. In that sense, the paper brings some originality to the field. The experiments show that the architecture improves some dataset representation tasksOn quality: A single idea was developed and well executed in this work.
Reject. rating score: 3. rating score: 4. rating score: 5. rating score: 7. <BRK>What are the pros and cons of each approach? * Minor comment: The "landscape of semantic representations" could be shortened, potentially leaving room for more comparisons to other methods/benchmarks of the attributions. Objective: Explain the output of high dimensional ML models in a human interpretable way by using Shapley values on semantically meaningful latent features. But a thoughtful discussion of the relationship of this work to methods like concept bottlenecks or CaCE is essential   and not present in the current version.<BRK>Summary: This paper develops a Shapley value approach to explanation that uses low dimensional latent features to explain the original input. The authors do a fine job of motivating this problem and their subsequent solution. If you just train an end to end model, f, on (x,y) pairs, how can we be sure that the generative, latent factors captured by z are actually used by f? Strengths  The paper tackles a difficult problem in practice: applying Shapley explanations to high dimensional data.<BRK>Summary:The paper proposes an approach to generate semantic explanations for high dimensional data. The proposed approach consists of two modules   the first module transforms the high dimensional raw data into lower dimensional semantic latent space and the second module applies Shapely explainability to this lower dimensional latent space to generate explanations in terms of semantic concepts. 1) the faithfulness of the proposed approach. 2) Shapely values over other methods. I think the authors need to back up their argument for using Shapely value explanations over other methods by comparing experimentally with other methods such as CaCE or even raw gradients. S4) The paper shows results for six different datasets. I think the paper might benefit with more precise definitions of these terms in the paper.<BRK>The paper describes a technique for interpreting the results of a neural net in human readable terms. Strong points: While there is work related to this idea (which the authors cite appropriately), the overall technique is new. (The adversarial example scenario and the celebrity data set use case were especially easy to understand.) Weak points: Shapley values are not the only way to exploit human interpretable coordinates (although they especially are made feasible by the reduction in number of features). The paper might be even stronger if it compared other methods. I recommend acceptance. It s not really an inverse, maybe more like a pseudoinverse (or maybe a left or right inverse, depending on notation). This would be particularly straightforward in the first Fourier transform example.
Reject. rating score: 3. rating score: 4. rating score: 4. <BRK>Reasons for scores: overall the paper is rather incremental and the idea is neither novel nor significant in my opiniont. Cons:  Overall, the paper does not have much novelty in my opinion.<BRK>Cons: 1) Scope: It seems the paper works specific on the left right warping consistency of semantic label and depth, while the major scop told in the title and introduction is about pseudo label for general multiple task learning, which is byfar not shown in the worked experiments.<BRK>The paper presents a joint learning strategy for simultaneous semantic segmentation and monocular depth estimation. The experimental evaluation is a bit lacking in the following aspects. Therefore, it is not surprising to see the performance improvement over the prior work.
Reject. rating score: 2. rating score: 5. rating score: 6. rating score: 7. <BRK>The authors present an analysis of previously proposed Dirichlet based models for adversarial robustness and empirically evaluate exiting methods on two image datasets, MNIST and CIFAR10, as well as 2 tabular datasets. While in principle adversarial robustness is an interesting topic, the scope of the paper is extremely narrow and I would have liked to see a broader set of Bayesian models being included. Furthermore, I would have liked to see a broader type of data   how about a text dataset (such as 20 newsgroup) or a different sequential dataset with a recurrent architecture? Even with the very limited set of experiments, the authors basically report negative results, showing that neither of the approaches could detect adversarial attacks, OOD samples or highly perturbed dataThe authors also propose a robust training strategy, but concede that this increases performance either for either ID data or OOD data, but not both. While in principle also such negative results can be valuable for the community, I feel in this case the scope of the paper is too narrow and a broader class of Bayesian and ideally non Bayesian but uncertainty aware methods should have been analysed.<BRK>This manuscript addresses an important question of how Drichlet based uncertainty (DBU) measures can be used to quantify robustness to adversarial label attacks. Robustness to OOD samples of these models were already shown in the papers they were proposed but this work differs from them in using adversarial samples as OOD samples. Via extensive experimental results on various datasets, the authors conclude that the uncertainty estimates are not good indicators for identifying correctly classified samples for adversarially perturbed data. Although I find this contribution valuable, I am still not sure how generalizable the conclusions are. My comments/questions are below:  What makes adversarial samples so different than other OOD samples? I am surprised that the authors did not mention calibration in the context of uncertainty estimation. I understand that the main focus of this work is Dirichlet based models but I wonder if it is possible to generalize these findings to other families of models such as deep ensembles (Lakshminarayanan et al., 2017)? Figure 1 was never mentioned in text. The authors mention that it is due to density estimation but it is not clear to me how it is possible. For the results in section 4.3, I do not understand why the authors decided to report results based on a threshold instead of computing AUCPR as was done in other sections.<BRK>The experiments were nicely guided by the research questions. The idea of attacking the uncertainty estimates is novel and provides an interesting research direction. Negatives: 	The scope of the uncertainty estimation to Dirichlet based uncertainty estimation techniques was limited. The authors already cited prominent techniques in the paper, ranging from MC Dropout to ensembles and including the results using these techniques would have made the results more compelling. The scope of the adversarial attacks employed in this study were also limited. Even though the authors claim that using uncertainty from DBU models for identifying correct estimates do not produce robust estimates under adversarial attacks, the experiments are performed only using PGD attacks. The uncertainty attacks provide an interesting contribution but I found the section describing the proposed attacks very limited. I would suggest adding a brief summary of the conclusions in the abstract.<BRK>In this work, the authors seek to evaluate the robustness and uncertainty of Dirichlet Based Uncertainty models (DBUs). This paper make a valuable contribution in that it performs a systematic analysis of the robustness and uncertainty of DBUs. The one minor weak point of the paper is some of its contextualizations with the literature. For example, there are several papers (albeit very recently published ones) which I think it would benefit the authors to reference. In [1] the authors analyze the robustness of a model which outputs the parameters of a Gaussian distribution and find that directly using the parameters of the outputted Gaussian can lead to a strong attack. In [2,3] the authors perform statistical and probabilistic certification of Bayesian neural networks which is strongly related to the claim that the authors are the first to certify methods for uncertainty estimation models. Of course, there is a distinction in the approaches, but it is one the authors should probably make explicit for completeness. I do consider the experimental evaluation in this work to be sufficient given that the authors consider many applications which already exist in the literature, and in my view it is out of the scope of an evaluation/methodology paper to necessarily advance the state of the art in applications of the method they seek to evaluate.
Accept (Poster). rating score: 7. rating score: 7. rating score: 7. rating score: 5. <BRK>This work shows that this function can be decomposed into a sum: C of the zero sum part + C of the coordination part. The paper could benefit from additional discussion and explanation of the claims (see below) so that the paper is more self contained. Or in general? Quality:The work is of high quality. Significance:Proving chaos is common in games is significant because it means we have to adjust our algorithm design to account for it.<BRK>It is very interesting that MWU and OMWU have totally opposite dependency on this quantity. Summary:This paper studies the chaos phenomena of learning in general normal form games beyond zero sum and coordination games. If not, shall we expect them to have chaos phenomena or not? This work extends the previous results for zero sum and coordination games to more general games in a non trivial way, and it demonstrates the existence of a broad class of games suffering from the chaos phenomena.<BRK>Summary:This paper studies Lyapunov chaos in learning algorithms for matrix games. It appears to extend earlier work by Cheung and Piliouras to more general sum settings with the conclusion that in these more common settings the learning algorithms considered exhibit chaos. It does not appear to be defined* (1) is confusing. * I found (4) onward (through the end of pg.5) very confusing and didn t really follow* Notation of "u_i" in Def. this is a little awkward for the readerOverall:While this paper does offer some interesting ideas, I cannot recommend publication at this time.<BRK>As far as I understand, the authors study Lyapunov chaos that occur in learning in non zero sum games. I believe that even notable and important contributions for the community must have a significantly better presentation, because otherwise it will be quite difficult for readers to obtain and understand these results. 2.Continuing presentation issues, I did not get at all the core of the contributions. 3.It would be interesting to get insights on how the paper is related to learning community: despite the authors articulate on this in Abstract and Introduction, it is still unclear how the obtained results can improve learning / learning algorithms and/or improve their application in practice.
Reject. rating score: 4. rating score: 6. rating score: 6. <BRK>Summary The paper argues for using differentiable simulators for policy optimization. First and second order methods are presented and evaluated on a version of payload on crane stabilization problem. Decision I vote for rejecting this paper for the following reasons. 1.There is little novelty in this work. The method can be seen as a version of Guided Policy Search (GPS) [1] where the trajectory optimization part is done using analytical gradients. In Fig.5, PPO and SAC show very bad performance. Given that the task is quite low dimensional, it is surprising that they work so badly. If that is nevertheless the case, then it would be beneficial to add experiments at least on the Classic Control environments from OpenAI Gym, such that one can compare to other papers. 3.The choice of the step size alpha is questionable. The paper says that unlike PPO and SAC, the proposed method does not require any policy update regularization. So, the proposed algorithm is still a version of conservative policy update algorithms, such as [1].<BRK>The analytic gradients are used to optimize trajectories. The known states and actions from the optimized trajectories are then used to improve the deterministic policy. ## Strengths  This work provides strong insights into the benefits of differentiable simulators for  trajectory guided reinforcement learning. The paper provides a solid discussion and evaluation of  the use of first and second order methods. Do local minima pose a problem, given that the method effectively does no true exploration? Can hard constraints be handled, e.g., obstacles or joint limits that are to be avoided? Why not also compare the results for some tasks which are commonly used for RL, e.g., pendulum and acrobot swing up? Explaining these connections could be helpfulto many. I think the algorithm is sufficiently novel, and it does really well on some difficult new problems. I see the paper as being about a new policy optimization algorithm leveraging differential dynamics, and not about sim2real. And so with this in mind, I am changing my score to a 6, i.e., marginally in favor of accept. ]<BRK>Problem formulation: Can analytical gradients form differentiable simulators be leveraged to formulate better policy optimization algorithms? Strengths: Tractable and stable optimization procedure due to the decoupling of the trajectory and policy optimization. Expectation under this viewpoint warrants comparison to similar (interleaving supervisedlearning with trajectory optimization) optimization paradigms like POLO(Plan online, learn offline: Efficient learning and exploration via model based controlK Lowrey, A Rajeswaran, S Kakade, E Todorov), IGOR (Interactive control of diverse complex characters with neural networks.I Mordatch, K Lowrey, G Andrew, Z Popovic), GPS (Learning neural network policies with guided policy search under unknown dynamics S Levine, P Abbeel), etc. Under the premise that PODS  primary contribution should be viewed as a paradigm for sim2real, we need to account for the training and test (true) dynamics being different. While SAC and PPO should train using samples from true dynamics for training and report performance on true dynamics as well. No results on commonly accepted benchmarks tasks are presented. I recognize that the authors claimed that these tasks can pose for future benchmark tasks. With both algorithms, as well as the tasks, being new, its hand to establish the strength and credibility of both using one another. This is used as training dynamics for training policies, and the resulting policy is evaluated for performance on equivalent OpenAI Gym mujoco as true dynamics. Introducing a more crisp definition separating the two will help with clarity. Information on the line search for the experiments reported will be helpful.
Reject. rating score: 4. rating score: 5. rating score: 5. rating score: 6. <BRK>The authors propose a synthetic graph generator to evaluate graph neural networks. This reviewer acknowledges the need for a benchmark for knowledge base completion. Related work about graph neural networks and knowledge base completion seems to be well covered. One aspect of related work that seems missing is previously proposed synthetic graph generators.<BRK>The "results" seemed little to do with the dataset (apart from being enabled by the dataset), but were comparing some models that were neither the current state of the art or particularly novel. Such reproducibility in science is important. I would like to see more discussion about why doing well on the benchmark would translate into doing well on real world problems. But it doesn t; they all seem to be just the same complexity. I think they are just meaningless names, and so there is no commonalities between the worlds. Is there any notion that a relation learned in one world are like the relation with the same name in other worlds?<BRK>The similarities and differences to existing benchmarks (I agree that there are some, e.g., support for continual learning) need to be pointed out. I think it might be good to define the notion of "compositional generalization" more clearly. However, I do not understand why the authors completely ignore the entire field of rule learning/ILP. Smaller Comments:  p.2 What is the difference between generalization and adaptation?<BRK>The relations are drawn from a fixed set of types. It s true that the rules may examine new combinations of relations, but the end result is just that one of the existing relations is inferred to hold between the start and end of the path. This permits the benchmarks to be used to evaluate multitask learning, robustness to distribution shift, etc. My hesitation is that it s a bit on the incremental side, and seems oversold in a few places, as follows:The work suggests that the proposed benchmarks examine the "compositional generalization" abilities of models, but it is not clear to me how this is so.
Accept (Poster). rating score: 7. rating score: 7. rating score: 7. rating score: 3. <BRK>The authors contribute to the recent research on whether neural network training (in particular, SGD) favors minimal representations, in which irrelevant information is not represented by deeper layers. Importantly, the network s output is conditionally independent of the color information, given the direction decision, so the color information is in some sense irrelevant at the later stages. Using this, the authors quantify the  relevant  and  irrelevant  information in different layers of the neural network during training. Interestingly, the authors show that minimal representation are uncovered only if the network is started with random initial weights. (The situation is different when the task involves predicting a non degenerate probability distribution P(Y|X), in which case the minimal representation   i.e., the sufficient statistics   can have an arbitrary amount of information.) In fact, it can be conditionally independent because it is type 2. Given the points made above, if I understand correctly, information about which targets correspond to which colors is just as irrelevant as the color information, when conditioned on the output. I would suggest referring to the second kind of information (the one mainly discussed in the paper) as "output information".<BRK>The new results on CIFAR 10 and CIFAR 100 with fine and course labels match the results on the checkerboard task. These findings are interesting, useful for understanding neural networks, relevant to the ICLR community, but lack evidence of generality. The new experiments also confirm that the level of noise in SGD has a key role in finding minimal representations. **Most of the experiments in this paper are done on the checkerboard task. Furthermore, they show that when training with SGD with enough amount of noise, the usable information with fine labels increases initially and then decreases. This increases the generality of the main claims. Opening the black box of deep neural networks via information. The authors also consider the task of predicting whether an MNIST digit is odd or even. Additionally, it would be interesting to consider cases when the training data is such that there is a small mutual information between the irrelevant factor and the target. P.S.I am willing to increase the score if the authors address the above concerns about generality.<BRK>Broadly, this work is an attempt to understand how neural networks can form generalizable representations while being severely overparameterized. This work proposes an information theoretic measure, called the "usable information", and use it to quantify the amount of relevant information in different layers of a neural network during training. The choice of the task was also appropriate: it provides a good intuition about the relevant vs. irrelevant information, as well as a bridge to neuroscience studies that may lead to insightful discussion. (ii) "a positive correlation with the minimality of the representation and generalization performances": this sounds vague. Overall, I think this is an interesting paper that presents a promising approach toward the understanding of how informative representations are formed by training, one of the most fundamental questions in deep learning. **UPDATE:** Most of my questions/comments are addressed in the revised version of the paper and the author responses. I maintain my support for acceptance.<BRK>This work introduces a notion of "usable information" in neural network representations (essentially, decodability of information by a neural network), and suggests that learned representations are "minimal" (discard task irrelevant information) when training begins from random initialization, but not necessarily when beginning from other initializations. It seems that the results of the paper must necessarily depend on several hyperparameters which were not explored. The result about generalization correlating with minimality was not confirmed on MNIST. More discussion of this is neededOverall, given that this is an empirical paper (no new theory is provided), it is important for the experiments to be extensive and comprehensive. The experiments in this paper, though they touch on interesting ideas, are not thorough enough to convince a reader of the authors  broader claims.
Reject. rating score: 4. rating score: 4. rating score: 4. rating score: 6. <BRK>This paper proposes the few shot edge detection task, which is similar to few shot segmentation but for the dual task of detecting semantic edges. For the proposed method, the authors:1) Propose a meta learning strategy based on prototypical learning and using intermediate low res segmentation prediction to generate attention maps for few shot edge detection2) Propose using a multi split matching which performs prototypical matching (for fg and bg) using many splits of the high dimensional featureThis paper is the first to propose the task of few shot semantic edge detection (dual task of few shot segmentation, which has been explored before). This is a relevant problem, and is well suited for few shot tasks instead of segmentation since learning a boundary can be easier (modelling high frequency info) as compared to modelling interior pixels (often containing low frequency info) that has to be done in few shot segmentation. I have **two major issues** with the paper in its current form:1. Quote   "Moreover, evaluation with non zero distance tolerance requires additional heavy computation. However, PANet + Sobel filter would only generate thin edges and there is no mention in the paper of the authors thickening its output edges according to the dataset (whereas thick edges are used to train CAFENet, so it has learned to produce thicker edges). 2.Multi Split Matching Regularization   The proposed MSMR technique is well ablated for the different choices of its hyperparameters, but at the end of the day is still an ad hoc technique. I believe this needs to be ablated more, and MSMR needs to be grounded more in existing work.<BRK>Pros:1 This work is well written and easy to understand. 2 Extensive ablation experiments are performed to verify the effectiveness of the proposed modules in the CAFENet. Cons:1 This new task is very similar to the few shot segmentation. 2 The proposed method was named “class agnostic”. So, I think the method is class aware rather than class agnostic. 3 In my opinion, the main technical contribution of this work lies in the split wise matching, which is employed to produce better segmentation masks. It is hard to find novel/specific designs in the edge detector part. So, I consider that the performance will significantly rely on the predicted segmentation masks, making this new task back to few shot segmentation. 4 Using PANet + Sobel as a baseline is unfair. Authors should replace the few shot segmentation branch with PANet and more latest advanced few shot segmentation methods for comparison.<BRK>A.Summary:This paper works on few shot semantic edge detection. Instead of dealing with the problem in a single stage, the authors decompose the problem into two stages. First, a few shot segmentation stage, where the foreground and the background probability are estimated via attention with the foreground and the background prototype (averaged feature vector on the foreground and the background region). In the experiments, the authors modify two existing datasets for evaluation. However, this paper is about semantic edge detection, where the internal region does not contain any edge. 2.The comparison with previous methods, especially the PANet+Sobel is unfair. The semantic response map corresponds to the attention map in the proposed method, so the author should also use sobel operator on their attention map to have a fair comparison, which can also serve as an ablation study to verify whether the attention map is one of the major contributions to the edge quality. 3.Another ablation study worth to be investigated is to apply the attention map directly on the edge map, which will further sperate the high level sub task(semantic attention maps) from the low level sub task  (edge detection). It also contains some issues in evaluation.<BRK>Summary:This paper introduces a novel problem of "few shot semantic edge detection" where semantic boundaries are to be learned/detected with a few labeled samples. Pros:  The motivation was effectively described and the problem was well defined which made it easy to read. Quantitative results show reasonable edge detection performanceCons/Questions:  The authors constructed the baseline by combining PANet (ICCV 19) with the Sobel edge detector while disregarding some of the better/recent approaches shown below:*  Canet: Class agnostic segmentation networks with iterative refinement and attentive few shot learning. (CVPR 19)** Prototype Mixture Models for Few shot Semantic Segmentation (ECCV  20)*** Part aware Prototype Network for Few shot Semantic Segmentation (ECCV  20)Would the proposed approach still be effective than the combined version of Segmentators+Sobel if the segmentors were updated to more recent models? Theoretical or analytical reasoning to support the effectiveness of MSMR would better convey the authors  claimMinor comments:  There might be some incorrect labels in Figure 3. In this figure, the diagram on the left is not supposed to be split wise matching, right?
Reject. rating score: 4. rating score: 4. rating score: 6. <BRK>While the paper attempts to study an important problem of scaling graph networks to large graphs, and authors show the proposed methods is capable of saving in terms of memory and runtime by sacrificing predictive performance (which is not surprising at all), it is not suitable for publication at ICLR. Comments: Novelty is clearly below the bar of ICLR. The proposed method "layer by layer training" is rather trivial. In fact, there are many easy counter examples to show this approach would fail. Moreover, another problem with this method is the lack of auxiliary labels for each layer.<BRK>**Summary**This paper introduces a method for decoupled layer wise training of graph neural networks. b) I did not find the code release with the paper. c) Emphasize the novelty of the method otherwise, greedy pre training seems like a lukewarm idea. **Weaknesses***Novelty and impact need to be strengthened*a) As mentioned by the authors, greedy layerwise pre training is an old idea. c) Absent theoretical convergence guarantees, the proposed approach is a heuristic as it relies on an auxiliary function that’s somewhat arbitrary. *Experiments can be strengthened*a) The results seem to trade off accuracy with speed and saved memory and results are reported on 3 small and one large dataset.<BRK>Pros:1.The layer wise decomposition is not a new idea but to optimize GNN, it is not as popular as in normal DL framework. Unlike CNN, inference of the node embedding in GNN requires information be propagated throughout entire graph for each epoch. In this paper, the authors proposed to decouple the inference and aggregation in each layer and learn the layer wise latent representation separately.
Reject. rating score: 5. rating score: 5. rating score: 6. <BRK>The knowledge distillation for noisy learning has been well studied and is not a significant novel contribution. Therefore, I am not very positive about this submission. The results are motivating, but as such there is not enough novelty in the proposed solution which stands out. .Both of these assumptions seems very strong.<BRK>The paper lacks the analysis on class specific performance and object size specific performance on different noise settings. Although the results are promising, the practical value is limited.<BRK>What about the effect on performance? So overall, the results are quite good and the approach is sensible and seems to work. The warm up phase appears to be critical and is also given short shrift. I would thus also dispute the description of this setting as being "more challenging and practical" than other settings considered for weakly supervised training (e.g.mostly relying on image level labels). Based on the final results, the method obviously must be doing something right but some distribution of IoU values before and after the correction/training process would have been interesting w/ some more qualitative examples of boxes that weren t successfully corrected.
Accept (Poster). rating score: 7. rating score: 6. rating score: 6. <BRK>My biggest issue with the paper concerns inference. The authors do say that the algorithm converged fast, but do not show this in any experiment. 2.The priors match current literature on suggestions for sparsity inducing priors. Cons:1.Very limited coverage of inference, which is an important aspect even if carried out by an external software. More generally, the comorbidity example is a bit superficial and could have been developed a bit further.<BRK>This manuscript revisits the hierarchical Poisson matrix factorization (HPF) promoting sparsity in both the representation and the decoding function with a Horseshoe+ prior. The benefit of the GAM decoding is not clear. It could help to perform more empirical comparison, and to study more the contribution in the context of full analyses. It develops well the theoretical point that sparsity in encoding and decoding is important.<BRK>** DescriptionThis paper provides a new approach for finding a sparse encoding of count data matrices and hence automatically achieve feature selection. It identifies a failing in traditional hierarchical Poisson matrixfactorisation (HPF) and proposes a solution. ** ConsFor me the weakness of the approach is that the solution proposed appears very ad hoc with no probabilistic basis. A less ad hoc model might allow a more principled approach to choosing hyperparameters.
Accept (Poster). rating score: 7. rating score: 7. rating score: 6. rating score: 6. <BRK>This paper proposed a novel method for knowledge distillation. The idea is to utilize the teacher’s pre trained classifier to train the student’s penultimate layer feature by adopting two losses: (a) the Feature Matching loss LFM and (b) the Softmax Regression loss LSR. The whole idea is easy and clear. why student classifier not included? The final model is thoroughly compared with state of the arts knowledge distillation methods. Despite in several tasks, the proposed model fails to succeed the CRD method, given the simplicity of the proposed method and its superiority over most of the tasks, I think the results are satisfactory.<BRK>This paper propose to leverage the frozen classifier from the teacher model for training better representations for the student model. By further combining with another feature matching loss, the proposed methods outperforms previous methods (such as KD, AT, CRD) on many benchmarks. Authors demonstrate the effectiveness of this method, on CIFAR 10, CIFAR 100, ImageNet, as well as transferring to STL 10. The paper is well written and very easy to follow. So I encourage the authors to add a section to discuss it.<BRK>**Paper summary**This paper proposes a new knowledge distillation method by enhancing the student network s representation learning. Experiments cover various network architectures on CIFAR and ImageNet datasets and show improvements over the other competitive knowledge distillation methods. The main idea to share the teacher s projection matrix $W$ for both the student and teacher s penultimate features seems new. 2.Proposed method is simple and easy to implement. Combining $L_{SR}$ with other KD methods (e.g, AT, OFD, etc) will verify the generalization and applicability of the proposed method.<BRK>#####################################################################################Summary:This paper proposes a new formulation of knowledge distillation (KD) for model compression. The proposed L_SR loss is similar to the standard KD formulation in Hinton et al., with the only difference that the pre trained teacher’s classifier is used for both teacher and student models. The approach is evaluated in a variety of scenarios, such as different network architectures, teacher student capacities, datasets, and domains, and compared with state of the art results. The paper is well written.
Accept (Spotlight). rating score: 7. rating score: 6. rating score: 6. rating score: 10. <BRK>They introduce geometric vector perceptrons as a way of summarizing geometric information for graph layers without loss of information as it happens in dense layers. They evaluate the performance of these architectures on MQA and CPD tasks, both relevant and standard benchmarks in the field. Positives:  The paper well written and sets up the problem and how it supposed to be solved very well. Ablation and hyper parameter studies are well scoped. For this reason I suggest including the MLP results in the main text, so that the performance improvement is clear. It is also relevant to discuss what drives the performance of the method such  that even with MLP, it outperforms sophisticated architectures introduced before. For MQA tasks, A discussion and ideally quantitative comparison with Ingraham 2019 ICLR (Differentiable Simulator), and AlphaFold (Senior et al 2020 Nature) seems pertinent.<BRK>This paper aims to leverage the geometric and relational aspects of the 3D structuresimultaneously with the proposed GVP layer. The experiments on two problems in protein structurelearning show improvement over CNN and GNN. However, some details are missing, and theintuition of the methods is not clear. 2.The learned vector features in the intermediate layer are interpretable. The schematic diagram needs to be refined. 2.Some equations are not clear. 3.The writing and format of the paper are hard to read.<BRK>This work uses a graph representation of the protein backbone and a GNNfor model quality assessment (MQA) and protein design. Strengths:The GNN architecture proposed has the important property that the vector and scalar outputsare equivariant and invariant with respect to composition of 3D rotations and reflections. The submission is clear and correct, and the experiments are reproducible.<BRK>In this paper, the authors introduce a novel procedure to predict or acquire insights from the structure of a macromolecule (such as a protein, RNA, or DNA), represented as a set of positions associated with atoms or groups of atoms in 3D Euclidean space. Their approach is divided into two steps: model quality assessment and computational protein design. The paper is powerfully written, and the approach is novel. I have a few suggestions for the Authors. In the results, please refer to the suggested approach as GVP GNN. The availability of a dataset, especially the synthetic dataset, will allow the paper to be reproducible by others.
Reject. rating score: 4. rating score: 5. rating score: 5. rating score: 7. <BRK>2.The experiments show that Transformer QL can outperformance Transformer XL and Compressive Transformer. 2.The authors should have some comparison to some recent models on processing long sequences, such as the models from "Efficient Transformers: A Survey"#######As no author response, I will keep my rating. The performance of Transformer QL is still far from the work "ADAPTIVE INPUT REPRESENTATIONS FOR NEURAL LANGUAGE MODELING" which does not target capturing long context. The authors should have done more experiments on other datasets, maybe refer to the paper, Big Bird: Transformers for Longer Sequences.<BRK>Summary Using multi scale hierarchical and compressive techniques, this paper examines a way to increase the context length of transformers. The results demonstrated in the experiments show an improvement over previous models. Negatives The experiments do not compare to many other approaches, even though those approaches are cited throughout the paper. Although the complexity analysis is thorough, I d like to see empirical results of memory/compute requirements as a function of the context length. Minor issues that did not affect score Figure 1 has some scaling/resolution issues that make it hard to read.<BRK>The work introduces the Transformer QL, a transformer based model that aims to capture long distance dependencies in the input. A thorough complexity analysis is included. The presentation can be improved, all the definitions are hard to follow. The paper describes the idea of multiple temporal scales. In the experimental sections, it seems that only two scales are used. I agree with the authors that extending the context is important. How effective is the method to capture farther long term dependencies compared to previous methods? The experimental results mainly address similar networks with similar context lengths.<BRK>This paper proposes Transformer QL, an improvement over Transformer XL architecture which allows to use longer context at reduced cost. The paper is well written and its good experimental results are strengthened by theoretical complexity estimation. The text of the paper, though well structured and concise, requires some polishing and misspellings correction (for example, I am doubtful that the proposed architecture is endowed with the ability to bite while section D of the appendix claims that "Multi scale Transformer has been widely bitten by Transformer QL").
Accept (Poster). rating score: 8. rating score: 7. rating score: 7. rating score: 7. rating score: 5. <BRK>This paper describes a number of improvements over the non autoregressive FastSpeech TTS model. One of the trends that TTS has seen since Tacotron was announced is a retreat from fully end to end modeling for TTS. The text input is first converted to phones, and prosodic information can be inferred from text (in a unified system) or can be conditioned by a user. It relies on ground truth predictors during training, but learns to infer these via multitask training so they are not required during inference. However, since the discussion of training time as a valuable measure is already begun, I would suggest that the authors include this number, with the explanation that it is not comparable. 1) there is variation other than duration, energy and pitch that can vary between realizations of the same utterance, voice quality, background and channel noise, and speaker effects. During inference, the process is still deterministic given the state of the model.<BRK>This paper is motivated by, and intends to address shortcomings of, the previously published FastSpeech paper, specifically problems with the internal duration prediction of that model, and its requirement for a teacher student configuration. The submission makes clear what the problems with that original approach are, and the evaluation later confirms this. If space is an issue, some of the material in the Appendix could be shortened, or just referenced. Some comments and needed clarifications follow:The paper says that the variance adaptor "adds" different variance information. *   There is, in fact, an extensive (and relevant, but uncited) literature on direct neural prediction of f0 with external f0 targets as the authors do here. The paper mentions that the training of FastSpeech2s required using shorter clips to deal with memory issues. The analysis/resynthesis condition (GT(Mel+PWG)) shows a drop of 0.38 MOS points with respect to the ground truth (4.3 vs 3.92).<BRK>This paper further extends the non autoregressive end to end TTS model "FastSpeech". The new work includes (1) removing the knowledge distillation from an AR model to the non AR model which was required in FastSpeech, (2) introduced "variance" representations such as energy and F0 in addition to duration. (2) improved the naturalness as well as controllability of synthesized speech. There are a few parts need to be addressed. It is not well discussed in the paper. The authors should also discuss it from this point.<BRK>Summary: In FastSpeech 2 and 2s, the authors propose several changes to the non autoregressive FastSpeech model. In FastSpeech 2, the authors do away with the teacher student knowledge distillation pipeline and instead utilize the following to alleviate the one to many problem while retaining a non autoregressive architecture:  ground truth mel spectrogram is used as the training targets  a force alignment tool is used to extract more accurate phoneme durations  pitch and energy information is also used as conditioning for the model as a means to introduce variance informationThese changes boost the speech quality scores of the FastSpeech 2 architecture over autoregressive TTS architectures while reducing both training and inference times considerably. The authors also introduce an end to end text to waveform variant of the model: FastSpeech 2s. This could be included in the text as well. Additionally, there are metrics proposed in prosody transfer literature[1] that can be used here.<BRK>This work presents several improvements over the original teacher student framework in FastSpeech: 1) training the model with ground truth target instead of the output from teacher, 2) extract phoneme duration, pitch and energy from speech and directly take them as conditional inputs in training and use predicted values in inference. The problem of FastSpeech (and other non autoregressive TTS models) is really due to the over simplified output distributions, which assume conditional independence between frames and frequency bins for mel spectrogram. The authors overclaim that "FastSpeech 2s is the first attempt to directly generate waveform from phoneme sequence". The CWT/iCWT based pitch predictor is interesting. 8, In Table 5, the durations from teacher model are extracted by teacher forcing ground truth mel spectrogram? Pros:  Good sample quality. Cons:  The proposed pipeline is far more complicated than existing end to end TTS model. The novelty is rather limited.
Accept (Spotlight). rating score: 9. rating score: 7. rating score: 7. rating score: 6. <BRK>Summary: This paper focuses on better learning of video text representations. To this end, the paper introduces a new generative task of cross captioning which alleviates the typical issue of contrastive learning by learning to reconstruct a sample’s text representation as a weighted combination of a video support set. 2) The proposed method sounds novel and interesting. The empirical evaluations on various datasets suggest that the proposed method is better.<BRK>This paper concerns the problem of video text representation learning and its application to video text retrieval. The source of pre training data comes from YouTube video ASR pairs. i) The motivation behind using a support set is still unclear, besides the obvious reason of empirical gain. ii) The overall scope of the proposed method is somewhat overstated. The authors have committed to include more datasets to enhance the result completeness and add relevant but missing related work.<BRK>## SummaryThe paper proposes a new way to train joint representation of video and text. To address this, they propose to combine the contrastive loss with a generative loss that tries to generate the text from one video based only on *different* videos within the batch. Interestingly, adding this objective improves performance of retrieval. This approach is novel to the best of my knowledge. **Is the support set idea always good? If yes, were there benefits? The paper is clearly written and the results are compelling and do support the claim of the paper. Does it hurt? Is this a problem that the authors have considered?<BRK>Summary:The paper introduces an interesting problem when doing noise contrastive learning. 2020.Video understanding as machine translation. To alleviate this problem, the authors develop a generative model to push these related samples together by reconstructing the caption from the weighted feature of support samples. My major concern is about the architecture of the proposed model, the performance on other tasks, and some additional ablation studies (see below). The comprehensive experiments on retrieval tasks show SOTA results with a margin. It is doubt that why the cross instance captioning needs such a heavy decoder. Does it mean training without cross instance captioning? 10.The support set is the samples of the mini batch. 2020.Actbert: Learning global local video text representations.
Reject. rating score: 5. rating score: 6. rating score: 6. rating score: 6. <BRK>This paper proposes a more sophisticated approach to computing importance weights, in the setting where the functions $f_i$ have a special form (essentially polynomials in the inner product with a fixed vector). There is a matrix $A$ where each row $a_i$ defines a function $f_i(x)   P(<a_i,x> )$ for some univariate polynomial $P(t)$, so that its gradient is of the form $\nabla f_i   P (<a_i,x>)a_i$. #### Pros* The general problem is clearly important and interesting. * There needs to be some discussion of why this is an interesting class of functions in this setting, what it covers, and what it doesn t. . * In general, the paper is very dense. I was unable to get a sense of what the technical novelty of the paper is, and where the authors  are putting together known ideas from previous work. Whereas the problem it tackles is of broad interest, in its current form, the paper will not appeal to a broad ML audience,<BRK>I think the paper remains on the borderline, for the following reasons:  I still doubt the practicality of this algorithm in any realistic settings. Thus I disagree with the "vastly superior" characterization; it is well known that the gradient estimator this paper seeks to efficiently approximate is a good choice; a convincing proof of concept is that it s useful to use sketching to efficiently approximate it (whether it s worth the computational overhead + approximation error). My primary concern, and the reason I believe this work could use a round of significant revisions before publication, is with clarity. Including an explicit convergence rate would be helpful, since the paper culminates in an optimization algorithm.<BRK>The present paper shows how to perform variance reduction for SGD (as well as stochastic second order methods) in a streaming setting. The first is that the theory requires the gradient of the loss to be of the form f(<a,x>) * a, where f is a polynomial. It would help if the authors could clarify this point, since I didn t see it explained in the experiments on logistic regression. Looking through the actual algorithm and analysis, I believe it is O(d^p) where p is the degree of f, which if we take a second order approximation to f means at least O(d^2). In many applications this would be infeasible (many models have d > n, so d^2 is worse than n*d).<BRK>I really would like that the authors give an intuition for the theorem 2.1 which is the principal building block of the paper. Is this new ? The goals are clearly stated and the paper reads rather well when we have understood the purpose of each block. I cannot yet imagine how the authors can modified the current version to make it more comprehensive, but it is sure that the complexity of the procedure is a real barrier for the reader. * Below the general comments and ways to improve the quality of the article. **Improve the clarity. How does you algorithm compare to them? behavior of your algorithm.
Accept (Poster). rating score: 7. rating score: 7. rating score: 6. <BRK>One could see this work as a variant of self attention networks instead of a new dropout technique. **Clarity:**_Pros._ The manuscript is well written and straightforward to understand the proposed method. Including the ImageNet and the VQA experiments is helpful to illustrate the applicability of the proposed method. (in "Contextual dropout module for fully connected layers" paragraph.)<BRK>Summary: This paper proposes a way of estimating data dependent dropout probabilities. This is done in each layer using a small auxiliary neural network which takes the data (on which dropout is going to be applied) as input and outputs dropout probabilities, which are sampled and multiplied into the data.<BRK>The paper develops a variant of dropout regularization which was shown to be effective in training deep neural nets in the past several years. The results in the paper show the contextual dropbox can improve accuracy on ImageNet and VQA2.0 datasets. I like the idea of data dependent dropbox and the authors showing its applicability to various neural network layers like fully connected, convolutional, and attention layers.
Reject. rating score: 5. rating score: 5. rating score: 5. rating score: 6. <BRK>### SummaryThe paper proposes DAC, an actor critic method exploiting the replay buffer to do policy entropy regularisation. Does such a policy always exist? You prove the convergence of your algorithm (I did not check the proof in the appendix), what are the assumptions for which the convergence is guaranteed? ### Pros  Formulating the diversity using the entropy of the replay buffer frequences is an interesting idea. The variance across different seeds seems to be huge for your method (as well as for the others). ### Cons  It is not clear, what is the problem the paper tackles. If DAC is for improving exploration, then it should be compared with other exploration methods, not with vanilla SAC. The paper is based on assumptions not challenged/tested by the authors, e.g.policy entropy regularisation is inefficient, because it does not take the distribution of the samples into account. A is the continuous action space . The paper has to have a clear research question and its motivation. This should define the experimental part of the work. "it is preferable that the old sample distribution in the replay buffer is uniformly distributed".<BRK>This paper considers the exploration efficiency issues in off policy deep reinforcement learning (DRL). To avoid repeated sampling of previously seen scenarios/actions, the authors propose to replace the current policy in the entropy term with a mixture of the empirical policy estimation from the replay buffer and the current policy, and term this approach as sample aware entropy regularization. The authors then propose a theoretical algorithm called sample aware entropy regularized policy iteration, which is a generalization of the soft policy iteration (SPI) algorithm, and show that it converges assuming that the empirical policy estimation is fixed. A practical algorithm based on the sample aware entropy regularized policy iteration, called Diversity Actor Critic (DAC), is then proposed. This algorithm is a generalization of the well known soft actor critic (SAC) algorithm. Some discussion about this is needed. 2.In (2), $s_t$ should be replaced by an arbitrary $s$ in the state space. So in short, the paper proposes an interesting modification of the max entropy regularization framework, but contains several technical and clarity issues. Hence I think it is not yet ready for publication in its current form. Some more explanations are needed.<BRK>This paper proposes diversity actor critic (DAC) for exploration in reinforcement learning. The main idea of the proposed algorithm is to take advantage of the previous sample distribution from the replay buffer for sample efficient exploration. The authors provide convergence analysis of DAC and conduct empirical investigations on several benchmarks. Pros The idea of using previous sample distribution from the replay buffer for better exploration seems interesting. The authors provide a reasonable method to optimized the proposed objective, which can be naturally combined with state of the art algorithms like SAC. The authors should make the definition of $\pi^*$ clear. 3.It’s worth to provide the results of SAC div with JS divergence as it’s more similar to the proposed objective (4). For example, [1] also uses a mixture of previous polices to encourage exploration with strong theoretical guarantees. Also, the experiment results are not very promising compared with the baseline algorithms based on SAC. Provably efficient maximum entropy exploration. Other suggestionsThe main idea of the proposed method is to make the current policy different with previous policies. The paper uses a nonparametric method  (2) to approximate the previous policies.<BRK>Compared to previous methods which do not take care into account the distribution of the samples in the replay buffer, the proposed method maximizes the entropy of the mixture of the policy distribution and the distribution of the samples in the replay buffer, hereby making exploration efficient. Reasons for scoreI vote for accepting the paper. The paper proposes an intuitive and efficient exploration method that generalizes existing methods, including them as special cases. The authors provide a theoretical guarantee (Theorem 1) that the policy obtained from the iteration of evaluation and improvement under this new regime converges to the optimal policy. In Section 6 in the 5th line, J(\pi) should be specified as “J(\pi) in (1)”. It is done in the next sentence, but I prefer that it is done when it first appears.
Reject. rating score: 1. rating score: 3. rating score: 4. rating score: 5. <BRK>The model is evaluated on SQuAD (top K answer prediction) and an internal Amazon dataset. There are indeed several multi span QA datasets (e.g., DROP, Quoref, Natural Questions) and I think the paper should experiment with those datasets and compare to previous approaches. First of all, it is mentioned in the paper that “To the best of our knowledge, a multi span QA architecture has not been proposed.” This is certainly incorrect. Third, the evaluation of the paper also should be improved.<BRK>I think the paper is not ready for publication for a few reasons. Such literature has not been mentioned or discussed in this manuscript. Second, although “multi” span extraction is the main motivation for this work, as described in the Introduction, there is no evaluation on public dataset. Experiment on Amazon internal data is included, however, as the detailed description or the data statistic is missing, it cannot be considered as academic empirical evaluation. There is no “image” involved in the model architecture or training.<BRK>The basic motivation makes sense to me, but I still have the following concerns about the paper:1) the proposed convolutional network though incorporates more information, it s constrained to be the nearest information in its sliding window, let s say 2 or 3, and the probability of p(i, j) thus depends on $h_{i 2:i+2}, h_{j 2:j+2}$. The errors the model made are mainly due to a lack of global reasoning, the more further away context is actually a critical issue for breaking the bottleneck. 3) the results in in house datasets are not quite convincing. I think you can add some heuristics to the BERT QA answer span selection to improve its top k results like adding diversity, prevent overlapping, etc. I m not sure whether the gap will still remain as significant as reported in the paper.<BRK>When the Span Image technique is applied to a base BERT model, the authors show performance gains on a single span dataset (SQuAD) and substantial improvements on a multi span dataset (an internal Amazon dataset). #### Negatives  The authors only compare to one baseline, BERT QA, and do not fully motivate how it is state of the art on the respective datasets considered; a few other observations call into question the generalizability of the method:      The authors claim that the technique is modular and can be applied to other architectures, however only one architecture is considered. There are other multi span “reading comprehension” datasets in the literature (DROP[1], Quoref[2]) that would probably have been appropriate datasets to which the authors could have applied their technique. Ultimately, the paper could be improved by further exploration of the method, whether via application to new datasets or more in depth analysis of the generalizability of the method. Did the authors consider the RC datasets specified above (DROP, Quoref), or is there a reason they were omitted. Association for Computational Linguistics.
Accept (Poster). rating score: 8. rating score: 7. rating score: 6. rating score: 6. <BRK>How good is this approximation in practice, and what  are the factors influencing it? I would assume that stability is a function of the  selected aggregation function *and* the Wasserstein embedding itself. I am fully aware that it is hard to satisfy both   theory and experiments in a paper; I have some more detailed comments   about what could be added (potentially in the supplementary materials,   if the authors think that it detracts from the flow).<BRK>##########################################################################Summary:This paper proposes to use a Wasserstein embedding to compare graph using also node embedding methods to convert graphs into vectors. As such the paper is self contained. The combination of node embedding with Wasserstein distance is new and show pretty good results.<BRK>Most OT libraries already include this variant, so there really seems to be no excuse not to incoporate it. The paper proposes a farily trivial combination of Wang et al s  linear  OT with simple message passing on graphs. How do the authors explain this?<BRK>2.Choice of the baseline methods. The result is an embedding into a Hilbert space, where the distance between two graph representations approximates the 2 Wasserstein distance between their respective node distributions. I have some comments and concerns about the experimental analysis, which does not seem to directly validate the paper claims.
Reject. rating score: 3. rating score: 3. rating score: 4. rating score: 6. rating score: 6. <BRK>Summary The paper proposes a trainable way to re order or recover the ordering of features from sets of examples, and use it as a way to build a common feature space (or embedding) for a neural net, the (initial) parameters of which can be trained by Reptile. Pros   The paper shows it is possible to recover information about the identity of coordinates in the input space, through a learned transformation, on several unstructured datasets. The similarity between such representations of individual coordinates can help identify similar features, either in a given dataset or across datasets. However, the paper then seems to consider individual coordinates in the input space only, and focuses on mapping shuffled subsets of these coordinates back to their initial position. Even then, the pre training of the "chameleon" alignment module seems to involve using examples of the meta test classes. The principle of the alignment module seems similar to (soft) attention mechanisms, in that there is a softmax trained to highlight which parts of an input vector should be emphasized (or selected) at a given point in the processing (here, in the aligned feature space). The parameterization of the first matrix makes the number of parameters depend on N, the number of examples in a given task. This could be quite limiting to be restrained to tasks of exactly N examples, especially if both the support (mini train) and query (mini test or valid) parts of an episode need to have exactly N examples. There is also no discussion of the  value or impact of or K, the size of the chosen embedding space). Recommendation I recommend to reject this submission. However, both the algorithm and the experimental set up are described in a quite confused way, and not well justified or grounded.<BRK>At the beginning of that section it says, “every task has to share the same schema of common size K” which seems to indicate “schema” is the number of features and then a few lines later, “ tasks with varying input schema and feature length F” which seems to indicate “schema” is *not* the number of features. In the related work section, few shot learning did not begin in 2017 as might be suggested by the citations. 2003A Bayesian framework for concept learning. If the meta training tasks are drawn from only EMNIST Digits, then how can the “re ordering” matrix be learned from EMNIST Digits such that it can re order features from EMNIST Letters? It would be helpful to have an experiment on a less toy dataset, both to demonstrate that the problem of “mis aligned features” exists in more complex data, and that the method can address it. I do not understand the problem statement or how the method is trained in the learned feature case. In my view, the clarity of this paper needs to be significantly improved to consider acceptance.<BRK>They introduced a feature transformation or re ordering matrix to align the features. While I agree with the authors that this problem is of significance in the meta learning community, the solution in this work, depending on the ground truth of re ordering matrix, is trivial and impractical. The paper is well written and easy to follow. Weaknesses:      The primary concern about this paper is its technical contribution, being limited and impractical. To align tasks in incommensurable feature spaces, projecting them into a common feature space has been a common practice. The empirical results are also not convincing. Why is only Glorot initialization compared in Figure 3? From both Figure 3 and Figure 4, and also the results in Appendix C, I see little improvement of the proposed over Frozen. Maybe it is features 2 and 4?<BRK>It proposes Chameleon model that learns to align input features from different tasks by learning a permutation matrix for each task, and shows that Chameleon can successfully learn good initialization. Strength:  It identifies and tackles a new important problem in meta learning: meta learning on tasks with different input features. The proposed approach is simple but shows improvements over the baseline method. Weakeness:  Supervised training for the permutation matrix is necessary for the model to perform well. Experimental results section can be more detailed. Given that Algorithm 2 is the major part of the method, how is the reordering training procedure constructed? are there / what are the shared features between different tasks? typo: Equation (9) is mentioned several timesI believe this paper proposed a new interesting problem in meta learning and provided a simple effective model to address the problem.<BRK>Chameleon: Learning Model Initializations Across Tasks With Different Schemas  The paper provides an interesting direction in the few shot classification field. The paper also demonstrates how current meta learning approaches can successfully learn a model initialisation across tasks with different schemas as long as they share some variables with respect to their type or semantics. The paper takes on an interesting facet of few shot classification: An encoder model that aligns to different predictor schemas to a common representation. Overall performance is quite good however, it would be a good study to have an analysis of the different datasets as to how balanced/unbalanced they are, how it affects the performance, the nature of the features etc. Also, I would like the author to discuss how suitable/adaptable this approach will be for multi label tasks and what kind of modifications (if any) are to be made. The idea of encoding different predictor schemas to a common representation is quite interesting and comprehensive experiments and supporting ablation study has been made.
Reject. rating score: 3. rating score: 4. rating score: 7. <BRK>This paper studies the implicit regularization effect of SGD from the Thermophoresis perspective. Overall, I cannot give high scores to this paper. Below are my detailed reviews   it is highly possible that I misunderstand something; if so please do clarify. Does Eq.(5) imply the expectation of the gradient is zero??? This is not at all true for GD/SGD. Intuitively this transformation changes a descent iterate to an ascent iterate and vice versa. In this case can you prune the network as stated in the paragraph? The title and abstract emphasize the paper concerns SGD.<BRK>Transferred to SGD, this would imply that the mean gradient is zero, which seems almost fundamentally at odds with the setting of gradient based optimization. The main argument of the paper relies on a physics analogy. For a theoretical paper studying a mathematical process (SGD), the mathematical reasoning seems very vague and I have problems following it in various steps. I am listing some specific questions below. I put a lot of effort into it, but I just can’t follow the chain of reasoning here. It seems to me that we are talking about the probability of transitioning into state $q$ from any other state. This restriction is not discussed in the paper at all. b) Is there an intuitive meaning to that subset?<BRK>1.Summary:This paper proposes that SGD has the implicitly bias of reducing gradient variance via the phenomenon of thermophoresis that masses tend to flow from regions with higher temperature / variance of random walk to regions with lower temperature / variance of random walk. The dependence of the rate at which this phenomenon happens on the learning rate and batch size are verified in the experiments. Because in the end, which indices belong to $U$ is not fixed, and the complement of a $U$ may also be a legitimate $U$ itself, for example. 3.Arguments for the recommendation and questions for the authors:The authors did a good job explaining the general theory of thermophoresis as well as the derivation of the effect on the two layer neural network model.
Reject. rating score: 3. rating score: 4. rating score: 5. <BRK>Improving predictions for tail labels is an interesting research goal that has not been thoroughly addressed in the literature, but I am not convinced of the theoretical results and the introduced algorithm. Theorem 1 does not hold because an important condition is missing. However, in practice, such a condition cannot be guaranteed. I am also not convinced of the algorithm that is introduced in Section 3.2. The method is very ad hoc, without any theoretical justification. Has such a simple solution been considered in literature? In the experiments it is not clear to me why only four XML datasets are used.<BRK>The paper presents a method for improving tail label performance in extreme multi label learning setup where the number of target labels can be extremely large. It is possible that the impact of RankNet proposed in section 3.2 can be achieved in a more simple way of reranking scores. In the code provided, it was not clear where RankNet as described in section 3.2 was implemented. The theorem 1 seems incorrect. There are some notational issues also, the W, and w symbols in the theorem don t match the preceding text. Also, the Table 4 does not seem to be of much consequence as the re ranking method can be potentially be applied to all the competing methods.<BRK>Summary: In prediction problems with millions of labels also known as Extreme Multi label Learning (XML) problems, e.g., recommender systems, the model predictions are not as good for the tail (rarer) labels. The first model is re ranking based, that is, it reranks the prediction scores of a standard XML model. Comments: The paper solves an important problem which has several industrial applications of extreme multi label learning. By CV?Since you can stack RankNet modules to make it deep, how many were used for results in Table 2? Also, it is unclear how one should split the data between head and tail labels?
Accept (Spotlight). rating score: 8. rating score: 7. rating score: 7. rating score: 6. <BRK>**Paper summary**The paper gives theoretical proof showing that the recently proposed data augmentation technique Mixup can indeed improve generalization and help in robustness. The paper also contains numerical experiments supporting some aspects of the theory. Currently, there is only a limited theoretical understanding of why Mixup works. Further, the paper proves that the mixup loss is an upper bound on the $2^{nd}$ order Taylor approximation of the adversarial loss, and hence reducing mixup loss reduces adversarial loss. 3.The paper supports its approximations and claims by numerical experiments. **Comments**Although the paper seems well written, I have a few suggestions:1. This paper provides theoretical guarantees for Mixup on two fronts   robustness and generalization; for both GLMs and ReLUs.<BRK>Based on this, it provides some theoretical analysis on the advantages of Mixup training for the generalization and adversarial robustness over one step attacks. This paper provides many insights on why Mixup works: e.g.connecting its 2nd order approximation with l2 adversarial loss; and shows that the Radmacher complexity of mixup adaptively characterize the intrinsic dimension of empirical data distribution. Though the techniques used in the paper were already developed by other works, the new results and insights on mixup in this work are worthy of being known by the community, particularly for that Mixup is such a popular data augmentation trick in deep learning.<BRK>The paper theoretically studies the beneficial effect of mixup on robustness and generalization of machine models. The mixup loss is  rewritten to be the sum of the original empirical loss and a regularization term (plus a high order term). For robustness, the regularization term is proven to be upper bound of first and second order terms of the adversarial loss s Taylor expansion. Hence, the mixup loss can upper bound the approximate adversarial loss. For generalization, the regularization term is used to control the hypothesis to have small Rademacher complexity. 2.The theoretical results are clean and insightful. The quality of the approximation is not explored in the paper.<BRK>SummaryThis paper has extensive analysis on mixup augmentation, which focus on the effect of adversarial robustness and generalization. In adversarial robustness, They try to make a connection between mixup loss and adversarial loss, On the other hand of generalization,  they argue that mixup is a kind of data apdaptive regularization. Good contribution about author s careful analysis on connecting between mixup loss and adversarial loss. 2.Good to community about having a connection between Mixup and Rademacher complexity, I think it can make some impact to discuss the high level connection between data augmentation and model generalization.
Reject. rating score: 3. rating score: 5. rating score: 5. rating score: 5. <BRK>Strengths   The paper demonstrates that on the synthetic dataset the proposed approach can infer many invariants. Weaknesses   The evaluation with a synthetic dataset seems very weak. Without a more direct evaluation, it is very hard to tell how useful the learned invariants actually are for these tasks. Thus the contribution in terms of model design is limited.<BRK>### Summary ###The paper presents a technique for inference of certain kinds of program invariants directly from the program’s source code. The basic idea is to treat conditional statements as hints for facts about the program state that should hold at a given program point. ### Strengths ###* This is a challenging problem and the paper shows some successful examples. * As a concrete example, take your own Figure 1.<BRK>Summary: This paper proposes a novel approach for training a Transformer model to predict program invariant. The evaluation with real world bugs focuses on “missing if guard” bugs. However, the effectiveness of the trained model to infer program invariants in a general way is not clear from the experimental results.<BRK>In terms of execution, the paper is well written and the techniques look state of the art from a machine learning perspective (although there are no baselines given). This is a novel idea that uses code with correct if conditions to guess the invariants for code that has the conditions missing.
Reject. rating score: 4. rating score: 5. rating score: 5. rating score: 5. <BRK>This paper proposes a training strategy for simultaneous translation to choose appropriate amount of look ahead information for each decoding. Based on the observation that the wait k method can be improved by training with longer information, the method introduces a function to determine its length given the current example (source x, target y, and the translation model f). The method is interesting and has promising improvements compared with bare wait k methods according to the experiments, but the paper seems to have some major questions which should affect the conclusion. I recommend to revise the paper appropriately, especially to resolve following concerns:  The proposed method is intuitively strange because of mismatching between training/inference strategies. Algorithm 1 involves a suspicious use of the development set: it is used directly to optimize a parameter. Minor comments:  The title sounds misleading: the proposed method still does not learn how to use future information because it uses only k look ahead information at inference (same as usual wait k methods), i.e., there is nothing special to represent "future".<BRK>The authors observed that some lookahead information during training time is helpful to improve the translation accuracy for simultaneous translation. Base on this observation, this paper proposes to use RL based methods to learn a certain number of lookahead words during the training of the wait k based simultaneous translation model. This paper proposes a new approach for improving the translation quality and the results indeed show some improvements over the baseline methods. However, I still have the following concerns:1) I think the proposed RL based methods are very completed (in terms of hyperparameter searching, extra training time compared with other baselines) but the improvements are quite marginal compared with wait k* and random. 2) For the experiments, the authors did not compare with other agent based or adaptive methods. In this way, I believe we could use regular wait k training, and use a controller to decide a smaller k during inference. I suggest the authors include this method s experiments as well.<BRK>This paper improves the wait k based simultaneous NMT by training on an adaptive wait m policy. The proposed method and experiments are clearly described. Given that the WMT dataset is much larger than IWSLT datasets, does this suggest that the proposed method may not work well on larger dataset? I’m also curious about the reason of using adaptive wait m only during training since it is a more straightforward idea to apply adaptive policy during both training and inference.<BRK>This paper proposes a new training method for wait k simultaneous translation. Rather than training on prefix pairs where the target prefix lags the source by k tokens, it uses an RL controller to determine an optimal lag for each sentence pair. The controller uses a small set of features intended to capture training progress, and is trained with REINFORCE to minimize wait k loss on a validation set, in alternation with main training steps. This method shows consistent gains over various wait k training heuristics, and some gains over other approaches that adapt the lag at inference time. The comparisons to adaptive baselines are less convincing. 2.For Mk, since this runs only over the validation set and you are using RL, why not use the actual wait k BLEU score?
Accept (Poster). rating score: 6. rating score: 6. rating score: 5. rating score: 10. <BRK>This work also shows an improvement in the transferability of adversarial perturbations by incorporating the interactive loss with the classification loss.<BRK>SummaryThe authors analyze the transferability of adversarial examples from the perspective of interactions based on game theory. This discovery leads to an explanation of the adversarial transferability by the interaction inside adversarial perturbations. 3 Results are very nicely presented, and writing is clear throughout the paper.<BRK>The paper mainly deals with the negative correlation between adversarial transferability and the interaction inside adversarial perturbations. I guess the authors  intention was to prove their claims empirically, and the paper provides some good results. However, I cast doubt on the theoretical significance of the central hypothesis. 4.Is high $\lambda$ always effective?<BRK>This work proposes that the transferability of adversarial attacks has a negative correlation with the interaction within an input perturbation. Overall, I think this work provides a new perspective of understanding transferability and presents solid analysis/experiments to verify the hypothesis.
Reject. rating score: 3. rating score: 5. rating score: 5. rating score: 5. <BRK>It is also completely obvious that forgetting older data, especially under the assumption of a distribution shift, makes sense and is the correct thing to do. This needs to be fixed. How are the graph snapshots created? Furthermore, the results only make sense for the specific time step chosen for each graph. For instance, it is mentioned that “GNNs achieve 95% accuracy with a small window size of 3 or 4 time steps”. And so all the findings in this paper and the discussion depend precisely on the data and the authors choice of how to create the time steps, and what granularity to use, which isn t discussed.<BRK>Temporal graphs can naturally model many real world networks, and many graph neural network (GNN) based methods have been proposed recently. This paper precisely considers this problem, and1) compiles three vertex classification datasets for future research,2) proposes an experimental procedure for evaluating performance under this setting,3) explores 5 existing GNNs, and concludes that incremental training for limited periods is as good as that over full timelines. ## Cons1) (Soundness) Tables 2, 3, and 4 compare accuracies of different static GNNs with varying window sizes (proposed idea) and with full graph (existing idea) which is informative. However, to increase the impact of the paper, the proposed idea (with static GNNs) should also be compared against state of the art temporal GNNs on full graphs (in all these tables). As also acknowledged by the authors, the paper explores well known existing static GNNs for temporal graphs. From this point of view, the paper is of limited originality since it explores well known algorithms in an unexplored setting.<BRK>This work empirically evaluates the sliding window strategy for training GNNs with temporal graphs. The findings are: (1) incremental training is necessary to account for distribution shift, compared to a once trained, static model; (2) incremental training with warm start does not always yield better performance than cold start; (3) the window size needs be large enough for incremental training to catch up with the performance of full data training (e.g., covering at least 50% receptive field); and (4) these findings extend to several GNN models. + The authors compile three temporal graphs, which enrich the availability of benchmark datasets. Minuses:  The empirical findings are very much expected, which means that they are not exciting. Since most of the presented results are naturally expected and there lacks theory/method contribution, the reader is unsure about the value of the paper. The challenge in this case is less about distribution shift, but more about how to handle edges and what are the consequences.<BRK>### SummaryThis paper proposes a paradigm which speeds up the training time of GNNs while not compromising too much performance. The decoupling training approach is useful if the memory can hold multiple feed forward layers’ outputs, which is not the case with previous methods [1]. 1.The actual memory used in LU DGL GCN lazy update. 2.The balance between stableness of LU DGL GCN and the large value of $T_{lazy}$ is hard to find. It would be meaningful if the authors could discuss how to set $T_{lazy}$ for the balance of stableness and time cost which is an important point the paper has argued for the approach. 3.Exact training time comparison should be stated. With this stated, it is also strange to find that the accuracy of LGCN is higher than LU DGL GCN in Table 2 while it is not the case in Figure 3 where Sequential_test is larger than any lazy update.
Reject. rating score: 3. rating score: 3. rating score: 4. rating score: 5. <BRK>I would not like to encourage such work to be published as a research paper. The main framework of this submission is very similar to the existing work [Scalable Transfer Learning with Expert Models] which has not been officially published but only on arXiv.<BRK>Recommendation: Based on my understanding of the paper I recommend a clear rejection. Did you find any pattern in the hyperparameters with the best model? 2) Experimental setup is clear and the motivation is valid.<BRK>[Weaknesses] The technical novelty of the paper is very limited. These paper should be clearly discussed with proper comparison in the experiments. Given the lack of novelty and convincing experiments, I vote for rejecting the paper.<BRK>Update after author response: I appreciate the authors  efforts to address my concerns and to raise some interesting points I missed. I still find the paper s insights are lacking some novelty to be published, but I think that this line of research is worth it!
Reject. rating score: 4. rating score: 6. rating score: 6. <BRK>In the work, the authors focus on tackling the problem of source free domain adaptation. The proposed method mainly has two parts, in which the second is nearly the same as the SHOT IM as in Liang et al., 2020 [1], while the first part aims at coping with this problem from a new perspective to align the distribution of target features extracted by the fine tuned encoder to that of source features extracted by the pre trained encoder. To achieve this, they utilize batch normalization statistics stored in the pre trained model to approximate the distribution of unobserved source data. They also provide a roughly promising theoretical analysis. 2.This paper is well written and crystal clear, making it enjoyable to read. Neither state of the art results nor comprehensive ablation studies are seen. 3.Further, since the proposed method consists of two parts, some basic ablation studies should be conducted to verify the effectiveness of both parts. source hypothesis transfer for unsupervised domain adaptation.<BRK>### SummaryThis paper proposes a domain adaptation technique when source data is not available. The exponentially weighted average of BN statistics from source training along with the trained model is utilized to align source and target distributions. Source model is divided into feature encoder and classifier components based on the presence of the last BN layer. Experiments on several benchmark datasets showed competitive performance with state of the art domain adaptation methods. Sound modelling with thorough experiments on benchmark datasets along with small scale datasets 2. Application of joint optimization ensures discrimitiveness between classes at the same time ensuring domain matching.<BRK>The first part seems interesting and novel, and to get strong results they need to combine it with an existing technique, which seems typical. I agree with the authors and disagree with R3: that their "method outperforms it (SHOT) in 7 out of 9 scenarios." This is too strong, domain adversarial training is only inspired by theory and not validated by it. However, I agree with R3 that there should be more extensive ablations to understand the effect of each part, and the sensitivity analysis of lambda should be done on more datasets. Additionally, I d like to see more detailed comparisons to related work like https://arxiv.org/abs/2006.10963 that uses batchnorm for domain adaptation. Page 2 says domain adversarial training “is also validated in theory”. #########################################################################Summary:This paper tackles the problem of unsupervised domain adaptation, where there may be privacy constraints on the source, so we have access to a source model but not the source data. This paper proposes in addition aligning the batchnorm statistics between the source and target data (inspired by domain adversarial training). The method even performs comparably to regular unsupervised domain adaptation methods (no privacy), that use domain adversarial training and are notoriously difficult to tune so this approach could simplify tuning in addition to preserving privacy. #########################################################################Pros:  I like the idea of aligning batch norm statistics when we do not have access to the source data and cannot do domain adversarial training. Are the regularization, augmentation, number of epochs, etc trained the same?
Reject. rating score: 4. rating score: 5. rating score: 7. <BRK>Given a latent vector sampled for one shape, they pass it to a decoder that predicts the SDF value at any point in R^3 for that shape. (This feature, which is clearly stated in the IGR paper, is quite obfuscated in this paper and should be stated much more clearly, though it is not a contribution of the current work per se.) Further, given the "each shape is a domain" interpretation, the final network architecture ends up looking suspiciously like a VAE, where the encoder (say with a PointNet or similar architecture) takes a point cloud of arbitrary size as input and outputs a local gaussian distribution over latent vectors, and an implicit field decoder that acts exactly as in this paper, taking as input a latent vector sampled from the encoder output. Any difference from a standard issue VAE, then, boils down to the exact way the encoder s gaussian is computed (and possibly also the training loss, but here the usual ELBO loss appears to be used). However, the similarities do seem to warrant an explicit comparison, where the overall architecture is as similar as possible (the current encoder with parallel MLPs with shared weights followed by an aggregator appears very similar    to PointNet, so something on those lines could be used), but the encoder explicitly outputs the \mu and \sigma of a gaussian. I suggest posing this as an ablation study of the proposed method, though, to eliminate any bias due to other architectural differences. But such sampling does not seem to be demonstrated anywhere in the paper. Some of these are novel though somewhat obfuscated, e.g.the "meta learning" solution; some have been used elsewhere, e.g.learning from point clouds only; and some are not relevant to the stated contributions, e.g.architectural differences other than the bare minimum to get the meta learning formulation to work. However, I still cannot recommend acceptance, because of the issue both R3 and I mentioned: the comparison to a more traditional VAE style encoder, which replaces the "aggregator" of this paper with max pooling plus some number of FC layers to output a mean and variance (as I mentioned earlier, this is almost exactly PointNet with the last layer outputting mean+variance instead of a single feature vector). I definitely do not buy the assertion that VAEs cannot support variable sized point contexts. Just about any shape encoder can be converted to a VAE encoder by modifying the last layer (and training with an appropriate decoder and loss)   this includes encoders that handle point clouds of varying size like PointNet. In the exchange with the authors, it was not clear that this point was fully appreciated. Hence, while I appreciate the many interesting aspects of this paper, and the additional results provided by the authors during the discussion session, I remain negative and am slightly lowering my rating to make this clear (if this was a journal submission I would mark it as "major revisions" and ask for additional experiments vs VAE baselines which share the bulk of the proposed architecture other than the different aggregator).<BRK>This paper tackles the task of point cloud completion, aiming to infer an implicit occupancy representation given a sparse input point cloud. The central task addressed here is to be able to infer a posterior distribution over the latent variable given some observed points $D$ i.e.$p_{\theta}(h|D)$. Following Maeda etal., this paper uses a specific form of this distribution and trains $(\phi,\theta)$ by optimizing the loss on held out surface points and using additional regularization. **Concerns/Comments**  While this approach uses a specific factored form of $p_{\theta}(h|D)$, the more  typical  way is learn a non factored encoder (e.g.a PointNet) that directly predicts a variational distribution $q_{\theta}(h|D)$. Two of the baselines reported (PCN, OccNet) do use this approach of using an encoder, but I am concerned the comparison is unfair. This is not as much of a concern for the proposed method because of the factored and per datapoint prediction of $h$. I would therefore really like to see results with the encoders of these baselines trained using a variable number of input points, similar to what would be used at testing. In the current form, the results merely show that encoders trained using many points work poorly when using fewer points for inference. I would assume this is a limiting choice (e.g.given a single point, say on a chair back, the possible shape space is arguably multi modal). I would appreciate some discussion on why the method still works despite this choice. Despite being novel in context of shape completion, I am slightly concerned regarding the technical novelty as the overall approach is a direct application of the model proposed in Maeda et. al.That said, the insight that implicit occupancy prediction from sparse point clouds can be viewed as a meta learning task is still interesting and I might still be in favor of acceptance if all the empirical concerns were addressed. I would instead encourage the authors to report alternate metrics such as F score. Overall, while I think the approach is interesting and novel in context of shape completion, I cannot recommend acceptance until the concerns regarding the fair comparisons to baseline methods are addressed.<BRK>This paper introduces a meta learning approach for the neural implicit representation of 3D shapes. The experiments show that the network can reconstruct the entire shape well even with a very small number of the input points, such as 50 and 100. For better reconstruction, the authors also proposed to use some implicit function regularizations, which are introduced in a previous work (Gropp et al., 2020). *** Strengths ***To my knowledge, this idea of using the meta learning technique in the neural implicit representation is novel, except for the following concurrent work (I hope the authors discuss this work in the revision):Sitzmann et al., MetaSDF: Meta learning Signed Distance Functions, arXiv:2006.09662. It’s interesting to see that the proposed method generates a plausible output even with 50 input points. There are many recent works about 3D shape completion and point cloud upsampling, but the authors only referred to and compared with PCN (Yuan et al., 2018): To name a few:[Completion]Huang et al., PF Net: Point Fractal Network for 3D Point Cloud Completion, CVPR 2020. I hope the authors address this concern in the revision. For instance, one can consider a multi view or sketch based 3D reconstruction network which, however, can still reconstruct the shape well even with a single image or a few strokes. In Figure 1, ShapeNet40  > ShapeNet? It would be clearer to show one encoder with variable ‘m’ for the input dimension of the last MLP layer and mention in the caption that the ‘m’ is 512 for ShapeNet and 256 for ICL NUIM.
Reject. rating score: 5. rating score: 6. rating score: 6. <BRK>This paper proposes a domain discrepancy measure and an algorithm for continuous transfer learning. The results seem to be interesting and the problem this paper studies is important. Domain decay rate: There is a domain decay rate in all the main results and the algorithm. This algorithm seems to implicitly assume the target domain at time 1 is more similar to the source domain than the target domain at time $t$, which seems unnatural and requires further justification. However, using  $\omega_j$ does not have this issue. Second, it is possible that the time evolving domain has a cyclic or seasonality pattern.<BRK>The paper proposed a transfer learning setting where the target domain varies/evolves over time and the source domain is considered static. The paper uses C divergence to measure label dependent domain discrepancy between source/previous target domain and the current target domain and provided a theoretical bound. ### Major Questions:* The reason for the label dependency is motivated as explained but I believe that the usage of adversarial semi supervised VAE was not properly motivated and related it the C divergence. * I believe the problem setting is very closely related to the Continual learning setting,  addition. To the Catastrophic forgetting, most recent work in Continual learning focuses on Negative transfer (such as Gradient Episodic Memory). * In many practical applications, for instance product reviews, if a new version of the product released periodically with new sets of features. Even though, the review data is  not only from the same target distribution but sample the different regions of this same target distribution. Can the current distance measure and  the CONTE algorithm be adapted to this problem setting. i have read the author(s)  comments and I have updated the ratings based on their replies. Thanks for very extensive clarification. Adding these comments in the final revision would significantly improve the quality of the paper.<BRK>(1) This article is more detailed in its theoretical analysis, and the C divergence has advantages over other divergence from the theoretical analysis. (2) This article proposes a new continuous transfer learning setting. It theoretically analyzes the upper bound of the target domain error, and utilizes the C divergence to measure the joint distribution discrepancy in the two domains. (3) Some parts of the presentation needs to be improved, such as the presentation of negative transfer in section 4.3. How to use other methods such as DAN to this new setting? (4) Although the results outperforms some other methods, the experiments are not sufficient. In experiments, the authors only do experiments in some easy task in DA, and no experiments are shown for the effect of the hyper parameter mu.
Reject. rating score: 3. rating score: 3. rating score: 4. rating score: 7. <BRK>I understand the process of counting the number of images. From that perspective, it is not clear what the new attack proposed offers.<BRK>While this claim might be true, the significance of this insight is unclear to me.<BRK>I also still do not feel that counting images in pixel space is a meaningful metric. I was, in general, confused by many of the choices in the paper.<BRK>The clarity of the paper is good.
Reject. rating score: 4. rating score: 5. rating score: 6. rating score: 6. rating score: 6. <BRK>The paper proposes a modification of the gradients of objective function wrt model parameters by changing the underling metric on the manifold of model parameters. The biggest criticism I have is that the baseline systems presented in the paper appear very far behind the best known results. This is a very large gap. Another weak aspect of the paper is the choice of output channel dimension for smoothing. It is empirically motivated based on the correlations in parameters along output dimension. However, since the method allows for selecting any subset of variables to smooth over, could there be more principled approaches for identifying optimal subset?<BRK>The paper proposes a method to compute the gradient of the loss function with respect to output channel directed re weighted H0 or Sobolev metrics, which has the effect of smoothing components of the gradient across a certain direction of the parameter tensor. However, there are some major issues with the proposed method and analyses:  In the proposed method, Sobolev gradients are formulated by considering the space of parameter tensors as a Riemannian manifold, and choosing the Sobolev metric on the tangent space. That is, the method is proposed as a new optimisation method on Riemannian manifolds of parameters. Parameter correlations for CNNs trained on Imagenet were analysed in Figure 1. Please apply the proposed method on larger benchmark datasets such as Imagenet, and provide these results as well. A reason of this observation may be difference in implementation of other parts. After the rebuttal:I checked comments of other reviewers and response of authors. This statement claims that the "structure" of the manifold of tensors changes by changing the metric on tangent space by changing geodesic or paths on manifolds.<BRK>This paper proposes an optimization method for convolutional neural networks that can be used to improve existing gradient based optimization in terms of generalization error. The method computes the gradient of the loss function with respect to output channel directed re weighted $H^0$ or Sobolev metrics. The paper has the following merits:+ The paper shows that defining the gradients along the output channel direction leads to a performance boost, while other directions can be detrimental. I wonder whether the method can be used for other networks beyond CNN? + The continuum theory of gradients, its discretization, and application to deep networks are provided. + The effectiveness of the proposed method is demonstrated by experiments on benchmark datasets, several networks, and baseline optimizers.<BRK>In this paper propose a stochastic optimization method for CNNs, which can obtain an improvement in terms of generation error. The proposed method is used to compute the gradient with respect to output channel directed re weighted matrices or Sobolev metrics, and some anabasis is also provided. 1.It is not clear that what s the relationship between input channel, output channel correlations and Sobolev gradients. The authors should compare the proposed algorithm with sophisticated methods such as manifold optimization algorithms.<BRK>This paper follows a very active line of research on gradient based optimization methods for convolutional neural networks. In particular, this work proposes a method that can be applied as an extension to popular optimization methods such as SGD and Adam. The main idea is to introduce a modification of the tensor space in order to encourage correlations in the output channel dimension. The theoretical foundation behind the paper seems to be sound and the overall writing is clear. This is shown using several datasets. Furthermore, the proposed method does not produce a significant overhead with respect to the regular operation of the baselines.
Reject. rating score: 6. rating score: 6. rating score: 6. rating score: 7. <BRK>This paper demonstrates that for a particular model SGD with label noise and proper learning rate schedule recovers the (sparse) data generating model while GD with or without Gaussian noise does not. (Of course, not every paper needs to be about neural networks, but that s certainly the motivation of the paper.)<BRK>### EvaluationI think this is a solid theoretical contribution on an important problem, and the paper should be accepted. The main result is that stochastic gradient descent with label noise, and without any explicit regularization will recover the ground truth. It shows a clean separation between the power of label noise and that of random Gaussian noise.<BRK>The authors study the problem quadratically parameterized (linear) regression and study the behavior of SGD to solve it when the stochasticity added is in terms of label noise. They show that SGD (with this kind of noise) with arbitrarily large initialization converges to the ground truth solution, whereas there exist settings where Langevin dynamics or gradient descent would fail to converge to this solution. The proofs are carried out carefully and are correct as best as I could verify. 2.The generative model assumes that the label y has no added noise. It would be insightful if the authors could add a comment about why this is the case, and if this was experimentally observed as well.<BRK>This paper considers the implicit regularization of stochastic gradient decent (SGD). The authors also prove that SGD with Gaussian noise (Langevin dynamics) does not converge to the ground truth at zero under the overparameterized regime. This is a solid contribution with theoretical insights on SGD with label noise. While the theoretical results are deep with long proofs, their outlines and meanings are well explained. The difference (affinity) between the deep neural network and the quadratically parameterized model is mainly discussed numerically (Figure 1).
Reject. rating score: 4. rating score: 5. rating score: 5. rating score: 6. <BRK>[Pros] The idea of converting nodes of a graph into feature graphs is novel and interesting (as far as I know). (3) Traditional methods also exploit the interactions between features in a neighborhood, by concatenating feature vector of the target node and the aggregated neighborhood feature vector and do affine transformation. This breaks down the original graph into individual nodes and their contexts which are then fed to the model to generate results. [Cons] However, some major issues still hinder the paper from publication: (1) There is a lack of necessary details. Moreover, at first the problem formulation says each edge is associated with a weight vector, but it seems finally it is just a scalar. This is a little confusing. The above issues affect the reproducibility of this work. For example, in the definition of the graph lifelong learning, the weight vector set $\mathcal{W}$ is said to contain weight vectors associated with the $K$ hop (Strictly speaking, I think here it should be $K$ rather than $k$.More about this issue below) neighbors. Does this mean these weights are associated with nodes rather than edges? I am not sure whether this is how $K$ is set. Is it possible that $K 0$? I cannot find the related content. Why do we need to change the number of feature nodes? The motivation is not clear. Furthermore, there is no comparison to using broadcast layers in experiments.<BRK>I am happy to improve my scores if the authors can further justify their proposed method. Specifically, the major contribution seems to be transforming the original graph into a feature graph so that the node classification problem is transferred into a graph classification problem with isolated samples. Meanwhile, feature interactions are modeled in constructing edges of the feature graph. Thus, more experiments on larger datasets may be needed. (The experimental results on the Flickr dataset in the Appendix are puzzling since the results show that not using memory outperforms using memory, indicating that graph lifelong learning may even not be a proper setting.) (4) There are also a few missing related works [6 7]. But since they are informal publications or very recent w.r.t.the submission, I only suggest the authors compare them in an updated version and do not consider this as a negative point. Based on the above comments, I am currently leaning towards rejection.<BRK>This paper aims to solve the problem of lifelong graph learning. The authors conduct experiments on three popular citation graph datasets including Cora, Citeseer, and Pubmed. It makes the graph learning applicable to the continual learning. This converts the problem of node classification to graph classification. In this way, the increasing nodes become training samples”. In the lifelong learning, this strategy will not only increase the node samples but also the edges between new and old nodes. Although the feature graph continuum and random sample rehearsal strategy are proposed, scalability might still be a concern. More justifications and theoretical analysis shall be provided. In addition, evaluating the performance of feature graph in some traditional graph learning tasks would be helpful. 3.In the experiments, the authors only compare their method with the modified GraphSAGE method. Although this topic about the combination of graph learning and continual learning is relatively new, there exists several relevant papers such as Continual Graph Learning (March, 2020).<BRK>In order to enable the continuous learning approaches to be directly applied to GNN, the authors propose Feature Graph, which converts the node classification problem into graph classification by converting nodes into graphs. Therefore, we can apply current lifelong learning techniques to GNNs. Experimental results show that the proposed method overperforms the baselines. The main contribution of the paper is the idea of the converting technique, which makes it possible to employ existing methods to deal with the problem of continuous learning on the graph neural network. Using the feature s cross correlation matrix as the adjacency matrix of the neural network on the node sounds reasonable, but the validity of this idea lacks experimental explanation. It is not discussed in this paper. This problem still exists in Feature Graph. The author should discuss it. 3.The usage of task descriptors should be clarified, since their different usages may have a great impact on difficulty of problem4. In addition, Cora, Citeseer and Pubmed are small graph datasets.
Reject. rating score: 3. rating score: 5. rating score: 5. rating score: 5. <BRK>This paper proposes effective gradient flow (EGF), which is a layer wise normalized gradient flow. The key idea is to compare  sparse and dense networks that have the same number of (non zero) weights.<BRK>The definition and use of symbol a_s is confusing. If the rank correlation is not used here, I would suggest the authors to present results on rank correlation. Specifically, the effective gradient flow can potentially be an objective to improve for innovate methods in sparse training.<BRK>In order to do this, it proposes a simple framework to compare sparse and dense networks (SC SDC) and a simple common sense metric to measure the gradient flow in sparse neural networks (Effective Gradient Flow). W3) The general appearance of the paper suffers from, probably, the last minute submission.<BRK>The authors come up with a new measure of effective gradient flow, which is important for good performance. There are previous studies of the effect of overparameterization, but much less about sparse networks vs. dense networks at the same size.
Reject. rating score: 6. rating score: 7. rating score: 7. <BRK>Main idea: The paper developed a practical second order preconditioned method, which is based on the Shampoo algorithm, to improve the wall clock time compared with state of the art first order methods for training deep networks. It appears that the iterative preconditioning method is only incremental improvement over the Shampoo algorithm. However, the iterative method is widely used in the similar spirit of the limited memory of BFGS or LU decomposition. The authors should directly show wall clock time. 4.Some minor comments: fonts in Figure 1 are too small; in other figures, the fonts in x axis and y axis are also very small; in figure 2 (b), the author should explain more about how to break down the latency of a single step into the parts mentioned in the figure. ##################################Updates: I have changed my score after reading the author s responses.<BRK>I am also unsure as to how software/hardware benefits could boost the performance of the current method without significantly boosting the performance of first order methods as well. Some of the modifications seem to be heuristic and I found it unclear why these should work well in practice. Thank you for running the experiments on small datasets and for clarifying my additional concerns. It would be helpful if the authors could comment on this. I felt similarly about the delayed preconditioner computation: The fact that computing the preconditioner only once every several epochs is interesting, but I didn t quite follow why this should have worked in the first place. For example, while I understand that there are several works using learning rate scheduling to achieve state of the art results, in most experiments, Adam can be used with a learning rate of 1e 4. For example, is the main suggested improvement to provide better support for double precision computations?<BRK>The paper tackles the important problem of making second order methods practical for stochastic optimization. Overall, the paper tackles the important problem of second order optimization in a more practical way. On the algorithmic level, computing the preconditioners is expensive both in terms of memory and compute and the authors suggest improvements theoretically motivated techniques to make this faster. Experiments are run on a varied set of tasks (translation, CTR, NLP, vision) and they show fairly significant reduction in the number of optimization steps.
Reject. rating score: 3. rating score: 4. rating score: 4. <BRK>Summary:In this paper, the authors design a method to evaluate the gender bias for natural language inference tasks. The experimental results show that those models indeed have a gender bias. Gender bias is an interesting and important topic in the NLP domain. The constructed dataset contains only entailment pairs. This point is my primary concern. In the constructed dataset, the hypothesis is generated by templates and looks like the context is not very related to the premise. However, in most of NLI datasets, the premise and hypothesis are usually related. It can be reasonable for models to perform not well and have the bias on the evaluation set, since the domain changes a lot. What is the definition of "bias" in Figure 1?<BRK>The paper s main contribution is the construction of an NLI style dataset for evaluating whether systems training on MNLI/SNLI are gender biased with respect to occupations. 3 systems are evaluated, all are found biased, and then a data augmentation approach from previous work (gender swapping from Zhao s coref bias paper) is used for mitigation with mixed results. For all we know, the models are correctly predicting that with high probability, but the measurement is forcing a renormalization between entailment and contradiction examples (Section 3.2). This seems like it would be a problem for the "delta P" and "B" measurement. How was the list of swaps constructed in this case?<BRK>The method involves setting up a NLI pair where the premise is a gender neutral statement about an occupation, and the hypothesis is explicitly gender specific. The paper also investigates how to reduce this bias by data augmentation. The hypothesis templates are interesting, but present a bit of a technical question. In other words, it talks about the form of the premise, rather than its meaning. In contrast, the word "text" in the hypotheses constructed in this work refers to the entire text of the premise, and not its entities and events. One way to fix the issue is to change the hypothesis templates to use the same (or similar) words as the premises, and replace the occupation word with a gendered word. The experiments suggest that the problem is perhaps in both. This paper seems to argue that the provenance of the stereotypes is the training data for the task. However, the final results suggest that this is not entirely the case, and the paper does say so in the section on debiasing.
Reject. rating score: 5. rating score: 5. rating score: 7. <BRK>This paper presents a new architecture for multi task learning that uses a top down control network to modulate the activations of the main (bottom up) recognition network. The datasets/tasks used herein are homogenous and therefore straightforward for multi task learning. How does the proposed architecture fair in a more challenging setting involving heterogeneous tasks, e.g., Misra et al., 2016? The target in this case is a 224x224 mask, where a single pixel, blurred by a Gaussian kernel (s.d.3 pixels) was labeled as the target location. To conclude, the proposed architecture is novel, the paper is clear, but the experimental work leaves some questions unanswered.<BRK>The paper propose a way to combine image information and task information as controllers for multi task learning. In this way, the authors expect to extract more task/image specific features in a shared backbone. The proposed method seems novel and intuitive. Though there are some typos and grammar mistakes, overall the paper is easy to follow. 2.While there is improvement in performance, but it is not clear what factors causes the improvement. 3.All the tasks are classification tasks /discriminative models. It is not demonstrated if this would be working with a mixture of generative/discriminative tasks. If possible, as the TD and BU s are sharing the same structures, it would be interesting to explore what are learned by visualizing the weights in each channels and layers.<BRK>In this paper a novel top down control network is introduced for multi task learning. After that, the BU2 is updated with the top down parameters. Weakness: Although the experimental results show the better performance on the image classification, there are exist several unclear parts:1. The definition of multi task in this paper refers to the different dataset’s classification? Or referring to the different tasks, e.g.localization, classification, and attributes predication. In my opinion, the authors should provide more details on designing the validation experiments. 3.If the top down stream would make the recognition be sensitive for the visual variations?
Reject. rating score: 5. rating score: 6. rating score: 6. <BRK>This paper considers and formulates a generalized version of the private information retrieval (PIR) problem, where a user aims to retrieve one of $M$ files from a dataset, but wants to keep the index $M$ private. Technical comments: Several statements in the technical formulation needs to be revised for correctness and readability. Post Rebuttal:I m totally fine with the topic, which is a very minor issue as mentioned earlier. My main concern was regarding the overselling of some presented results. I expected something beyond a simple application, such as determining an interesting condition where this approach can be used, but the provided statement does not seem to hold after revision. The condition actually needed by the authors is essentially random sampling does not hurt privacy, making the statement "random sampling can be applied when it can be applied", which is a bit trivial. For the experiments: I have a similar concern as mentioned by Reviewer 1, which is about the generalizability issue. Overall, there is no more direct evidence of generalizability.<BRK>Summary: This paper studies single server private information retrieval with user data distortion constraint. They also provide a information theoretic formulation of the trade off. Empirically, the authors demonstrate the effectiveness on the Gaussian dataset and MNIST dataset. Pros: 1.The trade off between distortion, privacy, the download rate is indeed an interesting problem. Cons: 1.The idea of using GAN for information retrieval is not new. Overall the paper is well written.<BRK>Summary:The paper aims at taking a new approach towards the problem of private information retrieval. The proposed method relies on the interplay of the three parameters: distortion (utility), leakage (privacy) and the download rate/cost. They try to decrease the download cost, by sacrificing some utility (lossy compression through GANs), which is an interesting and seemingly novel take on the problem. would the proposed methods also work on larger images? "I did not completely understand what the problem is. [I am not at all familiar with information retrieval and the work surrounding it, so I am not entirely confident in my review and I might update it based on the review of expert reviewers later on].
Reject. rating score: 4. rating score: 4. rating score: 5. <BRK>In this paper, the authors argue that the mini batch method and local SGD method suffers generalization performance degradation for large local mini batch size. An asynchronous method is proposed to improve the generalization performance. When the local batch size is increased from 128 to 1024, the learning rate also needs to be scaled accordingly. It is not surprising that we observe performance issue if we do not increase the learning rate. This makes the motivation of the proposed algorithm questionable. 2.It is not clear to the reviewer about the intuition of why local asynchrony can mitigate the generalization issue. These two symbols are used in many places in the paper. But there are no definitions of them. And, what is s_j^q in Algorithm 1b? It seems to the reviewer that you are switching between SGD and block coordinate descent. This can reduce computational complexity but worsen the convergence at the same time. 5.Theorem 2.1 does not show any improvement by using more workers.<BRK>Degradation in accuracy upon increasing batch sizes is a popular problem and there has been lot of important work in this work (e.g.LARS [1], LAMB [2] to mention just a few). I don t see what the advantages of the proposed method are in comparison to these approaches which approach the problem differently but end up ultimately solving the large batch accuracy issues. on whether they investigated this empirically? If so, how does proposed approach compare with other methods which aim to construct mini batches or sub sets of data on local workers more intelligently? such as [3] or [4]. I feel these related works also can extend to distributed decentralized SGD well. The scale of datasets is not large enough for distributed method. The non uniform distribution of data on various workers in my opinion can affect the optimization a lot and it would be good to see the authors comment on this. How does the accuracy look like in that case? (b) The gains are only marginal and don t seem significant. If so, what was the variance of this experiment? The paper obtains a convergence rate of 1/\sqrt{T} based on what I understand? I am curious if the authors could comment on what assumptions need to be changed or what fundamentally stops the method from achieving a linear rate? The Quality/Perf column in Table 1 is confusing   what to quantify "ok", "good", "poor"? I feel the paper does not meet the technical bar for an ICLR paper due to incomplete related work, unconvincing experiments. Proposed approaches do not seem very novel to me.<BRK>This paper  proposes a class of  asynchronous local SGD by combining the local SGD with shared memory based asynchronous gradient updates. Moreover, it proves that the proposed methods guarantee ergodic convergence for non convex objectives, and achieves the classic sub linear rate under standard assumptions. Some experimental results verify the efficiency of the proposed methods. Unfortunately, I do not find Appendix of the paper, so I can not judge the contribution of this paper. In addition, because there are too many notations in this paper, it is easy to be confused.
Accept (Poster). rating score: 7. rating score: 7. rating score: 6. rating score: 5. <BRK>Summary: This paper proposed a data augmentation method for training a classifier which is intended to have predictive parity between two identified groups. It is based on the “mixup” idea – samples from the two groups are interpolated between, and the smoothness of this path is encouraged. The authors recommend doing the interpolation in latent space. An optimal solution in a constrained setting is derived, and experiments show the empirical success of the model at this task on 3 datasets. Recommendation: I recommend acceptance of this paper. The application of mixup to this task is sensible, the paper is clearly written, and the theoretical and empirical work both seem solid. There is a literature on data augmentation methods for this task 	Explanation of the mixup method is clear and the theoretical work seems good 	The empirical results are pretty strong, beating each method at each task. Showing the tradeoff is good, this is the right type of diagram to use here 	I like that this method is non adversarial – easier to train and more reliable imoWeaknesses + Clarifications: 	The question of the latent variable model seems relevant and interesting. It seems that the mixup method is only as good as the model, and also the trained model might add its own biases to the classification task. It would be nice to see some discussion of this in the paper 	I am surprised that mixup improves precision on the adult task. It would be good to see some exploration of this 	For experiments, are all runs shown? A number of hyperparameters (e.g.regularization) are not given  	For all the latent path figures (eg Fig 3) why is the y value at x  0 always 0? Be clear in your description (or maybe I missed it) 	I would be interested in seeing some further analysis on this model, perhaps using the interpolations themselves<BRK>The idea is that we expect changes to sensitive attributes not to affect classification decisions. They introduce a number of regularization terms for both demographic parity and equal opportunity. They perform assessments on a number of datasets and modalities including Adult, CelebA, and  Jigsaw toxic comments. They find their methods improve fairness generalization compared to a few baselines, including directly regularizing the model for fairness. It could be good if Expected Output was written out a bit more explicitly or make it so that $\Delta DP$ is clearly the difference between the two points. I was somewhat confused about what was being shown at first. Section 4:  When introducing lemma 1, I d recommend stating explicitly that $T(x_0, x_1, t)$ corresponds to a function that outputs an interpolated sample between $x_0$ and $x_1$ at step $t$. What is this notation meant to say? I notice that this is equivalent to the expected value of drawing $x_0 \sim P_0$, so is this meant to indicate the likelihood of each $x_0$ as well? I don t think this assumption is poor by any means, but I think describing the desired properties of the latent space could be a nice addition. Could the authors try something like (1) decreasing the training time or size of the model or (2) just add another baseline such as a random forest or logistic regression to this graph? I think this would help us understand the effects of fair mixup a bit better. The authors compare ERM models trained at as many iterations as the fairmixup models. However, given that only one $t$ is sampled per batch, it might take quite a few iterations to get a good fairmixup model. Is this the correct intuition? Overall thoughts:I think this is a nice paper with a useful contribution to fairness generalization. If it is really the case that this method is flexible enough to improve both fairness and classification generalization, this method could be quite useful in general. I have a couple clarification questions from the body of the paper, and I d appreciate answers from the authors. Further, I d also appreciate it if the authors provided clarification to the experimental decisions I asked about above. One last point for the authors to consider is that for CelebA, the authors build classifiers to preform attractiveness classification. Though they do so from the perspective of discouraging classifiers to exhibit biases, it still could be worthwhile to include a brief discussion in the appendix on the ethical considerations of such models.<BRK>This paper provided a fair mixup strategy to improve the generalizability of fair classifiers. Extensive experimental results are presented to verify the effectiveness of proposed method. Mainly, I have two questions that need to be answered. 1) The authors claimed that "the proposed fair mixup can improve the generalization of group fairness metrics", and they "provide a theoretical analysis to deepen the understanding of the proposed method". However, it can not be seen from either proposition 1 or proposition 2 to show the improvement of generalization. Either proposition 1 or proposition 2 does not show the generalization bound. Could you please give some explanations? 2) In the theoretical analysis, the authors considered $f(x)$ as a linear function of $\Phi(x)$ where $\Phi$ is an embedding function. However, this is not true in many applications. Even for the experiments in this paper, this is not true. For example, two layer ReLu networks (this is not a linear function) is used in section 6.1. If so, the theoretical results can not fully deepen the understanding of the proposed method, which makes the results less important. On the other hand, if general non convex function $f(x)$ (for example, NN with ReLu activation function) is considered, then the optimal $v^*$ is very hard to obtain since this is a NP hard for general non convex minimization problem. Finally, I am wondering if the proposed methods can be extended to multi class classification since this paper is studying binary classification problem?<BRK>Inspired by the mixup algorithm, which was presented to improve the generalization performance in Zhang et al., 2018b, fair mixup pick two samples from two different sensitive groups. Instead of regularizing the gap (e.g., \delta DP), the authors regularized the derivative along the path between two samples. This algorithm can be applied either to the input space or to the feature space. The idea of using mixup for training fair classifiers is interesting, and the paper is well written and easy to read. However, I have a few critical concerns. Concerns:   E_{x from P0} f(x)   E_{x from P1} f(x) does not imply fairness:  My first concern is the choice of this relaxed metric, which the authors borrowed from Madras et al.(2018).Most surrogate conditions are sufficient conditions for the target fairness condition. A classifier can be completely unfair while satisfying this condition: f(x)   1 w.p. I would make sense if this metric helps impose the actual fairness conditions we care about. :  First of all, I haven t seen large gaps between the level of fairness measured in the train set and the test set. I believe that it really depends on the choice of training algorithm, and most of the methods I have seen in the literature do not exhibit huge gaps. Even when there is such a gap, one can choose the best model based on the validation performance (accuracy/fairness). The authors may want to add more evidence on the lack of fairness generalization of existing algorithms to justify the considered problem. Other baseline algorithms with early stopping:  The authors seem not to use any validation or early stoping to maximize the test performance. This is commonly done in most work in the literature, so please clarify this or do so. Most importantly, related to the above concern, GapReg s performance might be improved if used with proper validation and early stopping. Its train performance should be better than Fair mixup in every aspect. AdvDebias is not a good baseline algorithm as it s not optimized for a target task. post rebuttal:  The authors have addressed some of my concerns, but the experimental results are still missing several important baselines. Raising my score from 4 to 5.
Reject. rating score: 2. rating score: 5. rating score: 6. <BRK>I cannot recommend acceptance at this point. The main issues I see are the following:1. I do not approve of pushing them to the appendix.<BRK>The authors are suggested to give some analysis on this problem. **Post rebuttal**I appreciate that the authors answer my questions.<BRK>This paper proposes a new framework NSB GAN for high resolution natural image generation with a low computation budget. The proposed method tackles this issue in a simple but effective manner. 2) What about the inference time (or complexity) of NSB GAN compared to BigGAN? For example, the images of the cabinet (Fig3, 4, 5th row) have overly sharpened artifacts that BigGAN does not suffer. And lastly, since the proposed method is pipelining, there are some unexpected artifacts.
Reject. rating score: 6. rating score: 6. rating score: 6. <BRK>## SummaryThe authors propose a near minimax optimal linear estimator under distribution shift. They have estimators for when there is a covariate shift (i.e.the underlying data distribution) or model shift (i.e.the distribution of the label given the features of the data). ## Strengths* The authors provide bounds for both data coming from linear and non linear generative models* The authors analyze model shift in addition to covariate shift* Experiments showing the results on simulated data## Weaknesses* No experiments for the non linear data generation modelUpdate: after reading the arguments and comments from my fellow reviewers, I was convinced by their reasoning that the paper did not merit my original high score.<BRK>I raise my score to marginally above acceptance threshold, since I am still not convinced in the importance of domain adaptation for fixed design regression. This paper studies the problem of domain adaptation for linear regression problem. For this case the authors propose a linear estimator, and show that under assumptions which guarantees the reduction of the problem to Gaussian sequence, this estimator is within a constant factor from the minimax risk of non linear estimators. If not, what is the reason to introduce this assumption if the design is fixed?<BRK>### SummaryThis paper derives estimators for regression under additive Gaussian noise and distributional shift (mostly covariate and also some model shift) that enjoy minimax properties. The score is based on the fact that the paper can use some polish to improve clarity and the fact that some of the assumptions, even if traditional, are too strong (in essence, sample complexity bound will become meaningless if what is assumed known itself is too sample intensive to approximate).
Accept (Poster). rating score: 7. rating score: 7. rating score: 7. rating score: 6. <BRK>This paper proposed neural attention distillation (NAD), a defense aiming to erase trigger effects from backdoored models with limited clean data. The authors also demonstrated that NAD training is more efficient than MCR. Some discussion on the teacher student combination and the role of attention map is provided. Overall, I find the defense results descent and the performance improvement significant. The authors also have done a thorough comparison across datasets, attacks, and defenses. There are a few things I hope the authors can further clarify, and I believe the new results will add more value to this work. In the appendix the authors had some discussion on why using attention map over feature map for distillation loss.<BRK>The defense method, called neural attention distillation (NAD), first finetunes the backdoored model on a set of clean data to get a teacher model. The authors show that NAD outperforms or matches previous backdoor defenses over a range of attacks on the CIFAR 10 and GTSRB datasets. Pros:+Stronger empirical performance in defending against backdoor attacks versus previous methods. NAD’s premise assumes the presence of clean and validated data, having limitations especially for training datasets with a large size. Recommendation:NAD’s better empirical performance over prior art and the comprehensiveness of experiments in this paper would be valuable in the effort to tackle the threat of backdoor poisoning. Moreover, the proposed method did not include a component to detect the presence of a backdoor poison or consider an adaptive attack scenario.<BRK>In any case, I think the paper is a good contribution to the field and would still vote for accepting the paper. The authors show the proposed approach s effectiveness by comparing to three commonly used techniques leveraging six state of the art backdoor attacks on two datasets, namely GTSRB and CIFAR10. Introducing attention distillation as a simple yet practical approach for erasing backdoors in a poisoned neural network. Providing substantive ablation studies that further clarify the contribution of each step of the proposed approach and its variations (for instance, in the iterative NAD). ## Questions and comments for the authors1. ## Evaluation logicOverall, I have a high opinion of this paper and appreciate the work the authors have put into writing a comprehensive article.<BRK>This paper presents an empirical study on the backdoor erasing in CNN via teacher student alignment of the attention maps. S1: An unexpected way to erase backdoors without the need for additional information (provided that the experiments are done correctly). Detailed comments:The training progress and the degree of overfitting of the teacher model are important but not clarified in the paper. But I can’t understand why "4) C teacher and B student)" can work well too. Isn’t it prone to overfit? How does your approach compare with a baseline where only the teacher model is used and trained with both the data seen by the student and teacher models in your approach? What are their settings? The author is encouraged to conduct more experiments to clarify this.
Accept (Poster). rating score: 7. rating score: 7. rating score: 7. rating score: 7. <BRK>Summary: The paper calls into question the significance of previous results on Self Expressive Deep Subspace Clustering (SEDSC) models, which are touted as successful extensions of the linear subspace clustering (using the self expressive property) to non linear data structures. The authors present a set of theoretical results that indicate that the standard formulations of  SEDSC are generally ill posed. I believe this is a good paper worthy of being considered. As such, I feel the paper addresses a somewhat limited audience and the impact of the work appears somewhat limited.<BRK>The authors point out that the empirical improvements obtained by deep self expressive subspace clustering may be artifacts of post processing on the learned affinity matrix. The theoretical contributions of this paper are significant, and the critique of deep self expressive subspace clustering is timely and important.<BRK>This paper studies the flaws associated with extending subspace clustering methods to the nonlinear manifolds scenario. In particular, the authors demonstrate that the optimization problem solved due to the extension can be ill posed and thus lead to solutions which are degenerate/trivial in nature. Overall the outline of the paper is good and I found the discussed problem informative. Did the authors consider this ?<BRK>Pros:1)	Authors theoretically studied a class of self expression deep subspace clustering methods and found that the optimization problem is typically ill posed. 3)	Authors mentioned the solutions are optimal in many places of this paper. From the perspective of the nonlinear optimization problems, it is not proper to say “optimal solutions”. I change my rating after looking at authors response.
Reject. rating score: 2. rating score: 6. rating score: 6. rating score: 6. <BRK>This paper offers a new set of challenges for batch reinforcement learning coupled with a set of benchmarks, including autonomous robotics and driving domains. Furthermore, there have been many previous work which have already collected offline datasets as part of their work. I do not underestimate the importance the authors  hard work, nor the importance of the provided datasets to the RL community. While these factors may indeed be good for testing offline RL algorithms, they do not provide a complete picture of real world datasets. The non stationary behavior could be mimicked or simulated from real behavior. In a controlled setup, the datasets could be constructed so that policies are categorized (e.g., "level of non Markovianess`"). Real policies may act according to some causal structure in the background that is not necessarily known. One could collect or use large amounts of high quality datasets from the real world and add certain corruptions to the data as to lower its quality (e.g., removing certain trajectories). If the initial data is of high quality, the corrupted data could be controlled well. 5.Changing and/or very large action sets.<BRK>Several existing RL benchmarks are used, the results of several algorithms are presented. The authors claim that the benchmarks were specifically designed for the offline setting and are guided by the key properties of datasets in real world applications of offline RL. In particular it seems that so far no benchmark has been included that has the ambition to have the characteristics and complexity of a real application. On the other hand, the current status is already used by the research community, since there seems to be no test suite for offline RL apart from "RL unplugged: Benchmarks for offline reinforcement learning". I therefore recommend to accept the paper. Additional feedback with the aim to improve the paper:In Table 2 and Table 3 average results are reported over only 3 random seeds. Since no uncertainties, e.g.in the form of standard error, are given, the reliability of the results cannot be assessed. Since different algorithms require different computational efforts this approach does not seem to be in the sense of a real world application. In most cases there should be the willingness to use much more computational effort for especially good policies.<BRK>The paper proposes a standardized benchmark for offline RL research. The data collection is well motivated from real world scenarios covering many important design factors. The benchmark of existing methods is thorough and provides many useful insights. I believe this work will have a high impact on the offline RL community. I can expect this benchmark will be used by many papers in the future and will function as the starting point for many offline RL research. However, I notice that this dataset may not be accessible for underrepresented groups. As the authors note in the paper, each task consists of a dataset for training and a simulator for evaluation. In my understanding, half of the six tasks (Maze2D, AntMaze, Gym mujoco) depend heavily on the MuJoCo simulator, which is a commercial software and is not free even for academic use. I therefore view offline RL as a good opportunity for the community to get rid of commercial simulation softwares, making RL research more accessible for underrepresented groups.<BRK>D4RL: Datasets for Deep Data Driven Reinforcement Learningreview:summarization:In this paper, the authors consider the problems of offline reinforcement learning problems, and has a focus of dataset and shareable code base. While no novel algorithms are proposed in this project,a systematic evaluation of existing algorithms (offline RL algorithms) is proposed. This paper, while using simulated data, provides a general platform to benchmark these algorithms. 2.The project provides a comprehensive evaluation and discussion on existing considerations in offline RL. 3.The paper is well written. It is very clear what the purpose of the project is,and it is very clear why the authors make the dataset in the way they did. While I understand the difficulty of collection real data, it does raise some concerns that a simulated dataset can be generated by researchers themselves.
Accept (Poster). rating score: 7. rating score: 7. rating score: 6. <BRK>+ The paper is well organized and written. The MixStyle is relatively simple to implement, but effective. By mixing styles of different instances, which generates synthesized domain samples while preserving the content features, the proposed method achieves the generalizability of the trained model.<BRK>**Pros:**  Overall, this is a well written paper with a clear idea that is simple but intuitive. This became clear to me only after reading the discussion in the last paragraph of Section 3.2. The analysis where to apply MixStyle is good and makes intuitive sense.<BRK>I also found the experimental validation not fully sufficient to grant publication. The proposed approach diversifies the data implicitly and the experimental results show that the mix style can improve domain generalization. Overall the paper is well written with plenty of details. Some of my concerns are addressed in the response.
Accept (Poster). rating score: 7. rating score: 6. rating score: 6. rating score: 5. <BRK>**Summar**The paper proposes a spectral based graph convolution layer, called Simple Spectral Graph Convolution (S$^2$GC), which is based on the Markov Diffusion Kernel (MDK). The paper provides a spectral analysis on S$^2$GC and shows the connections to several previous methods, such as GDC, SGC and APPNP. The authors also show that their proposed S$^2$GC advantages from both spatial and spectral methods. Overall, the paper is well written.<BRK>In this regard, authors proposed Simple Spectral Graph Convolution with Markov diffusion kernel as a graph filter which combines strengths of both spatial and spectral methods. The paper is written well and easy to follow. Pros:Paper proposes Markov diffusion kernel as the new filter for GCN which can be seen as a variant (or combo) of Simple Graph Convolution and  APPNP.<BRK>Even better, the hyper parameters should be selected using a validation set. The proposed formulation in eq.13 is uncannily similar to the one proposed by Defferrard et al https://arxiv.org/pdf/1606.09375.pdf known as Chebynet, with a different choice of polynomials. S^2GC variance of 0.0 on MR looks strange. Pros: The proposed method is relatively simple and shows good performance In the conclusions and in the introduction, authors claim that their proposed approach avoids oversmoothing.<BRK>This paper also systematically analyses previous methods and discusses their relations with S2GC. The authors also conduct extensive experiments on node clustering, community prediction, node classification, and text classification. However, the proposed filter is not a new one. [a] also provided detailed spectral analysis as this paper does. 2.The performance of the diffusion kernel is not significantly better than other graph filters.
Reject. rating score: 3. rating score: 3. rating score: 6. rating score: 6. <BRK>However, it is unclear to me whether humans that are exposed to a generated prefix (which may already deviate somewhat from human language) would feel that the mistakes made due to exposure bias “match” the mistakes in the prefix. The authors begin by defining exposure bias as the decrease in quality and relevancy (to the conditioning text) in generations as the model conditions on its own output. A more appropriate reframing would be to talk about the kinds of effects exposure bias couldn’t possibly be having, e.g.that exposure bias may only result in strange word choice but not total degeneration. This over claim, and the lack of smaller claims to build up to it that might have been more appropriate, make it difficult for me accept the described conclusions. To calibrate our understanding of these metrics, the authors do a human study as well as comparing two GAN frameworks. In the discussion the authors discuss limitations of the work, mostly that the given metrics are not a complete notion of evaluation and connect their work to related literature. Their intuition that exposure bias should mean quality and relevancy are highly dependent on the prefix is good idea. The discussion section is rushed and does not make a strong argument that the author’s interpretation is supported. I recommend rejecting this paper, as the way exposure bias is quantified is dubious and the authors do not make a strong argument that they would have detected exposure bias if it is present. Corpus BLEU has been used in many papers, though I don’t know of any showing actual correlation with a human metric, but we can at least say that it has precedent being used cumulatively. However, using the corpus BLEU of _individual_ samples in a fraction creates the potential for completely inaccurate estimates. GANs do badly on EB C, but they are known to be significantly worse even on normal metrics, so it’s not clear what this proves. No intuition is given, simply “we didn’t find exposure bias when we looked at the data”. It especially shouldn’t be described as failing “to show the significance of exposure bias”. The authors show that their metric is not complete via a toy example, but talk about how MLE does not produce these kinds of solutions. That’s fine and I believe that part of their argument, but I still feel that there is not much evidence that the metric would actually show significant differences if the quality of the writing went down.<BRK>This problem is known to cause incremental performance degradation, and attempts to mitigate this problem have received significant attention in the community (using, e.g., RL and GANs). The paper claims that prior work has mostly focused on addressing the problem rather than measuring how severe the exposure bias problem actually is. That said, I have several concerns that make me question some of the claims of the paper:1) The human evaluation shows very little performance difference between generation with data prefix (D prefix) and model prefix (M prefix), suggesting exposure bias is not a problem. Since these generated strings are of length 30, there is plenty of room for exposure bias to crop in. As the paper’s automatic evaluation is done in a completely context agnostic way and relative to a large pool of references, it essentially only measures whether the model (with or w/o exposure bias) is able to generate plausible trigrams, but that sets the bar very low as we already have plenty of evidence showing neural language models are quite capable of generating reasonable trigrams (whether there is exposure bias or not), and in fact often much longer n grams. 3) Abstract: “Although a lot of algorithms have been proposed to avoid teacher forcing and therefore alleviate exposure bias, there is little work showing how serious the exposure bias problem actually is.” I find this claim a bit misguided, as the numerous papers addressing exposure bias are *empirical* ones. The term “corpus BLEU” or “corpus level BLEU” is generally used to contrast with various versions of “sentence level BLEU.” Now it appears that “corpus BLEU” in this submission refers to the version of BLEU used in SeqGAN (Yu et al.), which is therein not called “corpus BLEU.” That distinction should be made clearer, considering that the use of “a large number of sentences from ground truth data as references” is a significant departure from how BLEU was originally designed to work. The authors justify their use of BLEU as it is a “well established [metric] in the NLG literature,” but this is rather misleading as their specific version of BLEU is not well established and not what is commonly used in MT and NLG. Since this prompt supposed to make the two strings more comparable is absent from the actual evaluation setup, I gather from the authors’ own words that they implicitly admit that string comparisons in their evaluation setup are not so comparable.<BRK>This paper presents an empirical study of exposure bias, showing that it does not appear to be an especially significant issue. Designing metrics for exposure bias is a novel task, and this paper invented appropriate approaches. The experiments cover the two most important model classes (LSTMs and transformers) and use representative model settings and corpora. Also, they have been reported to exhibit more deviation from the corpus distribution (especially with respect to particular pathologies like repetition). Evaluating generation in these other settings would increase the impact of the paper s conclusions. The human evaluation is helpful, but needs measures of inter annotator agreement. Less significant: The qualitative experiment that starts the paper is very anecdotal, and I think the paper would be better off without it. The concrete examples in Table 1 are helpful for illustration, but I would not dedicate a section to them or call it an ‘experiment,’ as it is too limited in scope. It detracts from the stronger experiments later in the paper. Likewise, the model in Example 1 seems too inaccurate to be illustrative, and the associated discussion in Appendix D is already well known I believe (that the “Teacher forcing” MLE objective just aims to choose parameters that maximize the likelihood of the corpus, using the chain rule).<BRK>Summary:The so called "exposure bias problem" (EBP) is often cited as a serious issue when training sequential model with MLE and teacher forcing. Positive aspects of the paper:  EBP is often accepted as an obvious problem in the generation community, despite the lack of experimental evidence. Thus, a paper such as this one that tackles such evidence head on can be very useful. The paper however attempts to design (1) experiments and (2) quantitative measures that correspond with our general intuition of the EBP. According to these experiments and measures, and also to some human evaluations, the paper compares two situations: (D) the trained generator is provided with a prefix from (an unseen portion of) the dataset and asked to generate a continuation; (M) it is provided with a prefix that it generated itself and asked to generate a continuation. The conclusion from these experiments is that the continuations in the (M) case are only very slightly worse than in the (D) case. This contradicts the usual expectation about the EBP, namely that in the (M) case, the model would be "lost" in unknown territory and start to produce very poor text. The main conclusion of the authors is that: "although the mismatch between the data and model prefix distribution exists, it is still in the model’s “comfortable zone”, and is not large enough induce drastic performance loss during generation". Second, you do not describe exactly how the generation is done. The corpus BLEU measure that you use is most of your experiments was designed for Machine Translation and appears to me to be a very weak measure of quality for open ended generation. It would actually be informative to compute a baseline for that measure in terms of continuations not from the model, but from the training data itself, giving a measure of the average "quality" of a gold standard continuation relative to all the other gold standard continutations from the training data. While they are used in the human evaluations, it was not clear to me whether they were used in the EB M experiments, for example. The paper https://arxiv.org/abs/1906.05664 "Calibration, Entropy Rates, and Memory in Language Models" might be relevant as related work: in that paper the authors argue that neural language models tend to suffer from "entropy drift", namely the tendency to entropy of the next token prediction to be higher when conditioned on an (M) type prefix than on a (D) prefix. While I still believe that the *questions* that the paper raise are very worthwhile to the community, I agree with several reviewers that the *answers* provided in the paper are insufficiently supported by a convincing formalization and by experiments.
Reject. rating score: 4. rating score: 4. rating score: 5. rating score: 7. <BRK>From my perspective, the main focus of the paper feels too limited to have much impact. It seems to me there are a number of related issues that are just as pernicious which are not addressed at all in this work. So this paper focuses on what I believe to be an important set of issues. This is an equally obvious concern and similar theory to that presented in this work could be used to demonstrate this. Beyond this, I am left wondering why the authors restricted themselves to studying GANs. Beyond that, I do not see much value in the way these results are presented.<BRK>However the main arguments are actually fairly obvious and well known to all theorists that have worked on these problems. In fact I know multiple people that have discussed these points years ago, not as a paper contribution in themselves but as a jumping off point. The writing is clear and the problem is important. But the authors need to go deeper, clarify what precisely is the learnability problem, why their method is justified, and/or to contribute far more by way of experimental support for the proposed ensembling techniques.<BRK>(2020).Learning disconnected manifolds: a no GANs land. While the problem setting is an important one I fail to see the main novelty in this paper. The fact that continuous functions keep the connectedness of the input space in tact is not a novel finding and explains why a single GAN with an input vector sampled from a Gaussian (or similar) cannot model a disconnected data distribution.<BRK>**Summary of contributions:**This paper proposes to study how to model distributions with disconnected support. Finally they show experimentally that ensemble GANs outperforms other GANs formulation on CIFAR. I believe this work is significant and could lead to breakthrough in generative modeling or other settings where we might need to model distributions with disconnected support. **Cons:**  The theory seems to suggest that we only need to have an ensemble of generator and that we might not need to have an ensemble of discriminator. It would also be nice to have a comparison to GM GAN. Finally it s always nice to have some confidence interval for the empirical results.
Accept (Poster). rating score: 7. rating score: 7. rating score: 7. rating score: 6. <BRK>This paper studies the compression of recommendation models (RMs). That is new and relatively less studied in the model compression field, but of great practical value. The paper is very well written, and the notations and technical details are clearly presented. Question 1: My major concern is that, although the authors reported many baseline comparisons and ablation studies, all experiments are on only one dataset (i.e., Criteo AI Labs Ad Kaggle), and one task (CTR prediction). Question 3: in Eqn (1), why only enforcing structured sparsity for the input layer?<BRK>Studying the new problem of RM compression can save energy/latency in those practical systems, and hence has strong application promise. A typical RM consists of two components: a feature embedding sub model and a prediction sub model, both implemented by neural networks. To resolve this unique challenge, this paper proposed UMEC, the first unified optimization framework for the recommendation system scenarios. Overall this paper is technically sound. The writing is in general good and easy to follow.<BRK>3.Experiments:The authors compared with three groups of baselines: model compression, feature selection and embedding reduction. 1.Overview:The paper proposes a new unified optimization framework to solve the RM compression problem. I can understand that for binary classification, the last layer can perhaps be less parameterized. In this way, the importance of regularization terms is automatically tuned by two dual variables in minimax.<BRK>This paper proposes a framework of a unified recommendation system, which balances the compression degree of the model and the accuracy of the model by compressing the embedding layer model, while optimizing feature selection and neural network compression. The biggest problem of this article is that although it is proposed to optimize feature selection and model selection at the same time to achieve an effect of simultaneously ensuring model accuracy and model compression.
Reject. rating score: 3. rating score: 4. rating score: 5. rating score: 5. <BRK>The main technical contribution of this work is a new blinding algorithm that dramatically reduces the memory required to store the blinding parameters (decoupling it from the input/model size). Score Rationale:  The proposed blinding scheme does indeed provide a ~1.5x performance improvement over the Slalom baseline  There two concerns about the correctness of the security argument provided by the authors    The security proof as argued in the paper does not extend from the single pixel to multi pixel case and as such does not apply to real world images    The reviewer believes that this not a mere gap in the security proof. This statement critically makes no claims whether all the pixels in all the blinded images in their totality will leak information about a given pixel index in the source images. The statement is not true. Consider the case where the parameters are set as in section 5.2 with K 1. $x^{(1)}   \frac{\overline{\mathbf{x}}^{(1)}\alpha_{2, 2}   \overline{\mathbf{x}}^{(2)}\alpha_{2, 1}}{\alpha_{1, 1}\alpha_{2, 2}   \alpha_{1, 2}\alpha_{2, 1}}$  Thus the source image can be represented as a simple linear combination of the outputs and attacker job reduces to the task of guessing the two weighting coefficients  Given the strong priors on natural images this seems to be a very tractable problem.<BRK>Looking at Figure 2 and Figure 4, the spped ups do not appear to be consistently significant. While the authors make a note of them in Table 1, their differences with the proposed approach and where one is supposed to be better than the other is discussed neither conceptually nor experimentally (except Slalom)Novelty   The main algorithmic novelty of the paper is exporting the compute heavy linear computes to a GPU outside the secure enclave by using blinding unblinding techniques to protect the privacy. In my opinion,  while it is a very nice and concise thing to do (along with the results on its privacy guarantee), it is not significantly impactful or interesting to provide enough novelty to this paper. However, that is not the case either. However, I would encourage the authors to discuss the other techniques in more detail, compare with them and point out situations where this technique can have a larger impact. I have read the authors  response and my comments remain the same as above especially the paragraph regarding novelty.<BRK>Overall the paper is well written but could be improved in the presentation of its security guarantees and how it presents related work. The paper says that it is fine referring to the work by Bonawitz et al.; but they do so in federated learning setting which is different from the one considered here. Clarity:The paper is presented in an accessible manner. Some details however need to be explained further regarding generation of the masking variables for inference and training. For example, how are \alpha, \beta and \gamma generated. In the experimental section please clarify if blinded batches can be generated in a streaming manner. While Slalom considered only inference, this work considers training as well. But it seems, that integrity and privacy that the authors obtain in return is different. However, privacy and integrity guarantees are weaker than what one will obtain with Slalom (for inference) and SGX.<BRK>Also, they point out that their proposed scheme supports training phase as well, as opposed to Slalom which targets inference. This would be similar to what differential privacy and differentially private SGD provide. However, the suggested method in the paper seems to aim at providing secure training, in which data is not exposed to outsiders during training, however private information might still leak to the model, based on my understanding. I would like to know if that is the case. The privacy guarantees are not completely clear to me. It seems that the work is built upon the notion of Mutual Information, a heuristic approach. ", is this on average? or for each image, the guarantee is that no more than one bit leaks?
Reject. rating score: 4. rating score: 5. rating score: 6. <BRK>The Method section in the current version looks much better and clear. Weak points:  Since the main contribution of the paper is accounting for the uncertainty introduced by the optimization algorithm which is very novel, I think a more rigorous definition of this source of uncertainty is needed. Analogously, in standard posterior inference, $p(\omega|\mathcal{D})$ is defined through the prior $p(\omega)$ and likelihood $p(\mathcal{D}|\omega)$ which links the observed data $\mathcal{D}$ and the model parameter $\omega$ by defining how likely $\mathcal{D}$ is generated by a given $\omega$. Section 2, first paragraph: it says here that for these Bayesian optimization algorithms, "significant approximation is needed". Since I imagine that for different initializations, the optimal optimizer could be different. Also it would be interesting to see the proposed method applied to Bayesian optimization, although this is not an important point. However, I think this may not be the case.<BRK>The parameterisation for the posterior is defined using an LSTM neural network. ##########Pros:  The main high level idea seems clear. ##########Cons:  The presentation is not clear, especially in the method section. It is rather strange that instead of defining the prior over possible optimisers, the paper only talks about the posterior. It seems to me that this work relies heavily on the work of Ortega et al (2012), and if that is the case, a clearer presentation of the main arguments of that work would be very useful.<BRK>**(Recommendation)**  I think this paper may be suitable for publication if the authors improve the quality of the text and address my questions in this rebuttal. The use VI to approximate the intractable posterior distribution. Finally, they show that they demonstrate their approach on a diverse set of benchmarks. **(Additional feedback)**  The narrative is a bit confusing.
Accept (Poster). rating score: 7. rating score: 6. rating score: 6. <BRK>With FFNN and CNN, a theorem is given to show that the model is trainable only when the initialization on Edge of Chaos (EOC) and also provided a rescaling method to make the pruned NN into EOC regime. With Resnet, it proves the pruning satisfies the EOC condition by default and further provides re parameterization method to tackle exploding gradients. The experiments well support theoretical results for both FFNN/CNN and resNet.<BRK>### Contents of the paperThe contributions of the paper are three folded: 1) It proposes an essential for pruning at initialization, namely the NN must be initialized with EOC. 2) It proposes a trick to pull the pruned network back into EOC. The paper seems to be solid with enough motivation and proofs. 2.The experimental results show that the proposed algorithm achieves about 1% higher accuracy on ResNet than other algorithms. Lack of experiments on larger datasets such as ImageNet2. However, only results on ResNet are provided. 2.Initialization of CNN or other networks is an interesting topic which affects the performance of pruned models. However, there are few papers about the topic. I think the paper is a good example which may arouse more concerns about it.<BRK>The given paper carries two main contributions: 1) theoretical study of the pruning at initialization (i.e.before training); 2) proposing a new rescaling trick to avoid issues (namely, entire layer pruning) that are common for such pruning mechanisms. Major concerns:\  . However, I strongly believe that the layer pruning problem is commonly observed and studied phenomena (which is stated by authors as well) and it was theoretically studied before. For example, I suggest authors refer to the recent work from [1] where they call it "layer collapse". [1] Tanaka et al.Pruning neural networks without any data by iteratively conserving synaptic flow. Authors theoretically justify their proposed method;\  .
Reject. rating score: 5. rating score: 5. rating score: 5. rating score: 6. <BRK>This paper tackles the problem of temporal action proposal generation (TAPG). The authors address the problem from two perspectives: features wise and score fusion wise. However, as pointed by other reviewers, the novelty of the paper is marginal. For score fusion, they propose transformer based module to incorporate long range temporal information. The proposed method is evaluated on two benchmark datasets and achieved state of the art performance. Strength:+ The paper is well written and easy to follow. If you replace the two stream feature with more advanced video features, the performance will likely improve. What is the ground truth for these scores? Why not share the same transformer for both forward and backward? Although it has good performance, this paper seems kinda tricky and engineering.<BRK>Both modules rely on self attention and transformer architectures to improve the feature representation and the scoring component. Experiments on two well known temporal action datasets shows the potential of the proposed approach. Conclusion:Overall, the paper is well written and clearly presented. The authors have rebutted my concerns regarding the experimentation, which have gained in insight. The limited novelty and fit to the conference do remain pressing issues and it seems that this is partially shared with the other reviewers. Overall, it is interesting to see that attention and transformers can be applied to the problem of temporal action proposal generation, but the insights gained from the approach are modest. In general, action proposal generation is not the end goal, but the first step for the task of temporal action detection.<BRK>In general, it is an interesting paper to utilize multiple techniques to enhance two stream features and transformer to improve proposal scores, though all the techniques are not first proposed in this paper. The other reviewers also show the same concerns. Significance: This work has promising performance on action proposals, but its significance can only be evaluated after the detection performance is provided. It is mentioned at Page 5 that the input for backward are reversed. How does a backward transformer differ from a forward transformer considering that self attention is not uni directional? That makes the claimed novelty of proposing these modules trivial because attention and self excitation are existing works. ] 4) Though the paper claims to solve the problem of action proposal generation, the purpose of generating action proposals is to do action detection. So action detection performance is expected on the two datasets as well. From Eq.(5), all the 4 features can be directly concatenated, which means that they have the same temporal size.<BRK>  The paper proposed a feature integration (FI) module and utilized transformers to capture long range temporal dependencies, which is reasonable and interesting. However, these two modules focus on different aspects for action proposal generation. The proposed method was evaluated on two popular benchmarks, and achieved convincing results. More qualitative results and analysis should be provided. For implementation details, details about transformer, such as number encoder and decoder layers, are not provided, which could make the re implementation of the paper as a concern. Overall, it is an interesting paper, and I lean to borderline accept before the rebuttal.
Reject. rating score: 4. rating score: 4. rating score: 5. rating score: 5. <BRK>The idea itself is interesting, but to fully demonstrate the effectiveness of the proposed algorithm, there should be at least some comparisons with methods of similar flavor. More specifically, the experiment (section 4) only considered first order baselines (SGD and Adam type methods), while later in literature review (section 5), the authors mentioned that there are actually existing stochastic Quasi Newton methods and adaptive first order methods that can all be seen as using a sort of diagonal approximation of the Hessian. There are also discrepancy  in the text description and the algorithm presentation. There are plenty of examples showing that Newton without line search could fail to converge, even for convex problems. Therefore, the motivation is quite weak. There are many minor typos. My point about Section 3.2 is not that the correction doesn t make sense, but that the reasoning is not quite convincing from an optimization point of view. I would suggest the authors to simply say what they have replied me instead of trying to link this part to Newton s method. My major concern is that if the additional cost of other solvers is the main issue, then probably it would be better for the authors to directly show the training time as well so that the comparison can be straightforward.<BRK>While the combination of techniques is interesting, my main hesitation comes from the limited discussion concerning other quasi Newton methods for the same problem setting. To begin, a much more significant overview of the distinctions between this work and those of AdaQN and SdLBFGS is certainly warranted, as few details are provided to explain how they differ from the methods in this paper. For example, the comment made about AdaQN is that it "shares a similar idea but specifically designed for RNNs." In fact, the authors do not even compare with AdaQN in the RNN experiments, choosing instead only to run against Adam and RAdam. This brings us to a key issue with the paper: why are there no comparisons to any other quasi Newton methods, for any setting (RNN or otherwise)? As for other related methods such as AdaHessian, I agree that there is a distinction between quasi Newton methods and second order Hessian free methods in terms of the information that is accessed. Overall, the previous works on quasi Newton methods for stochastic non convex optimization have not been sufficiently addressed or compared to, particularly given how those works may also handle the issue of preserving positive definiteness of B_t. ."Newton s method usually employs the following updates to solve (1)"It should be clarified that convexity is important when trying to use (plain) Newton s method to solve problems such as (1). "Efficient full matrix adaptive regularization."<BRK>Some points that would be good to address regardless of outcome (no influence on my decision):  Does the algorithm work in the $\beta 0$ setting? Does that also hold for Apollo? **Pros**:+ The authors identify the main problems of QN methods in the DL setting and provides a good argument for using a diagonal approximation instead. In algorithm 2 you should replace $\lambda$ with $\gamma$ to be consistent with the rest of the paper. APOLLO uses the Frobenius norm (W I) which I would like some further comments on. This parameter is not mentioned in the language modeling experiment. I think this is handled in a suitable way as outlined in the appendix but it makes the comparison more difficult. The Rebuttal has addressed many concerns and the revised edition has further strengthened the paper in many ways but unfortunately lack in the empirical evaluation.<BRK>The authors give a clear introduction to Newton and quasi Newton methods, and summarize three main drawbacks of quasi Newton methods for nonconvex stochastic optimization which is the real case for many practical problems. As far as I known, there is no quasi Newton methods that could solve the above challenges simultaneously, and this paper give a solution that considering all the three aspects. Instead of depending on first order informations from m previous iterations, the proposed method approximates the Hessian matrix by  considering  the diagonal parameters of B_t which is more memory and computational efficient. The proposed method has advantage in all tasks, and for Language Modeling, the improvements seem to be very large. And more theoretical analysis is suggested to better understand the proposed method. I would like to change my rating from 9 to 5. The paper proposes an interesting idea and does achieve good results on several datasets. However,  after reading all the comments and feedbacks, I notice that the comparisons are not convincing enough, and I have some concerns about the performance of the proposed method on more general and challenging tasks.
Accept (Poster). rating score: 9. rating score: 6. rating score: 6. rating score: 6. <BRK>This is a great investigation on how to scale the gain of the inhibitory weights to balance the impact that the changes that the excitatory and inhibitory connections have on the layer’s output. I think it should be accepted. The Mushroom Bodies of the insects are the equivalent of the cortex and present feedforward inhibition.<BRK>Note that this does not mean that the action of a neuron is always excitatory or always inhibitory on all of its post synaptic partners. Although, I find the contribution interesting, my enthusiasm is tempered by the following two issues:1. Although the paper is generally well written, the authors could make it clearer. 2.The DANNs are shown to be just no worse than ANNs that do not respect Dale’s rule.<BRK>Summary: It is shown that Dale’s principle can be observed in feedfoward ANNs if one uses inhibitory neurons in the form of feedforward inhibition, while the other neurons are purely excitatory. Pros: This is a nice and new insight. Cons:  Apparently this insight provides no benefit for designing ANN. Furthermore the biological insight is rather limited because biological neural networks are not feedforward networks.<BRK>Most neurons in the brains are either excitatory (E) or inhibitory (I)   sometimes referred to as Dale’s law. Clarity: The writing is generally clear. Originality: As far as I can tell, the results are original. *I am worried that the experiments for the ColumnEi model was not treated fairly. In section 5.1, it is mentioned that 50 columns are negative. 4.The paper is fairly well written and the basic ideas are clear. This is not quite true in physiology. Also, would adding more I neurons decrease the performance of the network?
Reject. rating score: 4. rating score: 5. rating score: 6. <BRK>** Strengths(1) Improve the classic video texture synthesis method Video Textures by replacing pixel similarity with a distance metric learning to measure the transition probabilities (2) Extend the proposed approach to audio conditioned video synthesis(3) Outperform the competing algorithms on a set of evaluations** Weaknesses(1) It seems a strong limitation that the proposed approach is not able to generalize to different videos or has to be video specific (i.e., train a model on each input video). (2) It is not true that existing methods fail to generate more than a short sequence of frames, e.g., (Lee et al.2019) in theory can generate videos with arbitrary lengths. (3) The pre trained interpolation network (Jiang et al.2018) seems a quite important component for the proposed algorithm to generate smooth videos. (4) Lacks of analysis or comparisons to justify some important hyper parameters, e.g., (1) how the softmax temperature term impacts the synthesis results, (2) how t% is chosen to threshold the transition probabilities. (5) For video encoding, two separate subnetworks are used to break the symmetry between query and target embeddings, which makes sense considering temporal ordering. Unclear how this approach could handle general videos exhibiting with large actions. (7) It is claimed that the approach is able to produce infinite video, however the content is constrained in the input video, so the variation is limited. (8) What are the results if the classic methods also use the interpolation network?<BRK>Summary of this paper: In this work, the authors propose a method to learn to generate long range video sequences. The general idea is starting from a prior work (Video Textures) and extending this work with a learning framework. Specifically, during training a model is used to learn the transition probability between different video segments. To guarantee the smoothness of the transition between different segments, an existing interpolation method is used to connect these video segments in a sequential order. The authors also present sufficient qualitative results to demonstrate the superiority of their work. The analysis of the quantitative and qualitative results is convincing and logical. Cons:  originality: This work is more like a simple extension of the previous work (Video Textures) with limited novelty. It is highly recommended that the authors could present more comparison with the previous baselines in both general idea and model details. significance: I have carefully checked the quality of the generated video sequences, which are not so satisfying. Second, this method seems to be example specific, which needs retraining if fed a new video sequence. Third, the video content is directly sampled from seen sequence, where the diversity is constrained to the given video.<BRK>In this paper, the authors proposed a non parametric approach for video generation, i.e., video frame (un)conditional resampling. The proposed method is inspired by Video Textures (Sch¨odl et al., 2000), which synthesizes new videos by stitching together snippets of an existing video. The adjacent segments are regarded as positive pairs with high transitioning probability, yet other random sampled pairs are negative pairs. Similar to contrastive learning works, NCE loss is utilized to train the bi gram model. The authors made a trade off between the audio conditioning signal and the learned transition probabilities. However, I still have some concerns:In the experiments of unconditional setting, the authors included Classic, Classic+, and Classical++ as their comparisons. However, all recent (here, I mean in the past decade) methods that related to video generation are missed. First, although there may not exist any resampling method that can directly perform the video texture synthesis, I believe many related graph based methods could be used to model the transition probabilities of frames. In addition, the authors may further discuss the limitations of the proposed method.
Reject. rating score: 5. rating score: 6. rating score: 7. <BRK>Standard deviations are not reported in the expertiments. The bottom line is that the greedy optimization step helps significantly. There is not a lot of conceptual novelty compared to previous works: just the introduction of the greedy optimization step. There is technical work to extend gradient optimization and prove the theoretical results.<BRK>Numerical experiments are given to demonstrate that the approach substantially decreases approximation error compared to classical and naively learned sketches. The idea of learning sketches and the proposed greedy algorithms are simple. They are closely related to some of the existing related works on dictionary learning.<BRK>Pros: 	Paper is in general written well barring some notational issues and explanations which were not clear to me (explained later in “Cons”) 	The two stage idea of learning a sketching matrix with the same sparsity pattern as in CountSketch is interesting. Additionally, it is shown for LRA (under certain input distributions) that including the first stage is strictly better than not including it.
Reject. rating score: 4. rating score: 5. rating score: 5. <BRK>#### **Decision Recommendation**For now, I suggest to reject the paper. I think that it is interesting and valuable but feels that it needs to be solidified. Upon clarification of the questionable results I would consider changing my decision.<BRK>We find the title of the paper inappropriate, because the paper only consider the  graph neural tangent kernel. There are some spelling and grammatical errors that can be easily identified and corrected, such as "The descriptions in this section is".<BRK>b.)More details about the experiments would be helpful. Pros:a.)A good point of the paper is the compressive experiments and theoretical guarantees which make the paper complete and useful to the community. It is a good idea but the authors do not provide any new ideas for either NTK and GNN.
Reject. rating score: 4. rating score: 4. rating score: 5. <BRK>Thanks for submitting your work to ICLR. **[ ]** While the BvN decomposition might be of interest to the ICLR community, I am concerned the suggested differentiable approach to attaining the BvN decomposition is not. **[+]** The math seems correct.<BRK>The paper deals with the problem of finding an (approximate) Birkhoff von Neumann decomposition of a doubly stochastic matrix DS, i.e., given a matrix DS find a convex combination of permutation matrices that sum to the given matrix DS. However, as it stands I think both the "theory" as well as the "application" part of the paper fall below acceptance standards. As the algorithm is approximate: what about the approximation error? Why is it relevant here, if it is not used?<BRK>Of course, the application of a given method is extremely important, but in this case it shifts the focus of the paper. The notation for cardinality is never used as such, but the same notation is used for the absolute value several times. The Hadamard operations and pseudo inverse are never used. (2008).A dynamical systems approach to weighted graph matching.
Accept (Spotlight). rating score: 7. rating score: 7. rating score: 7. rating score: 7. <BRK>I think the method is well motivated and the solution is simple and portable (can be applied to many base methods). ##########################################################################Summary: Prediction sets are used to quantify the uncertainty of classification. 2.The proposed method is easy to implement and can be applied to general scores or be used to improve base conformal prediction methods. 2.More to the point: it seems that the proposed method is cut out for problems in which there are MANY classes.<BRK>This paper proposes a regularized generalization of adaptive prediction sets (APS by Romano et al 2020) that results in smaller prediction sets that still maintain the correct coverage level for statistical validity. The basic idea of the new regularization is very well explained and quite elegant: roughly speaking, there is an indifference between classes of low probability so we should penalize including them in the prediction set. I also found the experiments to be well chosen.<BRK>The proposed regularizer smooths top p scores with top k scores, which empirically results in more robust predictive sets. The authors also perform a large scale evaluation on ImageNet with modern architectures, which serves as a helpful benchmark for conformal prediction algorithms. Strengths  + Uncertainty quantification is a relevant and timely topic. + Conformal prediction and the resulting uncertainty sets they produce is also a practical and important instantiation of uncertainty quantification/prediction with performance guarantees. + The proposals put forth in the paper are interesting and potentially useful. It s also unclear what data was used to do the platt scaling. I suggest sticking to one. It should be clarified that the guarantee expressed in Eq.1 is *marginal*, i.e., taken over all permutations of (X_1, Y_1), ..., (X_{n+1}, Y_{n+1}).<BRK>The key feature of the method is that the size of the uncertainty sets are regularized via a penalty on the size. 2.The method satisfies some meaningful theoretical guarantees. (The proofs have been checked for correctness.) Furthermore, although the hyper parameters can be loosely interpreted as having to do with the size and the tradeoff with adaptivity, respectively, the precise relationship does not appear to be understood for general cases, making it more challenging to predict the behavior of the method. I think it ought to be included in the paper.
Reject. rating score: 4. rating score: 4. rating score: 5. <BRK>The most notable change is probably the introduction of an additional discriminator, $D_\phi(s)$, that is trained on the state marginal. The paper just argues that this regularizer makes "the overall algorithm well defined", without providing any reasoning. The transfer setting is different from the setting used by AIRL because the evaluation learns the policy and reward function directly in the test domain and it is not assumed that the original domain can be simulated. Hence, the evaluation tests the robustness of the imitation learning rather than the robustness of the reward function. Especially at the transfer experiments, CAIRL performed better than AIRL. I do not see how they are supposed to improve multimodality or transferability  (II) The evaluations seem to be termination biased by using positive rewards for CAIRL. CAIRL makes use of information about environment resets due to unhealthy states by assigning a value of zero to these states. It is not clear to me why removing this term improves imitation learning when learning under different dynamics. First of all, the fact that the expectation of a function is positive does not imply that the function itself is positive. On a side note, there is also a typo in that equation, because I suppose the log should be in front of the fraction.<BRK>The paper claims that the use of (state action) occupancy measure make IL and IRL methods brittle due to the high variance of these measures and their inability to transfer to other domains. This would show a) whether either of the proposed solutions can be used alone with benefits, and b) what is the individual impact of each contribution. It must be made clear why the reward formulated by AIRL is flawed. The BC line should be kept to highlight how similar the formulations are. Moreover, while the introduction of potential based shaping is justified in AIRL, it is not motivated here, and it remains unclear why it was introduced. About the experiments that are presented: the results of CAIRL are encouraging. It would also be interesting to evaluate the proposed IRL method on IRL tasks (reward recovery, as in AIRL), not only on IL tasks (perform well in environment). This observation directly echoes the main result of AIRL, which is that rewards robust to dynamics changes are only function of the state (not action).<BRK>The authors argue that the rewards learned by AIRL are potentially inefficient since they depend on the ratio of state action visitation distributions of the expert and the policy. Better motivation: It is argued that since AIRL rewards depend on the ratio of state action visitations, it suffers due to the “high variance” of this ratio. So why would CAIRL not be plagued with the same “high variance” issue? Note that DAC, which appears to be a much more principled way of handling the reward bias, does not add a modifier function (e.g.softplus) on the IRL rewards. ), and also comment on the role of softplus on their IRL rewards. 3.Transfer learning experiments: how was the algorithm (Algo 1.) The paper says it is required to “make the algorithm well defined”. Also, it would be interesting to do an ablation on this to measure its contribution to the overall performance of CAIRL.
Accept (Poster). rating score: 8. rating score: 6. rating score: 4. <BRK>##########################################################################Summary:The paper proposes a novel learning framework for robust (against adversarial attacks) fine tuning of pre trained language models, that is based on information theoretic arguments. It introduces two regularization mechanisms and investigates their efficacy on various tasks. The theoretical results seem to be sound. It also seems to outperform competitors in the field of adversarial language models. 2.The approach is tested on several standard datasets used in adversarial language models.<BRK>why is equation 7 true in the general case? I d like to see a proof for Theorems 3.1 and 3.2how do the authors define stability and robustness? The experiments demonstrate that InfoBERT consistently outperforms other adversarial training approaches on a variety of adversarial evaluations. The experimental results are convincing, however there are no ablation studies to disentangle the performance contributions of the two proposed objectives.<BRK>## Summary:The paper proposes two regularizers for finetuning pretrained mask LMs to improve the robustness of NLI and SQuAD models. The second regularizer has a similar motivation. This is not accurate. Formulation of the method: The authors cite the “localization” of the IB principle in the IB regularizer as part of the novelty of the method. Both are missing from the references.
Accept (Poster). rating score: 7. rating score: 6. rating score: 6. rating score: 6. <BRK>Specifically, authors claim that self supervised learning methods can extract task relevant information and discard task irrelevant information. Authors also do some controlled experiments and try to support their theoretical analysis. Recommendation: I recommend to accept the paper (rating 7). I liked the abstraction proposed by authors and particularly liked the way authors set up the Definition 1 and analysis afterwards. Ratings can be improved further if authors can relate experimental setup more to the theory which I find slightly disconnected. 3) In Proposition 1 and Theorem 3, it looks like d can be arbitrarily large and even scale with n. Is there any upper bound in d? Would this invalidate all claims? 5) Figure 3 c is not clear.<BRK>This paper explores both theoretical and empirical perspectives on self supervised learning. The authors attempt to prove that self supervised learning could extract task relevant information while discard task irrelevant information. They also provide a composite objective that includes contrastive and predictive loss functions along with an additional regularization. There are three major concerns for me. 1)	The experiments are conducted in a controlled way, including visual representation learning and visual textual representation learning. Traditional uncontrolled experiments, such as unsupervised learning on Cifar10 or ImageNet are suggested. 2)	The results in Fig.3 and Fig.5 demonstrate that the performance is sensitive to the hyper parameter of $L_{IP}$. How to set the hyper parameter $\lambda_{IP}$  in practice? Besides, the best performance achieved by $L_{IP}$ is only marginally better to those without it. As far as I know, InfoMin analyzed influence of different view choices using mutual information theory. Therefore, it would be helpful to compare the similarities and differences between this work with it.<BRK>Weakness:	I like the idea of discarding the redundant task irrelevant information to improve the self supervised learning. However, the composite objective proposed in this paper seems just like a simple combination of three tasks, which is not strikingly novel. another concern of this paper is the lack of persuasive experiment results to prove the effectiveness of the proposed method. In fig.3, the improvements on two dataset are marginally, which can not convince me. The \lambda (λ_IP) in proposed objective function seems not robust to different datasets, which makes me doubt about the generalization of this method. I hope the authors provide more explanation about this.<BRK>Pros:This work presents a very detailed theoretical analysis for self supervised learning objectives. The idea of inverse predictive learning for filtering task irrelevant information is interesting. How will the authors differentiate between these two formulations? The variation in the performance shown in Figure 3 is very marginal. Figure 5 a shows some results on Omniglot, but the improvement shown there is very marginal. Experiment on a large scale dataset, maybe ImageNet will be very useful to demonstrate the effectiveness of the proposed loss. The weights required for inverse predictive learning in the loss formulation is not trivial. As shown in Figure 3, it varies with different datasets and varies a lot (1 for Omniglot and 0.1 for CIFAR10). Is there a simple way to determine this weights without exhaustive search on target dataset? However, it is not clear from the experimental results if this is really effective.
Accept (Poster). rating score: 7. rating score: 6. rating score: 5. rating score: 4. <BRK>This paper proposes a member of the neural process family. The authors did not highlight the contribution made based on LieConv. This paper is the first to introduce the group convolution to NP to my best knowledge.<BRK>This paper addresses an important symmetry in meta learning. The method is somewhat novel; it may be described as a combination of two existing methods: permutation equivariant NPs (Garnelo 18) and LieConv (Finzi 20). The experiments achieve good results, but could be much more convincing. **Update Based on Author Feedback**I am grateful to the authors for their detailed replies to my questions. Should there be a measure on orbits?<BRK>The paper provides an extension of convolution conditional neural processes CNPs to more general Lie group equivariant CNPs. This is not a penalizing point but rather just an observation. The major concern with the body of work is the fact that the experiments seem lacking in application to more realistic data and scenarios. Indeed, the authors agree to this point in the discussion para.<BRK>2.The main weakness of this paper is the experimental section. The approach utilizes a combination of LieConv (Finzi et al., 2020)  and DeepSet (Zaheer et al., 2017) to achieve the equivariance in the data space and permutation invariance across the samples in a dataset. ### 2.DecisionI am recommending a weak reject for this paper.
Reject. rating score: 4. rating score: 5. rating score: 6. rating score: 6. <BRK>A convincing clarification of the above concerns could change my mind, but at this point I don t think the paper is ready for publication. While I find the motivation and the high level idea of using Attention weights to decide block relevance interesting, there are too many gaps in both the idea and the writeup at this point. This is because a block is marked relevant only if it contains answer words. The text mentions using the attention between the question and the block for assessing relevance. The precise equation for that cross attention would be more valuable here.<BRK>Summary: This paper presents the "Block Skim Transformer" for extractive question answering tasks. How exactly is the aggregation done between the question and a block of text? The key idea in this model is using a classifier, on the self attention distributions of a particular layer, to classify whether a large spans of non contiguous text (blocks) contain the answer. During training, none of the blocks are dropped. Is this improvement statistically significant? While I don t expect comparisons against all of them, atleast 1 2 comparisons should be done to ground the observations better. Overall RecommendationThe idea of dropping tokens is interesting, but I m not very convinced by the results.<BRK>Weakness of the paper:The most important concern I have is that the model requires pre defined supervision for tokens in order to train BST module. For instance, this paper trains the model to skim all the tokens that are not the answer token (which is the reason that it is specific to question answering). 2) The model is straight forward and the description is easy to follow.<BRK>This paper introduces a block skimming approach for QA. The main idea is to use a light weight module that uses the attention weights to predict which blocks of input are not useful for answering a question and use this information to avoid processing these blocks in further layers. The main idea is interesting and well motivated. Yet, there are no external efficiency related baselines compared (e.g.DistillBERT). 2.It will be useful to know the speed ups for some of the block sizes. This will highlight the fact that skimming method can be used to improve6.
Accept (Poster). rating score: 7. rating score: 6. rating score: 6. rating score: 5. <BRK>Summary:The paper proposes Dirichlet Neural Architecture Search which formulates Neural architecture search as a distribution architecture search problem. They derive a bound that shows that their formulation implicitly regularizes the Hessian norm with respect to the architecture parameters which has been shown to allow more robust architecture search. They demonstrate strong empirical results on several benchmark datasets and NASbench201. It proposes a new efficient distribution learning based NAS algorithm which regularizes the Hessian with respect to the architecture parameters. They provide ablation experiments that demonstrate the effects of different parts of their final algorithm. While the benefits of using the Dirichlet distribution could be somewhat better demonstrated empirically separate from the progressive search, compared to existing NAS distribution learning algorithms which don t discretely sample, it is beneficial that the trade off between exploration and exploitation can be controlled by a penalty term as compared to the popular Gumbel softmax based methods which arbitrarily anneal the temperature parameter. Pros:1.Proposed approach is shown to regularize the Hessian with respect to the architecture parameters which works have shown to lead to better network generalization. 3.Provides ablation experiments are which demonstrate the benefits of different parts of the algorithm. Cons:1.The paper would be improved greatly if the robustness of the search and final network architectures was empirically explored. This could be accomplished by analyzing the architecture distribution while searching on NASBench 201. 2.The paper would benefit from a proper baseline for the proposed progressive search method. Currently it is somewhat unclear if DrNAS with more time and memory would perform better if used to search the architecture space without a proxy or if the progressive search is regularizing the search to perform better. In table 4, do you know if there is a particular reason there is no variance in the results for DARTS, ENAS, and your algorithm. Updates:Thanks to the authors for addressing my concerns and responding to my questions. The newly added experimental results make the paper stronger and addressed many of my concerns. I recommend this paper to be accepted.<BRK>Summary: This work proposes a modified DARTS optimiser for NAS which assumes a  factorised dirichlet distribution over architecture parameters. It uses pathwise derivatives to learn an MLE estimate of these concentration parameters and adds appropriate regulariser terms to stabilise the training. The paper is easy to follow, and relevant experiments have been included. With the given probabilistic formulation of this work, it would be useful to include details on how the factorisation of appropriate distributions varies between PARSEC and DrNAS? On the same line it would be useful to get insights on how the obtained models qualitatively differ from ProxylassNAS, PARSEC and SNAS. The interplay of progressive learning with modelling assumption is a bit unclear. The number of parameters and test error in Table 2 are inversely correlated across SNAS, ProxylessNAS, PARSEC and DrNAS which is perhaps not as surprising. I was wondering if authors have any insights on what aspect of the algo (with and without the two stage progressive learning) contributed to network size. Would it be possible to employ the progressive learning policy mentioned in section 4.1 across other DART flavours and understand its impact on model performance?<BRK>This paper proposes a differentiable NAS algorithm based on the Dirichlet architecture distribution. Different from the previous differentiable and stochastic NAS algorithms that used the Gumbel softmax trick or the Categorical distribution, the proposed DrNAS does not require any temperature scheduling for balancing exploration and exploitation, and moreover it does not suffer premature convergence and instability during search. In addition, in order to reduce the memory consumption when searching based on the super net that mixes all possible operations, the proposed DrNAS applies the progressive learning scheme by combining the network widening with the operation pruning. Pros.The proposed algorithm is technically sound, and the motivation and both of the theoretical and empirical analysis are reasonable to support the use of the Dirichlet architecture distribution with the progressive learning. The experimental results also show that the proposed DrNAS consistently outperforms all previous NAS algorithms on CIFAR 10, ImageNet, and especially NAS Bench 201. Cons.My main concern of the proposed method is how to produce the sparse solution during search to reduce the architectural bias between the search and retraining phases. This naturally brings up the question of how and when it automatically changes the exploration to the exploitation during search, like a temperature annealing for the Gumbel softmax trick. When performing NAS on ImageNet, why did the proposed method use the proxy task with the reduced training set, even though it can retain a low memory overhead like a discrete architecture sampling such as DSNAS? How is the performance variance of the proposed NAS method on ImageNet? Minor: what if the proposed method searches different cell structures for each layer without the repetition?<BRK>##########################################################################Summary:The authors present a new differentiable Neural Architecture Search (NAS) method which places a Dirichlet prior on the edges of a cell for NAS: DrNAS. The authors adapt the original bilevel optimisation to incorporate this new prior over the operation mixing weight. The authors add a regularisation to the Dirichlet parameters and effectively study its effect in an ablation. The authors also introduce a progressive architecture learning to make the optimisation more computationally efficient. DrNAS produces very strong results on three different NAS scenarios and is compared to a wide variety of baselines. I think there are some issues surrounding the technical presentation of the work   see the Cons. Also I have some issues with the notation (see below), fixing these will help a lot with the readability of the paper. I have asked for some clarification and I am happy to raise my score in light of clarification from the authors. The experimental analysis on the other hand is very strong. Inference of the prior’s parameters is performed using the new pathwise estimator [4]. Implicitly controlling the largest eigenvalue of the Hessian is very important for producing good results in differentiable NAS [1]. The authors experimentally show that DrNAS is very effective in doing so. DrNAS produces very strong results compared to a variety of benchmarks on three different NAS scenarios. The L2 regularization can be formulated as a Gaussian prior about the Dirichlet parameters. Instead the Dirichlet prior is added to the bilevel optimisation in Equation 1 in an ad hoc manner. ##########################################################################Questions: Section 2.2, paragraph 1: if \theta is drawn from a distribution why is it less prone to overfitting? The references are text books. In the paper, for instance in the abstract, you claim that DrNAS encourages better exploration. Of course the model has some stochasticity by sampling from a Dirichlet, but what explicit evidence do you have that having \theta ~ Dirichlet leads to better exploration of the search space for NAS? You use the pathwise derivative estimator, which is not very popular (I might be wrong here), and you also work the Laplace approximation for Proposition 1 and in related work [3]. What is the advantage of the pathwise derivative estimator versus the Laplace for inference over Dirichlet parameters? You include stds in Table 4, but not in Table 3? How are the errors in Table 3 calculated? #########################################################################Some typos and notations issues and other comments**: Section 2.2, paragraph 3, first sentence no “the” before “exploration” needed. Although these works regard Continual Learning (CL): it is worth mentioning two related works including [5] uses a Dirichlet prior over networks for CL and [6] learns new neurons in a Bayesian NN using an Indian Buffet Process prior.
Accept (Spotlight). rating score: 9. rating score: 7. rating score: 6. <BRK>Overall, I strongly support this paper to be accepted and published in ICLR. The problem of universal multi agent reinforcement learning for multiple tasks is very interesting and challenging, and the methodology proposed in this paper is inspiring and the demonstrated experimental results are very impressive. Main contributions of this paper are as follows, which are very significant and impressive: [1] The proposed UPDeT based MARL framework outperforms RNN based frameworks on state of the art centralized functions by a large margin in terms of final performance.<BRK>### Summary and claims of the paperThe authors propose several Transformer based architectures that can be used in combination with a MARL (multi agent reinforcement learning) algorithm to tackle multi agent environments. The only thing I m unsure about here is the "vanilla transformer" architecture, which doesn t seem to be working at all. Is it possible that the transformer output is not conditioned on the different action types? UPDeT still performs reasonably well after the switch (zero shot generalization) and quickly recovers to full performance during fine tuning. The position of enemy units corresponds to 1 action each (targeting that unit) and the position of allied units (also controlled by the network, but not by this application of the transformer) corresponds to 0 actions. There are some references to "training speed" in the paper. I struggled to understand the details of the architecture for some time.<BRK>1.In this paper the authors proposed a transferrable framework for multi agent RL, which enables the learned policies easily generalize to more challenging scenarios. 2.However, the experiments seem to be insufficient. The authors only investigate scenarios for 3 vs. 3, 5 vs. 7, which are still the easiest cases in the StarCraft II combat tasks. I suggest the authors to try more on 20 vs. 30 StarCraft combat task or more challenging scenarios, or the hundres or thousands levels of multi agent tasks like that provided by MAgent environment[1]. Besides the performance gains, insightful understanding of how the designed model works is also necessary.
Reject. rating score: 3. rating score: 5. rating score: 5. <BRK>This paper proposes a reinforcement learning based model for aspect based sentiment analysis. 4.I am not convinced by the statement that the proposed model is more generalizable compared to baseline models. However, there are a few limitations that need to be addressed:1. The writing needs to be improved.<BRK>Summary:The paper addresses aspect based sentiment analysis by running reinforcement learning on the dependency parse of input sentences. The results in these 5 papers seem considerably better than in the current submission: e.g., for laptops, a number of these papers obtain over 74% F1 compared to the 71.48% in this submission. The state representation is learned with an LSTM. [1] and [2] were referenced in the paper, but their results were not included in the comparison.<BRK>The paper proposes an approach to aspect based sentiment classification, which is the task of identifying the sentiment of a specific phrase or entity in a sentence. The approach is evaluated on multiple benchmark datasets for the task and compared with many previously proposed approaches. They show improvements over these methods on most of the benchmarks. The proposed method is well explained and easy to follow. The results show improvements on many datasets. Due to the use of RL, the proposed approach will be more computationally expensive. Can you compare and contrast your approach to [1] and [2]? Can you explain if you found this to be an issue in your experiments? Many citations are not properly formatted.
Reject. rating score: 5. rating score: 5. rating score: 6. <BRK>This proposes a new measurement for the complexity of learning tasks. Strengths:  The idea of using the minimum expected number of questions that need to be solved for measuring the task complexity is an interesting idea to me. The paper is generally clear and well constructed.<BRK>Condition 2 in Prop3: we are assuming that y is categorical, and p_{Y|X x} is a distribution over the labels in Y? The paper claims that existing measures of complexity such as entropy are not suitable for measuring task complexity since they focus on the complexity of X rather than the predictive relationship from X to Y. The paper proposes a measure for task complexity based on the number of queries required to predict a label of an input.<BRK>This paper proposes a method to quantify the complexity of a learning task. The paper is motivated from the “20 questions“ game where an agent computes the answer (label) via a sequence of questions asked on the input data with answers given by an Oracle (simple functions of the data, in this case). This comes with certain caveats that are discussed in the detailed comments below. The definition of task complexity developed in this paper does not relate to “learning” tasks. Indeed the complexity of the same task under decision made by above over complete decision tree would be very large. This is exactly the benefit for using quantities like VC dimension.
Reject. rating score: 4. rating score: 6. rating score: 7. rating score: 7. <BRK>This work investigated a very interesting topic about generalization in decentralized deep learning. The authors identify the consensus distance as the key factor that affects the generalization performance of decentralized training. In general, the paper is well written and there are several interesting observations and discoveries involved regarding the generalization performance of decentralized learning. (Th1 is based on previous work.) 2.Also, Th1 quantified the convergence rate for SGD, while in the numerical results the authors used accelerated SGD and adam. In all, the discussion about the relation between the general error and critical distance is vague. In summary, I don’t think the theory part is very strong in this paper, and the relation between the critical distance and the generalization error needs to be further justified.<BRK>The authors consider the decentralized optimization problem and explain the generalization gap using the consensus distance. The conducted experiments are extensive and the delivered message is pretty clear   Critical consensus distance exists in the initial training phase and ensures good optimization and generalization, while a non negligible consensus distance at middle phases can improve generalization over centralized training. So I would say the abstract is a bit overclaiming, the authors better tune down their claims to practical only, without any theoretical guarantees   "We identify the changing consensus distance between devices as a key parameter to explain the gap between centralized and decentralized training. Overall speaking, I feel the motivation and message delivering is clear, though I am afraid that the main contribution falls into the practical findings (they are also important though) instead of the theoretical guarantees   there is a mismatch between theory and implementations. International Conference on Machine Learning.<BRK>It focuses on the so called "critical consensus distance" and how disagreement during different stages of training ultimately effects optimization (training loss) and learning (generalization error). Theory is provided for the case of synchronous symmetric averaging methods, and the paper is complemented with detailed experiments on CIFAR and tiny ImageNet. This is a nice contribution to the growing literature on decentralized training for deep neural networks. The connection between consensus distance and performance has previously been studied to a limited extent in various settings, so the contribution of this work is somewhat incremental. I expect the results to be useful to those working on decentralized training and am supportive of accepting it. I have a few suggestions and comments, about which I look forward to hearing from the authors. CIFAR 10 and ImageNet 32 are both relatively small datasets.<BRK>The authors describe an upper bound for dissimilarity of local variables that guarantees the performance of decentralized training is as good as centralized one. Moreover, some heuristic guidelines are proposed to control consensus during training process. Reasons for score:I believe the paper is well written and the results are useful for the literature. Although there is a connection between rates and generalization, one is not equivalent to the other. 2.The authors claim to analyze the problem theoretically. This is an issue, as the translation of the obtained results into the momentum method needs to be proven. How are the authors sure that momentum does not play a role into the dependency on consensus? 4.One main concept seems to be that \phi_t does not change too fast. Such a main concept needs to be spelled out in the main text.
Accept (Poster). rating score: 7. rating score: 7. rating score: 5. rating score: 4. <BRK>The model is that $N$ samples are made from a Bayes net on d nodes, out of which an unknown $\varepsilon$ fraction are changed arbitrarily. When there are no corruptions, it is well known that in $O(N)$ time (for sufficiently large $N$), one can estimate the Bayes net upto TV distance $\varepsilon$. This is without any extra assumptions. However, they also require two extra assumptions of "balanced" ness and "minimum parental configuration probability". The previous best running time (under the same assumptions) was $\tilde{O}(Nd^2)$. The key contribution of the work is that they show a clean reduction to robust mean estimation. Unlike previous work on this problem, they make their algorithm super simple and natural, at the expense of making their analysis somewhat more complicated. The second is a speed up for robust mean estimation that runs in time nearly linear in the number of nonzeros in the input. Overall, I am in favor of accepting the paper. One could argue structure learning is more  fundamental . Mention that it s the set of corrupted samples.<BRK>The authorsfocus on the fully observable case when all the variables are binary and theunderlying graph is given to the algorithm. The main results in this paper are(1) a nearly linear time algorithm for this problem with a dimension independenterror guarantee, and (2) a direct connection between robust mean estimation andlearning Bayesian networks. Applications of learning Bayesian networks could also be discussed inmore detail in the introduction of the paper. [ 2] Typo: "previous work"  > "previous works"  [ 2] Typo: "Previous algorithm"  > "Previous algorithms"  [ 2] Typo: "either has"  > "either have"  [ 3] It might be helpful to use  n  instead of  N  for the number of samples.<BRK>The paper studies the problem of robust learning of fixed structure Bayesian networks under the eps adversarial corruptions model. Fixed structure means a known structure of the underlying Bayesian network. The main contribution of this work is in improving the running time of the algorithm. On a d node Bayes net, let m denote the total number of parental configurations possible. Prior work of Cheng et al showed a robust learning algorithm using O(m/eps^2) samples and runs in time O(md^2/eps^2). The current paper reduces the running time to O(md/eps^2). The core subroutine is a robust mean finding algorithm for general distributions. A nearly linear time algorithm for this problem was recently given in the work of Dong et al which the current paper uses.<BRK>The paper considers the problem of robustly learning fixed structure Bayesian networks in nearly linear time. The authors have to modify the runtime analysis of the algorithm of Dong et al.to work in time linear in the sparsity, rather than dimension. I found the current contribution to be a bit limited. Once the reduction is established, the the paper leverages the algorithm of Dong et al.for robust mean estimation. However, the vectors arising in learning Bayesian networks are sparse, but Dong et al.does not exploit this sparsity. I also think that it would be good if the paper had experiments to back the algorithm it would especially make the paper more interesting and relevant to the ICLR audience.
Reject. rating score: 6. rating score: 6. rating score: 7. rating score: 8. <BRK>Likewise, I feel that some of the limitations of the proposed approach are understated and that its generality is overstated, especially in the abstract and the introduction. The modelling assumption (1) together with the model constraints (2) permit the problem to be cast in a form for which the celebrated Polyhedral lemma (Lee et al.2016) applies. 3) The paper is also lacking in clarity. I would encourage the authors to show results pertaining larger models (e.g.ResNet based architectures with > 10 layers).<BRK>Pros  The proposed method yields good performance in power in practice. The authors proposed an efficient algorithm to find certain kinds of attention areas. Cons & Questions:   Is the assumption (1) necessary for the proposed non asymptotic method? If it is necessary, please also address why this assumption is practical, or existing work where the same was assumed. The proposed method has several assumptions on the neural network, such as the layers need to be affine / max pooling / piecewise linear. For example, can one uses this method to test the alternative hypothesis that a region found by BERT s internal attention mechanism is significant?<BRK>I believe the goal and motivation of this paper are clear and very well useful. They also give some promising experimental results. The techniques used in this work (e.g., selection of extra conditioning for traceability, Merging results of condition sets) have a satisfactory amount of novelty and can be of independent interest to the ML community.<BRK>Considering that there are more and more interests in the research on neural representation learning, the problem that this work is trying to solve is pretty important. It also shows its novelty in solving the problem. The structure of the paper is well designed, and the writing is clear. Related to the previous point, the boundary between where this method can be applied and can not be applied is not so clear. It would be better if the authors could give such guidance for people to use this method.
Accept (Spotlight). rating score: 7. rating score: 7. rating score: 7. rating score: 7. <BRK>  SummaryThis paper presents a "physics informed" deep learning model of fluid dynamics. The only practical application demonstrated is in a simple control task, which is not explored too deeply. Reasons for scoreOverall, given the "pros" described above, notably the distinct loss formulation that allows the model to learn unsupervisedly to perform efficient, differentiable fluid simulations, I recommend this paper for acceptance.<BRK>The paper is generally a good contribution especially in the field of machine learning for physics. However, I do have some questions about the novelty of this work and what makes it different from other physics informed approaches. Below are the pros and cons and the I list out a set of comments and additional results that I believe would make the contribution of the paper much more clear. Does the introduction of $\Omega$ and $\partial \Omega$ in their input features make their framework generalizable? Sure, the a net outperforms v net when it comes to the loss. 6)  The problem solved in this paper is a very simple problem.<BRK>This paper proposes to learn the dynamics of an incompressible fluid via a physics informed loss formulation using an unsupervised training framework. It employs a custom solver that is executed at training time to learn a Navier Stokes residual with a incompressible (curl of a stream function) formulation. Here, the authors instead discretize the solution on a grid, while a network yields the solution for the next step, and the physical model is evaluated with FDs on the grid.<BRK>### Summary of my understandingThe authors propose an unsupervised (or rather, I would say self supervised) method to build a simulator of incompressible fluid flows based on neural networks. The proposed loss functions are designed to make the outputs of the neural net fulfill the Navier Stokes equation and boundary conditions. I could not find it in the paper. They show that the learned model outputs qualitatively plausible flows, even if the domain is not exactly handled in the training phase.
Accept (Spotlight). rating score: 8. rating score: 7. rating score: 7. rating score: 6. <BRK>**Pros**A gap in the literature of learning and evaluating reward functions is clearly identified and an explicit argument is made in favour of searching for alternatives to the rollout method.<BRK>The EPIC distance gives a bound on the regret between policies optimizing for one of the two reward functions relative to the other. The paper is well written and well motivated. The paper gives a deep analysis of EPIC in a reasonable setting.<BRK>Pros:   This paper proposes and analyzes a theoretically grounded distance metric to evaluate learned rewards. It has several properties appealing to practical RL tasks.<BRK>To me, the proposed approach is novel and interesting, and there is no similar previous work to my best knowledge, so I tend to give it an accept. This can be a proof of concept, but I admit that it might also be limited. You re now comparing the reward functions directly in the "reward space".
Accept (Poster). rating score: 7. rating score: 7. rating score: 6. rating score: 6. <BRK>The authors showcase improvements in 4 different datasets for long term prediction, planning and generalization. Weaknesses:* Evaluation and insight into how changing feature representation for interaction networks impacts long term prediction seems to be missing from the paper.<BRK>While the paper is well written, there are some inconsistencies in notations that are hard to follow. The paper is well written and is evaluated on several popular benchmarks. k has a direct effect on the number of parameters in the network W_p and it would be interesting to see how it affects the predictions.<BRK>The benefits of being able to make long term predictions when planning are clear. The method is clear, and the results are good. Reason for Score:I vote for accepting. I cannot see this size in the paper (should be in A.1?).<BRK>Folks outside the field may not understand what it means to "operate in the state space" and why it that not good ? 2.There are clear experiments that effectively demonstrate the contribution of the paper.
Reject. rating score: 4. rating score: 5. rating score: 5. rating score: 6. <BRK>The authors attempt to address an important problem of preserving both the global and local structures of  high dimensional data by applying two step optimization approach to UMAP. The paper is well written and this is an interesting direction, but unfortunately the results don t look convincing at all:1. It would be much more interesting to see that the clusters are still preserved, but they are positions relative one to another also correspond to a global structure. 2.MNIST and Fashion MNIST are certainly more realistic than simulated data, but would be nice to see the results on some real world datasets. For example, the authors give examples from biology in the introduction. Maybe they could show the performance and advantages of their method on one of these datasets. Therefore, I think it would be more fair to compare the proposed method with non random initializations used in practice.<BRK>### Summary: This work proposed a dimensionality reduction algorithm called Uniform Manifold Approximation with Two phase Optimization (UMATO), which is an improved version of UMAP (Ref.[3] see below). UMATO has a two phase optimization approach: global optimization to obtain the overall skeleton of data & local optimization to identify the local structures. On synthetic and three real world datasets, UMATO achieved comparable and sometimes better results, compared to baseline method PCA, and alternative non linear methods such as t SNE, UMAP, topological autoencoders and Anchor t SNE. This is an interesting concept, and related to previous works such as Ref. [4], which utilize the manifold ranking method to detect outliers. [3], however, feel several related works are missing, for example Ref.<BRK>The proposed algorithm has two levels for the recursion: first run UMAP on the selected hubs, and then run UMAP for the nearest neighbors for each hub. Strengths:The paper presents a substantially improved visualization of the synthetic Spheres dataset, where the original global structure is preserved in the visualization, as well as quantitative results on four datasets (Spheres plus 3 real world datasets). In particular, what do (B) and (C) refer to exactly? It may be better if it is clearly called out that t SNE and UMAP are good at local quality metrics, topological autoencoder and At SNE are good at global quality metrics, while UMATO is good at both global and local metrics.<BRK>Quality, clarity, originality and significance:Pro: The paper is well written and most aspects are clearly explained. It seems to me that the method is mostly designed for clustered data which is significant in many real world applications, but this is not emphasized in the paper, see also comments below. Second line on page 2: Should it be UMATO instead of UMAP? Due to the two phase approach to the embedding it seems that the method is much more adapted to clustered data than to the standard manifold embedding. How would the method perform for a continuous manifold that is fairly uniformly sampled such as the Swiss roll, would it create holes because of the two phase structure?
Reject. rating score: 4. rating score: 6. rating score: 7. <BRK>The trouble begins from the unnumbered equation in page 4, where the authors define $\widetilde{h}$. **Theorem 1:** In the proof of Theorem 1, the authors make an assumption about what the output of $f$ is. Even worse, they claim that the empirical average of some loss value is equal to its expectation (_what happened to generalization?_). 2020).Even then, the comparison should be done with respect to the statistical parity violation as defined in Dwork et al.2012, which is a more established notion of fairness. The addition of fairness violation/accuracy tradeoffs and also (Kamiran et al.2012) add a lot of value in putting this paper in the right context. On the other hand, the main claim of the paper seems to be theoretical optimality. However, I am still unable to recommend this paper in the current form for publication in the conference proceedings.<BRK>The empirical results show that the algorithm outperforms existing post processing approaches for fair classification. Existing post processing algorithms usually lack theoretical guarantees but not this paper. My main concern is the clarity; see cons. Pros:    1. Several symbols or notions lack explanations. E.g., in Theorem 2, it is better to explain each term of the right side, including why these terms exist and when they are small. E.g., why the update rules should be formulated as (2)?<BRK>In this paper, the authors propose a post processing method for removing bias from a trained model. The bias is defined as conditional statistical parity — for a given partitioning of the data, the predicted label should be conditionally uncorrelated with the sensitive (bias inducing) attribute for each partition. Subsequently, they propose an iterative solution to the problem, proving some theoretical properties as well as showing how the method compares to different baselines. 3.Right below, a bar is missing in the absolute value: “can be written as |.| < \epsilon”. Overall, this is a well written paper that tackles an important problem, and it would make for a good addition to the conference.
Reject. rating score: 3. rating score: 4. rating score: 5. rating score: 7. <BRK>Overall I vote for rejection of this paper in its current form, mostly due to the missing comparison with the very related method mentioned above, the quality of the writing and the weakness of the experimental results, as described above. Specifically, the property that is referred to in this paper as ‘disentanglement’ is very related to previous notions that have been studied in this context of pre training for few shot learning. [B] Unfortunately I also found the writing to be of poor quality.<BRK>do not prescribe a supervised pre training phase. * The term "meta few shot learning" is not widely used in the literature and appears to be introduced in this paper as far as I can tell. Can the authors point to work that provides empirical evidence for this statement?<BRK>Paper summaryIn this paper the authors provide a summary of the role of pre training in meta learning. Would be good to explain either in the caption or figure what is an SNN value for fully disentangled and fully entangled data. * To improve the strength of the paper, I would suggest focusing on the disentanglement properties of the SNN regularization and, as proposed above, explore it under a lot more settings.<BRK>The authors show that the SNN loss is lower when the regularization loss is in use in the meta training phase, which supports the authors  conjecture that this regularization helps disentanglement of the penultimate layer in the backbone network, thus enhancing episodic learning. Pros:1.The method is very neat and easy to implement2. Not sure if the term "pre training" is proper here. The main focus of this paper, is the regularization term $l_{reg}$. Do you think might be the reason for this phenomenon?
Reject. rating score: 2. rating score: 4. rating score: 4. rating score: 5. rating score: 5. rating score: 6. <BRK>The paper makes a number of comments about its method learning causal structure. It does not defined saturated until after it has used it in context. To me, these seem like big claims. “Another one is the curiosity based approach (Stadie et al., 2015; Pathak et al., 2017; Haber et al., 2018; Burda et al., 2018a), which is formulated as the uncertainty in predicting the consequences of the agent’s actions.” The paper previously said that there were two families so “another one” should be “The other” or something else signifying that this is referring back to the two families comment. The end of the sentence needs fixing. First, the paper gives no information (that I could find) about how it tuned the baselines or whether they tuned them at all. Both the quality of writing and the issues with seeds/tuning independently merit rejection. We hypothesize that the main reasons are: 1) the mean squared error (MSE) loss used for regression is satisfied with “blurry” predictions. The paper runs this experiment and discusses the results later in the paper. IE, the ablation studies and the discussions. “makes use of the bootstraps for deep exploration”bootstraps  > bootstrap“Here, we mainly focus on the problem of using intrinsic rewards to drive explorations.”Why is exploration plural?<BRK>Combining audio and video information is essential to improve the explorative behaviour of agents that explore multisensory environments, especially if the relationships between agent behaviour and sounds are causal. The paper is well structured, and the description of the methods is clear. Note, that the reviewer is not familiar with these environments. For example, in the Habitat experiment, how many sound sources have been used? Also, the reviewer found it challenging to interpret the results. For example, Figures 3 shows that the reward of the proposed methods is higher than the reward of other methods. Each action was randomly selected based on a uniform distribution. If so, then this may not be the most reliable way to estimate random exploration. The idea to use the prediction error to encourage explorative behaviour is smart 	Method evaluated in on different environmentsThe cons:  	Interpretation of performance is not straight forward 	A more systematic analysis of the relationship between relevant sound sources and environment sounds would be helpful to get an idea about the potential and constraints of the proposed method ### Update after discussion period ###Good idea, but the results don t clearly support the authors claims.<BRK>Summary:This paper proposes a novel type of intrinsic rewards for RL agents based on audio events prediction. The auditory event prediction error is used as an intrinsic reward for better exploration of the agents. ###Final Recommendation###Based on the discussions with other reviewers and AC, this paper is not ready to publish at this stage mainy due to the following reasons:1. the big claim of causality as also pointed out by R62. the writing should be significantly improved and the experiments lack details as pointed out by all reviewers3. the new problems found during discussion with the AC regarding the ablation study, and seeds, etc. The authors are encouraged to polish the paper in writing and experiments for future resubmissions. 2.Why not using bottom up clustering methods such as Agglomerative Clustering so that the number of clusters is not needed to be specified? See detailed comments. The current method mannually set K to be in the range of 5   30, and then decide the best K is somewhat cubersome. The results on more realistic environements are not very convincing. There is no event defined. Technically, audio might not even be necessary in this case. 2.Comparison to Dean et al.2020 should be more clear. 3.In figure 4, how to directly use sound clustering as an intrinsic reward is not clear to this reviewer.<BRK>The index of these clusters are the auditory events used during the second phase. The prediction error produced from this classification task is given as a shaping reward along with the extrinsic reward to the learning agent. Pros:Overall, the approach of using auditory events to drive exploration is novel. Cons: Some implementation details about the approach are not available. The approach seems to require a pre training phase that requires collecting a significant amount of data. Questions:I have a few questions related to the approach, experiment results, and implementation details, which would help clarify my understanding of the paper. Would be interesting to see if the performance still holds in this scenario. It seems unfair to the other methods when the introduced method has experienced more data because of the pre training phase. The choice of the number of environments would affect the batch size of the learning update and usually, it is common to use 16 or 32. Why was this choice made?<BRK>**Update**: Other reviewers have pointed out issues with this paper s ablation study. Additionally, it is difficult to trust the empirical results because they are based on only three runs. Why did the authors choose to use a texture based sound embedding (McDermott & Simoncelli, 2011) as opposed to something more standard like log amplitude Mel spectrograms? I would be quite interested to see how the performance of the latter compares to texture based embeddings. A caveat here is that I have limited experience on RL and defer to the expertise of the other reviewers in assessing the relevance of the baselines (ICM, RND, RFN, DIS).<BRK>Consider replacing it with “We follow the standard setting in Pathak et al, 2017; Burda et al., 2018b, where an agent can use external rewards as an evaluation metric to quantify the performance of the exploration strategy. The approach is clear and easy to follow. Comprehensive experimental analysis and convincing results. Consider replacing with: “We also compute the cluster distances of both models and find that the sound clusters discovered by active exploration are more diverse, thus facilitating the agents to perform in depth explorations.”  Section 4.3, sentence 1: Consider replacing with: “To evaluate the exploration ability of agents trained with our approach, we report the unique state coverage, given a limited exploration budget. Some more insights about the choice of audio features will be helpful to the reader. The writing requires more clarity (suggestions below). UPDATE: Due to concerns raised by other reviewers, and my own confusion about the computation of audio features, and clarifications on the causality stance, I have lowered my score from 8 to 7. ########################Questions during rebuttal:  Please refer to the questions in the Cons section and other feedback. How would the proposed work compare with other prior work where audio is used to aid RL (not necessarily as an intrinsic module), such as Aytar et al., 2018; Omidshafiei et al., 2018? ########################Some typos and other feedback:  Consider using the \citet command when referring to the authors of a reference paper in a sentence. Related Work, RL explorations: Tompson sampling  > Thompson sampling  Related Work, RL explorations: Osband reference missing the year in the references section and the citation.
Accept (Poster). rating score: 7. rating score: 7. rating score: 6. rating score: 5. <BRK>This paper leverage the idea from counterfactual reasoning and tried to ask and answer the counterfactual in IRL. I think it is a good attempt in an important direction and also help to bring the the gap between causal inference and the RL community. The proposed approach is run on a simulated and a real medical dataset, showing the effectiveness of the approach.<BRK>The problem tackled is extremely challenging from the theorectical point of view and to the best of my knowledge this is the first paper which tries to explain sequential decisions through counterfactual reasoning and to tackle the batch IRL problem in partially observable environments. I like the methodological framework, it is well structured and convincing, even if the basic assumprtions required to make the proposed approach working are strict.<BRK>Reading over the paper, it is not clear they make their case for interpretable decision making. They present results of two simulation studies, as well as an experiment on the MIMIC III data set. ## Questions for the authors  The problem formulation in section 3 is a bit light on details with respect to how counterfactual reasoning is present in the definition of the feature representation (cf.Equation 1).<BRK>This paper considers the problem of Inverse Reinforcement Learning in a batch setting. One of the main motivations of this paper is to provide explanations behind the expert’s decision making. I think the paper makes interesting contributions to the literature on IRL in the batch setting. Furthermore, the authors should discuss when the counterfactuals $E[Y_{t+1}(a_t) | h_t]$ are well defined.
Reject. rating score: 3. rating score: 4. rating score: 5. rating score: 7. <BRK>While neural networks have been applied to simulate fluid dynamics before, they are purported to suffer from poor generalization to unseen geometries for long time predictions. ### Detailed Review The following is the detailed review of the paper, organized into strengths and weaknesses subsections. ### Weaknesses #### Relation to Prior Art The paper does a reasonable job of presenting the prior art but fails at identifying the unmet need that the presented work fulfills. The claimed speedup over numerical solvers has been achieved before (Guo et al, 2016). It is not clear what advantages the present work is supposed to have over the SOTA and why. #### MethodologyThe proposed approach is a straightforward application of U net to predict a spatial field given past few spatial fields (stacked together). However, U Nets, LSTMs, conv LSTMs and other architectures have been tried before. It is unclear what the novel contribution in this paper is (gradient augmented loss function?) and why it would be instrumental in handling unseen geometries over longer periods of time. Further, these approaches have been tried before (see prior art) #### Empirical Evaluation There is no evaluation against the state of the art. ### Assessment Though the problem seems relevant and of significance to the research community, the paper suffers from a lack of novelty and a non existent comparison against the state of the art. I do not recommend the publication of this paper.<BRK>On the domains tested, the predictions remain accurate for 20 time steps and the method seems to provide considerable speed ups compared to a state of the art spectral/hp element numerical solver. Some encouraging results on generalization to new domain shapes and larger scale domains are also presenting in the experimental session. Weakness: 1) The paper lacks a novel contribution from the architectural and application side: UNets have been previously used on forward dynamics predictions (e.g.[1] Thuerey et al.) and there are other works using neural networks on wave prediction (e.g.[2] Fotiadis et al.). The architecture is a straightforward application of a Unet architecture + a loss function with MSRE of the predictions and its gradients. 2) There are no comparisons to other baselines. b) a history of the previous 5 time steps is used as an input to the network, no ablation or justifications are provided. c) in the training details, authors state "The time origin for the input sequences is not at t   0, instead it is randomly selected, and five time steps are performed before updating the network weights" but nothing else is discussed. e) In the generalization to a large domain experiment (section 4.4), the authors justify the relatively poor performance of the approach by stating "This problem could be addressed by training the U Net for longer output sequences and possibly increasing the depth of the network." 5) Most experiments are showing results for 20 time steps unrolls, and while for longer sequences dissipation was observed   which is a strong limitation of the model. Because of the points above, I think this manuscript does not pass the ICLR threshold for acceptance, although I believe this would be a good workshop paper submission.<BRK>The authors use a U Net architecture network to predict the motion and interaction of surface waves in an open and closed complex. The neural network based method runs much faster than the standard numerical simulation by directly solving the PDE. The predictions over more extended periods are not very accurate. 2.Some previous works also used the U net to predict wave dynamics as [3]. It is not clear what is the novelty (if any) in the proposed network architecture. 3.Not enough Experiments. How does the model generalize with more complicated initial conditions, for example, five or ten droplets? Furthermore, there is no comparison to other existing work. Is this because the wave reflection at the boundary is a local phenomenon, so the convolutional network only needs to learn the local reflection?<BRK>A deep net based on U Net was trained to predict the next state of the wave field given the five previous states. The results also show that the trained network is able to generalize to qualitatively different environments from the training set. If there was one word I would use to describe this paper, it is thorough. The introduction and related work do a good job going over related contributions in the literature and giving the reader, particularly a reader with an ML background and not necessarily a fluid dynamics background, a good understanding of what has been done to this point. The same can be said for the results. The weakness of this paper is that it doesn t present any novel techniques. It s an existing architecture (U Net) applied in a new domain (wave simulation). However, experimental papers provide value as well, and the field is grossly lacking in them. To that end, this paper could be a useful contribution. The results are thorough and even show the limitations of the model. However I would have liked to see have seen data showing how much faster this model is over the standard solvers. The paper claims in the intro and conclusion that their model is 10^4 times faster than the standard method, but no where in the paper is data provided to back this up.
Accept (Poster). rating score: 8. rating score: 7. rating score: 6. rating score: 5. rating score: 5. <BRK>In Instance Level and Episode Level Pretext Tasks for FSL the authors present a novel method to take advantage of auxiliary prediction tasks and consistency regularization tasks which have had large success in Self Supervised Learning settings to improve upon FSL approaches. Furthermore, the authors incorporated a transformer based predictor to improve upon to be used with multiple augmentations of an instance to improve upon naive averaging of multiple predictions. Empirically, the authors demonstrate the benefits of incorporating both instance and episode level tasks by showing significant improvements over the ProtoNet approach upon which this work builds and also achieving state of the art results on several tasks with different architectural backbones.<BRK>This paper presents a method for combining self supervised learning (SSL) (in the form of predicting the rotation applied to an image) with few shot learning (FSL) in the domain of image classification. Strengths:  The paper is well written, with extensive discussion of prior and contemporary work. A large part of the improvement over the baseline ProtoNet seems to come from the integration method (based on the ablation study). The technical details are presented in clear precise terms.<BRK>The paper proposes both Instance level and episode level pretext task. However, the novelty is limited. It is more like existing works (Gidaris et al., 2019; Su et al., 2020) plus the the regularization of consistency for images with different augmentations. In NeurIPS, 2017. Temporal ensembling for semi supervised learning. Comments after rebuttal: I know the authors develop two components for FSL, my concern is that these components are incremental and have limited novelty. Hence I increase my score to accept now.<BRK>This paper solves the problem of few shot learning. 2.Your ablation experiments are not complete. Unlike other trivial combination of SSL and FSL methods, this paper proposed instance level and episode level pretext tasks to bring on closer integration. Further, this paper proposed to use transformer to integrate features from different images and augmentations.<BRK>Two self supervised losses are designed based on the augmented episode sets – (1) recognizing different rotation transformations as an instance level pretext task; (2) ensuring consistent predictions of class labels across different episodes as an episode level pretext task. Strengths:++ Self supervised learning and few shot learning are two important techniques to transfer knowledge for addressing new tasks, but their combination is less explored. Suggestions and questions:	  Compared with the state of the art methods, the performance improvements of the proposed approach are marginal. In the design of a set of extended episodes, examples within the same episode belong to the same rotation transformation. Post comments to the author s response:After reading the other reviewers’ comments and the authors’ rebuttal, I am still concerned with the novelty of the approach, experimental evaluation, and performance improvements over previous work.
Reject. rating score: 4. rating score: 5. rating score: 5. <BRK>This paper introduces the skill action (SA) architecture, an instantiation of the options framework that can be optimized with standard MDP algorithms (such as PPO). I recognise that it is a recent paper but it should at least be discussed in the paper. The results on the OpenAI Gym are encouraging and improvements in HRL would certainly be significant and of interest to the ICLR community.<BRK>### SummaryThe paper proposes the "Skill Action" architecture with an MDP formulation, inspired by their novel discovery of an equivalence of Semi MDPs and MDPs under some assumptions. I suspect that the reason for increased performance may not necessarily come from having multiple skills and this can be verified by adding another baseline to all plots in Figure 2   the proposes SA + PPO algorithm with just 1 skill instead of 4 skills. Given that no significant gains were seen in the finite horizon setting, I am surprised to not see more experiments demonstrating the efficacy of the proposed method on more infinite horizon environments. The equivalence between semi MDPs and MDPs is also novel.<BRK>This paper shows that the semi MDP option framework has an MDP equivalence, by adding extra dependencies into the master policy. The proposed method (“Skill Action” architecture) trains a skill policy which marginalizes those dependencies, allowing the master policy to be updated at each time step. I think more experiments on finite vs. infinite horizon tasks would be useful to the research community, e.g., compare SA vs. baselines on the same task as you increase the time horizon. Do all of the methods have the same number of learnable parameters for fair comparison?
Reject. rating score: 5. rating score: 5. rating score: 6. rating score: 7. <BRK>This manuscript describes a connection between Potts models and attention as implemented in modern transformers. The connection between the Potts model and attention described in this paper should be obvious to those who already understand attention models and Potts models and the empirical results of the factored attention model don’t make this approach seem compelling. In the discussion, the authors make several broad future speculations. Maybe factored attention could be promising for better capturing dependencies between positions for deeper transformers on MSAs, but it isn’t likely that this work will be of broad interest to the machine learning community. This is equivalent to learned position embeddings as in BERT which is worth mentioning. Present a compelling use case for the factored attention model. There are a number of interesting pieces but the final picture of an improved protein model is not fully resolved.<BRK>Recently, some researchers tried to apply attention models into the protein field, using self supervised learning to predict protein contacts. This is an interesting work. Cons:1.The analog between the simplified attention model and the Potts model is intuitive but not rigorous. The authors claim that they provide a theoretical connection between the two models. 2.There are two assumptions in this work, which make the simplified model different from the attention models that the previous researchers used. Firstly, they train the model on multiple sequence alignment instead of the raw sequences. If they train the model on the raw sequences, the performance is unacceptable, as shown in Figure 16, which is consistent with the previous research. Then why we want to use the more expensive attention model?<BRK>Summary:This paper explores the connection between the classic Potts model based approaches and modern Transformer based approaches for protein contact map prediction. Pros:  The paper attempts to connect classic and modern approaches to protein contact map prediction, which might be interesting to the people working in this field. At multiple places in the paper, the authors give the impression that MRF models are close to state of the art. The paper is well written.<BRK>This paper draws parallels between Transformers and Potts models (fully connected pairwise MRF) the current standard approach for protein contact prediction and shows empirically that Transformers are competitive with Potts models. Understanding the differences and similarities between Transformers and Potts models makes Transformers less of a ‘black box’ and helps to establish them as a principled method for contact prediction. The paper is clearly written and the evaluation is solid. 3.You describe in section A.3 how you extracted protein contact maps from the attention maps of ProtBERT. 6.Section 4, first paragraph: The L of the precision at L metric is not the sequence length but the number of top sequences.
Reject. rating score: 4. rating score: 4. rating score: 5. rating score: 5. <BRK>3.In 2.1, the authors reasoned that it seems to them impossible to use actual human data, thus the paper is based on simulation. The idea and results presenting are indeed very interesting and potentially promising, but I found it is to be impractical and based on lots of assumptions that easily fail. I elaborate on this below.<BRK>This can be bad for many reasons including counterfactual inference for what the person would do as well as inference for welfare (which the authors do not discuss, more on this later). The overall point here is both one of being able to predict behavior and one of welfare.<BRK>For example, when mentioning Bayes  rule in low dimensional settings, it could be explained more clearly how the likelihood looks like, and how the specific irrationality model affects the likelihood (through the optimal irrational policy I assume?). The authors provide a metric to evaluate the quality of the inference procedure. Originality and SignificanceThe question, the paper asks, is original but the significance is limited. Quality and DetailsI am on the verge when assessing the quality of this work.<BRK>The authors also acknowledge that it is hard to infer irrationality directly from human data and real human data may contain multiple types of irrationality; it is important to discuss how this influences the contribution of this paper along with results from the above point demonstrating the effect of assuming wrong or incomplete irrationality when multiple irrationalities exist. The authors also demonstrate that knowing the general/approximate type of irrationality, instead of knowing the exact irrationality model, might be enough to improve reward inference.
Reject. rating score: 5. rating score: 5. rating score: 5. rating score: 6. <BRK>The paper proposes a model based posterior sampling algorithms with regret guarantees when the model is assumed to be drawn from a distribution randomly. The authors also provide numerical evaluations of the proposed method. The contribution of this work as theoretical work is limited.<BRK>Also, the paper shows how the proposed approach can be implemented in practice using model predictive control. However, the sampled MDP is continuous, and solving a continuous MDP is hard in general.<BRK>This paper studies the model based reinforcement learning. They propose a posterior sampling algorithm and provide a Bayesian regret guarantee. The dependence that regret is in polynomial of H is a nice property.<BRK>ReviewThis paper proposes a new model based reinforcement learning algorithm named MPC PSRL. Theoretically, the authors provide regret analysis of the proposed algorithm. However, it is not clear that the main contribution of this paper. Osband & Van Roy (2014) already provides the algorithm posterior sampling RL algorithm, named PSRL for continuous domain.
Reject. rating score: 3. rating score: 4. rating score: 5. rating score: 5. <BRK>While the general idea is appealing, several of the motivating claims are vague and the way these ideas are implemented (e.g.via classification to enforce similarity between vectors) are questionable. For example: on pg 2 they mention that "[sparse representations learning] were not proposed to be jointly optimized with other objectives". There is a large body of work that the authors seem to ignore. Weaknesses:  The idea of matching the representations for two different data in order to enforce some similarity (or in this context,  knowledge transfer ) has been extensively used.<BRK>The proposed distillation method is validated through several experiments. This paper proposes a way of utilizing sparse representation for knowledge distillation. The main idea of sparse representation matching (SRM) is the combination of sparse representation and knowledge distillation. In other words, All CNN is not proper to compare with other distillation methods. As a result, the performance gap between the baseline and the proposed method seems to have greatly inflated. A great gap against baseline (about 10%~20%) seems to be very different from the results of the other distillation papers.<BRK>It is not clear why, and the results may be different if initialized networks are used. Detailed comments: 	Page 2: “However, we argue that the intermediate feature maps by themselves are not a good representation of the knowledge encoded in the teacher to teach the students” – This sentence states a claim, which is a main claim of this paper. However, two drawbacks:o	Why are the student networks randomly initialized and not initialized with their trained ImageNet weights (at least for ResNet 18 weights are publically available, I believe).<BRK>One is at the pixel level, and the other is at the image level, which pools the coefficients from the pixel level. The idea of using sparse representation is novel and interesting. 2.The clarity is good and easy to follow. 2.The experiment part has a large room to be improved. The paper has some arguments in the introduction, but they are not convincing since the coefficients can be the same even though the dictionaries between teacher and student are very different.
Reject. rating score: 4. rating score: 5. rating score: 6. <BRK>The goal is to develop few shot adapting personalized frameworks for remote physiological measurements with limited data. Positives:The authors present their application domain and purpose very clearly, and the related work from literature are nicely explored. MetaPhys is (as far as it is presented in the paper) a framework on using model agnostic meta learning (MAML) on physiological measurement data in unsupervised subject adaptation settings. However these analyses are only addressed superficially (not described or analyzed in detail) at the end of the paper. Hence I would be questioning these as "contributions" of this paper.<BRK>Strengths  Few shot learning could be beneficial for remote patient monitoring applications. Experimental results on all three datasets are positive, with around 40% 50% MAE improvement compared to the baseline. Weaknesses   This is a straightforward application of meta learning / few shot learning to a specific domain. Other than showing that meta learning / few shot learning is useful in yet another particular application, it is not clear how is this work advancing machine learning. Some explanations could have been more specific. For example, it is not clear how exactly are ‘pseudo labels’ generated and how do they compare with the ground truth labels. Summary of the rating:While the experimental results are positive, there is little methodological novelty and the experimental results do not seem to provide some interesting insights that might spur machine learning research in some novel directions.<BRK>The authors propose a system called MetaPhys for personalized remote physiological sensing from videos. Their system combines a pre trained CNN with an existing meta learning method (MAML). Performance evaluation of their methods on benchmark datasets show their model significantly outperforms SOTA methods using multiple metrics as well as for different skin types. They further show that the unsupervised model achieves comparable results to the supervised model. This is an interesting paper, with extensive evaluation that demonstrates their model’s superior performance  to SOTA methods. The technical novelties of their model, however, are incremental since it is primarily built the existing models : a pre trained CNN, an existing meta learning method (MAML), and use of the POS method to generate pseudo labels for unsupervised learning.
Accept (Spotlight). rating score: 8. rating score: 8. rating score: 8. rating score: 6. <BRK>This paper presents experiments for acquiring words via fast mapping in an embodied environment. The technical contribution is interesting and solid, but the experiments fail to address some important questions that are yet scoped by the claims of the paper (namely, that learning is being done  both  fast and slow, as per the title). What does embodiment get us for the hypothesis that a network can be trained to do fast mapping? What challenges does embodiment present that are missing from that paradigm? 4.3 is cool but doesn t  quite  do the fast mapping test we d like to see, which is that a word used consistently applies consistently. That would look more like including a few object categories whose names are used "correctly" across episodes, rather than one of the random names that are used inconsistently.<BRK>This paper considers the problem of fast mapping: in a 3d environment, learning to explore an environment to learn a (new) mapping between objects and names, and then during a new phase, learning to pick up objects by their name ( pick up a dax ). Strengths:* The paper defines the task and situates it within the context of a 3d environment, which could be useful for future work to build off of. These generalization experiments provide us some insight as to what models learn in this setup. Weaknesses: At least to this reviewer, there are no major weaknesses (except perhaps that the dataset is a bit toy, which isn t a concern to me). However...* It s not quite clear how much the memory bank is learning as opposed to the LSTM hidden state (which is also something mentioned in the supplemental). It would be interesting to learn a probe to measure when the model learns to assign an object to a name (if this is indeed something measurable by the memory bank alone).<BRK># Overall reviewThe authors use a 3D world to explore grounded language learning, in which an agent uses RL to combine novel word learning with stably acquired meanings to successfully identify and manipulate objects. The results should be of interest to many working in grounded language / multimodal representation learning, and the experiments are thorough and well motivated. Pros:* Interesting environment for combining fast mapping with stable language learning in a grounded task. * Novel memory architecture, shown to improve memory efficiency. * Fast category extension: these results show that if the agents are trained to pick up different exemplars of a category, they can do so in testing. During training, there was one exemplar from each of three distinct categories. # UpdateI thank the authors for their thoughtful reply and for incorporating the feedback. The additional information is most welcome, and so I maintain my score of 8 for the paper.<BRK>**Summary:** An agent following instructions in a grounded world is a core task in AI. This paper studies agent that accomplish this using memory based architecture. Results are presented in a simple 3D domain containing several objects randomly sampled each time from a set of 30 objects. 3.How do "independent read heads" work? Interaction proceeds in episodes where each episode contains a discovery phase where the agent learns the phrase associated with each object, and an instruction phase where the agent solves a given instruction. **Strengths:**  The proposed architecture learns a near optimal policy, requires few memory slots than baseline and can generalize to novel object names (fast mapping). It is argued that the language and observation key value embedding can be used to compute a simple intrinsic reward that incentivizes reaching states (language+vision) whose distance in the embedding space is far from what has been encountered. 5.Does the background of the environment change across episodes (e.g., layout, colour of the wall, etc.) **Weakness:**  The paper only investigates generalization of object names. This is kind of a low hanging fruit for generalization in language space.
Accept (Poster). rating score: 8. rating score: 7. rating score: 7. rating score: 6. rating score: 6. <BRK>SummaryThe paper presents a method for tackling multi domain few shot image classification problem where it obtains a task adapted representation by weighing representations from pretrained domain specific backbones according to the support set at hand. Strengths  The paper is well written  The hypothesis is clearly conveyed, tested and is interpretable as seen from the attention weights  The model improves over the results of the past works that were based on conditioning backbones using FiLM layers   CNAPs [2], Simple CNAPS [3]. Is there a way to try mixing domains in a task? Selecting relevant features from a universal representation for few shot classification.<BRK>MotivationThe paper is well motivated. It reviews past work in meta learning as well as universal representations and transformers. It essentially sets up an optimization across multiple domain specific backbones to solve the multi task problem. ResultsThe results are convincing. They are carried out on datasets well accepted by the community. Quality, Clarity, Originality and SignificanceClarity   The paper is extremely well written. Those can be easily removed with a single editing pass. I would say the clarity of the paper is high.<BRK>## SummaryThe paper addresses the problem of multi domain few shot image classification (where unseen classes and examples come from diverse data sources), and proposes a Universal Representation Transformer (URT) layer, which learns to transform a universal representation into task adapted representations. The method proposed builds on top of SUR [Dvornik et al 2020], where a universal representation is extracted from the outputs of a collection of pre trained and domain specific backbones and a selection procedure infers how to weight each backbone for a given task at hand. The paper is also very well written and very well presented.<BRK>Summary Few shot learning on meta dataset is challenging due to the domain gap between train and validation. This combination is done with a transformer model that pays attention to the features extracted from domain specific backbones. * The text is well written and easy to follow. if it is, could you include this information in the paper for completeness? * Given that the proposed model is an efficient version of SUR, there are some questions that naturally come to the reader that are not answered in the current submission. For instance, how do the attention coefficients of URT compare with the coefficients learned by URT? What is the difference in training time? I also asked for a clarification on the efficiency of the method. Overall, even though I still think that this work lies in the application side, it is interesting enough to be published at ICLR, so I have accordingly raised my score.<BRK>Summary: The paper proposes a meta trained Universal Representation Transformer (URT) layer, which learn to dynamically re weight domain specific representation for classifying given target images. Weakness:   The idea of mixing pre trained representation to a universal representation was first proposed in SUR. While  this work shows good improvement over SUR with a learnable URT, I argue this work does not have sufficient theoretical or algorithmic contributions. The author is probably refer to the few shot domain generalization where the target class are from an unseen domain. Then, for test domain Birds and Textures, both assign high score to ILSVRC.
Reject. rating score: 5. rating score: 5. rating score: 5. rating score: 8. <BRK>Summary: This paper is an empirical investigation into the role of architecture and objective choices in Neural Process (NP) models when the amount of conditioning data is limited. Cons: There are some assumptions/statements that would be beneficial to elaborate upon in the paper. It would be helpful to formally define “posterior contraction” for the reader   is it referring to a reduction in posterior uncertainty?<BRK>6?Summary:I think the focus on the diversity of posterior samples is very interesting and highlights some important property of these kinds of models. For the paper to provide a clear value for the community, I think it would be good to extend the experiments to cover the whole combination space of {NP, ANP, SIVI} x {mean pooling, max pooling, mean+max pooling}, so that it becomes clearer what the influence of the different design choices are in combination with each other. Could these ideas (SIVI, max pooling) also be combined with more modern NP architectures (like Attentive NPs or Convolutional NPs)?<BRK>* I am not familiar with SIVI and I don t expect the average reader to be either. The low data regime is arguably where neural processes are most interesting, and in that regard the paper is right to turn to this setting. As it stands the paper feels a bit incomplete in this regard. I d appreciate some discussion on the choice of using it.<BRK>This paper proposes an improvement of the standard NP by using a mixture distribution \q_{\phi}, semi implicit variational inference, and max pooling to capture the multimodel structure of the posterior distribution. So, the improvement is weak although it is shown to be effective by the empirical study. More importantly, the authors have investigated the posterior contraction of NP. It is interesting. It is a classical property in Bayesian and this link will enable further theoretical analysis for NP.
Accept (Spotlight). rating score: 7. rating score: 6. rating score: 6. rating score: 6. <BRK>Clarity: 			a. 5.Similarly, it should be noted that there is an explicit limitation of this challenge task and the evaluation metrics to scenarios where the entire success or failure of the approach is purely based on the final team accomplishment. Many real world scenarios for human AI collaboration, differ by also requiring individual collaborators to do well or for the primary human user to do better with collaboration (than without). For clarity, it would be valuable to refer to the presence of this validation experiment in the main paper.<BRK>In fact the paper should be reduced in length, focusing on the AI related issues and selections adopted and how they were implemented, with the detailed platform description be included in the supplementary, if possible, or in a url where this information could be reached. The paper targets to demonstrate social perception and human AI collaboration in common household activities. This is an interesting set up which can model some aspects of human behavior.<BRK>Is it social intelligence for human robotics? So perhaps with that in mind this paper may be more suited to a conference like AAMAS (Autonomous Agents and Multi Agent Systems) rather than ICLR which is more machine learning focused. I really like Figure 6a.This is an excellent chart except for the y axis removal of sections between 0.6 and 1.6 and 1.8 and 2.0. I think this is misleading. Perhaps a log scale would be appropriate.<BRK>This paper introduces an AI challenge where an AI agent needs to collaborate with a human like agent to enable it to achieve the goal quicker. I think there are other contributions in the paper, but they are all presented in terms of such a challenge and the virtual environment. This work tackles problems related to human AI collaboration, a topic that poses several great challenges for the AI community, and other scientific communities too.
Reject. rating score: 3. rating score: 4. rating score: 5. <BRK>This paper presents a new method to defend black box attack based on an ensemble of sign activation neural networks. However, I have many concerns regarding this paper: The paper organization is very weird and confusing. I think the authors should evaluate a set of attack methods instead of only one method otherwise the results are not convincing to me.<BRK>This paper proposes training an ensemble of binary neural networks with sign activations using gradient free stochastic coordinate descent algorithm. The nature of the training method and binary networks leads to robust models with non transferable attacks. *Cons*:  It looks like the method is not scalable. There are multiple black box attacks that exploit different aspects of model vulnerabilities.<BRK>The paper proposes an architecture (ensemble of networks) aiming at being robust against black box attacks, based on the idea that crafting an adversarial example able to fool enough individual networks such that the majority vote changes is a more difficult task.
Reject. rating score: 4. rating score: 5. rating score: 5. rating score: 5. <BRK>It seems more accurate to refer to the proposed approach as one for counterfactual inference with confounders in the irregularly spaced time series rather than continuous time setting. For the first experiment on tumor growth simulation data, for which samples can be obtained at regularly sampled times it is not clear that 1) the linear interpolation for RMSN and TSD RMSN is necessary (please clarify, because the details of the experiment are quite vague) and 2) assuming samples are regular, why would the proposed approach be better than TSD RMSN?<BRK>2.The paper argues that we should consider the continuous case interventions. However, in the applications/experiments, it appears the interventions and problem settings are all discrete time spaced. I would assume to learn the effect of a continuous time process $a_t$, we need much more parameters, however, the paper said we use similar parameters, so would love to know more the size of the parameters.<BRK>For this, they employ neural ODEs as a latent time series model, which they augment with additional latent variables. Analysis of the method is lacking. I would have liked to see some discussion on auxiliary variables vs. non Markovian neuralODEs (f(x,u,t) vs f(x_1,…x_t,t))?. 3) Further, the results in fig. The curves are all close together, and it would be interesting to see whether the improvement is significant.<BRK>This is done by modelling interventions in continuous time with differential equations augmented by auxiliary confounding variables to reduce bias. + The proposed method is naturally constructed upon the neural ODE framework by additionally modelling the effects of incoming interventions. From the experimental results, it seems that the proposed method does not have an obvious advantage in comparison with the baselines.
Reject. rating score: 5. rating score: 6. rating score: 6. rating score: 7. <BRK>The paper proposes a Bayesian based approach to early prune parameters, which are predicted to have low saliency/importance, with the goal of accelerating the training of deep neural networks. The writing quality and clarity of this paper is OK, but I recommend the authors to include a table of mathematical notations. However, this paper only compares with some methods (e.g.SNIP and GraSP) which prune parameters before training, inspired by the lottery ticket hypothesis paper. Lots of pruning methods can prune parameters during training before the lottery ticket hypothesis paper appears. The paper should compare with those more superior methods, regardless of the fact that in Table 3 it is unclear if BEP can outperform SNIP/GraSP or not under the same "Time". Moreover, the method introduces new hyperparameters.<BRK>The proposed approach is novel in the sense that the vast amount of prior work on pruning has focused on either (i) pruning on network initialization (e.g., SNIP, etc.) or (ii) pruning after the network has been fully trained (e.g., Magnitude Pruning, among many others). Rationale for Score	As far as the negatives go, I take issue with the fact that the proposed approach seems to be highly complicated   requiring a multitude of hyper parameters/design choices, and tuning functions/ablation studies (e.g., for lambda). 2.The presented experimental results are not very compelling. Clarity   The paper is reasonably well written and organized overall.<BRK>This paper introduces a new method to accelerate training by saliency based pruning. The reviewer believes that the proposed method is novel, as it considers historical statistics during training to provide a more accurate saliency prediction. * What is the overhead introduced by the maximum likelihood estimation of the MOGP, and Bayesian early pruning (BEP, Algorithm 1)? * How accurate is the saliency prediction? If this is not accurate that one may expect the components in the pruning procedure can be replaced with simpler variants without a detrimental impact. Additionally, experiments should not only have averaged results but also provide standard deviations.<BRK>This paper presents a training time pruning method for deep neural network. The main idea is to include network elements only if they could improve the predictive mean of the saliency, where the saliency measures the efficiency of the network elements in terms of minimizing the loss function. The paper presents a clear objective of the early pruning, which is to preserve the sub network that can maximize the saliency function. This optimization problem is NP hard, and even approximation is very expensive. The authors proves that one can simplify the approximation by ranking the network element by predictive mean of the saliency function.
Accept (Poster). rating score: 7. rating score: 7. rating score: 6. rating score: 5. <BRK>[Overview]In this paper, the authors tackle the problem of unsupervised visual representation learning by combining the deep clustering technique and momentum contrastive learning. The authors presented a well written paper with some theoretical explanations and extensive experiments. It is easy to follow with some good insights in the words. 2.The proposed PCL is formulated as an EM procedure. Compared with MoCo, its main counterpart, PCL achieve substantial improvement over it across the board. Introducing a EM like procedure to mitigate the reinitialization issue has been proposed in previous works.<BRK>A EM like algorithm is introduce to train the model with latent variables. ### Strengths+ The paper is nicely written and provides a nice read. + The paper is well motivated and points to valid issue with current issues with (unsupervised) contrastive learning approaches. The qualitative visualization from the appendix are very nice and it is unfortunate that they cannot go on main manuscript. ### CommentsAlthough I generally like the paper, i dont understand why the authors did not include most recent methods in the experimental results (eg.BYOL, SwAV, etc). I don t particularly mind the fact that SwAV outperform the proposed approach (the fact the method is built on the top of MoCo AND outperforms it is not bad).<BRK>The paper mainly injects clustering into the instance discrimination work, MoCo, for unsupervised representation learning. From the arXiv upload time, this is an improved version of the manuscript submitted for NeurIPS 2020. Nevertheless, I would still give an acceptance rating as:1) This is solid work, I have personally implemented this work and the observation is quite consistent with what s described in the paper, at least for some major experiments. 2) Doing an EM style clustering with momentum encoder/key is indeed something that has never been explored to my knowledge, and there is some novelty to it. 3) It also proposes a prototype specific T, which is quite new and well motivated.<BRK>##########################################################################Summary:The paper proposes a new self supervised representation learning loss which is a combination of InfoNCE and EM clustering. The evaluated approach ProtoNCE actually consists of a sum of the standard InfoNCE loss and the proposed novel loss. I couldn t find other numbers in MoCo v2 paper https://arxiv.org/pdf/2003.04297.pdf surpassed by the numbers in this paper   although both papers evaluate on detection as well. Other numbers from MoCo v2 paper are not directly compared against. 2.It does not follow from the title/abstract/introduction that the evaluated approach is actually a sum of the standard InfoNCE and the novel loss. This is a clarity issue which needs to be addressed. I don t have an issue with this assumption, but it needs to be directly articulated early on. ##########################################################################Questions during rebuttal period: Please address and clarify the cons above. it is quite confusing to talk about semantic structures here, without clarifying what is meant by that.
Reject. rating score: 6. rating score: 6. rating score: 7. rating score: 8. <BRK>Here, the authors are using an ad hoc arithmetic language upon which deduction, induction, and abduction tasks are defined. With the aim of learning inductive biases for deep neural net architectures, this paper presents three synthetic experiments for learning primitive forms of mathematical reasoning in theorem provers. Overall, I think that the idea of training a learner with primitive forms of inference is interesting for improving its performance in mathematical reasoning. The experimental results corroborate the relevance of this approach.<BRK>Specifically, this paper design three synthetic tasks to teach the transformer model to first learn three primitive reasoning steps: deduction, abduction, and induction respectively. The three datasets are used in the pre training step, and the paper shows empirical performance gains on large mathematical reasoning tasks in the context of automated theorem proving. How the positive transfer from the proposed pretraining change as we increase the size of the model? The word “inductive bias” may have different connotations across different contexts and it would be nice for the authors to give a more formal definition of what they mean by “inductive bias” in the context of this work. For example, inductive bias may refer to the structural bias imposed by the architecture.<BRK>## SummaryThe authors propose LIME: a pretraining strategy for learning inductive biases for mathematical reasoning. They construct 3 synthetic datasets corresponding to 3 basic reasoning patterns: deduction, induction, and abduction. Technically, I believe it s correct. Then, any pretraining is "learning inductive bias." The authors do not have to address them in the rebuttal.<BRK>In this work, the authors introduce a method called LIME for imparting certain mathematical inductive biases into a model. The structure of the approach is to first pretrain the model on synthetic tasks that are designed around three principles of mathematical reasoning: deduction, induction, and abduction. Page 3 says “the BERT pretraining objective,” which suggests that BERT is the objective. Is that correct? Again, this is not a huge problem, but I think it at least deserves some discussion.
Accept (Poster). rating score: 8. rating score: 6. rating score: 6. rating score: 6. <BRK>The knowledge distillation (KD) approach is a two step procedure: first train the teacher model on the labeled data and then train the student model using the predicted class probabilities from the teacher model. Although Theorem 1 is useful, it does not directly address these key questions. Would this be possible for KD? However, classical semiparametric inference largely exploits the finite or low dimensionality of the target parameter to answer the questions mentioned above. 2) There is a notable difference between the settings of KD and classical semiparametric inference: the target parameter $f_0$ in KD is ancillary in the sense that the data generating model depends only on the nuisance parameter $p_0$. Nevertheless, the paper is a nice contribution, and I will keep my rating.<BRK>The paper under review presents a new approach to the theoretical understanding of konowledge distillation, an efficient technique that makes possible the training of small models given the predictions of a more complex model. Based on this re interpretation of the problem, a nice analysis of vanilla knowledge distillation is given and a correction is derived from a Taylor expansion of the loss which is then carefully investigated as well. All the proofs seem correct. In particular a more detailed presentation of the semi supervised learning background should be given in order to better substantiate the subsequent findings of this paper.<BRK>This paper formulates knowledge distillation as a semi parametric inference problem. That being said, there is still some room of improving the clarity of the paper. It would be better if this paper could provide a clearer justification of the bias and variance terms by the theoretical analysis. C3: The hyper parameter \alpha is introduced to balance the bias and variance of knowledge distillation in the experiment section.<BRK>### Strengths+ The semi parametric perspective and the attempt to have a bias variance tradeoff as part of the knowledge distillation procedure are well motivated and insightful. ### Weaknesses  The paper lacks clarity in notation and presentation. Namely, Theorem 4 requires a upper bound on the critical radii of *uniformly over all* classes that could be empirically selected by the teacher. But this is not the comparison that the paper makes when it expounds the advantage of the new bound. The only exception seems in the Adult Dataset, but only when the teacher has *low* complexity, which is also not part of the message of the paper.
Reject. rating score: 3. rating score: 5. rating score: 6. rating score: 7. <BRK>The paper introduces yet another testing environment for RL, PettingZoo, as a logic evolution of OpenAI but, this time, for multi agent RL systems, a feature not supported by the former. I think sections 2 4 could have been compressed to one section introducing PettingZoo, after which an actual research contribution using the system could follow, to create a proper research paper. •	Relevance/Novelty: I also wonder whether the paper really represents a significant advance in the AI field, and I guess its relevance may be below the ECLR threshold. Sections 2 to 4 gives an abstract overview of the main mechanisms in the API, environments and the interaction with the user, but all basically at the level of interaction at code level.<BRK>These are all positives. Your introduction should focus on introducing PettingZoo, describing the scope of the paper, an overview of the contributions made and giving the reader an overview of what is coming in the rest of the paper. You mention RLlib as an alternative multi agent RL library but you don t go into detail. This potentially could be your development methodology. However what is missing is evaluation criteria. #### APII feel you should have gone into more detail into your MARL API. A design or block diagram showing the architecture of #### EnvironmentsIts good to see a comprehensive set of environments available with the PettingZoo library.<BRK>2) Framework looks easy to use, familiar API for Gym users. 4) Nice detailed listing of all hyperparameters in Appendix for the included baseline results. Weak Points The paper could use a more thorough review of existing work and a more detailed comparison to make a stronger case for this framework satisfying a need in the research community that is not already satisfied. Many of these have different focuses, but may still be worth comparing to them. Now Figure 1 has "obs" but Figure 2 has "observation".<BRK>The paper introduces an API and library for multi agent reinforcement learning along with simple installation of a very diverse set of environments along. OpenAI Gym is overemphasized, and PettingZoo is underemphasized. (I realize that these are not the main points of the paper.) The paper states it has been shown that AECs are equivalent to POSGs but there is no reference. Paper could be polished more in general.
Reject. rating score: 4. rating score: 5. rating score: 6. rating score: 7. <BRK>Summary:The Bayesian Confidence Propagating Neural Network has recently been extended to the case of unsupervised learning (Ravichandran et al., IJCNN, 2020). This paper compares this extension to restricted Boltzmann machines, autoencoders, and a biologically plausible model proposed by (Krotov & Hopfield, PNAS, 2019) on the MNIST dataset. For evaluation the authors consider the learned receptive fields and the classification performance of a linear classifier. The paper is very similar to (Ravichandran et al., IJCNN, 2020) but with an extended experimental section. Positives:+ Biologically plausible methods for unsupervised learning are an interesting area of research. + There has been relatively little research on structural plasticity. Concerns:  The paper does not introduce anything new but merely compares existing methods. The comparison is not an extensive study, but limited to one dataset, MNIST, and few alternative proposals, of which only the KH model is deemed  brain like . There seems to be something off with the experimental results. Krotov & Hopfield report a better test accuracy of 98.54% in their original paper in spite of using less hidden units. BCPNN s performance is mediocre. It is even outperformed by random shallow networks with fixed, localized, random & random Gabor filters in the hidden layer (Illing et al, Neural Networks, 2019)  Lacking performance could be excused by greater biological plausibility as a neuroscientific contribution, which is however not the case here. Is it any more biological than the well known distributed and local representations?<BRK>Summary:This paper proposes a set of biologically plausible update rules that can be used to compute latent representations. Based on the additional experiments, I will be increasing my score to 5. The model is compared to RBMs, auto encoders and another recently proposed biologically plausible model. However, given that the main contribution of the paper is a comparative study, the paper can add value by doing a more thorough comparison against variants of AEs and RBMs that have otherwise similar properties (such as keeping the HC MC (softmax) architecture and sparsity levels the same). The experimental results include an interesting comparison of the learned features. Yet, the underlying probabilistic graphical model is not clearly described. It would be useful to have a figure in the paper that describes the model, including whether the model is directed or undirected, which variables are observed or unobserved, if the model is directed what is the generative model, etc. The paper does not include an explanation about why these assumptions make sense, or how they influence the model s inductive bias relative to other probabilistic models. In particular, I could not understand how to parse Eq (4) which has a sum over \\(x_i\\). The paper makes some very general assertions that do not add to the point being made. Therefore, instead of comparing to RBMs and AEs with sigmoid units, comparisons to RBMs and AEs with softmax hidden units will be more relevant. Overall, the paper can be improved along two directions : making the probabilistic interpretation more clear (specifying the graphical model clearly, deriving update rules), and doing experiments with softmax hidden units (so that the only thing changing is the update rules, and not the model architecture). Thank you for adding that. The additional experiments are also appreciated. This seems to indicate that it s not just the architecture but the learning rules that make the BCPNN model work well. It seems that the RBMs and AEs were not trained using any sparsity penalty. Higher sparsity makes the features look more like stroke like and localized, and less spread out all over the visual field (as they do in Fig 4, C and D).<BRK>This paper evaluated four unsupervised learning approaches (BCPNN, KH, RBM, AE) by training a supervised classification layer on top of the hidden representation. Specifically, the authors qualitatively compared the receptive fields and quantitatively compared the classification performance across four models. The authors emphasized the advantages of BCPNN since it applies biologically plausible local learning rules and requires fewer epochs for convergence. The description of the BCPNN model in section 3 was clear and comprehensive. But the authors did not provide sufficient details of key mechanisms in the other three models (especially the KH model, which also used brain like learning rules). The detailed introduction of the other three models should be an important component since this is a comparative study. The results were clearly stated, but the insignificant difference in the classification accuracy comparison (Table 2) can hardly lead to a reliable conclusion about which unsupervised method is better. In the original KH model (Krovtov & Hopfield, 2019), they also tested the classification accuracy on the MNIST dataset, and the result reached an error rate of 1.46% with 2000 hidden states. This is better than the reproduced result shown here (97.39% accuracy) with 3000 hidden states. Then is it fair to say that "BCPNN outperforms KH"? 2.Could you provide more explanation on Eq.11?Why this dynamic update of $k_{\beta}$ could be used as a desired bias regulation? 3.When the number of total hidden units is fixed, what would be the effect of changing the ratio $\frac{N_{MC}}{N_{HC}}$ in BCPNN? 4.The "hybrid" structure of BCPNN provides interesting receptive field results in Fig.3A.Is this structure generalizable to a model with multiple hidden layers? ##### Minor:Page 4. in 3.1 Bias regulation. Typo: Eq.6 should be Eq.11.And "the value of gain $k_{\beta}$ at around 1 when $P(y_j)   \frac{p_{MaxEnt}}{4}$," missing $\gg 0$ here?<BRK>Systematic investigation of biologically inspired algorithms and architectures is a very important research topic. The paper investigates Bayesian Confidence Propagating Neural Networks (BCPNN) on learning unsupervised representations on MNIST dataset. It presents a comprehensive comparison of four different commonly used unsupervised methods. The strong merit of BCPNN approach is the nice receptive fields of the hypercolumns (HC) and minicolumns (MC) learned by the proposed algorithm. Also, those filters look much cleaner than the counterparts of the classical algorithms considered in figure 3. Additionally, the authors demonstrate that their representations stand in line with previously published proposals in terms of the classification accuracy. The main weakness of this work is that the proposed method has only been investigated on MNIST and in one layer architectures. At the same time, given the novelty of the approach, I think it deserves attention even in this simplest setting considered in this manuscript. Small comments: 1. There are some misprints around equation 11, such as the use of p(y) vs. P(y) is inconsistent. 3.I also find panel B in figure 2 confusing. The way it is presented makes it look like the authors have combined the outputs of four networks to feed into the classifier, while in reality those four networks were evaluated one by one. 4.It would be better to designate a new variable for the left hand side of equation 12, since I_ij is already taken. I have read the discussion with other reviewers. A small comment: while I agree that it is reasonable to keep the classifier the same for all the models (softmax with cross entropy) for a fair comparison, I disagree that the activation function for the first layer should be kept as ReLU in the KH model. In fact KH explain that this is a suboptimal choice in Fig.4 of their original paper.
Reject. rating score: 5. rating score: 7. rating score: 7. rating score: 8. <BRK>############################################################################# SummaryThe paper proposes a Bayesian formulation of multi task learning in the classification scenario. The inter dependency of the tasks is enforced through Gumbbel softmax priors, with the weights learned from the data. The methodology is not formulated in a proper Bayesian way as claimed. ############################################################################# Pros 1) The use of the Bayesian approach is well motivated for the case of limited data. 3) The related work section is quite thorough. There is no clear separation between the model and the inference scheme. Specifically, the prior for one task is dependent on the variational posterior for the other tasks and is learned, hence it is not a proper prior. 3) The experiments do not have any error bars, only one value (i.e.one run).Thus the experiments only weakly support the claims and are not statistically significant. Is this an illustration of the model or the inference? Supplement: Eq.(15): The very last term should have expectation over both q(w) and q(z|x)Supplement B.3: "the prior of current takes is "  > "the prior of current tasks is"<BRK>Summary:This paper presents a variational framework for multi task learning, where for each image classification task, the feature representation z and the classification parameter w of the other tasks serve as the prior for the counterparts of the task. The priors are engaged as the regulariser in the variational inference framework, making the features and the classification parameters of the tasks close to each other. The proposed method outperforms several baselines on several image classification benchmark datasets. Pros:  The proposed way of using a Bayesian (variational) approach where the feature and classification parameters of other tasks are leveraged as priors is quite intuitive. As priors naturally serve as regularisations in Bayesian inference, it is natural and might be a principled way of sharing information across multiple tasks. The formulation of multi task learning with the Bayesian framework might not be completely new in the conventional Bayesian settings, but it seems to be new in the auto encoding/amortized variational inference settings. Although I m not an expert in the domain of multi task learning, I didn t find a similar point of view in the literature. I also think the proposal of the prior construction in Eq (9) and Eq (11) is interesting and clever, which is conducted by weighting the posteriors of the other tasks. To me, the components form an elegant model. The paper is easy to follow. I also feel that the datasets used in the paper are a bit small scaled in today s machine learning/deep learning areas. The authors have addressed my previous concerns. Therefore, I would recommend an acceptance of the paper.<BRK>This paper presents a variational based approach for multi task learning for the setting where there is a limited amount of training data and for each related task. Prior distributions on model parameters are defined using weighted approximate posterior distributions of other related tasks. This has been done both for classification parameter (w) and latent features (z). The classifier parameters are learned with amortized inference to make posterior inference more effective. The idea of using a variational approach for multi task learning is novel and interesting. Using posterior distributions of related tasks in a weighted sum for defining prior distributions for parameters of each task is an interesting method that could also inspire more future work in this area. Proposing this model for multi task learning problems in which there are limited input samples for each of the related tasks is a problem that has not been explored enough before in the literature despite its importance for many domain applications. Focusing on this problem is an aspect that makes this paper have a high impact on the field. The experimental results show a clear improvement of VMTL over the current baselines. I liked that the authors have analyzed the importance of each of the pieces that are used in the model or in the learning algorithm such as the effectiveness of the Bayesian approximation and the choice of softmax for priors. There are a couple of typos (takes instead of task in section 2.1 and B.3),.. Aside from a brief mentioning of cost reduction due to amortized learning, I don t see any computational time analyst. I think it would be very helpful if the authors could provide more information regarding running times compared to the baselines. Other applications such as NLP related ones could be interesting too as many multi task learning approaches have been successfully applied there, and it would be nice to see how this could compare to those. Overall, in my opinion, this paper presents a novel approach for a significant problem, and the experimental results support the claims.<BRK>In this paper, the priors for each task are conditioned on other tasks to enable knowledge transfer. The authors test their method in the small data regime and show consistently better performance than the baselines. As your method works comparably to MRN and can be classified as MTL papers with clever priors, I would like to understand more the differences + pros/cons of techniques. [Decision]I think this is a strong paper. [Decision after reading rebuttal]The authors appropriately addressed my concerns. I recommend acceptance of this paper and would argue that its quality is reflect by an 8, up from my original score of 7. The decision to cast multi task learning as a variational inference problem where task relatedness is modelled through use of priors and posteriors is very interesting and well developed. Why is a Gumbel Softmax necessary to learn the mixing weights when updating the priors based on the other tasks? In cases where they may only be task interference such that tasks should not be shared, how is this dealt with? 4.I am a bit confused about how the amortised inference is performed. For a given task $t$, there are $C$ specific labels. 5.Whilst not the focus of the paper, how does the method perform when there is more data accessible? This might point that Bayesian MTL methods, with clever prior construction are the way to go.
Accept (Poster). rating score: 6. rating score: 6. rating score: 6. rating score: 6. <BRK>The paper s main idea revolves around projecting the output of a NN policy to be within the set of a Lyapunov function (stability condition) (e.g., exponential stability). I have some concerns which are listed below,1. What are the limitations of the approach for larger dimensional control problems, such as a 7 dof arm? It s unclear if the method can scale to large dimensional control problems. Additionally, explaining the horizontal lines that start before 0 would be useful. 3.While the results show stable behavior, the loss in the adversarial setting  doesn t improve during training.<BRK>The key contribution I can see is Section 4.3 on deriving differential projections. 2.In control community, adaptive dynamic programming is exactly to deal with the problem proposed in the paper. Frank Lewis and many others had done a lot of work in this field. In this paper, the authors propose a model free approach to learning the Lyapunov function and policy simultaneously. I think this paper is more general than this ICLR submission. While the proposed method must need a model and the theoretical result depends on the model parameters<BRK>Edit: upgrade the rating to 6 with the clarifications from the authors, with which the submission is clearer and more convincing now. Main concern:  The results presented in the experiments section is not very comprehensive and not convincing enough to justify the claimed benefits of the proposed approach v.s. Due to above concerns, and also note that the examples provided in this paper is very limited rather than comprehensive, it is not convincing enough to claim the overall performance benefits of the proposed approach over the traditional robust control techniques.<BRK>The control is assumed to be from a neural network that takes in the state. In general, I like the idea of enforcing stability by introducing the convex optimization layer. However, it would be better if trajectories can be visualized in some way to show that the proposed method can stabilize the system. Also, the experiments for PLDIs and H infinity control settings seem to be missing (only description of the setup is found). After author s response The response addressed most of my concerns and included experiments results of trajectory visualization and run time comparison. I think the paper would be an interesting contribution to the conference.
Accept (Poster). rating score: 7. rating score: 6. rating score: 4. <BRK>The paper proposes an ego centric representation that stores depth values and features at each pixel in a panorama. As such, it is hard to recommend acceptance. However, I am willing to increase my score if the information necessary to understand this part of the paper is provided. ### After rebuttal phase ###The answers provided by the authors and the revised version of the paper sufficiently address my concerns. I am still concerned that the experimental evaluation is very packed with multiple experiments while lacking details on the experimental setup and explanations of the baselines. Does it only use depth and no features? * I am confused by the statement "In contrast, ESM by design stores features in the memory with meaningful indexing.<BRK>The representation can be used for several downstream applications ranging from semantic segmentation as well as action learning for robotics applications. The formulation of the proposed approach builds on a Kalman Filter setting and encodes memory in a spherical structure. Both the formulation and design makes sense, and the downstream applications show that the proposed approach is indeed plausible. While I think the value of this paper is well justified, I do have a few comments on the paper:  I think it ll be worthwhile to elaborate on the "Mono" method, including the network architectures as well as training details and such. It will also help understand which part of ESMN improves over this baseline method. Could the authors elaborate on Sec 4.1.1 Fig 5?<BRK># Summary This paper presents a method to build an ego centric spatial memory map from an agent s viewpoint. # Pros + The idea of combining the ego centric representation seems interesting and novel. # Cons    *  _[Major]_ The Method section is very confusing. In all experiments, the experimental set up is not well defined. For example, in the first experiment, we find out that the authors use imitation learning only from the appendix. Do these approaches show the state of the art performance on the examined tasks? In the second experiment, the authors only use the LSTM network to train the RL agent. * _[Minor]_ The simple approach to directly quantize pixel projections leads to artifacts in the map.
Accept (Spotlight). rating score: 7. rating score: 7. rating score: 7. <BRK>The submission proposes to defend against model stealing attacks by dataset inference. However, there should be an extended discussion under which situations/models the method is feasible. this is a vital part of a paper and not an appendix.<BRK>This paper tackles a timely problem of detecting model stealing attacks. Pros: 	The proposed method of dataset inference for model stealing detection is novel. Adaptive attacks are considered in the paper as well. The paper is well written and easy to follow. Prada: protecting against dnn model stealing attacks.<BRK>What are the countermeasures against such a scenario? Would it still be a problem? Evaluation over multiple model stealing attacks is a great way to both show the robustness of the proposed attack method, as well as identify strong (fine tuned models) as well as weak (label query attacks) points of the algorithm. Insufficient literature review. Please search for some works on  property inference : the current problem can be posed as differentiating between the dataset suspected to be used, versus some other dataset. The term  dataset inference  is slightly misleading. Please address and clarify the cons above
Reject. rating score: 4. rating score: 4. rating score: 5. <BRK>I suspect that the generalization proposed in this work does not lead to include additional interesting losses. I have some concerns about the significance of the results. It is not even clear whether the theory applies to the setting used in the experiments. The assumptions require Lipschitz continuity of the Bregman divergence.<BRK>Obtaining these with single step of GD offer a challenge, but allowed multiple gradient calls, one can use them to compute x_{t 1}^* offline. A guarantee of the second kind is strictly stronger in that it implies the first (and not vice versa). + The work mentions that previous works required a Lipschitz constant bound. + Now, let us just focus on the first objective as this paper does.<BRK>*** post response ***Thanks for the clarifications and revisions in the manuscript. The fact that the Lipschitzness of the loss functions isn t assumed is also not necessarily a feature. Could the authors clarify? A3: This example makes sense (though in this example, the function is L Lipschitz, not L smooth).
Reject. rating score: 4. rating score: 5. rating score: 6. rating score: 8. <BRK>The paper seeks to address the problem of novelty detection under the circumstance of having high corruptions in the training data. To address this issue, a VAE based approach is adopted in this paper, with several modifications made to the vanilla VAE to promote the robustness of VAE in detecting outliers in the corruption circumstance. Among the modifications, the paper assumes the posterior is approximated by a two component Gaussian mixture distribution, with each having a low rank and full rank covariance matrix, respectively. The paper hopes that the posterior of inliers (normal data points) can be represented by the low rank covariance matrix, while that of outliers cannot. Another notable modification is that the Wasserstein 1 regularization is used to replace the KL regularization in the ELBO, which is claimed to be more suitable to the low rank modeling. 4.I also have some concerns over the choosing of hyper parameters. The paper sets the dimension of latent representation (d) to be 2, which, I think, is too small. Moreover, I also think that it is not a good idea to set the mixture coefficient $\eta$ to be a fixed value.<BRK>This study proposes a novel method that can work well even the training data is corrupted by partial data from the unknown domain. And its arguments and statistical assumptions are followed by mathematical proofs. Overall, it is an interesting approach and I believe it would give a good way to ML practitioners who are struggling with noisy datasets in real world applications. Is it possible to model the outliers as multi modal distribution such as MoG? This study aims to generate the model to be robust to corrupted training data. Additional Comments  The readability of Figure 2, 3 is not good. This study shows the superiority from four datasets (image, non image). However, there is more dataset widely used for novelty detection such as (Fashion) MNIST or MVTech. The authors may compare the method not only to the novelty detection methods, but many previous works which also aims to be robust to noisy data(or label) in the training process.<BRK>Compared with conventional VAE, this paper incorporates the following strategies to improve VAE for novelty detection from corrupted training data. 1) By considering that outliers tend to have more complex structures, this paper assumes inliers and ourliers lie on Gaussian distributions with different ranks, and proposes a Gaussian mixture model for posterior q(z|x) including two component: a low rank multivariate Gaussian distribution component for modeling inliers and a full rank multivariateGaussian distribution component for outliers. 2) Applying the Wasserstein distance (W distance) between q(z|x) and p(z) instead of the KL divergence in the original VAE. The paper also proves the superiority of using W distance over KL divergence, by showing that the minimized value exists in W distance between a Gaussian mixture distribution q(z|x) and Gaussian prior p(z). 3) 3)For Rrconstruction loss, this model adopts ||x D(z)||_2 as described in (3) instead of ||x D(z)||^2_2, which is used in conventional VAE. This practice helps to alleviate the problem that the loss item in conventional VAE being too small when data point deviates from the center of the Gaussian distribution. Outliers/novelties are supposed to be rare in the dataset, while the paper assumes that a nontrivial fraction of outliers existing in the data.<BRK>This paper proposes a robust novelty detection method ("MAW") to model the distribution of the training data in the presence of high fraction (corruption ratios up to 30\%) of outliers. 2.Model the distribution of latent representation as a mixture of Gaussian low rank inliers and full rank outliers, both using full covariances instead of diagonal covariances as commonly used in previous VAE based methods for novelty detection. Under a special setting, it theoretically proves that using the Wasserstein 1 metric for regularization yields outliers robust estimation and is suitable to the low rank modeling of inliers, while the commonly used Kullback Leibler (KL) divergence does not. In Appendix D.3:In line 2 above Proposition D.2, ``the ill posedness of (11) with $\mathcal{R}  W_2$": The $\mathcal{R}  W_2$ should be $\mathcal{R}  KL$. Please rephrase ``the KL divergence fails is unsuitable for low rank covariance modeling", by e.g., removing ``fails" or inserting ``and" between ``fails is".
Reject. rating score: 4. rating score: 4. rating score: 5. rating score: 6. <BRK>Specifically, the authors borrowed the idea of upsampling from X net in CNN and update step simplification from SGC. What is the possible explanation of it? The reported empirical prediction performance of the simplified models on several standard datasets is comparable to that of the baseline models. However, I have some concerns regarding the methodology and the experimental settings, which, at this stage. **Below listed are the main concerns or confusion regarding the paper. The meaning of "sparse" is not clear. As a result, the output $\mathbf{H}^{L}$ should also be a zero matrix.<BRK>The paper is quite well written and easy to follow. Several sparse variants of the update step, including its complete removal and expander graphs with varying density, are compared in a thorough and comprehensive empirical study. 4.The findings are not strongly justified by theory.<BRK>#### **Post Rebuttal** I thank the authors for the elaborated answers and additional experiments. Unfortunately, I believe that although this work introduces some interesting insights it needs a little more work to achieve the ICLR publication bar. To answer the question two GNN variants with simple Update steps, expander based sparse and activation only, are suggested. The paper presents a wide empirical evaluation on various datasets, demonstrating the abilities of the suggested models,  showing that in many cases, a simple Update step is good enough. **Weaknesses**   Novelty and Contribution   I have some concerns regarding the novelty and contribution of this paper. Evaluation   the paper motivates the design of sparse Update steps by the reduced computational costs.<BRK>The paper does provide some interesting empirical evidence showing that GNNs can be simplified to by sparsifying, or even removing, the learned weights for combining different channels between message passing steps. As other reviewers indicated, the insights provided here are intriguing, but it is not clear whether they meet the level of significance and impact expected from an ICLR publication. How does the sparsification strategy affect the results? Perhaps a comparison of several sparsifiers could help establish the main argument of the paper, or justify the use of expander graph sparsification, even though the graph here has nothing to do with the underlying data graph. With this in mind, I consider this paper a borderline case, while slightly leaning towards acceptance in order to give the authors an opportunity to improve the presentation here by considering and studying the impact of sparsification strategies and presparsification hyperparameter tuning on presented results, in order to address my concerns mentioned above.
Reject. rating score: 4. rating score: 4. rating score: 5. rating score: 6. <BRK>The paper proposes creating diverse ensembles of neural networks using an evolutionary method for finding base learners with high performance as well as mutual diversity. The proposed method can lead to overfitting because the search seems to be based on a fixed set for evaluation. This paper can improve its literature survey by citing more directly relevant work in ensemble search using diversification. Trans.Evol.Comp Khurana et al.2018.Ensembles with Automated Feature Engineering. Tao, 2019. Yao et al., 2008. Evolving artificial neural network ensembles, in IEEE Computational Intelligence MagazineOverall, it is a good problem, but this paper falls well short of the threshold. Update: I thank the authors for their response.<BRK>The paper explores whether one can use Architecture Search to enhance ensemble diversity. They then try out a couple of architecture search methods to find ensembles with diverse architectures that minimize the loss. While the novelty is incremental, I like the idea in general. My main objection is that critical baselines are not compared with. Ensemble diversity is a well explored topic with multiple easy to implement regularizations to increase diversity [1, 2, 3] and several more that should be compared with.<BRK>The paper suggests a new approach to the construction of ensembles of deep neural networks (DNN). Unlike previous methods which usually deal with multiple DNNs of same structure authors propose to form an ensemble of networks with different architecture. The main claim is that using diverse architectures increases diversity and hence the quality of predictions. Currently the authors only compare their method with deep ensembles to which no special selection procedure was applied. This causes bias and it is not clear whether the improvement in NES is due to the usage of different architectures or due to the selection procedure which encourages diversity in resulting ensemble.<BRK>Additionally, it is encouraged to compare to hyper parameter ensembles. The authors addressed my concerns in the rebuttal. Another missing part in this paper is the cost analysis of NES and how much does it increase compared to deep ensembles. I have raised my score. The empirical results showed that the proposed NES methods outperform commonly selected baselines. Improve ensemble diversity by exploring the hyper parameter space. The authors did a self contained introduction on ensembles & uncertainty and AutoML. The improvement of the proposed NES RE/RS method over fixed architecture ensembles is consistent and significant. It would be more convincing if the authors can include other baselines which include ensembles with different architectures (without neural architecture search). This highlights the need for neural architecture search.
Reject. rating score: 3. rating score: 4. rating score: 6. rating score: 6. <BRK>The authors derive the corresponding policy gradient objective for the PMOE policy. The authors apply the PMOE policy on top of SAC and PPO, and perform experiments on the continuous locomotion tasks in the MuJoCo environments. The results indicate that in some tasks, the PMOE policy outperforms the naive policy baseline. The authors also include a wide suite of experiments with both on policy and off policy RL algorithms to demonstrate the performance of the proposed method, and various ablation studies and visualizations to demonstrate the behavior of PMOE policy. Moreover, as described in the paper, the PMOE model is a fairly well studied model, and it seems like the only contribution in this paper is the application of it in the reinforcement learning setting. Therefore, due to the lack of novelty and significant performance improvement, I cannot recommend acceptance of this paper.<BRK>The paper studies the problem of differentiating through the policy return when the policy is a Gaussian mixture model. The main contribution of the paper is a heuristic approach for computing this gradient. Having defined the policy update, the authors integrate it to two RL algorithms: PPO and SAC. Is it that the solution is a consistent estimator of the gradient? Since these two are very different, what ends up being used in the paper? Is it task specific or was it also observed on the Mujoco tasks? For instance showing sampled trajectories for the Hopper as a replacement to Fig.4 could be very encouraging.<BRK>I would like to thank the authors of "Probabilistic Mixture of Experts for Efficient Deep Reinforcement Learning " for their valuable submission. Summary of the paper The paper proposes an end to end method to train probabilistic mixture of experts policies in RL agents. They show that the approach can be applied in the context of popular on policy and off policy algorithms, and that it compares favourably (performance and sampling efficiency) to using the same algorithms to train the corresponding unimodal policies. Suggestions  * Figure 6 could be made more readable by plotting a parameter study instead of a bunch of learning curves, e.g.plot AUC as a function of K.* It would be helpful for the authors to better discuss the relation, similarity and difference between the proposed approach and popular HRL approaches, in order to better assess the novelty of the method, and to ensure that it is placed in the appropriate context.<BRK>In this paper, the authors propose to use mixture models of policies and present good experimental results. The first and second listed contributions are about "the undifferentiability problem", and the third is a verification of the propose algorithm. I agree the mixture models have great potential, but could you please justify your topic/target issue first, and then present your solution? C).Regarding your experiments, besides the three questions raised by the authors (which are reasonable). (Q1 mentioned stability, while the figures do not fully address it, right?) Also, as for mixture models, would an adaptive algorithms be more appropriate solution? Therefore, the current experiments do not fully justify the value of this paper.
Accept (Spotlight). rating score: 9. rating score: 8. rating score: 7. <BRK>This is a paper in theoretical computer science which considers what many would consider a very hard problem   learning a latent simplex lurking  underneath  a cloud of data. It patiently shows that this problem is lurking behind many different high dimensional data analysis challenges, and then carefully lays the groundworkfor discussing a proposed algorithm and  its performance characteristics. Nevertheless, the authors present results onreal data which seem to suggest that the algorithm really can be usedin empirical representation learning. So it would be interestingto hear from the authors, for example in a talk or in an eventual journal paper,to what extent the data actually obey the stated assumptions. In that case, what are the true conditions needed for the algorithm to work? Obviously the significance this work will have in the eyes of potential users depends a lot on the just discussed  Q and A.<BRK>The paper considers the problem of learning a latent k vertex simplex K, given a collection A of n points that are obtained by randomly perturbing latent points in the simplex. It improves the prior work [Bhattacharyya and Kannan, SODA 2020] by removing the multiplicative k factor in the running time given an (necessary) assumption about the mass of the top k singular values of A. Empirically, their algorithm is faster than the prior algorithm on both the synthetic and the real world datasets. Overall, I vote for accepting. The work is tough and technically novel. Specifically, the idea of obtaining a low rank approximation to A introduces a small angular distance is interesting and accelerates the running time. The experimental results that your algorithm not only outperforms on the running time but also produces solutions with lower least squared loss are interesting.<BRK>The paper considers the problem of learning/finding a simplex with k vertices in R^d from a set of points "close" to the simplex. This problem is considered by Bhattacharyya and Kannan (SODA 2020) and they gave an O(k nnz(A)) algorithm assuming that the data follows certain assumptions: 1) well separateness, 2) proximate latent points, and 3) spectrally bounded perturbations. The current paper adds an additional assumption of significant singular values and presents an O(nnz(A)) algorithm for this case. The paper argues that in some important machine learning applications (e.g., LDA), the newly introduced assumption holds. The paper is very well written. It is very interesting that LDA and stochastic block models satisfy this assumption. Moreover, the analysis of the algorithm is novel and interesting. I am wondering if this method can be used for learning a general V polytope. and what would be the running time?
Accept (Poster). rating score: 8. rating score: 7. rating score: 7. rating score: 5. <BRK>I m also a little concerned about whether the right baselines were used in the empirical evaluation. The performance of the network is compared to a number of baselines, some of which are ablated versions of the proposed system, and some of which resemble systems from the literature with similar goals. Overall, I think this technique is likely to have a significant impact on future work.<BRK>Experiments reveal that the proposed method outperforms several baselines and also generalizes well to zero shot unseen tasks (where the goal is to craft new combinations of seen components). 2.Related to the above point, have you tried providing a few past frames to the input as well? Weakness: One aspect that can improve the paper is some more analysis to tease out the effects of using ‘natural’ language vs variations (see points 7, 8 below).<BRK>The paper has still some weaknesses such as,    some analysis could have been explored further, e.g., better stating hypothesis beyond the experiments, better complement accuracies with other quantitative    Information is dispatched in multiple places, and it takes some time to have a global understanding of the experimental protocol and models.<BRK>Could you compare the results of the proposed model from Table 9 and Figure 5? Instructions are only used as latent variables that are fed to the policy to improve the overall performance and to have some interpretability. Experimental results show that language improves the performance, especially in zero shot learning setting, compared to baselines where language is not incorporated or used naively in a discriminator network. Please clarify the paper if and where you use human instructions during testing.
Reject. rating score: 4. rating score: 5. rating score: 6. <BRK>The updated version of the paper has addressed some of my comments. However, I still fail to understand why the method should 1) converge in general, and 2) converge to a good solution. Original Review:Summary:This paper proposes a method for causal inference given observational data. The authors formulate this task as a domain adaptation problem and propose a self training algorithm to address it. The algorithm imputes the counterfactual outcomes (to simulate a randomized controlled trial) in an iterative process that supposedly gets better and better at estimating treatment outcomes. I have many concerns about the paper; I touch on some of them in the following:  There seems to be a misunderstanding in the literature review. It is unclear why the proposed algorithm, namely Counterfactual Self Training, should perform well; i.e., why a model trained on a new dataset with randomly imputed counterfactuals should do any better …   It seems that the second term in the objective function in Eq.(1) is always zero; since, $\hat{r_{i, p}}$ is equal to $f_{\theta}(x_i, p)$ according to the line just above Eq.(1).Therefore, the objective function reduces to an unweighted factual loss, which we know would not result in a good model, since it does not account for selection bias. Algorithm 1, on the other hand, says that $\hat{r_{i, p}}$ is sampled from $P_{\theta}( r | x_i, p )$. However, the authors do not discuss what the convergence criteria are. Definition for $M$ in Theorem 1 is vague: which values for $P$ and $P_{\theta}$ are used?<BRK>The paper proposes to use self training to tackle the fundamental problem of causal inference where only one potential outcome is seen. The proposed self training method is iterative: after training a model on the observational dataset, they run points with different actions (treatments) through the trained model and collect the predictions, which are the pseudo labels. The paper experiments with two versions of the method   one with deterministic pseudo labels (CST AI) and another with soft pseudo labels sampled from a probability distribution (CST RI). It is assumed that there are no unobserved confounders. The rather bad (and consistently bad) performance of CST AI on the synthetic data is a little surprising. Do the authors have any explanation for why this is the case? The run time analysis is interesting but its meaningfulness really depends on the setting in which the authors envision that this work could be used. References: in the classic causal inference literature, there are learners such as S, T, and X learners (this terminology is recent, but the models besides X learner have been around for a while; https://www.pnas.org/content/116/10/4156.abstract has some good explanations.The S learner learns a single model on observational data, then takes the data, switches the actions (treatment), and runs it through the model to collect the pseudo labels. Hill 2011 cited in the paper is actually a S learner.<BRK>Thanks the authors for the response. The additional discussion regarding convergence is appreciated. The explanation of how BanditNet is trained and the additional baseline makes the empirical comparison more clear. I still have the following concerns. The generalization error bound is too loose to be informative. There is no guarantee that the algorithm will continue to improve the performance. Original Comments SummaryThe paper proposed to use self training, a semi supervised learning technique to solve batch learning from biased and partial bandit feedback problem. The self training process is as follows:(1) Train a reward prediction model from observed data. (2) Impute rewards for unobserved actions given a context with the reward prediction model. (3) Train a reward prediction from the imputed rewards and go to step 2. The paper presents empirical analysis on both simulated data and multi label classification dataset. This leads to a generalization error that depends on M, which can be very large in practice. 2.There is no guarantee that the algorithm will converge or continue to improve. Similarly, it seems not fair to use BanditNet but without modeling the selection bias. Additional feedbackIt would be great to mention that V is the VC dimension in the main paper. In the generalization error bound, minimizing the right hand side does not equation 1 which uses the observed reward instead of imputed reward for actions selected by the logging policy.
Reject. rating score: 4. rating score: 5. rating score: 7. rating score: 8. <BRK>The authors then continue by highlighting the inconsistencies observed across different papers and summarize them in Table 1. None of the considered papers had the goal of proposing a common benchmark evaluation. They were all proposing different backdoor attacks, under different scenarios. Of course they are. And this is true also for many other different research topics and areas. Their goal was different. In addition, the whole paper only considers deep neural networks, and not other models. A clearer taxonomy of the whole family of data poisoning attacks should also be reported (e.g., in the form of a table to help understand the existing different types of threat in the area of data poisoning). "Clean" attacks are sometimes dirty;4. Proper transfer learning is less vulnerable;5. Attacks are highly specific to the target image;9. 4, the attacks may fail, and I agree with the arguments posed by the authors in this section. I am only concerned by Issue no. 3 about the need of "clean label" attacks in realistic settings. As the authors of this work seem to be quite concerned on the realism of these attacks, this point should be better discussed, as well as the need of considering the perturbation model to be l inf with size 8/255 (or anyway fixed). In this respect, note that one more pertinent motivation for requiring small perturbations may be the detectability of the attack by automatic tools (rather than imperceptibility to the human eye); see, e.g., the discussion in https://arxiv.org/abs/1802.07295 and consider expanding the paper to discuss this issue. A final comment for the part destruens is that, eventually, it is not well systematized. And, as we will see, this impacts the development of a proper evaluation framework. Again, I agree that providing a benchmark to assess poisoning attack effectiveness is a valuable contribution, and the authors have done a good job highlighting the factors which may impact the performance of these attacks. However, I am also concerned about the proposed benchmarking framework. In particular, as the authors have shown, factors such as the type of the target model, the training dataset size, and the size of the perturbation that the attacker can inject into the poisoning samples substantially impact the attack effectiveness. However, in the proposed framework, those factors assume a single value that may unreasonably favor an approach rather than another. This would indeed give a much more detailed understanding of how an attack/defense performs w.r.t.previously proposed or existing ones.<BRK>**Summary:** This paper studies 4 different previously proposed (clean label and targeted) poisoning attacks and compares them in a systematic way. This incentivizes them to use a benchmark to test all in a unified way. They show that these 4 attacks are not robust to changing parameters of learning algorithm or the experimental setting. They identify several issues with the experimental settings of previous poisoning attacks and suggest some guidelines on  for evaluating poisoning attacks. **Evaluation:** I like the paper s analysis of different attacks and showing when they fail and more importantly, when they succeed. However, In my opinion, I don t find the findings of the paper in lack of robustness for the attacks surprising. These attacks are all proof of concepts that show a serious security concerns for real world applications. In general, I think designing adaptive benchmarks for attacks is not a very good idea. There might not exist a single attack that succeeds in all the scenarios in benchmark and this can give a false sense of security. Even if there is an attack that passes all the tests in the benchmark, one can try and find a scenario where the attack fails and add it to the benchmark. The authors might argue that the goal of the benchmark is not to show that the security threat is not real or that the attacks are weak, but to give a way to compare different attacks and provide insight for when attacks are (un)successful. I think this goal would be much more valuable as it poses a lot of open question for future research, but the current tone of the paper seems not to focus on that. I think the focus on image classification is disproportionate to real world applications of machine learning.<BRK>The authors study a number of existing data poisoning attacks, ablating different design choices of these attacks and evaluating them on a common benchmark. Overall, I found the paper quite interesting. As far as I can tell, the experimental methodology is solid and clearly demonstrates the brittleness of many of these attacks. Similarly, the benchmark proposed presents a reasonable and realistic setting to evaluate attacks on. I believe that such unified benchmarks are important for making progress in such an actively growing field. While early attack approaches serve as proof of concept for different vulnerabilities, understanding when these attacks fail and where should one focus to improve them is necessary to make progress. Other comments (not affecting score):  S2, 1st paragraph: Early backdoor attacks were not clean label because they also explicitly flip the label of the poisoned examples. The CLBD attack does examine a modified trigger that works in the presence of data augmentation. If I understand correctly, the success of CLBD increases since the overall accuracy of the model improves with additional data (the model is more likely to rely on the trigger to perform well on the poisoned examples which, after all are inconsistent with the clean classifier). POST RESPONSE UPDATE  After reading the authors  responses and the rest of the reviews, I still stand by my original score. That being said, I do agree with the other reviewers that the framing of certain discussion points could be improved to avoid misconceptions. It would be good to explicitly state the scope of the paper relatively early (e.g., "clean label attacks on deep learning for images") and explain this choice. Clarify that this is not meant to be the ultimate poisoning benchmark but rather a first step that allows us to make progress in terms of attack development and evaluation. It would be great if these could be incorporated in the next revision of the manuscript.<BRK>This paper discusses many issues in the data poisoning literature. They call into question the real world applicability of data poisoning attacks and discuss the short coming in the literature. Notably they show that many attributes of a data poisoning attack assumed to be irrelevant to the success of the attack are in fact statistically significant, such as a dataset size. Finally they present a new benchmark which attempts to standardize the comparison between data poisoning attacks. I think this paper will benefit the data poisoning literature by illustrating its short comings and presenting a path forward. Suggestions: in the  “Clean” attacks are sometimes dirty   section the authors present two examples, but a human study could be set up in order to establish how many  clean  attacks are actually identifiable. A small Mechanical turk study would greatly increase the strength of this argument and should not be financially prohibitive. The section on  Performance is not invariant to dataset size   convinces the reader that this attribute thought to be irrelevant to the success of the attack is relevant, but proposes no mechanism by which the data set size could effect the attack. A discussion section attempting to provide an answer to this question would be interesting, but is perhaps out of scope. The section on data augmentation does not mention if the augmentations are also applied to the poisoned inputs. If the clean data is augmented but the poisoned data is not, it seems expected that the success of the attack would drop.
Reject. rating score: 3. rating score: 4. rating score: 5. rating score: 5. <BRK>The paper also does not describe or explain the motivation and interpretation of anything, but instead just lists equations.<BRK>For example, the paper states, "For example, unbalanced label initialization on $D_0$ usually leads to an uninformative prior,". In appendix B, what sort of mathematical objects are A and B?<BRK>I like the cleaness of the proposed approach but I think that the paper in the current status is not yet mature for such a relevant venue as ICLR.<BRK>The effective combination of these two is a promising direction of the research and it is addressed in the paper.
Reject. rating score: 3. rating score: 4. rating score: 4. rating score: 6. rating score: 8. <BRK>The authors claim that BO can be solved more efficiently using their method and showed promising empirical results on some low dimensional benchmarks. And I hope the authors will consider the reviewers  opinions before submitting somewhere else. I still think this is a very interesting idea and a quite promising direction to incorporate human knowledge in our used to be black box machineries. Cons:  This wasn t the case of the previous version, but I found it unsettling that the 1st contribution point was "For the first time, user prior knowledge can be combined with standard BO probabilistic models, such as Gaussian Processes (GPs), Random Forests (RFs), and Bayesian Neural Networks." Even in the narrowest definition of prior, this paper is a perfect demonstration of how to incorporate human knowledge in GPs: https://papers.nips.cc/paper/2015/file/4462bf0ddbe0d0da40e1e828ebebeb11 Paper.pdf. Otherwise there will be bad consequences. It is still not clear what different probabilities mean to the authors. Can the authors describe a coherent generative model of what they proposed?<BRK># cons1.My major concern is the construction of the "good" prior and the "bad"    prior. The input data to KDE is {x_i}. # pros1.This paper provides an interesting direction in BO, i.e.incorporating    prior knowledge. 2.In Figure 2, the authors use a misspecified prior to show their proposed    method is able to recover from a "wrong" prior.<BRK>Summary:This paper incorporates a prior distribution given by experts into Bayesian optimization (BO), to leverage useful human knowledge to accelerate BO. The algorithm uses an intuitive approach to combine the prior with the probabilistic surrogate model of BO to derive a pseudo posterior, which naturally leads the EI acquisition function. Strong points:  The design of the algorithm is intuitive and natural. The paper is well written. I feel more real world experiments are needed, given that I m doubtful as to whether the proposed algorithm is truly useful in practice as mentioned in the first point above.<BRK>The goal of this paper is to enable the introduction of prior expert knowledge in Bayesian optimization. Extensive experiments are conducted on toy examples as well are more realistic hyper parameter test cases. This paper could interest practitioners, but some aspects need to be better detailed and more comparison to existing methods should be added.<BRK>The paper presents a novel method to incorporate experts  knowledge into BO. This is done through introducing  Prior guided Bayesian Optimization (PrBO). The design of PrBO enables it to guide the search in the early iterations and as optimization progresses, more emphasis is given to the model and the effect of the prior is washed out. The paper is well written. Weak points:  Although the experimental design is extensive and covers several aspects. I am sure that this kind of work will encourage more researchers to use BO in their problemsQuestions:  It is clear how PrBO is incorporated in TPE, but what about GPs and RF?
Reject. rating score: 3. rating score: 3. rating score: 4. rating score: 5. <BRK>A lot of information is missing, what are the sizes the $D_S$ and $D_L$? # Pros1.Compared to “(Ge at al.) What is the accuracy for $D_L$? However, unfortunately, I still think more needs to be clarified in the actual paper write up, notably on the points of non adversarial as well as the method description. 2.The scope of the method is not clearly presented either: I was under the impression that the proposed method was comparable to other unsupervised auto encoders while in fact it requires label supervision3. What I don’t understand is what makes it “perfect”, can’t it be further improved? We propose a different solution to empower precise attribute controllable synthesis ability on autoencoders: DEAE”.<BRK>Thus, it would also be good to mention that the approach is supervised (like GSL AE). Reason for scoreOverall I vote for reject, this paper has limited contribution and lack of consistency (disconnected ideas: DEAE and UDV), clarity (respective contributions of GSL AE and DEAE) and justification (spurious and not convincing experimentations). Pros1.The general topic of having more controlable synthesis by having more disentangled representation if of clear interest for the field. 4.Results are not convincing.<BRK>It uses interpolation in a disentangled latent space using Group Supervised Learning. ###########################################################Strengths: The problem of controllable image generation is important and current approaches are far from perfect. There are several other approaches that are not mentioned in the paper. They provide a more comprehensive evaluation on CelebA and Flowers datasets while this paper use relatively simpler datasets, Fonts and RaFD. It would be better to perform experiments on datasets such as CelebA which has attribute annotations. ############################################################Reason for Rating:Overall, while autoencoders with disentangled latent space are a promising candidate for controllable generation, experimental results and novelty of the proposed approach are inadequate for publication.<BRK>Strengths:* I appreciate the intuition and examples throughout the paper, regarding the method itself and description of the experiments and desired outputs. This makes the paper easy to read and follow. A more exhaustive presentation would be more convincing. Although the paper builds on previous work, I find it quite interesting and significant to have a method for controllable synthesis. Other baselines:  Generative models: at least some comparison to vanilla VAE, GANs as a sanity check (if the claim is generative AE)  Why is GSL AE not used in 3.4?
Reject. rating score: 5. rating score: 5. rating score: 6. <BRK>The paper studies recent approximation bounds of coreset based pruning strategies for neural networks. This whole discussion just seems very strange and confusing. I would expect some discussion here. They are among the first to show that coreset based methods allow any kind of theoretical approximation guarantees, but they do not claim that the bounds are good predictors of performance. 3)  fine tuning can recover large amounts of accuracy while simultaneously maintaining of increasing approximation error" ... That doesn t make sense. What is the  importance , how is it computed? A more informative intro would be much appreciated.<BRK>This paper investigates when neural pruning approximation bounds useful from several perspectives. Showing that approximation bounds are loose and require a large sample size. It also explores the influence of fine tuning after pruning by approximation bounds. However, I a bit concerned about the novelty and impact of the paper.<BRK>I like the idea of analyzing the practical performance of coreset based pruning methods and discussing the relation between sparsity, approximation bounds, and accuracy. Pros:1.The paper provides an empirical evaluation that shows the theoretical bounds of coreset based algorithms could be loose. This is an important issue for the practicability of coreset based algorithms. 2.The paper shows the tight approximation bound is not a necessary condition for accuracy by analyzing fine tuning. The quantitative results that fine tuning increases approximation errors are interesting. I suggest the authors to give more discussions on both sides. However, due to the empirical results of fine tuning, we can recover accuracy after post pruning.
Accept (Poster). rating score: 7. rating score: 7. rating score: 6. rating score: 4. <BRK>The paper is well written and clear, despite its rather technical nature. This embeds the infinite dimensional space R^\infty with a norm. Specific comments:  $I$ is never defined as the input space dimension  p2: "shows that the for Gaussian Processes "  there are several instances of the sentence "proving the existence of: i) a continuous Gaussian process, indexed by the network s width n, corresponding to the fully connected feed forward deep neural network" (with variations, in Abstract, Introduction and Discussion).<BRK>Overall, I think the topic of this paper is a good fit for ICLR (most of the new work in this direction was presented at ICLR in the last three years), and that the paper is trying to address an important open question. This paper is a theoretical investigation of the asymptotic behaviour of deep fully connected neural networks (FCNs) as the number of hidden units in each layer goes to infinity (simultaneously in all layers). The authors approach this problem by recognising that with additional Lipschitz assumption on the nonlinearity, both the finite network induced processes and the asymptotic GP limit have a version with paths in a space of continuous mapping between two Polish spaces (at least in most of typical use cases).<BRK>**Summary**: The paper is based on recent research about the limit of neural networks to Gaussian processes. The main result is the proof of the limit from a function perspective: neural networks are considered as random processes with the input space being infinite dimensional. However, in the discussion section, the authors pointed to the possible convergence of expectations where their approach can be used. Overall, I find the paper well written and not difficult to follow.<BRK>This paper makes a deep analysis on the large width functional asymptotics of the fully connected feed forward deep neural networks where weights and biases are independent and identically distributed according to Gaussian distributions. It is clear that these results are theoretically significant. However,  these weights and biases are assumed to be random as noises,  the network will not be learned with any task and cannot contribute to machine learning. Therefore, I cannot recommend this paper for the conference.
Reject. rating score: 5. rating score: 6. rating score: 6. rating score: 6. <BRK>Instead of generating positives pairs using different transformation of the same image  as it is standard in contrastive learning , positive pairs are generated using those similar images to the anchor image: after clustering the representation space using k means, the nearest neighbours that are closer to the corresponding center of the cluster where the anchor belongs to are selected. Finally, the paper also proposes a multi resolution augmentation, which consists in random zooms in (ie.random crop + resize) at different scales to enable scale invariance. The experimental section is good, which includes evaluations for different tasks and an ablation study that analyses the contribution of the different components of the proposed approach. ## ConsI didn’t find that the proposed approach was properly motivated in the paper. I have to disagree with this statement, since I personally consider these results to be on par: it only outperforms MoCo by 0.4/0.3 points in AP. I would suggest the authors to rephrase this claim. I understood this from the explanation in sections 4.1 and 4.3, but this is not clear for the semi supervised training experiments in Section 4.2.<BRK>The main contribution is 2 fold: a) use nearest neighbors from the same cluster which are closer to the centroid than the anchor as positive samples; b) use more complex augmentations, i.e.[CutMix] and multi resolution during training. ### Pros+ The paper is written well and easy to understand. + I appreciate quite detailed ablation studies (but not all of them). [CutMix] augmentation is a previously published work. And the contribution of this paper is applying CutMix in the context of contrastive learning, which is yet another augmentation among a huge variety of possibilities. "* It is not very clear how the proposed method differs from the clustering based methods in this sense. I understand that it is computationally demanding, however, it would provide a better picture of the performance contribution of the final components (after tuning them using a shorter training schedule with 200 epochs), since improvements brought by some components on the shorter training schedule can become less significant when the network is trained longer. ## After rebuttalAfter reading the authors comments  and other reviews, I think that this is a **borderline** paper that could benefit from more rigorous experimental validation.<BRK>The paper presents an improved positive sample selection and data augmentation method for unsupervised, contrastive representation learning. Authors propose two improvements to be used in contrastive representation learning: a positive sample selection scheme (called center wise sample selection), that improves over previously proposed kNN or k means methods; and multi resolution data augmentation which is an extension of a known crop augmentation technique with multiple scales. Strengths:The impact of the proposed improvements was thoroughly evaluated on several unsupervised feature learning benchmarks. It was shown that the proposed improvements advance state of the art. Ablation study proves positive impact of each of the two proposed improvements on the final performance. Weaknesss:The proposed improvements show rather limited improvement over state of the art. Wording and writing style needs to be revised as some sentences are difficult to understand. In introduction authors write:"....instance discrimination (Wu et al., 2018) based methods are rapidly closing the performancegap comparing with the supervised counterparts" and in the following sentence:"Following this paradigm, self supervised models are able to generate features that are comparable or even better than those produced by supervised pretraining." The second sentence claims, that self supervised models can be even better than those produces by supervised methods.<BRK>From appendix C, is it clear there is a big difference in accuracy from 200 to 800 or more epochs. Two components are proposed: First, to select semantically similar images that are pulled together in the contrastive learning, the paper proposes "center wise local image mixture" (CLIM)   both k means clustering and knn neighbors are computed, and then for a given anchor image x, and images x  that fall within the same cluster and are a knn neighbor (and additionally closer to the cluster center than x) are selected as a positive match to x. This selection is further modified by the use of cutmix data augmentation, where (x, x ) are combined via a binary mask. Positives:+ interesting proposed method for expanding the neighborhood space of considered positive matches for contrastive learning+ ablation study provided to show the improvement from each proposed component (sample selection, cutmix, multi resolution)+ generally good empirical performance on several tasks   linear evaluation on imagenet, semi supervised learning with few labels, transfer learningNeutral:  overall novelty is moderate; I would consider the main novelty to be in the selection of positive matches, as the cutmix and multi resolution augmentations are largely leveraging existing ideas.
Accept (Poster). rating score: 8. rating score: 7. rating score: 6. <BRK>This is achieved in two steps. First, the joint action space is clustered into different role action spaces that reduce the action search space for each role. The role selector is now used to learn a bi level hierarchical assignment to map the action observation history of each agent. This is inspired by QMIX, previous work on multi agent learning. 3.The paper is well written, and the technique is extensively tested on all the maps with useful ablationsMinor issues:1. Update   Thank you for the response and updates to the paper<BRK>This paper introduces a bi level hierarchical framework for achieving scalable multi agent learning. In this framework, the high level policy (role selector) coordinates role assignments in a smaller role space and at a lower temporal resolution. And the low level policies (role policies) explore strategies in reduced primitive action observation spaces. Strength:  The paper is well written and easy to follow.<BRK>This paper proposes a new method called RODE, to learn roles for multiagent systems to improve the learning efficiency of MARL. ********This paper is highly related to the subject of ICLR and is well written. But I have some specific confusions listed below, waiting for the authors  reply. I agree with the authors that the joint action space can be decomposed or classified based on the action effect or action semantics. Moreover, these two works both use SMAC as benchmarks.
Reject. rating score: 4. rating score: 4. rating score: 5. <BRK>Using three phase performance modeling to estimate computation time. Weakness1, The main problem is the lack of Novelty and technical contributions. Using a DL based model to predict the performance is not novel. Many NAS methods using the same trick to estimate the performance of one architecture ahead of running it directly on the hardware. 2, Using a DL base regression is better than normal regression when the sample size is large. The experiment results are not surprising. 3, Box cox transformation is a common technique in regression. That is not new. It is hard to generalize it to other hardware/platform. 5, Besides, the method only considers some common operations such as conv, pooling, and FC layer. However, more operations should be considered for example ROI Pooling, NMS, Spatial to depth.<BRK>Summary: The authors design a specific ResNet for predicting the model execution time on different platforms. Pros  conduct extensive experiments, particularly collect a large scale dataset for measuring different architectures, which can be helpful for further works if it can be released publiclyCons  The idea is not novel. The main idea is to utilize a ResNet to perform regression on network latency data, which can only be considered as a normal application of ResNet. The motivation is questionable. The proposed ResPerfNet should be applied in network evaluation and search stages in Neural Architecture Search (NAS) area, and validate that a more accurate model performance predictor is helpful for architecture search. But I didn t see any supporting experimental results in this paper.<BRK>The paper presents a method, called ResPerfNet, to predict the performance of deep neural networks. The paper evaluates the proposed method on three networks, LeNet, AlexNet, and VGG16, in two different frameworks, i.e., TensorFlow and TensorRT. The results are promising, but comparison with other approaches is weak. For example, the proposed method, ResPerfNet, is only compared to one other approach, PerfNet (from a paper that don t seem to be published yet, at least I couldn t find it), using TensorFlow (but not using TensorRT). For example, the following issues need to be addressed:* Motivation for the selected structure / architecture of ResPerfNet.
Reject. rating score: 2. rating score: 3. rating score: 4. rating score: 4. <BRK>Perturbations are aimed to be semantically invariant to preserve the meaning of the source sentence.<BRK>Why is fastalign able to align some words better than the transformer?<BRK>In this paper, the authors compare their approach to the following baselines: SWAP, DROPOUT, BLANK, SMOOTH. All baseline methods (SWAP, DROPOUT, BLANK, SMOOTH) have an element of randomness in them and are not strong baselines.<BRK>As it is, no attempt to do significance testing is made.
Accept (Poster). rating score: 7. rating score: 7. rating score: 6. rating score: 5. <BRK>This paper presents a two stage representation learning approach to deep one class classification. In the first stage, a mapping f to a versatile high level latent representation is learned using self supervised learning for a contrastive learning proxy task. This paper is overall a good paper that will be interesting to a certain audience at ICLR. + It is well written, well motivated, with a clear argument and as far as I can see, technically correct. + The experiments are well designed, valid and exhaustive, with comparison to a range of baselines as well as an ablation study. + I appreciate the comprehensive grounding of the contribution in both new and old related work.<BRK>This paper proposes a framework for deep one class classification (an example application being anomaly detection). The basic idea is to combine self supervised representation learning (eg through a proxy task such as rotation prediction or contrastive learning), with a classical approach to one class classification, such as one class SVM or KDE. In describing distribution augmentation and contrasting it with standard data augmentation for contrastive learning, it is clarified that the two sets of augmentations are disjoint. I would it have found it helpful if the paper was explicit about which data augmentations were used for the contrastive learning, as this did not seem to be stated in the paper. Overall I found this to be a nice paper with strong empirical results.<BRK>The main contribution of the paper is the feature representation learning that relies on contrastive learning to optimise a self supervised loss function which minimises the distance of the samples from the same image augmented with different data augmentation functions and maximises the distance of samples from different images augmented with the same augmentation functions. This paper has outstanding results on the datasets CIFAR 10, CIFAR 100, Fashion MNIST, and Cat vs Dog, but it is missing result on a challenging dataset, such as Mvtec [51]. In terms of the proposed method, it is quite similar to [20,21], with the difference that it uses more data augmentation functions and rely on contrastive loss. Therefore, it scores slightly below acceptance on novelty as well. One argument that seems contradictory is the one for class collision and uniformity. Of course, this would probably require a change in the OC SVM classifier. Can the authors comment on that?<BRK>Summary: They investigated the effectiveness of self supervised learning (SSL) for one class classification (OCC). Strength:The paper is well written. Concerns:The uniformity argument is weak. The authors state the empirical improvement on OCC using their method hinges on the DistAug technique, which is motivated to reduce the uniformity of the learned representation. They authors used MMD to a uniform distribution to measure how uniform the representations are. The less uniform (i.e.higher MMD), the better it should be for OCC. The  correlation between MMD and AUC does not  seem  to be very strong. If this is not why, then we should find another explanation for why there was  an improvement. Overall, a fairly thorough empirical investigation into better techniques for using SSL for OCC.
Reject. rating score: 4. rating score: 4. rating score: 6. <BRK>This concern is critical for the given score of the paper. 2.Besides of that, it would be also important to emphasise the novelty: why do we need to have another saliency method, does it give any new insights other than the existing ones mentioned in the related work section?<BRK>The finding in this paper suggest a new view based on ablation path optimization. + Reasonable results can be obtained on several example images to show the decision of the model.<BRK>(3) As is known, the saliency mechanism is closely related to concepts in the field of neuroscience, so it would be better if the proposed formulation could be discussed based on its essences of cognitive neuroscience mechanisms.
Accept (Poster). rating score: 8. rating score: 6. rating score: 6. rating score: 6. <BRK>Extensive experiments are performed across various datasets and architectures. I had to write out the results this way myself to aid understanding. While the paper does put forward a hypothesis regarding why, that the difficulty of learning routing operation in NNs may lead current approaches to fail to discover solutions which generalize systematically, little evidence is provided to support this conclusion. This is related to point (2) above, and I m not convinced this is the only possible conclusion. The paper does provide some evidence this may not be occurring. ### RecommendationI recommend acceptance. Although I have several concerns with the paper, I found the ideas and analysis presented very interesting. I believe the community would benefit from further discussion, scrutiny, and exploration of the ideas presented. I believe the techniques and analysis presented here for assessing reuse could be an important step between observations and explanations for the failure of NNs to generalize systematically. Ideally, the paper would provide more evidence to support this assumption.<BRK>The experimental evidence is provided for both synthetic, language, and image classification tasks. Concerns: The biggest concern I had is whether the conclusion reached in the paper is invariant to different neural network architectures, the size of the network, and the complexity of the task. It might be better to learn the binary mask for each subtask with a certain accuracy objective (e.g., less than 1 2% lower than the original network) for each subtask and compare the learned mask of different subtasks. In addition, it is useful to see the results on a more complex task such as ImageNet. In ImageNet, there exist many similar subtasks as many categories of the images are actually belong to one big category. Is there a significant difference (e.g., the stability of the training) in training the binary mask with the Gumbel Sigmoid and threshold function? I would consider raising my score if the authors can address my concerns.<BRK>However, this can be easily addressed   thus my full review below:Summary of the paper:The paper aims to analyze if and how neural networks learn modular representations. They argue that results on the SCAN dataset show that the representations are of a sufficient quality, concluding that the network did not learn the correct composition. The second is that compositional modularity is a highly desireable property, and it is important to know if neural network exhibit it. This makes the paper a little difficult to follow, if one is familiar with this literature. Conditional computation in neural networks for faster models.<BRK>The paper presents an empirical study of whether modularity can emerge within neural networks. It might not be possible to undo the permutation with only one layer. Also, source code is provided, and the supplementary material provides sufficient details to reproduce the results. One may argue that the weights in the input/output layer should be excluded from the module. The task is basically to learn the function f(a,b,s), where a and b are two numbers, and s is a switch indicating whether addition or multiplication should be performed for a and b.
Accept (Poster). rating score: 7. rating score: 6. rating score: 5. rating score: 4. <BRK>Because of these strong empirical results, I am recommending to accept this paper, but I am not advocating for a strong accept. While the results are strong, the work does not improve our understanding of these models, or introduce any particularly novel techniques. I think the contributions of this work could be made greater with some experiments on some other applications such as OOD. This is the main contribution of the paper and the authors should take more care to ensure that their effort is easily understood. Particularly interesting are the very strong results in the image translation task. The figures are not very clear on this. When sampling high resolution images, do we first generate lower resolution samples, and use these to seed the high resolution samples? While the experimental results were strong, I am curious why the authors did not present any results on out of distribution detection (aka anomaly detection).<BRK>In addition, authors demonstrate successful application of EBMs to unpaired image to image translation. #### Pros* Strong experimental results. Scaling to 256x256 images is a great advancement of the field. This is much faster than denoising score matching with Langevin dynamics or denoising diffusion probabilistic models, which typically need thousands of Langevin steps. The numbers in tables are therefore not rigorous. The processing methods can be quite different in different papers, and the number of samples used for FID computation can also be different. I completely agree that FID is a flawed metric and lower FID scores do not necessarily indicate better sample quality. I am still marginally inclined to acceptance, but will leave it to the discretion of the AC on whether this paper should be rejected due to flawed FID computation.<BRK>Strengths:  The presence of a saliency map in image to image translation is a benefit that is harder to get out of other methods. Weaknesses:  The novelty here is very limited. The difference between this and previous energy based models is the scheme of progressively generating at 8x8, 16x16, 32x32, etc... which itself is a well established technique from the Progressive GAN. Given neither of these things is new, the novelty lies in just using the one with the other. The qualitative results from the GANs are much poorer than can actually be achieved. The characterization of GANs in the previous work section is very poor. Also masquerading as a "critique" is that GANs have a "fancy architecture": I don t think the authors could devise a definition of fancy that excludes their model.<BRK>This work proposes a method to improve generation with energy based models. While [2] uses a separate hierarchy of multi scale EBMs, this work uses the sampling scheme in [3]. My second most major concern is other the theoretical framing of the proposed approach and well is issues with statements made in the paper. But also the approach claims to train a maximum likelihood objective, where Langevin steps are used to approximate the energy landscape. 3) The related work is still missing older work in the area of EBM training. I do not think this is the case. 4) I didn t find the out of distribution results to be a particularly compelling application of the model (although its good that it performs similarly to past approaches). Minor Comments:Why is the proposed approach much more efficient than past approaches for image to image translation?
Reject. rating score: 4. rating score: 5. rating score: 6. rating score: 7. <BRK>#### Paper summary:In this work, a reinforcement learning (RL) approach is proposed to solve the active feature acquisition (AFA) problem (as well as the active instance recognition problem). Comparing to existing RL approaches for AFA, the main difference of the proposed approach is to introduce a generative model (utilizing the existing ACFlow model) to learn the transition function, in order to provide additional rewards and auxiliary information. #### Advantages:  I think the paper is well written. It is a reasonable approach to solve the AFA problem. The idea of learning a generative model is interesting. No results are shown to show what the generative model learned in these experiments. This is very important for understanding the true effective tasks for the proposed approach.<BRK>Strong points:  The paper tries to solve an important problem that deserves additional attention from the researchers. However, I could see that some of the concerns regarding novelty, relation to the prior work and experimental results are shared among several reviewers. However, this is a very special type of the features and many methods that rely on the inductive bias about images could be used in addition. Then, the authors mention the superiority of the adaptive feature acquisition over selecting a fixed set of features for all datapoints. I am not entirely convinced that using the proposed "surrogate model" shares a lot in common with model based RL methods.<BRK>This work present a RL and ACFlow based solution for feature wise information acquisition. Pros:1.Clarity: The paper is very well presented. Very clear presentation of the problem formulation  and methods. 2. significance: the paper is addressing a very important question in real world applications where information are associated with cost (which was clearly motivated)3. significance: the method obtained improved results comparing to current state of the arts methods. Thus, the only main difference comparing this paper to ODIN is the choice of surrogate model* Lastly, the surrogate model is not new.<BRK>This paper studies the problem of active feature acquisition (AFA). The authors formulate AFA as a Markov decision process (MDF) and use reinforcement learning to resolve it. The research direction of this paper is important and interesting. In real scenarios, there are many situations in which people cannot observe all features before making a decision or constructing a model. Readers can reproduce the experimental results based on them easily. For example, when they introduce the surrogate model. In summary, this paper conducts interesting and vital research. GSMRL is flexible and effective for many tasks.
Accept (Poster). rating score: 8. rating score: 7. rating score: 7. rating score: 6. <BRK>This paper presents a novel class of associative memory models. The model is expressed as a network with two body interactions (synapses) and a well defined energy function, and it is shown to generalise and unify several existing approaches (Hopfield Networks, Dense Associative Memories and Modern Hopfield Networks). I only have a few nitpicks concerning the "biologically plausible" angle. More specifically, while I agree that the proposed model is *more* plausible than some of the other approaches discussed, its absolute level of biological plausibility remains limited. The authors recognise this point and devote a paragraph to discussing it at the end of section 2 ("For the purposes of this paper we defined biological plausibility as…"), but it seems odd to have this passage buried at the end of the mathematical derivation, rather than up front in the introduction. I suggest that this passage is moved forward to a more prominent location. Furthermore, given that this is essentially a paper on the theory of abstract associative memory systems, the emphasis given to the biological angle in the title seems eccessive, and the choice of the word "neurobiology" somewhat puzzling. In my opinion the title would be a better description of the content of the paper if the reference to biology was toned down. Finally, in the introduction: "typical synapses are not highly reliable, and a cortical synapse stores no more than one or two bits of information". I am not a specialist on the matter, but this seems surprising in light of work showing that the capacity of hippocampal synapses can be up to 3 to 5 bits (Bartol et al 2015, Bromer et al 2018). Bromer, C., Bartol, T. M., Bowden, J. Post revision update: my concerns have all been addressed.<BRK>Summary: In the current work, the authors describe a novel memory structure, and mathematically show how this is a superset of previously published models. Strong Points: Unifying the multiple discussed architectures, as well as relating back to RBM and classical Hopfield networks is a nontrivial task. Additionally, the use of integrator units (eq 1) provides a clear path how such an energy optimizing network might be implemented in biological networks (eg: LIF neurons). Weak Points: Biological plausibility is discussed at several points in the paper, specifically regarding the number of units involved in an interaction. Additional Comments:Regarding the above: If the feature units are to represent distributed patterns (eg: BERT features), then it should be possible for more than 2 abstract units to interact at a single "synapse", so long as the interaction of the actual units (neurons or RELU units) has only two members? If so, is it possible to derive more general estimates of capacity?<BRK>The authors proposed a dynamical system that unifies several associative memory models, including the classical Hopfield network and two recently proposed modern Hopfield networks. The dynamical system is described as interactions between two groups of neurons (feature and memory neurons), providing a more biological interpretation of modern Hopfield networks. This manuscript provides sufficient details for understanding, its derivations are correct, and its results are useful in bringing modern Hopfield networks closer to biology. I understand that these statements have been used in the literature. However, since the authors are explicitly trying to interpret modern Hopfield networks as network of neurons, it would be appropriate to discuss the memory capacity with respect to both $N_f$ and $N_h$. Related to the previous point, the number of memory neurons needed should be clearly described. In equation 2, the authors introduce an energy function containing two Lagrangian functions, and demonstrate that it reduces to energy functions from several previous models when using different choices of Lagrangian functions. I believe the reader would have a better understanding if the authors provide a stronger motivation for the energy function. For example, is the energy function constructed by explicitly trying to connect the Krotov & Hopfield 2016 model with the Ramsauer 2020 model? Here are some examples:In the intro, “a cortical synapse stores no more than one or two bits of information” should be followed by a reference since this is not common knowledge. ### Minor:P.2 “integrates our some of the degrees of freedom”  “integrates out…”In P.1, the sentence “a small part of a high resolution photograph may contain only 1000 pixels, but the number of describable “objects” which might occur in such a fragment is far larger” is a bit confusing. Post revision update:All concerns addressed.<BRK>This paper provides a mechanism by which recent extensions to the classic Hopfield network model, which as written involve many body interactions, can be implemented by a more biologically plausible network that uses only two body synaptic interactions (but more neurons). Pros:  The derivations are sound and well explained, from what I can tell  Given that Hopfield networks are an important model in the neuroscience community, it is nice to see these extensions to them brought back in to the realm of biological implementationCons:  My main concern with this paper is that I feel like it has not established its significance to neuroscience modelers   and since the paper is primarily concerned with connecting existing algorithms to biologically plausible implementations, rather than introducing new algorithms, I think it is important to do so. Dense associative memories and modern Hopfield networks are appealing because of their ability to store memories beyond the O(N) scaling limit of regular Hopfield networks (N   # of neurons). But in these two body implementations, the models once again have O(N) scaling in the total number of neurons due to the addition of hidden neurons. Are they still more efficient in some way? The connection to attention mechanisms feels a bit off in that the equivalence only holds when the update rule is applied exactly once and no more times (I realize this was first shown in prior work). But given that this paper is attempting to unify several mechanisms under a common framework, it feels like the paper should concern itself with models that really are variants of that framework, rather than introducing additional modifications like cutting off the dynamics early.
Reject. rating score: 3. rating score: 4. rating score: 5. rating score: 5. <BRK>Summary: In this paper, the authors investigate the inner loop optimization mechanism of meta learning algorithms. The analysis shows the effectiveness of the multi step adaptation and (1) the key of meta learning is how to design a well differentiated classifier. Pros:1.Empirically investigating the performance w.r.t.the change of optimization mechanism on both head and body is very important in the meta learning field. Cons:1.My major concern is about the contribution of this paper. The discussion is interesting. I think the goal is the same as metric based meta learning, which is not new for me. In addition, the baselines and experiments are not sufficient to support the goal. Especially, the proposed methods (both MCL and RDP) only show comparable performance compared with metric based methods (e.g., MetaOptNet). Thus, I feel the overall contributions are not enough to be accepted. 2.Besides the Cons 1, if the authors focus on gradient based meta learning. Most figures are not clear.<BRK>2.The idea of introducing contrastive learning is reasonable and leads to performance improvement in experiments. The proposed methods have little improvements over base models; Random Decision Planes (RDP) actually obtains worse results than the basic MAML. Although it is reasonable for the proposed method to use contrastive learning, its novelty is quite limited. I think this might be important. 2.I do agree that low accuracy head may provide bad signals for the model body. But the heuristic nature of the analysis is not convincing enough. Secondly, the parameters in the body might not change “efficiently” as the head parameters, because the gradient with respect to the body parameters (especially the low level features) might be small. Overall, question about the importance of multi step adaptation is still not well answered. 3.In Table 2, results of RDP are shown, which are worse than  the basic MAML. Is the classification based on RDP or using a new fully connected layer (as head)? This will allow us to see how the contrastive learning leads to features that can better distinguish/discriminate samples from different classes.<BRK>This paper adds to a series of papers that look at understanding how and why MAML works as well as it does, and what exactly is going on in the adaptation process. Do you have any intuition as to why RDP works better on the FC100 dataset compared to MAML, but not on the other datasets? Does that dataset have different properties? (Model 1) How is the multi task model trained? How does this work in the few shot learning setting? Could you explain why you chose to compare to the methods in Table 3? (Model 2) How is the multi head model trained? It s not clear to me what the purpose of the proposed method is   illustrating a point or proposing a useful new algorithm? UPDATE:I have read the other reviews and the author s response. I can see how you can do this for a small, fixed number of training tasks, where you just learn one head per task over the entire course of meta training. Overall, I still tend towards rejection   even with the updated version, I still find the contributions of the paper difficult to tease out and evaluate, and not all claims in the paper are sufficiently backed up / analysed by experiments. Section 4.2: In this section you ask *why* multi step adaptation is important. Sufficient in what sense?<BRK>The paper explores different variants of MAML, and provides empirical analysis to argue that the multi step task specific adaptation of a network head in the MAML algorithm is important in learning good representations and thus enabling effective few shot learning performance. **Overall Comments:** This paper consists of some analysis on task specific adaptation, and presentation of two related few shot learning methods that build on some of the insights from the analysis. In contrast, the second algorithm presented is clear and interesting, with notable computational benefits and performance improvements over baselines. In its current form, due to a weaker analysis section and limited analysis of MCL, I do not think this paper is strong enough for acceptance. **Detailed Comments:** * The analysis about the importance of multi step adaptation is a bit unclear at points, and the experimental evaluation may not support the claim that the multi step task specific adaptation is important. * One aspect of the multi head setting is a bit unclear   for every task, do we initialise a new 5 way head and then perform only 1 gradient step to update these parameters? * The RDP algorithm is an interesting investigation, but I am not convinced of its utility or that it supports the overall argument of the paper. I think that this method itself is a good contribution.
Accept (Poster). rating score: 7. rating score: 6. rating score: 6. rating score: 5. <BRK>This work proposes and evaluates reward randomization as a strategy to discover efficient policies in multi agent games. The figures are helpful and detailed. Some analysis of the sensitivity of this method would have helped. The authors demonstrate that both theoretically and empirically, Reward randomized Policy Gradient (RPG) outperforms standard baseline techniques.<BRK>This paper proposes to use reward randomization to explore the policy space in multi agent games. The experiments are done on three games and show the interest of their approach in comparison with several baselines. ___________________________After the rebuttal I raised my score.<BRK>Am I missing something here? So the importance naturally extends to the multi agent RL world. Extensive experimental evaluation. The paper considers a total of four test bed games and shows the benefit of using reward randomization in these settings. Having said that, here are some of the weakness of this paper.<BRK>The paper focuses on an important problem, the existence of multiple Nash equilibria in strategic games, and proposes a relatively simple mechanism, reward randomization, which allows agents to discover higher payoff equilibria in games where normal decentralized policy gradient methods would converge to suboptimal ones. That said, I appreciate there is value to the theoretical results that provide some more general evidence for the potential importance of the method.
Reject. rating score: 4. rating score: 5. rating score: 5. rating score: 6. <BRK>##########################################################################Summary:The paper proposed a new way for training models that stack the same basic block for multiple times   share the weights first and then untie the weights. The author tried to provide theoretical insights on why it can be better. Also, ablation study shows that the proposed algorithm has marginal improvement over the baseline. In addition, after the weights are untied, the training process becomes exactly the same as that in BERT. The idea of first train the model with shared weights and then untie the weights is interesting. The improvement over the baseline method is not very substantial. 2.I m not convinced by the theoretical analysis.<BRK>Weight sharing idea is not new, it has been investigated in ALBERT as mentioned in the paper and CV areas (see refs [1],[2]). Though novelty of this paper is not strong enough, their experimental results are adequate to show the effectiveness of proposed method. The writing and description are quite clear, but you need to clarify the definition of  iters  you used in  SWE exps, does it mean iters_of_weight_share + iters_of_wight_unshare? But for deep learning based models, non linear operations is required, which still leads to gradient vanishing or explosion even with weight sharing. So in my opinion, you d better take a TRUE deep learning model as an example to analyze the effectiveness of weight sharing during training. For example, you can try to analyze deep feed forward neural networks with ReLU activation, which can be treated as a deep piece wise linear model.<BRK>This paper provides a simple way to speed up training deep networks which contain repeated structures, such as the transformer module, by sharing the weights first and then unsharing. The authors did experiments on BERT large for SQuAD and the GLUE benchmark, and showed that their approach outperform the baseline at the same iteration of 500k, and is on par w/ the baseline training at 1M iterations. Basically I think this approach could be a practical way to train deep networks w/ repeated layers. It s like between BERT large and ALBERT, the results are not that surprising. The main contribution of this work is that it provides the empirical results of this training method on several tasks. In Section 3, the authors provide analysis of weighting sharing can achieve better convergence rate and for good weight sharing each layer s update needs to well correlate with its gradients for deep linear network. 2.For untie weights between adjacent layers, SWE A and SWE M, what s the motivation of setting a unified threshold for all layers? Will we get better accuracy if we do not limit to a unified threshold? Basically the experiments will show that \tau should be carefully designed, an arbitrary number may not work, we still need to train some iterations w/ weight sharing for the good model performance.<BRK>The key idea is to start training with shared weights and unroll the shared weights in the middle of the training. The authors report that this strategy accelerates the convergence speed. For example, QQP in the original paper reaches 72.1, whereas this paper reports 71.4, QNLI was 92.7 in the BERT paper and 91.7 in this paper. The paper claims that it uses the same BERT implementation and training procedure as the one used by Devlin et. Some clarification on the accuracy results is needed. First, the paper does not provide an analysis of how the weight distribution between the weights trained with and without weight sharing, so it is unclear what "not be far" means. If so, prior work [5] identified that model weights and gradients at different layers can exhibit very different characteristics, which seems to contradict the argument that "The optimal models are supposed to not be far from weight sharing". 4.The paper says "This means that the weight sharing stage should not be too long", but it is unclear how long is considered as not too long. However, there are some major concerns about the paper. From the text, it is unclear how this hyperparameter has been chosen or will be chosen when training a new model. It would be better to test the sensitivity of the hyperparameter on another model such as GPT 2 to verify the effectiveness of the proposed method. However, from the text description, it seems it employs the original BERT as the baseline ("We first show experiment results English Wikipedia and BookCorpus for pretraining as in the original BERT paper"). As Pre LN has been studied in several prior work [2,3] and has been demonstrated to also have the effect of accelerating the convergence speed, it is unclear whether the observed speedup in this paper is an effect of PreLN or weight sharing/unsharing.
Reject. rating score: 3. rating score: 3. rating score: 5. rating score: 5. <BRK>There are two main concerns about this paper:1. For a defense method, it is necessary to carefully design a convinced adaptive attack and demonstrate the effectiveness of the defense under the adaptive attack. The authors should be more updated on these related progresses.<BRK>Proceedings of the Genetic and Evolutionary Computation Conference. This can usually bedone by adding the detection objective to the loss function for attack. Additionally, although the paper claims to detect zero day, or unknown attacks,in evaluation the selection of attacks are quite limited.<BRK>Summary: this paper is about adversarial detection based on counter attack. The main intuition is that the adversarial sample lies closer to the decision boundary and hence if we do counter attack to the data, the perturbation is expected to be much smaller than the clean data.<BRK>The paper is well organized and easy to understand. 2.Zero day attacks are challenging but more practical than other adversarial samples detection problems. AttackDist can perform significantly better than the baselines to distinguish zero day adv examples. 3.The authors show experiment results across different attacks and adversarial distances; AttackDist is consistently better than baselines. 2017.[2] Li, Yandong, et al."Nattack: Learning the distributions of adversarial examples for an improved black box attack on deep neural networks."
Reject. rating score: 4. rating score: 5. rating score: 5. <BRK>The authors study the problem of training a regression model when only for a subset of the datapoints (those for which their label lie above the current model prediction) the correct labels are available. They would also benefit from writing down the existing work on classification more explicitly, and connecting it to the regression setup in the paper. It is unclear if the classification literature treats a similar (or exactly the same) problem in the classification setting and what is the hard part of translating these results into regression. This means that Theorem 1 is not very informative. In section 3.2, there is little explanation as to why using a \rho multiplier in (13). This does not seem to be in accordance with Theorem 1. The experimental evaluation is thorough.<BRK>I admit this is a pratical algorithm, but it differs substaintialy from the first half of the paper. Summary: This paper considers a regression setting in which the missing values are observed with lower values than the true values. They rewrote the risk and provided an unbiased gradient estimator. Authors provided appealing application for this problem setting.<BRK>In this paper, the authors address a new weakly supervised regression problem. Pros:1.To the best of my knowledge, this paper is the first to solve the weakly supervised regression problem presented in the paper. 2.This paper proposes a consistent learning algorithm to solve the above problem. Cons:1.The presentation of this paper needs to be improved. For example, I understand that in the introduction section, the authors try to justify that the weakly supervised regression problem (where upper side and unlabeled data are available) is reasonable and could be encountered in real world settings. In addition, for the order of Eq.(4) and Eq.(5), I think it would be better to present Eq.(5) before Eq.(4), as Eq.(4) relies on Eq.(5).2.For the proposed consistent algorithm, I would admit that it is novel to some degree, while the technical contribution of this algorithm is limited. In summary, this paper proposed a novel problem setting and a novel learning algorithm, while the problem setting is not well justified and the technical contribution of the algorithm is limited.
Reject. rating score: 3. rating score: 4. rating score: 5. rating score: 5. <BRK>*Summary*: This paper proposes a new dataset based on textbook / classroom chemistry questions for complex knowledge retrieval and aggregation. The authors scrape several thousands questions from online repositories and add additional natural language annotations signifying the quantities to be solved for in each question, as well as the declarative knowledge. Two baselines, one end to end neural and another symbolic, both fail at this dataset. *Recommendation*: 3 . This benchmark is motivated by the lofty goals of encouraging the development of models that can combine knowledge retrieval, complex reasoning, and language understanding. Can you give more details about the annotation process, beyond the short paragraph near the end of section 2.2? Can we see more (random) samples of the dataset, so we can better assess its quality?<BRK>This paper proposes a new dataset for the chemistry domain as a real world QA task. There are some typos and formatting issues, which reduce the paper strength. My primary concern is on the quality of collected questions. The purpose of employing two baseline models is not explained well. In the introduction, the authors say that "to verify the dataset is consistent with the purpose of evaluating AI  comprehensive capability". Comparison with existing datasets can be elaborated more.<BRK>Experimental results show that a neural encoder decoder model and an extractor plus solver do not work well. It is difficult for crowdsourcing workers to generate such complex questions. Weaknesses:* The dataset seems small to acquire the ability to perform complex calculation and reasoning. * The authors uses a pre trained BERT as the encoder of the end to end solver and trained the decoder from scratch. I think pre trained encoder decoder models such as T5 and BART are better as the baselines of the end to end solver than the model used in this paper. ChemistryQA can be a useful dataset to evaluate the ability of chemistry calculation and reasoning, while the dataset seems small to acquire the ability.<BRK>The topic distribution is missing from the paper. Experimental results and analysis is not enough. Overall: The idea of curating and annotating a new dataset for Chemistry QA dataset is good. Also the question complexity analysis/topic distribution is missing. It will be interesting to see how neural symbolic machines/Neural module network perform on this dataset ?
Accept (Poster). rating score: 8. rating score: 7. rating score: 6. rating score: 5. <BRK>The authors propose SCOFF, a novel architectural motif, one with memory, which, as they describe, can serve as a drop in for an LSTM or GRU within any architecture. It is inspired by the notion that when modeling a structured, dynamic environment (such as one with objects moving around), one must keep track of both declarative knowledge and procedural knowledge. (3) Is the factorization of knowledge into object files and schemata helpful in downstream tasks? (4) Does SCOFF outperform state of the art approaches? This is a very interesting paper, with a natural, novel motif. While the evidence put forth is useful, I do think there is considerable follow up work needed to really demonstrate the efficacy of this system. I recommend acceptance. The paper is interesting, well written, and the experiments are useful. A more minor comment, I think more details could be given for the RL task, both in model implementation and in exactly how the test task is specified   apologies if I missed but I only see train environment details in the appendix.<BRK>Review:The authors focus on a very important question of current neural networks: systematic generalization. The authors introduce their method as a way of modeling multiple objects, some of which may share behavior. However, it is not clear how this attention works. However, the way the input image is handled makes this very implausible. Specifically, the problem is in step 2 of Algorithm 1:The object files compete for the input. Can’t it infer the schema to use just from the motion itself? On page 7, “Multiple objects with multiple dynamics”, the authors use the dataset proposed by “Van Steenkiste et al.”, and claim that they outperform their baselines. But they do not compare to the original method proposed in the same work. That is the only way of having a different transformation of the input (assuming the same initial hidden state). I still consider the paper interesting. Why does this make sense? Is there an intuition behind this?<BRK>This paper proposes a new type of recurrent neural network architecture called schema / object file factorization (SCOFF). This model contains multiple weight sharing GRU cells. The input information is fed into each GRU cells through an attention layer. This model demonstrates superior performance than other modular RNN architectures such as RIM on specific tasks. + All experiments results covered in detail including hyperparamters and experimental setting. This is not novelty or contribution. The only difference from Recurrent Independent Mechanisms (RIMs) is that the modules has shared weights   an inductive bias is be useful for specific tasks. The novelty is weak. It s obvious that the proposed model will perform worse than RIM when not all objects sharing same rules. POST REBUTTAL COMMENTS  I thank the authors for the response. All my concerns are addressed. I will increase my score to 6.<BRK>The motivation and the proposal for splitting the schema from the procedural (representational) block makes sense. This is a good idea. A the authors build on top of RIMs, which have shown reasonable ways to model dynamical systems. However the paper itself needs to be improved and we need to evaluate the model more before publication. Firstly, the proposal that SCOFF is a direct alternative for LSTM or GRU and showing that it beats them is not entirely correct. SCOFF comprises of a GRU with a sequence of CNNs operations i.e., its doing more than what a GRU does? Am I missing something? Firstly, a bulk of the notation in the SCOFF presentation is very confusing. The description in the steps is helpful but the motivation and sequence of operations within each step needs better explanation.
Reject. rating score: 4. rating score: 4. rating score: 4. rating score: 6. <BRK>***Summary***The authors present a way to learn the action of an arbitrary orthogonal matrix on a vector via a map from $\mathbb{R}^{n\times n}$ onto $\operatorname{O}(n)$. They show that the map is surjective, and give conditions under which they can invert this action. "Is an follows"  > Follows***Conclusion***I like the idea of the paper as it is conceptually simple and fairly well implemented. First, I do not think that it is competitive with other approaches when it comes to efficiency. The paper has too many computations in it, which does not add to the point it tries to make. This does not help elucidating where this method could be of use over other methods, as mentioned in the previous point. How does the method in this paper compare time wise with this implementation? Even if that is the case that is fine, but please add them to the paper for a fair comparison. This is not the case.<BRK>This paper shows that it is sufficient and, of course, computationally faster to use only one reflection. In the current version, the motivation for me is not clear. The word “only” does not really motivate, does it mean that it’s not that good in terms of accuracy? What is actually the accuracy of such representations? Neural networks represented through Householder reflections are one layer neural networks with an identical activation function. **Minor**:   A lot of missed articles   “Much previous work attempt to alleviate the additional computational resources it requires to constrain weight matrices to be orthogonal.”   There is a problem here. Abstract: no commas before “if”  “because the d reflections needs“  “It is the evaluation of these sequential Householder reflection that cause”  “allows us simplify”  > to simplify   “to train invertible neural network as generative models”  > networks   “Newtons method”  > Newton’s   “Previous work demonstrate”  “Section 2.3.2 then present”  “invertibiliy”  “is simple connected”  “continously”  “we finally arive”  “a single auxiliary reflections”  “where the 6orthogonal matrices where attained”  “The RNNs with 1 auxiliary reflection attains”<BRK>This paper presents a method for representing orthogonal weight matrices of neural networks by simulating an arbitrary number of Householder reflections using an additional neural network to compute a single auxiliary reflection. It does not explain why orthogonal matrices are important. I am not an expert in this field and it is possible that this is obvious, but I think it could be better explained to convince a reader of the importance of the work. It would be nice to see more comprehensive experiments.<BRK>#### Summary:Constraining weight matrices to be orthogonal is a useful but resource intensive task in DL. This paper presents a simple yet powerful approach that shows the sequential Householder reflection method can be replaced by a single learned reflection to achieve orthogonality more efficiently. Not being sequential in nature, the proposed method is more efficient on GPU. Since it is claimed that other methods trade off expressiveness for compute efficiency, a quantitative analysis will help. Also, as a result of the compute efficiency, can the current approach tackle larger datasets?
Reject. rating score: 4. rating score: 4. rating score: 5. rating score: 7. <BRK>This work proposes a probabilistic model which disentangles view, class and shape attributes explicitly (it does not rely on an emergent phenomena of disentangled factors in the latent space). In comparison to a similar approach, GVAE, CIGMO additionally disentangles the content factor into category and shape factors. Obtained results are reasonable and show advantage of the proposed method over related approaches. I imagine the model that can additionally incorporate the data where additional factors are present such as explicit camera viewpoint. The plate model is not sufficient since details that make the model converge in the first place cannot be presented in such a diagram. Other comments:  Overall, the paper is well written, however the authors should focus on typos present in the paper. It is also formulated in such a way that one would argue that mammals are limited in terms of how many objects they can recognize by shape.<BRK>Strong Points:1) The paper was fairly well written and easy to follow. 2) The results are quite promising. 2) It is unclear how \pi_c in equation 2 is determined. Does it follow the same distribution of the dataset? 5) For figure 3B (swapping), based on data preparation, shape should capture identity, but it is not obvious from the figure. 6) For the experiment with MultiPIE, grouping by emotion category might reveal more about the model. 7) The model is C(category) times larger than GVAE.<BRK>Strength:  The proposed model can simultaneously model the category, shape, and view factors of general object images. The supervision is only from the view grouping information, which can be easily obtained. Weakness:  The proposed model is a mixture of disentangling models, which is not in an end to end manner. The authors only compare the proposed model with other methods in two down stream tasks. Experimental results are not convincing. The quantitative comparisons of invariant clustering and one shot classification are interesting settings and clearly show good performance of the proposed method. As the GVAE baseline does not model category (C 1) and the mixture of VAEs are trained with ungrouped data (K 1), it is easy to see why the proposed method performs better in such tasks. In Figure 3 (B), the qualitative results of different views are not clearly shape invariant. It is important to demonstrate whether the proposed method performs well on real world datasets (e.g., LFW face, Pascal3D+). What’s more, the model is trained in a weakly supervised manner. The supervision information can be easily obtained. For example, as the authors claimed, the proposed method can model the category, shape, and viewpoint of images at once, but other previous models cannot achieve this goal. Contribution: The contributions of this paper are good, which can simultaneously discover those factors of general object images.<BRK> SummaryThis paper proposes a categorical invariant generative model (CIGMO) from a set of 2D images that tries to disentangle the factors of data category, intra category geometry, and rendering viewpoint. Also, the paper is very easy to follow. 2) This paper brings in a novel approach that explicitly models data category as a crucial factor for learning disentangled representations. And, with 1 shot supervision, the classification performance is better than baseline methods. 4) The learned geometry and viewpoint factors are well disentangled, as shown in the manipulation tasks. Cons1) Can you show some quantitative evaluations or quantitative user studies to show the quality of the learned geometry and viewpoint disentanglement? Can you provide more discussions/explanations on this? Experiments comparing to baseline methods on unsupervised object category clustering and 1 shot object classification is quite convincing to me.
Accept (Poster). rating score: 9. rating score: 7. rating score: 7. rating score: 6. <BRK>This paper addressed an essential task for large scale application of object detection   semi supervised learning. It introduced a simple but effective Unbiased Teacher to solve the traditionally problematic data imbalance issue. Pros(1) Provides strong evidences and analysis on the class imbalance problem inherited in pseudo labeling methods;(2) Proposed       (a) an interesting learning paradigm where the teacher is the temporal ensemble of student networks;      (b) focal loss in place of cross entropy;(3) The experiment and ablation suggested that the resulting teacher model is not prone to class imbalance induced overfitting and the improvement from SOTA is significant.<BRK>Paper summary:This paper focuses on the pseudo labeling bias issue in semi supervised object detection (SS OD), and proposes an Unbiased Teacher framework to address this issue. Experiments on COCO and PASCAL VOC show that the proposed method obtains the state of the art semi supervised object detection results. + A very simple but effective solution is proposed. + Very solid experiments are conducted.<BRK>+This paper presents a good work on semi supervised object detection (SSOD), which is a very challenging task. This paper shows very good results over the supervised baselines, even when all annotations are used in COCO. +The proposed method is very simple. It seems the paper is easy to be reproduced. +It is a good idea to use EMA of mean teacher for SSOD. However, FL is not well ablated.<BRK>2.NoveltyNevertheless, some contributions seem a bit straight forward and without significant novelty. Pros:1.Revisiting of pseudo labeling bias issue in semi supervised object detection is good. 2.Experiments are solid and convincing. Table 1 & Table 2 show that unbiased teacher consistently improves state of the art methods CSD and STAC by a large margin on both COCO and VOC dataset.
Reject. rating score: 3. rating score: 4. rating score: 5. <BRK>The authors enforce this disentanglement using a regularizer that penalizes the difference between affine transformations on latent representations. The method is difficult to follow with many critical details left out. I can’t find any reference to it in the text. 5.How do you ensure that affine transformations of the latent vector actual produce affine transformations of the image? This does not appear to be a simple zoom transformation. Fully explain the method, neural network architectures, and training parameters.<BRK>This is done by adding an additional "affine regularization" on top of the InfoGAN formulation. The authors experiment on MNIST, dSprites, four shapes, and CelebA data. Does ADIS GAN guarantee alignment in some way that is more reliable than in previous works? Isn t the scalability aspect (modeling horizontal and vertical zoom) explicitly encoded by ADIS GAN, whereas previous works attempt to learn this from data? I could not follow the exposition in the paper. I guessed it might be elements of c, but there are 5 elements in c and 6 A_{ij} s.    There seems to be derivations of a maximum likelihood estimate for constructing c , but these depend on knowing what the A_{ij} s are.<BRK>The proposed ADIS GAN learns to extract affine parameters by adding an affine regularizer on the top of InfoGAN. + The experiments show that the proposed method can learn a disentangled representation with explicit affine parameters. Writing of the paper can be improved. For example, Section 4.1 needs improvements. The notations are not completely clear, and their roles in the whole algorithm are not clear. **Minor issues**  The paper emphasizes scalability, but it is not completely clear what scalability the proposed method achieves and how it is achieved. **Post rebuttal**Thank the authors for the rebuttal.
Accept (Poster). rating score: 7. rating score: 6. rating score: 5. rating score: 5. <BRK>The paper contributes to the growing body of work on self supervised representation learning approaches. It appears to have a strong theoretical foundation based on the concept of predictive information. Mostly clear with adequate detail for the method and experiments.<BRK>This is an interesting paper with several proposed theories on how to improve self supervised learning objectives through a "predictive information" type objective, combined with a masked reconstruction loss. The hypothesis is interesting, and has some benefits, specifically that it does not require contrasting with negatives unlike many recent SSL methods. 3) eval on Librispeech is only on the clean set.<BRK>This paper proposes Deep Autoencoding Predictive Components (DAPC), a self supervised representation learning approach for sequential data. 2) Based on this interpretation, it would also be interesting to investigate the effect that the predictive information regularizer has on the autoencoding model. Strengths:* The paper is written clearly and is easy to follow.<BRK>It is further improved by summing the PI loss applied at multiple scales, and by adding a masked reconstruction (MR) loss. Further analysis for the ASR models would help the readers understand different cases when the MR is required. The paper presents results on three domains, with speech recognition as the main one. Given that the results of the proposed model is worse than those larger models, it is not clear why the authors didn t pre train larger models of similar capacity for fairer comparison to their proposed approach.
Accept (Poster). rating score: 8. rating score: 7. rating score: 6. rating score: 4. <BRK>The analysis is first done theoretically for an early stopped linear least squares problem with different scalings of the features as well as a two layer network. Thus it may have significant impact for the practical performance of neural networks and may be adopted by the community. I vote for acceptance. Strengths:  The paper provides a novel theoretical explanation of the epoch wise double descent phenomenon, which recently has gained some interest in the community.<BRK>This work proposes an explanation for the double descent phenomenon observed by Nakkiran et al.as a function of training time, in models with significant label corruption. A similar effect is shown for wide 2 layer networks, using wide network theory to relate this network to a linear model. I do think that the explanation suggested for the origin of double descent is plausible and valuable. The two convolution networks shown in the paper point to different conclusion.<BRK>This paper studies the phenomenon of double descent. The bound depends on the covariance structure of the model and, in case such a structure is known, one can optimize the learning rates associated to the various components so as to minimize the risk. (2) Analytical results for two layer networks in the "lazy regime". The analysis for linear regression is quite simple. However, I am not fully persuaded of the impact that the results of the submission will have on the community.<BRK>The paper provides an interesting analysis and direction to improve generalization capability by eliminating double decent during training by setting learning rates differently for each feature and using early stopping. I like the idea and potential of this direction. The main theorems are proven only in very simple unrealistic settings. Theorem for neural network training is really theorem for shallow linear models with the fixed kernel, NTK. This is not for neural network training. The assumption that k >  n^10 in terms of the order is unrealistic, and known to make the neural network to be basically a shallow linear model. At the first epoch, training error is also better for the case ii, instead of only testing error.
Reject. rating score: 4. rating score: 4. rating score: 5. rating score: 5. <BRK>Practical performance and the properties of disentanglement are demonstrated in experiments on real data sets. Learning disentangled representation might help obtain a model that is useful yet interpretable and controllable. 2.Consistent and superior disentanglement performances on all datasets. Cons:1.While being neglected by most literature. The setting of controller metric experimentation seems to be trivial or unfair. 5.The presentation of the model is confusing.<BRK>##########################################################################Strength:  Interesting topic and practical in real world application  Relatively extensive experiments are conducted##########################################################################Weakness:  Technical contribution/depth of this work is a bit limited  Execution of the experiments and evaluation may need some improvements  The overall presentation can be further improved##########################################################################Detailed Comments:This work proposed an interesting angle to disentangle item representations and I personally like the pitch of recommendation interpretability and give control back to users.<BRK>The author claims that the approach is distinguishable from the existing work [20, 21] since the existing work relies on data with attribute labels. The critiquing based recommendation has a long history, and many classic approaches could be compared. MAP deduction (or so called F MAP) in [20] is a good example to evaluate.<BRK>Third, there are already plenty of existing works on weakly supervised disentanglement and are neglected and not compared by the submission. Weaknesses:W1: The abstract states that the submission is interested in how to recommend *less* violent movies, *funnier* movies, etc. Second, 0.5+ (achieved via 1% labels) vs 0.9+ seems to be a large gap.
Reject. rating score: 3. rating score: 5. rating score: 6. <BRK>This work aims to study so called “comprehensive” robustness by considering natural accuracy, sensitivity based robustness and spatial robustness simultaneously. However, it is not clear why this makes sense.<BRK>In addition, the paper investigates the relationship between the sensitivity based (lp norm based) and spatial robustness, and proposes a training method called ‘Pareto Adversarial Training’ to find optimal combination between natural accuracy, sensitivity based and spatial robustness. 2.The presentation of the experimental sections needs to be improved in order to reach the acceptance bar of ICLR.<BRK>This paper first provides explanations to the inherent tradeoff between rotation adversarial attack and sensitivity attacks/spatial transform attacks, through their differences in saliency maps. Further, the authors proposed to utilize pareto training to find the best tradeoff among the four dimensions: natural accuracy, robustness against sensitivity/rotation/spatial transformation attacks. ICLR, 2018. The tradeoff between rotation and sensitivity attacks has already been discussed in many previous works (e.g., [2]); the rotation and spatial translation attacks are defined in previous works [3,4] and the combination of the two is kind of trivial; the authors directly apply pareto training [5] in the adversarial tradeoff task, without any modification to fit the new problem.
Reject. rating score: 4. rating score: 4. rating score: 4. <BRK>#### SummaryIn this paper, the authors evaluate the performance of classifiers trained and then later tested on both adversarially generated perturbations as well as more natural perturbations. A more in depth analysis of these phenomena and whether they hold for broader classes of natural transformations would be quite interesting. The figures are quite hard to parse. I m not sure how to interpret Fig 5.<BRK>I read the paper a few times, and I m hoping I understood the authors claims and results correctly (which I believe I did). The authors stress that natural perturbations are designed with care, so that they induce a control loss in accuracy while being similar enough to the original samples so that they are recognizable by humans. However, what follows is an equation, and not any optimization problem to be optimized.<BRK>The authors propose a technique to “standardize” the robustification process across different perturbations. First, as the authors acknowledge, there have been several recent works studying robustness to natural vs. adversarial perturbations. For instance, an alternative would be to tune the level of the shift based on the drop in accuracy of the *unrobustified* network. In addition, not all the curves in the figures are labeled.
Accept (Poster). rating score: 7. rating score: 6. rating score: 6. rating score: 5. <BRK>This paper proposes a VAE based hierarchical model for video prediction. The model employs recurrent model to predict intermediate representations (in the form of label maps) and these representations are mapped to pixel level information, i.e., videos. The authors demonstrate the value of modeling temporal evolution of these representations which enables long term video prediction. Overall, the paper presents impressive results on long term video prediction and the evaluation of the paper is thorough.<BRK>  Summary  The paper extends video to video translation model of (Wang’18) to video prediction by first generating a sequence of segmentation masks and then translating them into videos. The model produces impressive high resolution and long horizon results, and is extensively evaluated on Kitti, Cityscapes, and dancing data, outperforming some previously proposed methods. Decision  The paper proposes a relevant method for hierarchical video prediction with impressive high resolution and long horizon results. Strengths  The paper presents a modern version of Villegas’17a, powered by the advances in probabilistic video prediction and generative adversarial networks. The real authors of this paper are Nevan Wichers and Ruben Villegas.<BRK>###Summary###The paper proposes the hierarchical video prediction model. Specifically, the model first generates a sequence of semantic segmentation maps. And then it generate a sequence of future frames corresponding to the semantic segmentation maps. Since the sequence is generated stochastically, it can generate various futures given same condition. Is it the result of learning the approximate posterior distribution of the following semantic segmentation map, or the result of the hierarchical generation mechanism? Which means, training with the generated semantic segmentation maps as the condition.<BRK>Summary: This paper proposes a hierarchical framework for long term video prediction. The proposed method, which firstly predicts the categorical map and then translate  the map to video frame, is very straightforward and clear. quality: Regarding this aspect, my major concern lies in the lack of the ablation study. In the experiment, the authors present sufficient and extensive results compared with other methods. [`1] Predicting deeper into the future of semantic segmentation.
Reject. rating score: 4. rating score: 5. rating score: 5. rating score: 6. <BRK>This paper propose to utilize a hyper structure network to generate the architecture of the main network, which can help to capture inter channel and inter layer relationships. MCH shows state of the art ImageNet performance in pruning setting. The idea to use a hypernet to help pruning is not new.<BRK>Experimental results also validate the performance of the proposed method. Overall, I think the paper is easy to follow and the derivation is also clear to understand. Nonetheless, there are still some problems that need to be further explored, given as follows:1. This paper only investigates the channel pruning, but the title uses the "model compression".<BRK>This paper proposes a channel pruning method to compress and accelerate pre trained CNNs. * Good results that are competitive with the SOTA.<BRK>## SummaryThis paper proposes hyper structure network for model compression (channel pruning). However, the related work section can be further improved. The other novel aspect of the paper is using GRU for designing the network. ## Strengths  The paper proposed a novel formulation towards the channel pruning problem. They are all relevant and strong channel pruning methods and the authors should have cited them and discuss the main differences (can be used to prune a pre trained model or not) in the related work as opposed to omit them entirely.
Accept (Poster). rating score: 8. rating score: 7. rating score: 4. <BRK>This paper proposes a reliable multi view classification mechanism equipped with uncertainty, called Trusted Multi View Classification. The goal is to dynamically assess the quality of different views for different samples to provide reliable uncertainty estimation. The idea is clear and well motivated. + The paper is well written and clearly presented. The method seems to be of great potential in real world (cost sensitive) applications.<BRK>The idea of transferring classification output to the parameter of Dirichlet distribution to give uncertainty in output is novel and interesting. Paper is well written and mathematically sound. To my knowledge using Dirichlet distribution for bringing uncertainty in output in multi view multiclass classification is new. It will be good to discuss the reason behind that. It would have been better to have in the main paper.<BRK>My main concern with this paper is the experimental results. First of all, the paper is a kind of Bayesian multi view learning method and should compare with Bayesian and deep CCA based method as well. Second, the paper used a Dirichlet distribution for each view. Concerns/questions/comments:1  The authors added Gaussian noise to half of the views. 6  A comparison of the methods in semi supervised classification would be great.
Reject. rating score: 5. rating score: 6. rating score: 6. rating score: 7. <BRK>This paper develops a framework for unsupervised learning of graphs. The representation is computed with an encoder $E$ applied to a graph data $(X,A)$, containing vertex data $X$ and adjacency matrix $A$. The GraphTER  considers perturbation of node data $X$, but also perturbation of adjacency matrix since they define the adjacency matrix as a function of the node data, that is $\tilde{A} f(\tilde{X})$. The current paper focuses only on adjacency perturbations. I don t see the benefit in the new loss formulation   if there is such a benefit it should be highlighted and demonstrated. Is there a way to quantify how equivariant is it? What is the benefit in going through the mutual information formulation? If indeed these classifiers are trained with labeled data   why is this method is called unsupervised? I am confused by that part. UPDATE: I would like to thank the authors for their rebuttal. I have read it, however unfortunately, I am not convinced the indicated differences from previous work is sufficient to warrant publication at ICLR. I am also not completely clear about the equivariance point.<BRK>This paper presents the Topology Transformation Equivariant Representation (TopoTER) as a learning method for the (uncommonly) unsupervised learning of node representations  in graphs, with applicability to Graph Convolutional Neural Networks. The paper falls well within the scope of the conference. Its writing could be improved and would benefit from a thorough language revision. To the best of my knowledge, and not being an expert on graph data analysis, the maths seem sound. The experimental work, including comparison with alternative approaches is quite detailed (and well informed of the competition). I guess it verges on cheekiness, but given, the particularities of accepted formats, assume it is fine.<BRK>#### Goal  This work considers the graph task of learning node representations that are invariant to small edge perturbations. It achieves this through a data augmentation procedure that samples new “fake” edges and regularizes the GNN equivariant representations to be unable to predict these fake edges. While the first paragraph of 3.2 states the fake “edges” are given by a random matrix \Sigma obtained from a Bernoulli distribution with parameter p. I am not sure what that means. Isn’t A fully determined by \mathcal{E}? It would be good to clarify. The transformation $t$ and its homomorphism \rho are given in Def 1 but not given a formal definition later in the paper. In the proposed algorithm in Section 3.4, what are t and \rho? The difference between what the authors did and data augmentation should be front and center at the experimental evaluation. The Representation Encoder looks like an equivariant node representation to me. The authors need to give a solid theoretical reason for their approach, otherwise it is not believable. GraphSAGE has an unsupervised version that also adds fake edges at random as a form of data augmentation. In International Conference on Learning Representations.<BRK>The paper propose an unsupervised method for self training of graph neural networks (GNNs). The authors provide information theoretic justification to their method using maximization of the lower bound of the mutual information on their objective. Their approach is based on maximizing the mutual information between a perturbed graph topology and its node representation. Strong points:  very good results (some of the results are comparable to supervised method and the improvement achieved on the other unsupervised methods is significant)  simple approach with theoretical justification  paper is nicely written and easy to follow
Reject. rating score: 4. rating score: 5. rating score: 6. <BRK>It proposes an interactive code retrieval system, called iPTR to perform cross language retrieval with minimum code pairs. The idea of training autoencoder for each language and fuse encoder and decoder of different languages is also very interesting, though questionable as mentioned below. Concerns:  The major concern is that this paper does not made significant novel and technical algorithm/theory contribution, but rather like proposing a system. In the experiment, what is the database to retrieve from? Is it that all training code form the database, or is it all training and testing code form the database? In practical situation, the desired translation is very likely to not exactly match the some code in the database. Therefore, I don t understand why the program accuracy of retrieval method can achieve so high if the database doesn t overlap with ground truth codes a lot.<BRK>The authors say that "theWord2vec and Code2vec variants of PTR cannot outperform PTR with our proposed feature representation based on structural features", but I think code2vec encodes very similar structural features in a potentially more concise way. This work proposes a retrieval based approach for program translation. Specifically, iPTR includes a query transformation model (QTM), which generates the feature representation of the target code, given the feature representation of the source code as the input. The idea of using the paths is similar to the code2vec paper. Specifically, encoders and decoders of different programming languages could be trained in a similar way to the training of autoencoders. Program translation is an important application, and the authors did a good job of evaluating on existing benchmarks and comparing with different types of baseline models. If this is the case, one clear limitation is that the retrieval approach could only search for existing code in the corpus, while the synthesis model could combine lines from different code snippets to construct the output code. Do you compare among the entire training corpus? For example, are variable names included in tokens, or are they simply denoted as "identifier"?<BRK>This paper proposes a program translation (retrieval) method by combining  syntax tree, transformer with specific encoder/decoder, and interactive signals from user. The novelty of each used techniques is limited, but they are reasonable for the code translation task with high accuracy, even for unsupervised version without interactive signals. The experimental results are solid and demonstrate the power of representation of the proposed variant of syntax tree. The idea of AE/AD with specific language is quite simple, but it seems that the QTM successfully transfer the representation cross different languages. The performance gain achieved by interactive signals is reasonable. Could it be applied to other downstream tasks after pre training?
Accept (Poster). rating score: 8. rating score: 8. rating score: 7. rating score: 6. <BRK>A clear accept. This paper is basically well written and easy to follow what they have done. According to experimental results, I find the proposed matrix decomposition based method outperforms several attention based methods in mIoU for semantic segmentation and FIN for image generation.<BRK># Summary:The paper presents a method based on matrix decomposition (MD) for encoding global context in computer vision tasks. Also the method shows advantages with respect to self attention as far as efficiency and memory requirements are concerned. Regarding writing quality, the paper is clear and easy to read. # Strengths:The paper presents a novel and simple method for capturing global context, demonstrated on two challenging computer vision tasks, namely semantic segmentation and image generation.<BRK>This paper proposes to use matrix decomposition to construct low rank representations to find the long distance correlations in context, which is demonstrated more effective than popular self attention mechanism. They conduct experiments on semantic segmentation and image generation to demonstrate the superiority of their methods regarding modelling global dependencies and computational cost. I believe that there is clear novelty in the proposed method. The paper is well written.<BRK>MD can be implemented with vanilla matrix factorization or non negative matrix factorization. To validate the performance, they experiment on semantic segmentation and image generation. Furthermore, it is hard to find sufficient reasons that MD performs better than attention based models. It is hard to understand that low rank will be helpful. ", but the paper is only for the "self" attention and MD. Are there results for Encoder Decoder structured tasks (such as Translation)?
Accept (Poster). rating score: 6. rating score: 6. rating score: 6. rating score: 4. <BRK>### SummaryThis paper proposes a hierarchical memory structure, where each layer of memory stores information about the corresponding level of sub task and uses this structure for better behavioral cloning. By looking at changes in each level of the memory module, which means changes in sub tasks, we can find the task decomposition. The experimental results on the Craft World (grid world) and Jaco arm dialing environments demonstrate that the proposed method can learn better task decomposition in both unsupervised and weakly supervised settings. ### Strengths  The idea of using the ordered memory module to learn the subtask structure in multiple levels is novel and intuitive. The paper is well written and easy to follow. The learning curves and final performance in the Dial task are missing in the paper. It is not clear whether the hierarchy in memory is important or not. Ablation study on a different number of levels in the memory module can help to understand the importance of the hierarchical memory. It is unclear from the text and figure. However, the paper needs a more thorough analysis of the results, and an additional ablation study can make the claims stronger. In summary, the reviewer thinks this paper can be accepted with a few more experiments and analysis.<BRK>*The authors introduce a new architecture, the Ordered Memory Policy Network (OMPN) that has an explicit inductive bias for modelling a hierarchy of sub tasks. They claim that OMPN discovers task decomposition on demonstration from imitation learning, both in an unsupervised and weakly supervised setting, and its performance compares favourably with that of strong baselines on the Craft and Dial tasks. There is some analysis of performance, showing that the model can indeed identify the sub task boundaries in both the Craft and Dial tasks. Perhaps the authors could try to communicate their motivation for the design choices in a revised version of the paper   but I leave this up to the authors. For experiments in Craft, it is not mentioned how the experience is collected. The paper presents a novel method that outperforms other baselines on a difficult problem and does provide some analysis as to how the model works and some ablation results. Overall, the paper is well written, but there are a few points that I believe need to be addressed, if the paper is to be accepted. *Ask questions you would like answered by the authors to help you clarify your understanding of the paper and provide the additional evidence you need to be confident in your assessment.<BRK>Summary: Paper introduces Ordered Memory Policy Network (OMPN) with an objective to learn sub task decomposition and hierarchy from demonstrations in the context of Imitation Learning under unsupervised and weakly supervised settings. The proposed solution views sub tasks as finite state machines represented as memory banks that are updated via top down and bottom up recurrences. Strengths:Paper is well polished and easy to follow. OMPN is shown to be effective in two domains. For example, a comparison to the Relay Policy Learning[1] (which the paper claims to be the most related work) is missing. Selected tasks have a very shallow task hierarchy and sparse task structure. The ceiling of the approach and limitations aren t clearly outlined and are less evident. This is also evident from the tasks considered in [1] and [2]. These methods have shown to be effective with play demonstrations in rich scenes such as kitchen and study table scenes where are underlying task structure is much more convoluted to uncover. I d strongly advise on including experiments under similar settings which will make the submission really strong.<BRK>#######################################################################Summary:In this paper the authors propose a new method for task decomposition. First a new neural architecture is proposed which includes a set of  memories  who s operation is inspired by the HAMs architecture in which individual memories correspond to subtasks which can be internally updated, call the next level subtask in the stack, or return control to the operating subtask. #######################################################################Reasons for score:While I think the proposed method is interesting concept there seems to me to be insufficient evidence to suggest that it is general. The authors seem to explicitly control the depth of the memory stack and the expected number of subtasks in subsequent analysis but do not present alternate results. I think the current work would be notably stronger given broader experimental support. While I understand that this is to some extent inevitable when implementing new methods, I think the paper could be notably bolstered by discussing these further. For example I am not sure how the number of memories  n , or the dimension of those memories  m  ought to be chosen in general, or what the effect of different choices might be.
Reject. rating score: 5. rating score: 6. rating score: 7. <BRK>Experiments are performed in small and large scale datasets to demonstrate the effectiveness of the proposed algorithm. Comments:This paper has clear writing and well established methodology. However, the main drawback to me is the lack of novelty. The [DomainBed](https://github.com/facebookresearch/DomainBed) is a suite which could conduct systematical evaluation for domain shift algorithms. In its [public results](https://github.com/facebookresearch/DomainBed/blob/master/domainbed/results/2020_10_06_7df6f06/results.tex), ARM s performance is consistently worse than ERM (like many other algorithms who claim to improve on OoD setting), in all three validation method (training domain validation, leave one out, oracle). I am familiar with DomainBed s implementation, so for me this difference indicates that the authors might pick the hyper parameters and datasets based on testsets, as DomainBed is different from regular evaluation for its random parameter picking and extensive datasets searching.<BRK>The novelty of this method is the incorporation of meta learning style training and test time adaptation for unlabeled data. The method is called adaptive risk minimization and there are two meta learning approaches provided, contextual and gradient based, in the paper. More should be discussed in the paper. And many should serve as baselines. Given the weak points, I recommend weak rejection for this paper. Instead of the group shift assumption, I think a more interesting question is: what is the assumption that will make the proposed method to work? This seems to be the case from the data used in the paper.<BRK>The main contributions of this work are as follows:1. The authors propose ARM by employing meta learning approaches to solve distribution shift problem. I still have a few key concerns below that prevent me from giving an acceptance. Firstly, the overall contribution seems incremental to meta learning approaches. The authors highlighted the difference between ARM and ERM as the constraint in eq.(1). It would be great if the authors could explain in more details of the technical contributions, if I missed anything here. I increased the rating to 7.
Accept (Poster). rating score: 7. rating score: 7. rating score: 7. rating score: 6. rating score: 5. <BRK>##########################################################################Summary:This is a nice experimental paper on comparing various methods for MBRL. Overall, the paper provides a good benchmark for MBRL. The writing is very clear and easy to follow, also without redundancy. The paper is also well structured. 2.The seven properties and metrics are mostly well motivated and well defined. 3.The authors discussed the results clearly with implications, not just stating which one is better, but also discussed when and why it is better. I think the result of the necessity of probabilistic vs. deterministic models in different scenarios is a good contribution to this field.<BRK>This paper analyses the random shooting control strategy in combination withvarious generative models \( p(\by_{t+1} | \mathcal T_t) \), which modelobservations \(\by\) conditioned on the history of observation action pairs. Both statistics on thedistribution associated with the generative model under a fixed policy andreward dependent metrics were defined and analysed for a range of models. The paper s contribution are twofold. Reward based ``dynamic  metrics are evaluated under a random shootingcontrol mechanism. This framework heavily simplifies model evaluation byproviding evaluation metrics and fixing the environment and control strategy. Second, this work applies the conceived frameworkto evaluate a range of models on two different environments. The authors conclude by claiming that 1. There s an inherent trade off between simplicity of the study and generalityits conclusion. The future directions outlined by the authors of extending the results tolarger systems and other planning strategies are very relevant to reduce theconcerns of generalisability of the results and applications to problems ofhigher dimensions.<BRK>This paper tackles a key problem in model based RL that is to identify the essential properties a predictive model needs to have to achieve good control performance. An important finding from this investigation is that deterministic models, when trained with a likelihood score in a generative setup, has comparable performance with the probabilistic modes. The findings lead to state of art sample complexity on the Acrobot system by applying an aggressive training schedule. Strength: 1) The paper is well written and structured. 2) The paper provides some good practices for evaluating model based RL which will help to advance the field. The authors did a great job in clarifying all the concerns raised by the reviewers.<BRK>Paper Summary:This paper performs a detailed ablation study over different mechanisms for predicting dynamics for model based control. The paper proposes its own metrics for models, evaluates how different types of uncertainty impact predictions, and measures control performance with random shooting MPC. By implementing a new hyper parameter schedule, the paper shows new SOTA performance (in terms of sample efficiency) on the acrobat task. Score Summary:The analysis in this paper is very warranted, but the methodology executed raises many questions as to if the claims will be generalizable. The paper evaluates models on only one task and on their own metrics, so there is difficulty in matching the paper to the literature.<BRK>The paper discusses the quality of surrogate models for model based RL. There are several things I like about this paper:   There is lack of existing work in throroughly comparing surrogate models in model based RL  The authors do a great job defining and arguing for evaluation metrics  (Section 2 is excellent)  Defining requirements to the model that are falsifiable (2.1) are good to augment quantitative comparisons with qualitative onesOn the other hand there a several things I think that could be improved:  Introduction:  The introduction  was quite confusing: Not only was it intermixed with conclusions that seem out of place. Especially when talking about both uncertainty in a Bayesian sense (uncertainty over parameters or functions, epistemic) and "randomness" (stochastic effects of the environment) it is very easyto be confusing. It is not clear here what you mean by "probabilistic":  Parametric uncertainty (like in GPs) or models that can model multimodalities (like MDN)? International Conference on Machine Learning. "Learning multimodal transition dynamics for model based reinforcement learning." Overall I think this paper is written quite well,  deals with a topic that is relevant.
Reject. rating score: 3. rating score: 4. rating score: 5. <BRK>The authors propose to use the same formulation of adversarial losses over space and time as done by Xie et. The paper reads more like a report as opposed to showing unique insights for the niche problem the authors have tackled. There is little to no novelty in the MSE and adversarial loss formulation over space and time. The use of a generator and two discriminators is exactly the same as done in tempogan. I can see two differences:1. The evaluation is contrived in my opinion. It is natural that tempogan will underperform wrt to MSE metric. The frequency based metrics are derived by the authors and the sparse qualitative comparison to tempogan on a single example make it hard to be confident about the improvement over tempogan. I would like evalaution with more neutral metric which exists in literature beyond the ones in the paper. There should be ablation studies that discuss the suggested improvements in architecture over tempogan. As the MSE loss is evaluated over paired samples,  the usefullness of GAN should be validated in the paired case. Some discussion on directly optimzing the frequency metric proposed in the paper using a GAN will be useful to the readers. Overall, the originality of the work is very weak and experiments are insufficient.<BRK>**Summary**This paper presents a GAN framework to learn spatial and temporal representation on complex physical surfaces to apply to simulation. The method represents data in a SDF like way so it is agnostic to the properties of material and simulation model. A loss function is proposed to evaluates on a grid based Fourier transform of the output and ground truth to better preserve high frequency details (temporally and spatially). **Strength**Using generative model for physical simulation is an interesting task. Physical simulation contains high temporal frequency / variance that would benefit from GAN that is known to preserve high frequency features. **Weakness**      Method on Fourier domain supervision lacks more analysis and intuition. It s unclear how the size of the grid is defined to perform FFT, from my understanding, the size is critical as local frequency will be changed using different grid size. Is it fixed throughout training? In figure 7 result and supplemental video result, SurfGAN produces smoother results (MSE seems closer to the red ground truth in figure 7). This seems contradicts the use of Fourier components for supervision   what causes this discrepancy? It s not clear what the columns mean   it is not explained in the text or caption.<BRK>This paper introduces the adversarial generative network into a physical simulation task, i.e., reconstruction and refining of complex surfaces. Several losses are applied in the proposed framework, such as MSE, adversarial losses, low resolution reconstruction loss. Experimental results seem good. Pros:+ As far as I know, this is the first work that uses GANs to refine complex surfaces. Specifically, MSE loss is introduced to help the low frequency reconstruction, a spatial discriminator and also a temporal discriminator are used during the training progress, the adversarial losses also provide the ability of learning from unpaired samples, and to overcome the shortage of unpaired samples that cannot learn an exactly map function, a down sampling reconstruction loss is applied as a cycle consistence loss. Second, many loss functions are introduced in the proposed method. An ablation study is desired. The frequency part is interesting. Do you use the frequency block for reconstruction loss, or only use it to guide the training progress? Most of the losses have been proposed previously.
Reject. rating score: 4. rating score: 4. rating score: 5. rating score: 5. <BRK>The paper proposes to use class wise similarity to measure task uncertainty in few shot learning. Experiments on mini imagenet  and CUB 200 show that it can improve MAML and ProtoNet’s classification performance and uncertainty estimation. With the current method, I think if the data set gets bigger and more complicated, the classification performance will drop significantly. Because the current method would filter out the hard tasks and ignore them during training to some extent. As for the experiment part, the paper also only conducted experiments on relatively small and simple data sets, mini ImageNet and CUB 200. It is a false claim. There are a lot of  post training methods to measure uncertainty at test time. I think the rebuttal has not addressed my concerns, especially the one related to the  ill defined  tasks. The  ill defined  tasks detected from the proposed method could be  hard but good  tasks benefitting the test. I think if the authors can take a further step to disentangle uncertainty estimation and classification, this con of the proposed method can be removed in the future.<BRK>Overall, this paper studies uncertainty in few shot classification. The main contribution of this work is not well motivated. The main contribution of this work is imposing the class wise similarity on the meta update rule in Equation (4). The meta update is part of the model agnostic meta learning pipeline as shown in Equation (2) [Finn et al., 2017]. There are many challenges mentioned in this work, such as aleatoric and epistemic uncertainty, few shot classification, distributional mismatch between support and query data, the trade off between calibration and the performance of a model. It is not clear what is the main problem that authors aim to solve and how it promotes the contributions of this work.<BRK>This paper presented a task calibration (TC) method, which introduces the notion of "distributional uncertainty", for few shot classification. I think this paper deals with an interesting idea but should be improved further. The authors should do some more work to demonstrate their contributions. My comments are as below. The authors should evaluate and compare in terms of the computation time to demonstrate the proposed method is really computationally efficient compared to those in a Bayesian fashion. 2) Regarding the contribution (2), there are a variety of meta learning models that have been presented in recent years (e.g., MetaSGD, TAML, and many more). Which of them can be combined with the TC method? It would be great if the authors investigate the applicability of TC to other existing methods and see if TC improves their performance by experiments as well. How, and in which aspect, could you say the proposed methods are "significantly" better? For example, "That is, modeling episteme is not much helpful for the calibration in few shot classification." and the sentences that describe the main contributions. i ve upated my rating after the authors  response.<BRK>The paper presents a task calibration method to for meta learning, aiming at better task uncertainty estimation. The method modifies the MAML meta learning approach by weighting the task specific loss using class wise similarity, measured by the cosine similarity of the task features. The authors further propose to use this weighting on metric based models for few shot classification. However, some comparisons to existing uncertainty calibration methods are recommended to show a more comprehensive evaluation. Probabilistic MAML [1] and Bayesian MAML [2] could be readily applied as additional comparisons to the author’s proposed method. The authors motivated the method with the discussion of distributional uncertainty. The presentation can be improved for better readability in section 3.2. post rebuttal I am downgrading my score as the authors did not address most of my concerns.
Reject. rating score: 5. rating score: 6. rating score: 7. <BRK>It proposes Semi Relaxed Quantization (SRQ) that uses a multi class straight through estimator to effectively reduce the bias and variance, along with a new regularization technique, DropBits that replaces dropout regularization to randomly drop the bits. Extensive experiments are conducted to validate our method on various benchmark datasets and network architectures. Pros:  Reducing variance of gradient estimator in Gumbel of RQ using multi class STE. A novel dropbits technique to reduce bias in gradient estimator of SRQ, as well as supporting the mixed precision scheme. A "quantized winning tickets" is introduced to train probs of binary masks so that learning proper bitwise for each layer. Cons:  The SRQ process seems not to be novel enough. Actually here pi is just a multi class sigmoid output, and y is derived directly by argmax(prob_pi) in forward process (and calculate gradients using STE.). Thisis quite similar to existing quantization methods using softmax + STE. [1]  Only 3/4 bit results of Res18/MobileNetV2 showed in ImageNet. I d appreciate it if authors could offer more quantitive analysis on more architectures and tasks. More comparisons on latency/energy/flops during the training and evaluation process should be provided to validate the SRQ + DropBits. [1] Hardware aware Softmax Approximation for Deep Neural Networks. Xue Geng et al.***After rebuttal and discussion This paper proposes a new network quantization framework. In particular, the proposed DropBits is somewhat novel. For example,  why SRQ can reduce quantization error has not been well motivated and explained. The definition of distribution bias is still unclear. I would like to decrease my rating to 5.<BRK>### SummaryThis work presents 1) Semi Relaxed Quantization (SRQ), a method that targets learning low bit neural networks, 2) DropBits, a method that performs dropout like regularization on the bit width of the quantizers with an option to also automatically optimise the bit width per layer according to the data, and 3) quantised lottery ticket hypothesis. SRQ is an extension of Relaxed Quantization (RQ), which is prior work, in two ways; firstly the authors replace the sampling from the concrete relaxation during training to deterministically selecting the mode (which is non differentiable) and, secondly, they propose a specific straight through gradient estimator (STE) than only propagates the gradient backwards for the elements that were selected in the forward pass. DropBits is motivated from the perspective of reducing the bias of the STE gradient estimator by randomly dropping grid points associated with a specific bit width and then renormalising the SRQ distribution over the grid. This essentially induces stochasticity in the sampling distribution for the quantised value (which was removed before by selecting the mode in SRQ). ### Pros  This work provides a set of additions that improves upon prior work  The DropBits method is novel and allows for learning the bit width in a straightforward manner  The results improve upon recent works that learn quantised neural networks### Cons  Some claims from the authors are misleading while others are not precise  The computational complexity of the method is not discussed   Some experimental settings might not be consistent with some of the baselines### Detailed feedbackThis work tackles the problem of learning quantized neural networks and the authors show empirically that their proposed method achieves good results. Nevertheless, I believe that there are still some important aspects that need to be addressed before I recommend acceptance for this work. First of all, the comparison against the prior work on figure 1 is misleading; the authors compare the *entire categorical distribution* (i.e., the one obtained after discretising the logistic onto the quantization grid) of SRQ at Fig 1(b) with a *single sample* from the concrete relaxation of the same distribution at Fig.1(a) right for RQ. In fact, the underlying categorical distribution will be the same for both SRQ and RQ in the specific example of figure 1. Furthermore, it is worthwhile to notice that the underlying categorical distribution (i.e., pre relaxation) does have support for the value of  a (as the p(g_i    a) is nonzero at Fig.1 (b)), thus it is not unreasonable that there are specific samples which lead to the quantised value being  a, thus incurring larger quantization loss. Furthermore, I believe that the discussion about SRQ misses some important points that would improve the clarity of the work if they are addressed. Selecting the most probable point in the categorical distribution for the forward pass is equivalent to rounding to the nearest grid point, which can be done much more efficiently than computing the entire categorical over the grid and then taking the argmax. Finally the authors argue that their novel multi class STE reduces the variance of the gradient estimator but no formal justification is given apart from some hand wavy arguments. Furthermore, for the second point with respect to the benefits of the multi class STE; while it does seem desirable that it aggressively clusters the weights and activations around the grid points, I wonder how much can that hinder convergence. Do you ever observe that the weights can be prematurely “stuck” (and thus lead to a bad local minimum) and do the weights ever move further away than just the closest grid point? DropBits could potentially help with the latter part, but it would be interesting to see what happens without it. DropBits in my opinion is the main novel idea of this work, and it is an interesting way to learn the bit precision of each tensor in the network. The main motivation behind DropBits seems to be converting the sampling distribution of SRQ (which is deterministic) to a stochastic one. If this is the case, then why have it be deterministic in the first place? You could just sample from the original categorical distribution (by, e.g., using the Gumbel Softmax STE which gives samples exactly on the grid) in the forward pass and use your multi class STE approximation in the backward pass. It would be interesting to see how that fares with SRQ + DropBits and would highlight whether the main benefit of DropBits was the regularization aspect (and not that of improving the sampling distribution). As for DropBits in particular; it seems that you drop bits independently with each of the gates z_1, z_2, … (i.e., figure 3). Finally, a couple of other things that I believe should be addressed; the authors don’t make any discussions about the computational and memory complexity of the resulting algorithm. This doesn’t seem to scale very well. As a result, their QLTH seems to state the opposite than what the original LTH was about; it states that a QLT is obtained when you manage to find a network X that a.) has better performance than a network initialised to the bit width of X and trained to convergence. Based on the aforementioned points, I cannot at the moment recommend acceptance for this work. Nevertheless, as I believe DropBits is an interesting idea, I would encourage the authors to put in the effort and rework the paper by addressing these points over the rebuttal.<BRK>This paper proposed a novel network quantization method to reduce the bit lengths of the network weightsand activations, which is one of the most important problem for resource limited devices. The presentation of this paper is well written and organized. The problem definition is clear, i.e., relaxed quantization with the Gumbel Softmax relaxation suffers from bias variance trade off depending on the temperature parameter of Gumbel Softmax. The proposed method to solve this problem is reasonable. The experimental results are also convincing. In Table 3, "MNIST" should be LeNet 5. 2.Section 3.4 is a little dense. 3.I understand that there is little space, but embedding formulas in sentences is difficult  to read. 4.I recommend you to show the result only using SQR for  ImageNet  in Table 2 as in MNIST and CIFER10 in Table 1.
Reject. rating score: 4. rating score: 5. rating score: 6. rating score: 6. <BRK>Hence, the latency experiment should compare the power of two quantization that is an important baseline in the paper. 8.In conclusion, the basic idea of the paper is to learn the quantization levels for each quantizer, which is a trivial and direct motivation. However, the proposed solution is inefficient and not elegant.<BRK>Despite all these merits, the experimental results shown in the paper might not be very compelling compared to the state of the art (e.g., [Esser et al., 2020]). This counters the authors  claim that the uniform quantization is a serious limitation.<BRK>Any impact on accuracy loss in the quantized model? While there are a few glitches in experimental evaluation, they do not hinder the overall novelty of the proposed approach. Weaknesses of the paper:   Experiments seems to be too conservative to get a noticeable improvement in memory footprints.<BRK>3.Extensive experiments on ImageNet and CIFAR 10 demonstrate the effectiveness of the proposed DDQ on various advanced networks, such as ResNet18 and MobileNetV2. Please provide more detailed discussions about that. 3.The experiments are insufficient in Section 4. 6.Bit operations (BitOps) [1][3] is an important metric to measure the computational costs of a quantized network. More results in terms of BitOps are required.
Accept (Poster). rating score: 7. rating score: 7. rating score: 7. rating score: 7. <BRK>## SummaryThe paper proposes a convex formulation for shallow neural networks with one hidden layer and vectorial outputs. This is an extension on a line of previous works (Ergen & Pilanci, 2020a) and (Ergen & Pilanci, 2020b) where similar results have been established for the case of scalar outputs. Therefore, I feel the results presented in this paper might be better suited for a journal version of these previous works.<BRK>Summary: This paper generalizes the results of Pilanci and Ergen (2020) showing that the non convex optimization problem corresponding to the training of a one hidden layer muti output ReLU neworks can be solved using convex programming. It looks like the proposed algorithms are currently only of theoretical interest. it would be great to understand if the solution obtained with convex programming translates to better misclassification error compared to sgd. Currently how much time does the convex program take to solve? at least this is what I would find most natural.<BRK>The draft is a vector extension of [1] on studying how to approximately solve the global optima of a two layered Relu network. I like the idea of linking 2 layer NN with copositive programs, the writing is good, and the proofs seem to be correct. Overall, this is a good extension of [1], but the improvement in complexity only works in trivial case. The sj^(k) is not used anywhere. Maybe it can be more clear.<BRK>This paper showed that a two layer vector output ReLU neural network training problem is equivalent to a finite dimensional convex copositive program. I understand the analysis of multiple layer neural networks can be potentially much more difficult. It might be good to compare the current paper and their result. I think this is a good theory paper that shows an interesting connection between two layer network training and copositive program.
Reject. rating score: 2. rating score: 5. rating score: 5. rating score: 5. <BRK>However, these issues are not convincing as no citations or theoretical proof is provided in this paper. Why the so called inducing point GP can speed up inference in GP model? All these important points are not clarified in the introduction. The author fails to prove that the spectral normalization as a regularization scheme can be uses to mitigate uncertainty collapse. 4.I also have some concerns on the experimental results of causal inference. The authors should fully explain the uncertainty settings in causal inference, as most of the causal baselines are not proposed for uncertainty settings.<BRK>Why does it compare only to deep ensembles? Strengths: The idea of using deep kernels within GP is a good solution that allows benefiting from both the expressiveness of the kernels and uncertainty estimates for GP. In general, the whole presentation of related work and positioning of this paper in the uncertainty literature is not clear. The presentation of the method should be better structured.<BRK>Experiments show that vDUQ is effective in uncertainty quantification tasks. ##### Reasons for score:The idea is clear and the paper is easy to follow. This perhaps explains how uncertainty collapse happens in out of distribution data to some extent, but is this the primary cause of undesired uncertainties? This seems a different conclusion form the results in Amersfoort et al.2020.It would be better to provide more discussion about it, which seems not to be expected. (2) The authors claim the vDUQ can be trained in an end to end fashion in Section 3.<BRK>The main contribution of this paper is methodological. Having said that, the proposed approach can be seen as a modification to [Liu et.al, 2020], with different approximation and different regularization. The authors discuss [Liu et.al, 2020] in related work, highlighting the important difference that [Liu et.al, 2020] is a parametric model, but it is similar in that both formulate it as a GP and regularize for distance awareness. Yet, the experiments miss the point of elucidating how much spectral normalization compared to other normalization schemes. The authors mention as a strength that a low number of inducing points is good enough, so showing evidence for that would strengthen the paper.
Reject. rating score: 4. rating score: 5. rating score: 6. rating score: 7. <BRK>The authors are to commended for undertaking a proper evaluation, including human experiments, which are often painful to conduct but quite helpful. As someone unfamiliar with the original paper from Ming et al, I found it easy to get up to speed. Can the authors give some clarity on this, for IMDB as well as the other datasets? I also suspect that averaging the outputs, rather than feeding them through an RNN would be comparably accurate, and simpler. 5.For the model diagnosis human experiment, the users were only shown three examples. Something like RoBERTA would likely have stronger results, and be more representative of SOTA.<BRK>The authors propose ProtoryNet,  a prototype based model for paragraph classification that associates each sentence in the paragraph with a relevant prototypical sentence from the training data. **Update**Thank you to the authors for the revisions, and great to know that the experimental results have improved significantly. 2.Related to the above question, it is common to fine tune BERT models on the dataset of interest. In the absence of an updated manuscript, it is difficult to update my score appropriately, so I will leave it as it currently is.<BRK>Summary: this paper presents an RNN sequence classifying model that generates a prototype for each sentence in a paragraph. Concern on user evaluation:  There is some improvement but the error bars are large, it is not clear if the differences are statistically significant. Experiments found improved accuracy compared to a previous model that generates only one prototype for a paragraph. The idea of generating a prototype trajectory for sentences in a paragraph is interesting and novel to my knowledge.<BRK>This paper presents ProtoryNet, a framework for text data that classifies and explains the prototypes  results. PloS one, 10(7), e0130140. The idea presented in the paper is really interesting: it allows for an interpretation based on prototypes that is simple and understandable while achieving acceptable prediction performance. The prediction performance of the model is fine, but not excellent. The last part of the framework uses a LSTM to predict the sentiment of the sentence.
Reject. rating score: 4. rating score: 4. rating score: 5. rating score: 5. <BRK>The article presents a new approach for learning representations of dynamic graphs. Model violates the temporal sequence. 3.Complexity analysis is given. 4.The paper is poorly written, and needs to be checked for language consistency throughout.<BRK>The major contribution of the paper is the Fourier temporal state embedding.<BRK>The paper studies dynamic graph embedding and proposes the method that first transforms the signal of certain edge s existence to its frequency domain by DFT and then considers it as edge features followed by the simplified vanilla GCN. This is one limitation of the paper. However, for the baseline which uses vanilla GCN + RNN, GCN only has O(|E|) time complexity. 5.My major concern is the complexity analysis.<BRK>This paper presents a new method called Fourier temporal state embedding. The motivation of this work needs to be stated clearly.
Reject. rating score: 4. rating score: 4. rating score: 4. rating score: 7. <BRK>Summary:The paper studies the structure of the Hessian matrix of loss functions by approximating Hessians using Kronecker factorizations. Interesting observations on the models trained with batch normalization. The main paper contribution, optimized PAC Bayes bound, is not emphasized enough; many important details are moved into the Appendix section, while most part of the paper is devoted to eigenspace overlap. One of the most interesting properties of the Hessian structure is the gap in eigenvalues distribution around the number of classes, but this is prior work. UPD:Thank you for your answer and for addressing the points raised in the review.<BRK>This structure can be used to approximate the principal subspace of the Hessian using certain "averaging" operations, avoiding the need for high dimensional eigenanalysis. We corroborate this claim across different datasets, architectures and sample sizesTo my knowledge arXiv:1901.08244 goes over some of the same territory as the present manuscript, but at full scale and in a much more penetrating way. The analysis of the spectrum of the Hessian of the loss function gives us significant insight, and it is rather amazing and beautiful that the spectrum has definite mathematical properties which also have mathematical explanation. Since the company is Google, that means that their work is fairly well known. The examples shown in the paper under review, in contrast, are of quite limited scale, and to my understanding do not represent the current state of the art.<BRK>## SummaryThe paper studies the empirical properties of the Kronecker Factorization based approximation to the Hessian of the loss. Top eigenspace of the layer wise hessian has a high overlap for neural networks trained with different initializations and hyper parameter setting. ## EvaluationOverall, the paper is not clearly written. Having said that authors do make new empirical observations with regards to the layer wise hessian approximation but they don’t do a good job in explaining why these observations are relevant in a more general context. There is no discussion in the paper about the approximation. How close are the matrices in Eq(6) in terms of the actual norm on matrices? 3.For the PAC Bayes bound, how the posterior variance is parameterized?<BRK>They, to me be very surprisingly, observe that the subspaces formed by top eigenvectors of those layer wise Hessians have non trivial overlap between independently initialized and trained models. * The question is well motivated and interesting. * The proposed simplifications (Kronecker factorization) are validated on real networks. * The obvious application to PAC Bayes is nice! I am not suggesting you work on ImageNet, but more experiments of even the easier variety would make the paper even more convincing. * I am missing a summary table or plot showing how well the approximations proposed match the reality for different architectures and datasets. It might be relevant.
Reject. rating score: 6. rating score: 6. rating score: 6. rating score: 6. <BRK>The work addresses an interesting problem that is relevant for full scale automation of deep learning. The paper is easy to follow with a thorough literature survey (to my knowledge) and contains relevant experiments. I find the study can improve by addressing the following   Firstly, it is difficult to argue for generalisability of this work as currently this work relies on only one algorithm to find the optimal combination of regulaisers. One would need to check how much influence does BOHO have on the claims made within this work. In particular, the third claims about gains on small datasets stands out as not so surprsing but challenges the effectiveness of the cocktail system as DL models often find use in large scale problems. Did the authors find any explanation for this? It would be interesting to see how Figure 4 changes if these are included along with BN, DO and WD. Finally, I think the paper can improve with some additional information and more discussion. For instance, a brief description of BOHO, details on what the scatter plot correspond to in Figure 4, insights and discussion of results from all figures in the supplementary etc. It would also be important to establish statistical significance of the experiments with multiple re runs.<BRK>The results show that the best regularizers mix is dataset dependent, and that regularization matters most when limited data is available. I am not aware of a specific paper that has explored using a parametric mix of regularizer. *Significance:*This paper is part of the AutoML trend. As such, it provides recommendations for best practices in machine learning. *Strengths:* * The paper carefully explicits the hypotheses being tested, and design experiments to probe them. * The experiments include a reasonable amount of regularization techniques in the mix (and for comparison). *Weaknesses:** The results are somewhat expected. * Experiments only consider tabular datasets for classification. The intuition indicates that the results would generalize to 1d audio, 2d image, or 3d point cloud data (as well as for other tasks beyond classification); but it would be better to have some data points on this aspect. I am not aware of a specific related paper should be discussed (and a quick online search did not show either). The text provides enough detail to be able to reproduce the overall system. *Specific feedback:*  Section 3.1, footnote: hold out validation versus cross validation might not be a detail, since it affects what is considered the training set of the baseline. It would be good to include results of the baselines with “default parameters” trained over train+val. Section 3.2: please mention the total number of additional parameters added to the system (or at least its order of magnitude). Consider sorting by frequency of usage. This is different from _strong_ regularization. That would be expected, but I did not see it presented. Figure 4: please mention what the points represent (a dataset each ?). Also use scientific notation for the number of examples. *Updates after reviews and authors feedback: *The updates from the author are appreciated and make the arguments of the paper clearer.<BRK>I was also convinced by authors responce on paper novelty, technical contribution and (after the re focusing) potential usefulness to the community. I am surprised with the results of the googling, but have to admit that authors are technically right and I was wrong. We demonstrate that regularization cocktails achieve a higher gain on smaller datasets;4. > 4."It is well known that the regularization/augmentation/ methods need to be tuned ... I don t see the support for that claim in the paper. I appreciate the fact, that unlike the TabNet, RegCocktails were using a standard deep MLP and not the attention model, yet one needs to add that reference. ****I would like to point out the TabNet paper https://arxiv.org/pdf/1908.07442.pdf, which claimed "beating GB methods for the tabular data". The paper has been significantly refocused and now sells itself as a way of deep neural networks being competitive versus gradient boosting methods, which are dominating the tabular heterogeneous tasks.<BRK>Summary:This paper provides an empirical study of combining different regularizers. Fourteen regularizers including batch norm, weight decay, etc. Strengths:  To my knowledge, this is the first paper that does an empirical study of combining regularizers. The paper is very well written and easy to follow. The figures are nice and easy to understand. Weaknesses:I think the biggest weakness of this work is that it is not very useful practically. All experiments are run on tabular data. I think it’s fair to say that most practitioners who deal with regularization are interested in non tabular data, given the fact that the regularization methods were individually developed for training on non tabular data. The paper frames itself as a “methods” paper more than an analysis paper, where the main claims revolve around the superiority of the regularization cocktail. The fact that the regularization cocktail does better than individual methods, or a combination of a few, is very believable in the tabular setting, since the tuning can be done sufficiently. The fact that more regularization is needed for smaller datasets is also well known. I recommend reject because of the lack of practicality of the paper. I am curious because I think for a small dataset, the need for a state of the art regularizer diminishes, and just one good regularizer suffices. update  I have read the revised paper, and decided to update the score (see response to authors  comment).
Accept (Spotlight). rating score: 8. rating score: 8. rating score: 7. rating score: 7. <BRK>The authors present a fun and effective idea to translate a peer s message in terms of one agent s own experience. Strengths:+ The motivation is clear, and the key idea is well presented. + Evaluation is thorough and indicative of the authors  claims. Major Concerns:  The reviewer has yet to discover a major issue with the paper with regard to its correctness, contribution, novelty, and effectiveness.<BRK>My questions regarding this point are:  Do you agree that this could be a problem in richer environments than those presented in the paper? Positives:  The problem of multiagent communication and how to learn it is important and relevant to the ICLR community. The paper is well motivated and well written. My only real negative was that I am excited to know more about what comes next. Specifically, what if by updating the old communication action to one chosen by the current policy, we present a communication action that no longer aligns with the old environment action, which we do not update?<BRK>The paper considers a multi agent reinforcement learning (MARL) scenario where agents take actions based on the current observation alone. The paper proposes a communication correction mechanism where, during the centralized training, messages there were received in the past from other agents are reevaluated according to the updated policy. This way old messages can be updated instead of discarded, which is more efficient overall. The paper is clearly written and easy to follow. The experiments are enough to convince that the communication correction idea is effective. The idea is not very deep or insightful, so the significance of the contribution depends on how useful this trick will be in practice. what if for some period of time the new actions are worse than the old ones? I think it can be insightful to also experiment with a scenario where communication is noisy.<BRK>  Summary  The paper proposes a method for modifying an experience replay when learning in communication environments, by relabelling messages using the latest policy. The correction proposed is simple, and effective in the domains it is tested in. My main concern is how broadly the method applies, which I am uncertain of from the paper. Pros  The paper provides a simple way to better leverage replay data for communication environments, addressing non stationarity in those environments. The experiments show improvements in learning speed and final reward in some communication domains. The OCC algorithm suggests that a necessary condition is that the message graph is acyclic. However, this limitation is not explicitly discussed, and I am also unsure whether an acyclic message graph is a sufficient condition.
Accept (Poster). rating score: 7. rating score: 6. rating score: 6. rating score: 5. <BRK>## SummaryThe paper describes an algorithm to tackle model bias in the MPC. They address the question of optimal horizon length as well the model errors observed in MPC based systems. The paper is well motivated and written in clear concise manner. 2.Although citations from the machine learning community seem to be covered the   standard MPC literature seem to be completely ignored. These are not recent developments but classic   position papers that address lot of questions you pose and attempt to answer   in the paper.<BRK>##**Quality, Originality & Significance:**The idea of combining MPC and model free RL is straight forward and not novel (the paper also does not claim this). In this setting it would be natural that the pole would fall down if this state is maintained for a longer period (which it is in the video). My biggest concerns are the experiments. The cartpole experiments show the improved performance compared to MPPI on the biased model and the impact of the lambda and model bias. For this setting the gravitational constant really does not seem right. For the simulation studies some doubts remain, but the authors improved the paper.<BRK>2) The authors may consider including these two other papers that relate to model based RL, model bias, and MPC horizon https://arxiv.org/pdf/2009.09593.pdf, https://arxiv.org/pdf/2002.04523.pdf. Do the authors consider this difference at all? I was unaware that MPQ was not the proposal of this paper until section 4. Rebuttal update: the authors have gone beyond the normal scope of a rebuttal phase to update their experiments and the motivation of the work, and for that reason I have improved my recommendation to be above the acceptance threshold. I now will address my conceptual comments followed by more minor suggestions. I am breaking this section of the review into it s own section because it is where the majority of my questions are. 4) there is a lot of visuals in Figures 2 and 3. Very important to standard dev.<BRK>Blending MPC & Value Function Approximation for Efficient Reinforcement Learningreview:summarization:In this paper, the authors consider using a blending of Q value which is predicted directly from the neural network,and a Q value which is predicted by unrolling the learnt dynamics. I also think the connection to GAE is quite natural and interesting,where both algorithms consider trade off between bias and variance (in this paper’s case, bias in learned dynamics). 2.The supporting experiments consider some of the interesting questions.
Accept (Poster). rating score: 8. rating score: 6. rating score: 6. <BRK>The paper proposes an actor critic neural network architecture for autoregressive generation of 3D molecular structures with reinforcement learning (RL). The main contribution is the introduction of a covariant state action representation extracted with Cormorant (Anderson et al., 2019) and a covariant, spherical distribution for the orientation of newly placed atoms (w.r.t.a focal atom) replacing the internal coordinates of the former approach. This seems to improve the model by Simm et al.(2020), especially when it comes to symmetric structures such as $SF_6$. The proposed covariant formulation could be beneficial for other existing or future generative models for 3d molecules, as well.<BRK>This work presents an approach for 3D molecular design using reinforcement learning that exploits rotational symmetries in molecular conformations. The manuscript is well written and provides ample context and appropriate references. The generative formulation directly builds on that of Simm et al.2020 to choose a focal atom, choose an element to add, choose a distance, then choose an orientation. An ablation study that decouples the effects of the rotationally invariant critic and the rotationally covariant actor could be informative. How sensitive are either of these models to hyperparameter selection?<BRK>### Summary of the paperThe paper utilizes a actor critic framework for 3D molecular design. How is the runtime of your model compared to Simm et al.? The central part of the approach is a rotation equivariant network (Comorant). I also realized that the validity calculation is different from standard graph generation methods. The actor critic formulation is also standard in RL. Are the generated compound stable? What is the RMSD of generated compounds? In the paper, authors state that the choice of focal atom, element and distance have to be *invariant* to rotation. How does this method solves this issue? RMSD must be added to evaluate the stability of compounds.
Reject. rating score: 3. rating score: 4. rating score: 4. rating score: 5. <BRK>Instead they find that the budgeted training benefits from variety in the sampled introduced by data augmentation. ++ The literature survey on budgeted training as well as importance sampling is detailed and clear. Weaknesses:  Inappropriate title. The majority of the introduction looks like a duplicate related work to me. The paper is not well organized. There is a negate in the front.<BRK>This experimental contribution concludes that importance sampling strategies do not improve training with budgeted constraints. Data Augmentation seems to be a better strategy in this case. The paper is easy to read and the experiments are clearly stated. It seems to be important to validate that data augmentation is beneficial for both SGD and Sampling technics. The budget aware version proposed by Li and Al should be detailed and as in the paper, it should be interesting to report some training/val loss vs epoch to compare the convergence of the tested strategies<BRK>While I appreciate the idea of comparing the sampling methods and learning rates in a budgeted training setting, such a study should be more comprehensive or suggest a new method as a conclusion. The paper provides an interesting comparison of the speed/accuracy trade off of several methods. # Questions to the authorsPlease provide more details on the data augmentation you used, e.g.how many samples are shown to the network during training, including the variations through data augmentation. Introduction is missing a common leitmotif connecting the paragraphs and seems to anticipate some content that would better be placed in the related work section. ICML.[http://proceedings.mlr.press/v80/jiang18c/jiang18c.pdf]Methods for estimating the importance are also highly used in active learning, relating to that, e.g.margin sampling, ensemble variation etc. Deriving statistics from three runs seems to be not very informative, I would suggest to use a least 5 runs.<BRK>This paper investigates the use of importance sampling in budgeted training. Uniform sampling with and without replacement are used as baselines, and experiments are performed on cifar 10 and cifar 100. Does importance sampling not work for budgeted training, or is cifar data not amenable to the technique? Similarly, I m not sure exactly what to take away from the data augmentation experiment. Still, the experimental setups are a little too narrow (one dataset family, budgets starting at medium length schedules) to draw any larger conclusions. Additional comments:  I feel the title is a little misleading, it implies that importance sampling is important, whereas the findings are the opposite.
Reject. rating score: 4. rating score: 5. rating score: 5. <BRK>##########################################################################Summary:This paper proposes a new framework based on noncommutative Lie groups to learn irreductible representations. This paper proposes a very generic framework for learning equivariant representations. The authors must explain this point. On the other side, a large part of the paper is used to introduce the Lie groups. Perhaps a good way to reduce the complexity of he paper would be to take one example as introduction and lets the more formal parts in the appendix. For example on section 5.1 how the structure constants are given.<BRK>  Summary  This work studies the problem of learning irreducible group representation without prior knowledge, and such algorithm (LearnRep)  is further used to build an object tracking model (SpacetimeNet), which has guarantee of Poincaré group equivariance. The technique of using optimization for finding irrep is simple and appears to be novel and effective. *Weakness*: One contribution claimed in this paper is the SpacetimeNet for object tracking task.<BRK>Space could also be made for this through a more concise presentation of the background material in the first 5 pages. In addition to this, the paper proposes SpaceTimeNet, a Poincaré equivariant neural network architecture, and applies this architecture to an object tracking task involving MNIST digits moving uniformly through space. Strengths and weaknesses:The paper proposed a well motivated algorithm for learning irreducible group representations and performed sensible checks against well studied Lie groups. The proposed Poincaré equivariant convolutional network was similarly well motivated, and the experimental results were promising.
Accept (Poster). rating score: 8. rating score: 7. rating score: 6. rating score: 6. rating score: 5. <BRK>##########################################################################Summary:This paper proposes O RAAC, an offline RL algorithm that minimizes the Conditional Value at Risk (CVaR) of the learned policy s return given  a dataset by a behavior policy. ##########################################################################Pros: The paper is well written, with comparisons with competing methods throughout. This makes the connection with those methods clear and easy to understand. More clarification either in Table 2 or in 4.3 would be great. In this paper, actor loss   risk aversion, so maximizing risk aversion makes sense, but I wouldn t call it a loss.<BRK>Usually, the parametric uncertainty is the main source of worries in offline RL, and dealing with the stochastic uncertainty on top of it, in order to account for risk aversion, is very challenging. The authors expose clearly their method and algorithm, even though we sometimes would have liked a bit more argumentation on why this and why not that. Since no theoretical analysis is provided, the only validation is empirical. It is rather complete regarding both the settings and the domains. The results are quite impressive, in particular in the offline setting. For all these reasons, I recommend to accept the submission. Please answer/address these comments in the rebuttal/final version:* 1  It is unclear whether d^\beta is defined as the empirical distribution in the batch, or the true distribution. * 3  First sentence of 3.2: It is frequent in offline RL to use stochastic policies in order to leverage the risk taken in the face of parametric uncertainty. Even more when we notice in 3.3 that the actual policy is stochastic, since b is sampled from the stochastic behavioral policy. * 4  Please indicate the performance of the behavioural policy (average and cvar) in the experiments.<BRK>It proposes “Offline Risk Averse Actor Critic” (ORAAC) which performs competitively as risk neural agent, and outperforms D4PG based baseline as a risk averse agent on D4RL benchmark. The paper studies safety aspects of learning algorithms which are competitive as a risk averse agent in offlineRL. I would strongly encourage the authors to include numbers for baselines like CQL/BEAR. Though not risk averse, these algorithms are conservative by design and are strong baselines in offlineRL. The authors perform extensive experiments, ablations and provide empirical evidence where ORAAC outperform baselines like OD4PG on D4RL benchmark tasks. + Though risk averse by design, the learned policy is competitive as a risk neutral agent. It would be critical to know how ORAAC performs in comparison to such baselines. Elaborating on this in section 4 would significantly improve the readability of the paper. [1] CQL : https://arxiv.org/abs/2006.04779[2] BEAR : https://arxiv.org/abs/1906.00949<BRK>Later, the full algorithm (O RAAC) is tested on 3 MuJoCo tasks, using offline data from the the D4RL dataset (Fu et al., 2020). The paper is clear and easy to follow. The proposed approach seems to outperform the baselines on the optimized measure: moreover, as noticed in also in previous work, introducing risk aversion seems to be useful also to increase the expected return. The authors claim that their approach is general for any risk measure, however, the experiments are conducted only for the CVaR case and with a specific percentile. The results in the appendix does not seem to match the ones in the main paper. ## RecommendationMy reccomendation is to accept the paper since it provides an interesting approach which puts together, in an original way, techniques from different areas of RL, even if it is not clear whether there is a novel methodological contribution. Is it possible to evaluate in a qualitative way the behavior obtained by the agents in the learned task when using the risk aversion or not? ## Additional Feedback  It would be useful to state the dependecies of your approach in a clearer way.<BRK>The authors propose an RL algorithm for learning risk averse policies from offline data. Empirically, it is shown that it can outperform some existing risk neutral approaches on a number of challenging robotic control tasks under risk sensitive performance measures. Although the empirical results are encouraging, the theoretical properties of the proposed algorithm are unclear and therefore it is not clear how easy it can be implemented in other tasks. Overall, the paper is easy to read and the presentation is clear. The authors address a very important issue that is faced by RL practitioners. So the main contribution in terms of new idea is rather incremental. Furthermore, the theoretical properties of the proposed algorithm are largely unclear. For example, with respect to a fixed distribution for the start state $S_1$, the optimal CVaR policy for Eq.(1) can be non stationary. Now suppose that the neural network is over parameterized, and assuming arbitrarily large training set, which policy does the algorithm converge to?
Reject. rating score: 4. rating score: 5. rating score: 5. rating score: 6. rating score: 7. <BRK>That being said I can provide some feedback. Moreover the suggested applications to ML (a single line in the paper) is not particularly convincing without more discussion and context (e.g., a fully worked example). This paper would be greatly improved by an experiment that showed the improvement of this algorithm over vanilla gradient approaches on a Riemannian manifold, even if the experiment was totally synthetic. I am surprised that any of the parameters of the manifold do not appear in the bound. It would be good to add a discussion of this, or be explicit with the dependency in the bounds if there is one.<BRK>Therefore, the results should be interesting to a broad audience of the conference and deserve some merits. It is important as the gamma constants are involved in the complexity in Theorem 2.4 (and hence in Theorem 2.5). 3.The setting of manifolds of constant curvature seems to be quite restrictive. And, as pointed out in the paper, it seems difficult that the technique could be extended to other manifolds. I understand that this paper focuses more on theoretical side. However, it would be good to demonstrate that performance/behaviour of the proposed algorithm does corroborate with the theories, at least in some toy examples such as optimization problems on the Poincare disk. On page 3, the \tilde{v} (vector of the same norm) is not well defined. I believe such implication requires geodesic completeness of the manifold.<BRK>The paper claims to provide a first order gradient algorithm that achieves (global) equivalent rates of convergence than in Euclidean space on two particular models of geometry (though important) the hypersphere and the hyperbolic space. I would suggest an important rewriting of the paper; Ideally, the main ideas of the paper should be illustrated in the fist technical part section in a simple and enlightening example. Other comments on the readability:In the contribution section, the third point on reductions is not clear at all. The method uses geodesic maps and in particular  …maps geodesics from the constant curvature space to geodesics in the  Euclidean space… This condition is very stringent. For instance, there is a theorem by Kobayashi which quantifies that affine maps (a condition which implies geodesic maps) are often isometries. I would suggest the authors spend more time on the definition of a geodesic map and provide a discussion. … Helping the reader with equations or more precise definitions would be helpful.<BRK>Summary: This paper provides a generalization of AGD to constant sectional curvature spaces (or subsets of them), and proves the same global rates of convergence that hold in the Euclidean space. They have a clear description of the general techniques applied in their work, and push overly technical arguments to the appendix. It appears that the restricted scope and lack of experimental results is quite a problem within this community and venue. They provide global rates which also apply to g convex functions (not just strongly convex).<BRK>This paper proposes a global accelerated method on Riemannian manifolds with the same rates as accelerated methods in the Euclidean space up to log factors. Reductions have also been studied on Riemannian manifolds. I think this paper is too technical and some descriptions are not clear. I am not sure whether there are literatures studying the optimization algorithms (either accelerated or non accelerated) on the constant sectional curvature before. My comments may be too strict for the analysis on Riemannian manifolds. 1.Previous literatures have studied the optimization on Riemannian manifolds of bounded sectional curvature, while this paper focuses on the special hyperbolic and spherical spaces, that have constant sectional curvature.
Reject. rating score: 3. rating score: 6. rating score: 6. rating score: 6. <BRK>In this paper, the authors present a generative model based Laplace mechanism. In experiments, the VLM training set is directly extracted from the private dataset (MNIST). Though the method is novel, the privacy guarantee of the proposed method is not clearly stated and proved. To my knowledge, I believe the work is useful for the privacy research community. Therefore, I doubt if the experimental results are useful for proving the effectiveness of a private algorithm. At the beginning of Sec.2, the authors reason the usage by "as it provides strong theoretical privacy guarantees". How does the Laplace mechanism guarantee privacy better than the Gaussian mechanism? The setting has to be clarified. In page 3, the authors briefly mention that the local version of the Laplace mechanism can be epsilon LDP if the sensitivity is accordingly defined. The VLM encoder has additional information about the data distribution or the noise (by back propagation in VLM training). (Phan, et al.2017).Adaptive Laplace Mechanism: Differential Privacy Preservation in Deep Learning Especially, there involves a non private training on stage 1. The experiments are run with pretty week baselines. For the specific task, how well is the proposed compared to the SOTA CDP private learning algorithms? For example, (Abadi, et al., 2016), or (Phan, et al.2017).Especially, (Phan, et al.2017) also proposed an adaptive Laplace mechanism without depending on pre training of the mechanism. Only DP SGD was discussed. Thus, the privacy guarantee is not straightforward. Seems the VLM training is using a non DP optimizer at stage 1.<BRK>Summary:This paper presents a new privatization mechanism for Local Differential Privacy based on representation learning. The proposed VAE based method is used for the low dimensional latent representation of the data and uses the Laplace mechanism to satisfy Local DP. The paper shows this mechanism can be used across various applications such as private data collection, private novel class classification, data joining, etc. The proposed method provides a great solution for data sharing with the local DP and can be used in many real world applications. The experimental results can be improved by adding more baselines. The authors are expected to state the differences between the existing work and the proposed work. Also, they mentioned these techniques need labeled data. DP GAN models need access to real data for training (no label is required) and then it can be used indefinitely for generating synthetic data. In an existing work (P3GM paper mentioned above), it is shown that VAE’s objective function is too sensitive to the noise of DP SGD, how the authors tackle this problem? It would be better if the authors use other techniques such as some recent work on LDP on high dimensional data or other general DP classifiers, DP SGD, PATE, etc. as the baselines for this experiment. It would be better if the authors also showed the performance of the proposed method on higher dimensional data (e.g., Lending club has only 23 features)Minor comments:          In the introduction, please list the contribution of this work.<BRK>They achieve significant gains in performance for high dimensional data. In this paper, authors have benchmarked results against such a mechanism, in which add Laplace noise to all continuous features, and flip each of the categorical features with some probability. A more effective approach to privatization involves noising a learned lower dimensional representation of each datapoint using a generic noising mechanism. Applying the Laplace mechanism thus ensures the encoded latents, as well as reconstructed datapoints satisfy LDP. At inference time, they show that this classififier can act on either clean or privatized datapoints,. For the experiment, when epsilon< 10, the accuracy is not very good. The related work and comparisons are not enough. They are quite a few work about LDP learning can be literature reviewed. By the way, we usually use private data rather than privatized data.<BRK>Strong point 1: The idea of putting noise insertion (via noisy data generation models) and optimization of good representations together to obtain LDP representations and/or synthetic data seems to be effective. When there is a label, the privacy budget is split and random perturbation is used on labelsStrong point 3: It outperforms naive LDP baselines (with noise added directly to features) a lot in experimentsWeak point 1: The proof of the most important result is missing: It is said that "sampling from $q_\phi(z|x)$ produces a representation $\tilde z$ of $x$ that satisfies $\epsilon$ LDP. I don t think it is a trivial result and the author needs to everything together (including the analysis of sensitivity, the optimization algorithm, and so on) to formally prove itWeak point 2: A minor issue: in figures of experiments, by "clean accuracy", do you actually mean "accuracy" (for some algorithms in the figures, it is privacy accuracy?) W1 is the main reason for the rating of 6 but not higher ones   highly encourage the authors to fix it before the publication
Reject. rating score: 4. rating score: 6. rating score: 6. rating score: 6. rating score: 9. <BRK>I would encourage the authors to look into the literature on scattering networks (e.g.https://arxiv.org/abs/1203.1513) for another approach to explaining networks from first principles that, I think, does a better job of making the connection to realistic neural network architectures. This allows the authors to compute the update using layerwise fourier transformations. There were several aspects of this submission that I liked a lot. I think the construction seems interesting and the rate reduction metric seems like a reasonable thing to optimize. I found the relationship of coding rate maximization to ReduNet to be quite clever and the experiments seemed to suggest that the approximations employed in the paper (e.g.the estimated membership in eqn 9) did not spoil the mapping. However, it is not at all clear to me that standard convolutional networks or residual network architectures are well described by ReduNet. 2) Since the relationship between ReduNet and commonplace CNNs seems tenuous to me, it seems there is a significant burden on the authors to probe this question empirically. A few more minor points:1) I may be missing something, but given the construction of ReduNet, I feel as though the emergence of a convolutional structure subject to translation invariance is not terribly surprising. Indeed I feel like this result has high overlap with Cohen and Welling [1]. Can the authors provide some intuition here? While I find the architecture proposed by the authors to be interesting, I do not think they have done enough to motivate the connection with neural networks.<BRK>The novelty of the paper is in that formulation of the feature optimisation is baked in into a deep architecture. MCR^2 principle seems like a sensible approach to learning, especially given that embedding algorithms (such as face encoding) use it already. It’s nice to see some rigorous mathematical treatment on this. However, I get confused pretty early on by the notations. And from then on, including equation 11, f(x,\delta) \psi^L(z_L+\etag(z_{L 1},\theta_{L 1})…which would make it seem f(x,\theta)\in \mathca{R}^n. Is that a neural network, or some model, with parameters \theta_l? I have also questions about equation 11, where number of layers is equivalent to iterations while maximizing MCR^2 and the width of each layer corresponds to m, the training points in the dataset. I also don’t quite understand how \psi^l(z_1,\theata_1) works   how does \theta_l change over iterations? Experimental section is not helping me with this, since it’s stated that E, C^j are computed for each layer…but there are no details on what \theta_l is and how g(z_1,\theta_1) is evaluated. Is it just based on definition of \hat{\pi}^j(z_l) from page 4? Finally, I am not sure if the result of obtaining a convnet architecture in ReduNet when translation invariance constraint is added the embedding is all that surprising.<BRK>The paper proposed a network derived from maximizing of rate reduction, known as the MCR^2 principle, where  all parameters are explicitly constructed layer by layer in a forward propagation fashion. Compared with exiting work by Yu et al.(2020), the proposed work is more like a "white box" that each layer is more interpretable. Results showed the proposed network can learn a good discriminative deep representation without any back propagation training. The derivation of the network also suggests that the network is more efficient to learn and construct in the spectral domain. Overall, the proposed work looks reasonable. The paper is well structured. The derivation and experiments seem convincing. Unfortunately, the proposed work is out of the reviewer s expertise and therefore it is hard to provide valuable comments.<BRK>#### SummaryThis paper proposes a theoretical understanding of neural architecture using the  principle of rate reduction. Yu et.al 2020 and the derived optimization steps naturally leads to operations such as network layers and identity residual. By enforcing shift invariant, it can also lead to convolutional operation. The network can be constructed with a forward propagation fasion, which conserves good discriminative ability. #### NoveltyTheorectical guidance of network design is one of the key direction in representation learning. The paper proposes a novel perspective (rate reduction) in network construction that generalized to the design of networks such as resnet and resnext. However, there are a bunch or other networks such as denseNet, non local network etc., which are also performing well in practice and designed with huerestics of larger context and better gradient propagation, will the author also include these representations within the objective of rate reduction? Or does the generation process from rate reduction (compact discriminative representation) come with additional guidance of what network, or answer the question in introduction, what is the object is optimal for network design, which could show to be effective on multiple tasks. I think the major issue of current result is that we can see the objective explain  that the set up of networks (resNet) seeks a compact representation, but the author has not shown strong experiments that their yielded alternatives by optimizing the objective (rate reduction) has positive relation with the network generalization. However, in realistic senario, we may seem equivalent disentangled representation rather  than invariance, is it possible for the objective leads to convolution without explicit inducing shift invariance ? #### Writing and referenceThe writing is good and easy to follow, checked few derivatives which are correct. In general, I think the objective is novel, but not generalized enough to explain lots of high performance networks yet. However more exploration of theoretical study should be encouraged.<BRK>The authors propose a deep network approach using the principle of rate reduction as a loss under a gradient ascent approach, avoiding traditional backpropagation. Besides, the work attempts to interpret the proposed framework from both geometrical and statistical views. Then, shift invariant properties are discussed. The innovative method allows the inclusion of a new layer structure named ReduNet, which could benefit the deep learning community. Though the experiments are not challenging concerning the studied databases, the authors aim to probe the concept without a complete implementation tuning. Overall, the paper is illustrative enough regarding the mathematical foundation.
Accept (Poster). rating score: 8. rating score: 7. rating score: 6. rating score: 5. <BRK>The work uses diffusion probabilistic models for conditional speech synthesis tasks, specifically to convert mel spectrogram to the raw audio waveform. The paper is very well written and it is quite easy to follow. The study of the total number of diffusion steps and two different ways (continuous and discrete) ways to feed it in the network is very interesting. Using this, authors are able to find a 6 step inference procedure that yields very competitive performance to WaveRNN while still being computationally feasible. Pros:1.Great results for the neural vocoding task2. It would be great to substantiate it with evidence/comparison?<BRK>The paper is well motivated and easy to follow. Though the fundamental approaches of this paper have been applied in other domains, making the approach work well for waveform generation is relatively new and not easy. The authors also had a deep dive on noise schedule, and proposed a variant of WaveGrad which conditioned on continuous noise level. However, one minor concern is that, if inference computation is not a bottleneck, when using linear schedule, for both 50/1000 iteration scenarios, WaveGrad conditioned on continuous noise level does not show clear benefits over conditioning on discrete index. When comparing with adversarial counterparts, most models are with fewer parameters than the proposed WaveGrad.<BRK>The proposed method is a direct application of Ho et al.(2020) for waveform synthesis. BTW, the authors may discuss WaveFlow (a SOTA flow based model), which also introduces a trade off between autoregressive and non autoregressive models. One may either introduce in more detail, or skip some content as this work focuses on a particular parameterization of diffusion probabilistic model introduced by Ho et al.(2020).Pros:  Solid technical contribution. Good empirical results. Paper writing can be improved. I would like to raise my rating if my concerns are properly addressed.<BRK>There are really limited innovations on the model side since the proposed method just combines some techniques from recent score matching and diffusion probabilistic models. The authors should also include the results of the WaveNet Vocoder. *** Post Rebuttal ***Thank the authors for responding to my concerns in the rebuttal. For the contribution part,  the major technical contribution is the continuous noise schedule. As also suggested by R1, the authors should carefully revise the paper to make it more clear. After reading the paper, my first impression is that the paper just uses a new model on image synthesis to address speech synthesis.
Reject. rating score: 4. rating score: 5. rating score: 5. rating score: 6. <BRK>This paper proposes NAHAS for co designing neural network architecture and hardware architecture. It shows performance improvements on ImageNet compared to previous hardware aware NAS methods which only optimize the neural network architectures. Pros:1.Co designing neural network architecture and hardware architecture is a promising and important direction. Cons:1.The technical contribution of this paper is limited. The main difference between this paper and previous hardware aware NAS papers is that this paper has an additional hardware search space beside the neural network architecture search space. 2.Co designing neural network architecture and hardware architecture is not new.<BRK>##########################################################################Summary:The paper proposes an algorithm to search for hardware designs and neural architectures jointly. The results show that joint optimization improves accuracy and latency, reduces the search samples. ##########################################################################Pros:  The paper introduces a new dimension, hardware design, to the neural architecture search domain. ##########################################################################Cons:  The used techniques are not novel  The hypothesis "joint optimization is better than two phased optimization" seems obvious. Can you describe more of the hardware. Is it a publicly accessable platform? 3.I also have concerns about the motivation.<BRK>The authors describe an automated system for co designing neural architectures and HW accelerators. The system is able to find the best solution under latency and chip area constraints. Results are compared to MnasNet, platform aware NAS and EfficientNet. Does the work outlined in the "co design" paragraph of section 2 not attempt to do this? Limited information is provided regarding the architecture of the accelerator. Is there anything to learn by a discussion of the good hardware configurations that were found? The work is interesting but the claimed contributions perhaps need to be clarrified.<BRK>##########################################################################Summary:The paper presents NAHAS, which is a combination of Neural Architecture Search (NAS) and Hardware Architecture Search (HAS) for software hardware co design. It uses PPO with joint search space: model accuracy and hardware constraints. In the beginning of section 3, it is unclear about the entire workflow. Tested only on Imagenet, mobilnet and efficientnet. It also needs to revise the claim that it demonstrate effectiveness of hardware aware NAS for first time.
Reject. rating score: 2. rating score: 4. rating score: 4. <BRK>Its contributions include developing an embedding model to capture clinical prototypes (CPs), via supervised contrastive learning, and presenting experimental evidence of these learnt CPs capturing attribute specific semantic relationships and being helpful in subsequent clinical natural language processing task of information retrieval and clustering of clustering of physiological signals. In addition, to feel convinced of the presented experimental evidence, I would have wanted to see statistical significance tests, confidence intervals, effect sizes, or similar presented.<BRK>The work could be greatly improved by clarifying some important aspects of the proposed system and by conducting more appropriate experiments to back the result claims made. 3) Points that were unclear to me:  	3.1 The construction of D_hat and the ground truth D in section 3.3 in “Arrangement of clinical prototypes” . Effectively the work shows that doing k means on representations learned by the paper is not as good in terms of ACC/AMI as compared with those learned from two proposed proposed variants of contrastive learning.<BRK>##########################  Summary:This paper proposes to tackle the problem of retrieving and clustering physiological signals by learning clinical prototypes via supervised contrastive learning. Why do we need to learn embeddings for such task? Overall, the writing can be improved.
Reject. rating score: 4. rating score: 4. rating score: 5. <BRK>Interesting approach using ideas in chaos theory to deep learning but its merit is not clear yet. Summary:This paper connects ideas in chaos theory to understand training dynamics of neural networks. The authors show that the largest Lyapunov exponent corresponds to the most negative eigenvalue of the Hessian. Then the paper claims that this provides an efficient method to estimate the largest eigenvalue and connect to using learning rate related to largest eigenvalue. Using this method, the paper claims SGD finds loss landscape regions where the largest eigenvalue of the Hessian is similar to the inverse of the learning rate. Proposed method of estimating largest eigenvalue of Hessian is presented without any comparison to well known methods such as Lanczos and proposed quasi Newton method’s utility is unproven as is. If the learning rate is too small, convergence will be too slow, if it is too large SGD can diverge. Also there’s evidence that a very small range of learning rate is critical for improving performance [1]. For example [1] have shown that the largest learning rate based on eigenvalue estimation is not sufficient for explaining neural network training dynamics especially in the well performant ones. ICLR 2017. For one thing there are various methods to estimate large eigenvalues of large matrices. For example, naively I would have used Lanczos to estimate the top few eigenvalues very efficiently using Hessian Vector product. Proposed quasi Newton does not have sufficient analysis that the idea works. There are numerous ideas for proposing new optimization and without careful, through comparison to baseline, well known methods.<BRK>Objective of the work: The paper uses the chaotic theory to study the dynamics of SGD. It provides algorithm to compute the most positive and the most negative eigenvalues of Hessian based on analyzing the Lyapunov exponents. The paper shows that the largest eigenvalue of the Hessian similar to the inverse of the learning rate. Strong points: The paper proposes an algorithm that can fast estimate the largest eigenvalues of the Hessian based on the analysis of the Lyapunov exponents. The technical derivation is sound. Weak points: 1. Chaos theory provides a way of computing eigenvalues but does not give much understanding on the neural network optimization. However, the paper also propose setting the learning rate according to the Hessian leading eigenvalues. Is there first eigenvalue or first the learning rate. I would not recommend the acceptance for now.<BRK>### 1.Brief summaryThe authors use an insight from chaos theory to derive an efficient method of estimating the largest and smallest eigenvalues of the loss Hessian wrt the weights. Then they use on the fly estimated largest eigenvalue to automatically tune the learning rate of SGD. While those methods seem useful, I do not see why chaos theory was needed to derive them. I appreciate the link and believe that more good stuff could come out of it, but as is I don t think this paper provides much new to the field on its own. 2) The simplest method for estimating the top eigenvalue   the power method   is also linear in the number of parameters. 3) The power method tends to be unstable (in its naive implementation) when used to get the less than highest eigenvalues. 4) The connection between the top negative eigenvalue and the rate of departure of nearby points in the weight space from each other (the same for gradient ascent and the top eigenvalue) does not seem very surprising to me. The highest negative eigenvalue will be the one pushing you out as exp(|lambda| t). 6) It seems that what you are describing with your adaptive optimization is very similar to some existing algorithms. This is a minor point, no need to address it.
Reject. rating score: 3. rating score: 4. rating score: 5. rating score: 7. <BRK>Summary:This paper proposed a new approach for modeling multi domain dialogue state tracking by incorporating domain slot relationship using a pre trained language encoder. Two kinds of special tokens are proposed to represent domain slot pair, DS_merge token for each specific pair, and tokens for every domain and slots separately Comments:1  what is the role of segment embedding (Fig3) for DST tank? However, it is not clear how much of the performance drop belongs to turns with cross domain related slots. The figure 4 only present one example of this case, which might not be correct for all wrong predictions. 4  Since the proposed approach can be used on any pretrained encoder, the evaluation on BERT and/or Roberta is helpful to understand the robustness of approach to the choice of pretrained encoder. Based on results in Table 3, there is inconsistency in which domain slot relation does not always results in better performance,  and it depends on the choice of encoder too. Overall, the proposed architecture is very similar to TRADE model, in terms of using an encoder for dialogue history, slot gate and slot value classifier. The only difference is in using a much powerful pretrained encoder.<BRK>[Summary]In this paper, the authors proposed a multidomain state tracking model that leverages the relationship among different domain slot pairs. This is done by leveraging the full attention step over the [CLS] special token and by providing all the domain slot pairs as a special token to a pre trained language model (Figure 2 is very clear). The authors studied different format to represent $D_{i,j}$ DS merge (i.e., one token per domain slot) and DS split (i.e., one token per slot and one per domain, thus more scalable). The reported performance is state of the art at the time of submission. [Pros]  The paper reads well and it is easy to follow for people working on Task Oriented dialogue. The proposed method is simple and effective, and it would be easy to reproduce. [Cons]  The idea of using domain pairs as input to a large pre trained model is not novel (Wu et al., 2019; Zhang et al., 2019; Lee et al., 2019), as also pointed out by the authors, but the authors do not explicitly clarify this in the methodology section, leading the reader to believe that the domain pairs is their own contribution. Same for the slot gate (Wu et al., 2019)  The authors claim to learn relations between slots, but the analysis section is very thin and it just shows an ablation by masking the attention between the slot. Can the authors show this ablation for all the model size? Although, MWoZ is the current benchmark for DST in ToDs, there are also other datasets for this task that can be considered (e.g., Schema Guided Dialogue (SGD) (Rastogi et.al.2019))[Reason to Reject]The main contribution of this paper is very thin, adding the [CLS] token as input, and the main technical contribution is not well explored (missing an in depth ablation). I suggest to better format the dialogue.<BRK>Pros•	This paper incorporates the multi domain domain slot pairs into the bert input so that the relations between sentences, domain slot are modeled. Cons•	It’s better to experiment on more dataset to prove the method’s effectiveness. Comments•	This paper   https://arxiv.org/abs/2006.01554 seems get better performance on the same dataset. •	In the domain split/merge method, does the domain/slot used as vocabulary tag or word embedding? •	The slot gate classifier’s output is fed into the slot value classifier, how does this affect the performance?<BRK>With their encoding and using strong pre trained initializationsthey are able to improve the joint goal accuracy by almost 1.5 points which is impressive. Reasons for score: This is a very well written paper and will be a good resource for people working on the task of Dialogue State Tracking. The authors show how they can model relationships between domain slot pairs and how they can encode them effectivelyusing pre trained representations. Pros:1.Good dialogue representation which helps with the task of state tracking2. 3.Clear ablation study showing the value of 1) pre training and 2) modeling relationship between domain slot valuesCons:1. This approach, like other popular approaches, suffers from the problem of having a fixed output vocabulary for slot values   hence limiting its scalability. While this cannot be addressed in this work, this is a drawback of this approach. 2.Some of the design decisions are stated but not well explained  Only one pre training method compared  Authors mention they drop "dontcare" from slot gating but don t show the affect with or without it. Not much details on the setup and how it was trained. I would suggest showing more analysis. Which turn in the dialogue does the error decrease the most. Adding another layer to make DS split work should be trivial, there is no reason to leave that to future work. Could you show how the results look with that?
Reject. rating score: 3. rating score: 4. rating score: 7. <BRK>1.Is the paper trying to propose a novel way of measuring the similarity between inputs or to improve the vector representations generated from the last layer to be more reflective and indicative in terms of the fine grained structure of the data? Let s assume that both reasons are valid, then why bother using neural networks for comparing similarity between datasets? 3.The activation function applied in the experiments is tanh, which squashes values to be in between  1 and 1, and the authors proposed to apply BatchNorm after tanh activation function. IMO, I don t agree with the reasons listed by the authors.<BRK>Moreover, a variant of Batch Normalization is proposed. EDIT: I am thankful to the authors for their insightful answer to my concerns, and for the though work in reviewing the manuscript. CONs:  In the experimental part I could not see (even in the appendix and in the code) any consideration made in relation to the hyper parameters of the neural networks, and of the used learning algorithms. The choices made in the paper (and reported in the appendix), e.g.on the learning rate, on the number of epochs, on the regularizer coefficients, seems rather arbitrary and would be better selected on a validation set.<BRK>I love the problem of neural information decorrelation which is so widespread in neural systems precisely for the reasons that the authors point out to detect novel information. I think the main contribution of the authors is combining random projection initialization, training with orthonormality regularization (as proposed by others) and drop out. The three models compared should be explained better in the experimental section. So, people can read the paper faster. An experimental section illustrating the impact on the model performance with each of these components would help us learn more.
Accept (Poster). rating score: 8. rating score: 6. rating score: 6. rating score: 6. <BRK>  *  Summary of the paper. It would have been interesting to do so. * The classification task considered by the authors in Table 1 (CIFAR 10) is not easy to interpret. What is an outlier for a neural network ? **  *  Questions:    * It is very weird that TERM works in Table 1 even when there are 80% outliers , as my understanding of outliers is that they must be in minority. * The fact that the objective is strongly convex when t>0 is fairly important (this proves the convergence), I think that you should include the whole proof and not an abridged version of it (see proof Lemma 3). I am not sure what is the task and what it is that your algorithm do in the 80% noise context.<BRK>This paper considers a unified framework named TERM for addressing a bunch of problems arising in the simple averaged empirical minimization. One thing that I do not understand very well is the paragraph under Lemma 1. Note that outliers can be arbitrary, say adversarial. Is it possible to show certain convergence results of the algorithms for solving the TERM? Especially, the TERM has the nice property that it is always smooth (depending on the value of t). In addition, the authors conduct a series of experiments to show the good performance of TERM for different tasks such as robustness to outliers, handling imbalance, and improving generalization, etc.<BRK>The paper is well written, contains pedagogical illustrations and detailed properties of TERM. The success of Term heavily rely on a *magical* tuning of the parameter $t$ depending on the application. Grid search was used in this paper which considerably increases the complexity of the algorithm without necessarily improving significantly the accuracy when compared to competitors. Averaging the test accuracy after several random splitting would be beneficial for clarity. The classical ERM formulation is written as sum of functions, which allows several advanced stochastic optimization algorithm. Such structure is destroyed in the tilted formulation.<BRK>This work analyzes the LogSumExp aggregated loss (named tiled empirical risk minimization, or TERM, in the paper). Provide theoretical analysis on the properties of TERM2. However, I am not an expert in this field, thus not sure how much the new analysis will contribute to the community (or the analysis may exist somewhere). The interpretation 2 is constrained to be t<0 for avg min trade off (and t>0 for avg max, as shown empirically in Fig.9). Since Algo.2 is using a non trivial averaging for the normalization term, the convergence of the stochastic version is unclear.
Reject. rating score: 3. rating score: 5. rating score: 5. rating score: 5. <BRK>This work proposes an efficient graph neural architecture search to address the problem of automatically designing GNN architecture for any graph based task. Comparing with the existing NAS approaches for GNNs, the authors improves the search efficiency from the following three components: (1) a slim search space only consisting of the node aggregator, layer aggregator and skip connection; (2) a one shot search algorithm, which is proposed in the previous NAS work; and (3) a transfer learning strategy, which searches architectures for large graphs via sampling proxy graphs. For the graph classification task, authors also excluded a lot of pooling methods, such as the Graph u Net [3], which achieves the better performance than the proposed approach. The paper organization is clear, but some expressions should be improved. 2019.[2] Xu, Keyulu, et al."Representation learning on graphs with jumping knowledge networks." The details are listed as below. The experiment results show that their framework greatly reduce time, comparing with the GraphNAS, Bayesian search and random search. It might be much smaller than the search space of CNNs. (5) In the node and graph classification of the experimental section, the performance improvement over the human designed is marginal. This would not justify the motivation of applying NAS to search graph neural networks. Currently, the authors’ search space is based on the traditional message passing approaches. They should consider more the recent developments in GNNs to further improve the performance.<BRK>This paper presents a differentiable NAS method named EGAN for automatically designing GNN architectures. The main contribution is searching GNN architectures efficiently with an one shot framework based on stochastic relaxation and natural gradient method. Pros:+ Paper is well written and easy to follow;+ The proposed search space with node aggregators, layer aggregators is interesting;+ The design of the baseline methods including random and bayesian search is appreciated;+ Empirical results on different datasets and tasks are very strong. Cons:  Limited novelty, the proposed search method is very similar to SNAS (Xie et al., 2018) except the search space;  A similar one shot search method for GNN has been proposed in SGAS (Li et al., 2020) which weakens the claimed contribution of being the first one shot NAS method for GNNs;  It is not clear why stochastic natural gradient method is needed;  The performance of GraphNAS with EGAN’s search space (Table 11) is close to the performance EGAN (Table 2). But it is not clear which aggregator is using;* The contribution in terms of transfer learning is weak since the proxy graph is a subsample of the large graph. SNAS: stochastic neural architecture search. I keep my rating as 5.<BRK>Summary: The paper proposes a framework for efficient architecture search for graphs. This is done by combining a differentiable DARTS like architecture encoding with a transfer learning method, that searches on smaller graphs with similar properties, and then transfers to the target graphs. The experimental results look promising, but there should be more care taken to assess statistical significance. Many things were addressed, and I have increased my score to 5 to reflect that the paper is not far from acceptance threshold. What is the empirical impact of the reduced graph size on running time? As the proposed method seems to be very fast, is it unfeasible to run the search on the full graphs? It would be better to perform a statistical significance test, and make the statistical significance clear by bolding several joint top values.<BRK>The paper presents a NAS method for graph structured learning, which focuses on constructing a search space tailored to Graph Neural Networks (based on different node aggregators, skip connections and layer aggregators). The experiments look promising, showing both the strength of the proposed approach in terms of performance as well as in efficiency compared to related NAS approaches. The training architecture and the model parameters are learned simultaneously (based on a Gumbel softmax formulation) via gradient descent. I personally think that the transfer learning idea has potential, but may need more work in order to see clear benefits. Which architectures generally perform better than others? [1] Corso et al.: Principal Neighbourhood Aggregation for Graph Nets (NeurIPS 2020)  Post Rebuttal Comments  I would like to thank the authors for their insightful rebuttal and clarifications. However, since the authors train a "SuperNet", it is not particular clear to me why one even needs to decide for a specific aggregation scheme (in contrast to PNA), e.g., by simply using the softmax function instead of the gumbel softmax formulation. Therefore, my rating will stay the same.
Reject. rating score: 3. rating score: 5. rating score: 7. rating score: 8. <BRK>The paper uses the bandit learning framework to study the online learning problem for routing in a city network . The authors do not compare the proposed algorithm with any baseline algorithms. However, there are a number of issues in the paper that makes the overall contribution questionable, as I list below. Since the optimization problem is minimizing the delay, so it is understandable that the standard UCB is replaced by the LCB. The first issue is that the authors is missing an entire line of very relevant result work and results. The contextual CMAB problem is studied in [2]. There is no more discussion on the factors related to the regret bound, such as |E|, and \beta. There is also no explanation on the outline of the analysis.<BRK>In this paper the authors study the problem of routing users according to their requests through a network with unknown congestion functions over infinite time horizon. I checked the proof, and it appears solid. The authors design algorithm 1. I think the authors mean that the regret R_t is \sum_{r 1}^t E[c_r c*_r].<BRK>This work introduces an interesting generalization of stochastic combinatorial semi bandits for routing in a static graph. The main contribution is a novel UCB like algorithm with a dynamic regret bound after T steps of |E|T^{2/3} (ignoring log factors in T). This is a nice idea which is not standard in the bandit literature. The analysis of the algorithm is quite involved and apparently novel for the most part. The source of the T^{2/3} dependence should be independent of the combinatorial nature of the semi bandit problem. The definition of x_{max}^t on page 2 looks wrong because the max is over t.<BRK>In this submission a routing problem is studied. These observations can then be used for future routing decisions. The main result of the submission is an algorithm that achieves a cumulative sublinear regret of O(|E|t^{2/3}) where the regret is defined as the difference between the expected path length chosen by the algorithm and the length of the shortest path. It is assumed that the congestion functions are unknown (which makes sense). However, if I understand it correctly it is assumed that each driver knows exactly the current load on all the edges. It is not clear to me why this makes sense.
Reject. rating score: 2. rating score: 5. rating score: 6. rating score: 6. <BRK>This paper presents a linear time attention model based on importance sampling and locality sensitive hashing. The idea is to use Bernoulli sampling to approximate the self attention   which is quadratic in complexity. There is not even a sparse transformer or local attention baseline in the experiments. why?I also find other flaws with the model. I think the authors could comment a little on this. I also find it difficult to understand the choices of tasks. My constructive feedback to the authors to improve the paper is to have reasonable baselines for comparison. The datasets are also not appropriate.<BRK>Strength:A new idea of applying  locality sensitive sampling to approximate attention matrix in the transformerWeakness:1. Comparison of Complexity: [1] presents the complexity of different efficient transformers. 2.Experiments: YOSO takes linformer as baselines. However, the pre training experiment part does not provide steps vs ppl of linformer with YOSO in Figure 4. 3.Efficiency: YOSO demonstrates an advantage over linformer and longformer in memory and runtime. On the other hand, linformer introduces a more global view for attention by the low rank projection.<BRK>### SummaryThe paper proposes to replace the weighted average of the values in standard self attention with the average of values sampled in a way that the expectation is close to the result of self attention. The LSH formulation where the values are averaged in the bucket is a clever way to avoid the pairwise interactions between queries and keys. Is it the gradient of the expectation? ### Reasons for recommendationI find the idea very elegant and interesting however, the experimental section is somewhat lacking.<BRK>Pros:1.This paper provides a thorough solution of how to accelerate self attention via an approximation by Bernoulli sampling and LSH. 2.The experiments show that the proposed approach could achieve considerable speedup while preserving model performance. Adding more tasks such as MT or autoregressive/causal LM would make the experimental part more solid and convincing.
Reject. rating score: 4. rating score: 5. rating score: 5. rating score: 6. <BRK>Finally, it s unfortunate that the paper sticks to these two easy environments. Understanding the failure modes in the easiest environment that can t be solved by this method would yield significantly more insights on the methods than near perfect scores on easy tasks. Influence based multi agent explo ration. MAVEN: multi agentvariational exploration. ### Particle environmentsThe environments are introduced in [1], and the authors directly compare to the results of the corresponding paper. The authors provide comparison with the methods proposed from [1], EDTI and EITI. Similarly, reducing the batch size could improve the sample complexity as well.<BRK>My main concern was regarding the scalability of the method to larger environments, e.g.w/ visual state space. I agree with the other reviewers regarding limited applicability of the method, and maintain my original score (Weak Reject). The paper introduces a coordinated multi agent exploration method that selects goals based on normalized entropy from multiple projected state spaces. I’d be willing to raise my score if, in more complex environments, using level 0 subspace still results in improved performance over existing work.<BRK>The paper proposes to improve the exponential sample complexity of finding a coordinated multi agent strategy by learning an exploration policy for each agent that conditions on a shared goal. Strengths:  The paper is well written. Weaknesses:  The proposed counter mechanism relies on being able to manually identify entities in the environment, such as the box in the push box environment. Similarly, it seems that deciding which subspaces are equivalent requires a significant amount of domain knowledge into each problem, and does not seem to be generally applicable. How many steps between updates were used for the proposed technique?<BRK>The authors test their method in two domains: 1) a series of coordinated multi agent exploration challenges in grid worlds, and 2) the Starcraft Multi Agent Challenge (SMAC). The paper uses interesting tasks, and the experiments are generally well structured. 2) The authors’ method seems to me to build in a lot more hand crafted knowledge than they let on (and more than any of their baselines, as far as I can tell). Can the authors comment on the fairness of comparing their method to the baselines they use, as well as the scalability to less structured observation spaces? 2) Maybe I missed it, but how many subspaces (K) are there in these experiments?
Reject. rating score: 5. rating score: 6. rating score: 6. rating score: 7. <BRK>The ablation study shades more light on some of the properties for the training dynamics of an image based model free RL algorithm (attention map, layer convergence)Cons:I’m a bit skeptical about the generality of this approach. It seems that the approach requires storing 4 data augmented observations per env observation and the replay buffer size is equal to the number of training steps. Clarity:The paper in general is clearly written and well organized. Post rebuttal:The authors has addressed several of my concerns regarding the method s generality and some experiments. There are several much simpler things one can do to tradeoff compute or memory (for example re render observations on the fly from stored low dimensional states).<BRK>After freezing latent vectors are stored in the replay buffer instead of images (and any existing images are replaced by them). This leads to both better compute and memory utilization. The CNN is frozen all at once instead of frozen iteratively. One paper that should be cited is Fang 2019. Overall      While I like this paper and think the idea has a lot of potential, I don t think it is quite ready for publication yet.<BRK>This manuscript proposes to reduce the intensive computation and memory requirement in reinforcement learning trainings by freezing the parameters of lower layers early. The motivation for this work is strong, and the results are impressive. However, for a certain network with various structures, is there any applicable conditions or limitations? Is it always the case that lower layers consume a significant amount of computation and memory? 3: this paper also proposes to store latent vectors instead of high dimensional images.<BRK>Section 5.4 convincingly argues that the freezing method is empirically justified. That is, in the proposed experiment setting the choices were reasonable but nonetheless a little arbitrary, so knowing a little bit more about learning dynamics with this approach would probably make the paper stronger and more robust for future readers. ### Good things  The approach is extremely relevant to most of the RL community. The method is simple, but clever, and the manuscript quite nicely details the steps taken to improve learning stability (which can arise due to the obvious possibility of bad model freezes).
Reject. rating score: 4. rating score: 5. rating score: 6. rating score: 7. <BRK>The paper studies the problem of learning to autonomously navigate the web, such as filling out web forms. It proposes a curriculum learning method that uses Adversarial Environment Generation (AEG) to build a curriculum of challenging web navigation tasks. It is based on the idea of creating training environments for RL agents. The problem in this work is an interesting application of RL. The idea is not very new and extends from previous works using AEG. It is better to have more details about the adversary architecture.<BRK>First and foremost, it is really not clear what are the motivations behind the research work. I understand that you want to learn a policy for filling in a web form but, honestly, I am not sure I got that. and then pay using a credit card. The second question I have regards the relevance of the results. It is true, though, that results are much better than the baseline and for this reason I believe the paper has some merit. > This sentence is not clear<BRK>This paper presents a technique for adversarial generation of environments for the interesting problem of web navigation, and provides an environment that enables learning to design complex websites out of a set of compositional primitives. Then, it also proposes a method to adversarially generate a curriculum of increasingly complicated websites, and uses it to train agents which can navigate more challenging, high dimensional websites. The training of agents with an autoregressive adversary policy is interesting.<BRK>This paper improves upon existing approaches for learning to fill forms on the web automatically. The main idea is to train an adversary to generate a curriculum of environments to train an agent to learn to fill forms on the web. The empirical analysis is also quite satisfactory and clearly shows the superiority of the proposed approach over existing baselines. I dont have any major criticism for the paper as I quite like it.
Reject. rating score: 5. rating score: 6. rating score: 6. rating score: 7. <BRK>Summary: This paper proposes a new sampling method for LIME based on user defined Boolean subspaces. They additionally propose a new metric for measuring the quality of an explanation. Pros: The authors create a novel link between the explainability literature and Boolean Satisfiability. They propose simple metrics for evaluating the quality of samples and the quality of an explanation. Minor: Equations 1) and 2), if they are describing the usage in Ribeiro et al., should include a weighting function. Examples which are important to the points being made should be brought back into the main body of the paper. In the adversarial attack experiment, it is not completely clear what is done: did you use CLIME to generate explanations of the adversarial classifier from Slack et al.? ###post rebuttal###I have read the updated version of the paper and still feel that this paper may have errors regarding the flexibility and purpose of LIME. By sampling outside the data distribution it becomes apparent that the classifier is using x1 only.<BRK>The paper presents augmentations to the LIME explanation system using logical constraints. The paper claims that this procedure is problematic because the samples in the neighborhood of x can be out of distribution (OOD), so learning from the OOD samples are not useful. However, the contributions of the paper are unclear, and the experimental results were not very exciting. But for 1) it seems they are simply calling off the shelf tools that sample from a logical constraint. Unfortunately, the constrained spaces for Recividism doesn t seem to be motivated at all, and I don t see why I should believe that CLIME explanations are better than LIME just based on Fig1.<BRK>This paper presents a method   CLIME   to generate constrained explanations using LIME. The method relies on boolean constraints to dictate the sampling region for LIME. Second, the method adds a level of robustness against adversarial attacks. The authors demonstrate their approach on a number of data sets and find useful insights conditioning on different features and greater robustness to attacks proposed by Slack et. The extension of the attacks to only discrete data for LIME is interesting   making it much more similar to the SHAP attack in some sense because this attack only relies on discrete data. I m a bit confused about the claim that  the methods help detect adversarial attacks. If they re using the adversarial classifier from Slack et. Overall, the constrained explanation approach seems pretty useful for conditioning explanations and could be a valuable contribution.<BRK>The topic of this paper is highly topical, as it focuses on providing explanations for black box classifiers. The paper tackles the known issue with out of distribution sampling in Ribeiro et al.s LIME.The approach taken is to introduce the use of Boolean constraints in LIME algorithm, which allows for providing some guarantees on the quality of the explanations. Related work is sufficiently covered in the main text, and more discussion is provided in the appendix. The authors might take a look to a recent work Bjorlund et al.that also tries to remedy the out of distribution sampling problem using an approach based on robust regression. Since the approach taken is to use Boolean constraints, it would be relevant to know the effort needed to perform the sampling, i.e., does the runtime of CLIME differ much of to that of LIME. Pros:1.Very relevant topic (explanations) and demonstration of the importance and the relevance of the data distribution in explaining black box classifiers2. 1 and Alg.
Reject. rating score: 3. rating score: 4. rating score: 5. rating score: 7. <BRK>High level view:I don’t think this is necessarily a bad paper, but I think it’s unacceptable for ICLR in its current form. Now consider a prior accepted ICLR 2020 paper, Hoppity (Dinella et al.), which trained on nearly 300k code change commits in GitHub. After thinking over the concepts in the paper more, I might lean more strongly toward rejection or toward acceptance (if the authors can address the issues I raise below). I provide details below examination of how I’ve come to my evaluation rating below. Summary:This paper principally focuses on the idea of decompilation. On top of that, the only data is coming from LeetCode. Low level concerns:The language in the paper seems to use many strong and ambiguous claims: “N Bref outperforms previous neural based decompilers by a large margin.” First of all, what is a “large margin”? There’s not quantitative measurement in the word “large”. This kind of language is not what I expect from tier 1 publications. But I absolutely do not agree that those are the *only* two problems that stand in the way of decompilation. N Bref has a number of components that it relies on to perform its decompilation. Also, there seems to be some lack of understanding of the field of machine programming, from my perspective. The authors empirically evaluate their N Bref’s accuracy on a number of problems from the open source LeetCode problem set and generate 25,000 pairs of high level source and low level source which are broken into training (60%), validation (20%), and testing (20%). Verified lifting is principally focused, as I understand it, is focused on language to language translation. This feels largely incremental to me. On the other hand, one could argue that N Bref is novel because it combines a number of existing components in a unique way to achieve better performance that prior work. The second major concern I have with this paper is the small dataset they are using.<BRK>Major concerns: 1. The paper identifies 4 challenges for decompiling, but some of them are generic, such as long dependency problems and data augmentation. For the other two challenges (datatype and control/dataflow), the paper proposes to decompose the generation into two subtasks: source code generator (SC Gen) and data type solver (DT Solver). This appears to be a ragbag of existing and known models: Transformer [Vaswani et al., 2017], Tree Transformer [Sun et al., 2020], and Memory Augmentation [Cornia et al., 2020]. I do not feel this paper very exciting. The performance of a model is measured by "token accuracy" when the authors "expand the decompiled AST from the root into the same structure as the golden AST (AST_G)." The authors assume a correct program is there, and predict the next AST token/rule assuming previous partial AST is the same as AST. Such measure of success could be drastically different from the real performance of generation. With token accuracy given the correct partial AST, I would not agree with the claim that "N Bref successfully reverts human written" programs. That seems to be better than Ins2AST (w/o attention). Decompilation tasks is  > Decompilation task is<BRK>The paper is interesting and addresses an interesting topic. The results seem promising, though the evaluation is done on relatively small test cases. The proposal to divide the problem into two sub tasks, i.e., data type solver and source code generation, is promising and can probably have impact on future decompiler proposals. The main problem with the paper is that it is hard to read and understand. I m aware of the page limitations, but the authors have crammed so much inte these pages that is hard to follow. For example, the authors claim that one thing their proposed method extends beyond earlier methods is to handle pointer references. The usefulness may be limited of this work, since they only work on unoptimized code (see e.g., page 4, 2nd paragraph). However, in reality, most code have went though substantial optimization during the compile phase. Some other comments / questions:* I lack information about the execution time of the training and inference. What do you mean by that, and how du you support that claim? * Fig 1b.Here are a number of strange / confusing things:    Why are not all asm instructions shown in the data flow graph? You have mixed up lines 7 and 8 in the graph (show it as mulss  36(rbp),xmm0), which is confusing.<BRK>The authors present a neural based decompilation framework. They generate synthetic input data in order to make sure source code examples have a consistent code style that can be more easily learned. That seems backwards. They predict types of variables and the actual code in two separate steps. Strengths:  The authors propose an end to end system for neural decompilation  Interesting use of graph neural networks to increase sensitivity to structure in a transformer. Weaknesses:  It is not so easy to fully understand the approach end to end. Perhaps the presentation can be improved. The paper says that DT Solver and SC Gen are both based on the same architecture, but DT Solver is not really discussed in detail. As an example, it is not stated if the types of variables are chosen from some fixed set (which one?) or if the DT Solver generates a type AST, but Figure 1 suggests a fixed set. (E.g., there is no variable of type `int *`), and variable declarations are shown as part of a single AST of the program even though later they are treated separately. The evaluation metric is explained rather vaguely, so I am not sure if I fully understand what is meant, but this is crucial to interpret the results. How do you "expand the decompiled AST from the root into the same structure as the golden AST"? It would also be interesting to understand a bit better the distribution of the results, e.g., how do the results change if you count the fraction of results with perfect token accuracy instead of computing averages over token accuracy?
Reject. rating score: 4. rating score: 5. rating score: 6. rating score: 8. <BRK>This paper aims to propose a parse tree embedding that correlates with brain activations better than existing measures on sentences. It is an extremely important topic as it can draw the link between Artificial NN and real NN on the problem of syntactic processing. However, the paper leaves with a major quesiton: why? All these models are embedding trees in different ways. Why the proposed model should be closer to the brain activity? This should be definetly clarified in the description of the parse tree embedder.<BRK>This paper derives various types of graph embeddings to encode aspects of syntactic information that the brain may be processing during real time sentence comprehension. The main concrete conclusion drawn from these analyses is that complex syntactic information is encoded in the brain. A related conclusion made in the paper is that syntactic information is represented in a distributed fashion, but the effort based metrics seem to suggest a similar conclusion (if I m correctly interpreting Fig 3f), so again this is not unique to the proposed representations. I do recognize that the proposed syntactic representations are predictive of activity over and above the effort based metrics in some voxels, but it s not clear exactly what we learn from this fact. A secondary conclusion made in the paper is that regions that process syntax are not specialized for syntax. This conclusion seems to be made based on the fact that BERT embeddings are stronger predictors than the syntactic embeddings in many of the regions in which the syntactic embeddings outperformed other predictors. However, as the authors acknowledge, BERT embeddings also encode syntactic information, and this makes it more difficult to interpret this pattern of results.<BRK>SummaryThe manuscript focuses on understanding the features of syntax that are processed by different brain regions as captured by fMRI. The proposition is to move beyond effort based metrics to subgraph embeddings for modeling syntactic structure. I liked reading the paper and it reports interesting results. It is known that syntactic processing and semantic processing are mixed and that several brain regions contribute to constitute an understanding of language (see e.g.1,2).It is also known that the brain not only processes structure seen thus far but also predicts future structure. However, this, and other measures, such as syntactic violations, information gain, etc. Maybe the authors could be a bit more specific in what exactly is novel in the present manuscript. There was nothing in the study setting that would have controlled for other effects. For example, what if there were some other factors in the sentences that explain the effects. Without a proper control condition or other analysis of the data, it is not evident that all of the conclusions hold.<BRK>This paper presents a neuroimaging study investigating the way syntax is represented. They find that syntax and semantics are computed/represented in overlapping brain regions and that "complex" syntactic information is decodable. The authors give an exposition of their research questions, however I think these can be phrased even more clearly in some cases. For example, for Q1 do they mean for humans in general (as in in the brain) or do they mean us as scientists researching human cognition? Why is it in and of itself surprising that (complex) syntax is encoded in the brain? In other words: "Several regions of the brain’s language system were predicted by ConTreGE, hinting that the brain does indeed encode complex syntactic information." Please do not get me wrong, this is obviously important/required to be shown but the research herein actually has even more value (or could have) and can be framed and discussed as such.
Accept (Spotlight). rating score: 8. rating score: 7. rating score: 7. rating score: 6. <BRK># General statementsThis paper has a special flavour, in the sense that it provides new light on a very established training method for energy based models: contrastive divergence. This is done through a connection between CD and adversarial training. All in all, I would say that the paper is a very nice read, and its english usage is good, as well as the references that are appropriate. however, and if I m not mistaken, it should not be too difficult with autograd mechanics to include this dependency in the updates. Did it break the algorithm ? * The neural net used for the toy data looks impressively large (8 layers of FC+leakyReLU with 512 hidden size).<BRK>This leads to an approximation. This work shows that the resulting algorithm can in fact be viewed as an exact algorithm targeting a different, adversarial objective. The derivation in this paper also shows how Markov chains which are not reversible w.r.t.the posterior distribution of interest can be employed within the algorithm. Strengths and noveltyTo my knowledge, the derivation of the relationship between CD and conditional noise contrastive estimation (CNCE) is novel. However, this appears not to be the case in the toy example. However, it seems to me that for $p_\theta$ reversible Markov chains, if the chain is either fast mixing or the number of iterations $k$ sufficiently large, the gradient in Eq.3 is proportional to the gradient of the log likelihood (multiplied by $ 1$) in expectation because in this case, the first term has expectation zero.<BRK>Summary:This paper presents a way to view contrastive divergence (CD) learning as an adversarial learning procedure where a discriminator is tasked with classifying whether or not a Markov chain, generated from the model, has been time reversed. Specifically, when the contrastive distribution is chosen such that the detailed balance property is satisfied, then the CNCE loss becomes exactly proportional the CD 1 update with the derivation further extended to CD k. Practical concerns regarding lack of detailed balance are mitigated through the use of Metropolis Hastings rejection or an adaptive weighting that arises when deriving the gradient of their time reversal classification loss. Strengths:The paper is well written. The results of this work appear novel, and proofs seem correct. Concerns:I understand performance comparisons and experiments were not the focus of the paper. Which one is in Fig.(3.4)<BRK>### SummaryThis paper formulates CD k training as an adversarial game, where the EBM parameterizes a discriminator which tries to classify whether a k step Markov chain is reversed or not. It is technically incorrect to say that CD s update steps don t correspond to the gradient of any objective. In Eq.(15), the subscript in $E_{q_\theta}$ should probably be dropped, to be consistent with the rest of the paper. Sohl Dickstein, Jascha, Peter Battaglino, and Michael R. DeWeese. "Minimum probability flow learning." arXiv preprint arXiv:0906.4779 (2009).
Reject. rating score: 4. rating score: 5. rating score: 6. rating score: 6. <BRK>Inspired by the concept that ``flat minima generalize better than sharp minima", this paper proposes to search the flat minima architecture by considering the performance over the neighborhood architectures. A random search and differentiable search method based on the neighborhood principle are proposed, which achieve comparable results with SOTA NAS methods. The biggest concern lies in the main idea of the neighborhood aware search formulation. It is known in many previous methods that the flat minima of the loss function of neural network training generalize better. However, this paper hypothesizes that similar architectures with similar performance point to a flat solution. This is of critical importance to support the main concept of the paper. It is not clear whether advantages of the method actually exist. The proposed method does not show evident advantages over others, e.g., worse or similar compared with PC DARTS but taking much more cost, 10x than PC DARTS. The experiments are based on the DARTS like search space. As the flat solution has been explored in [1], the main contribution seems to be the newly defined loss function ensemble with similar architectures. Understanding and robustifying differentiable architecture search. In ICLR, 2020.<BRK>This paper introduces a searching framework of neural architectures search by modifying the objective function to optimize the aggregated performance over the neighborhood of an architecture. From the observation that flat minima architecture $\alpha$ generalizes better than sharp minima architecture (Zela et al.(ICLR2020), the author proposes the objective function considering the neighborhood to enforce the flat minima. The author supports their method by providing ablation studies and architecture performances from CIFAR 10/100, ImageNet, and NAS BENCH 201. 2.The authors support their methods with various datasets such as CIFAR 10/100 and ImageNet3. The experiment results are incremental improvements (or par) to the existing NAS algorithms. The experiments don t include NAS Bench 201 (a popular benchmark for NAS) and small experiments with NAS Bench 1SHOT1 only comparing with DARTs, PC DARTs. And PC DARTs result is better than NA PC DARTs according to Table 6. The possible neighboring architectures from a given subnetwork even with hamming distance 1 may be much larger than 10. Overall, this paper proposes a neighborhood aware objective function that can be adopted in the existing NAS search. Despite the paper well written, the performance of their methodology is incremental improvements (or par) among various existing NAS algorithms. I would recommend a marginally below acceptance threshold for now but may change the decision based on the rebuttal. Chen, X et al.Progressive differentiable architecture search: Bridging the depth gap between search and evaluation.<BRK>** Summary The authors proposed neighborhood aware neural architecture search, where during the evaluation phase during search, the neighborhood of an architecture is considered. This is built upon the assumption that `` flat minima generalize better than sharp minima’’ and the authors verify it in Appendix C. The authors conducted experiments on CIFAR 10/100 and ImageNet, and obtained promising improvements over the standard baselines. Towards "Due to the property of the total variation distance, when $d$ is an integer, the neighborhood contains all the cells that have at most $d$ edges associated with different operations from $\alpha$". 1.There is a possible “ensemble’” baseline for your method. Let us take DARTS as an example. 2.Improvement not significant: In Table 4, compared to DARTS+, the improvement is 0.1/0.4 on CIFAR10/100. I think the "NA DARTS ES" should be implemented to see whether your method and  (Zela et al 2020) are complementary to each other. But my concern about "Improvement not significant" is not addressed, which is also mentioned by R1 and R4.<BRK>This paper proposes a neighbor aware method in the neural architecture search (NAS). The paper states that by optimizing a neighbor of the neural network, it can search the result in a flat minima, which is more stable than the sharp minima. The experiment results in further support that the proposed NA RS and NA DARTS outperform the current SOTA in various tasks. Overall, I think the paper raises an interesting opinion, which stresses that during the NAS process, a stable flat minima has a better generalization ability. Below are some detailed points of my opinion about this paper. Pros:1.The paper compares with a lot of NAS methods and outperforms most of them. 2.Stabilize the search result can reach a better performance seems reasonable for me. However, from the empirical result, the sample based NAS result is relatively weak compared with the differential based. 2.PC DARTS has reported its ImageNet performance, top1 error, 24.2%. However, they only show an S3 search space result in the appendix, it would be more convincing to show the neighbor aware method s generalization ability if you can apply it on the standard search space and directly compare with PC DARTS s public performance. I agree that different DARTS paper usually uses some different settings, the lower PC DARTS performance in the paper could be reasonable. 3.I also read other reviewers  opinions. This is an acceptable paper, but still has something to do in the future, like a more comprehensive experiment part.
Reject. rating score: 3. rating score: 3. rating score: 4. rating score: 5. <BRK>It is better if authors can provide evidence where the proposed adversarial settings can actually prevent filter bubbles, for example, by recommending different opinions than before. While deep learning based news recommendation systems work very well, these systems tend to recommend contents of the same site, and are likely to develop extremist tendencies. Second, while this paper starts with a good motivation, I was not sure that preventing the same outlet from adversarial learning procedures.<BRK>No technical novelty would be acceptable if some insights for the application area are generated. While I really like the general application area and the attempt to tackle such relevant issues, I think the paper is not ready. A strong point of this paper is the motivation and application area since it tackles a problem with direct societal impact, something that is typically not attempted in general machine learning papers.<BRK>Its novelty is not enough, especially for ICLR. 4.The paper doesn t seem to ready for submission. 3.Experiments of this work are quite limited. How about using muti label to train the model?<BRK>Weak points:1) The paper is a simple application of the well established adversarial training methods and there is very little technical novelty in terms of the core contributions. Would advise authors to submit this work in an applied data sciences conferences like SIGIR, CIKM, ECIR etc. Authors propose an adversarial training task for tackling the problem of filter bubbles in news recommendation problem.
Accept (Poster). rating score: 7. rating score: 7. rating score: 7. rating score: 7. <BRK>Review:This paper proposes a novel approach to handle the recovery of dirty data in fully unsupervised scenarios. The corrupted data considers both missing data and noisy samples. Questions/Comments:  It feels to me that the main focus of this paper is on missing data rather than in other types of corruption. Something I would have appreciated in this work is to observe the performance of the authors method in scenarios handling missing data and corrupted data separately. Nonetheless, it is interesting to see experiments with both effects combined, which is not so common in the literature. The notation is clear and the proofs in the appendix look sensible.<BRK>Summary: The paper proposes a method for reconstructing noise free data instances without assuming any ground truths for this, using a variant of auto encoder that avoids posterior collapse by utilising a newly proposed reduced entropy condition. The problem itself is important and the proposed method seems to offer good empirical performance for the task, outperforming a good selection of recent methods. Reasons for score: A good paper with no obvious flaws; I do not have concrete improvement suggestions. Nevertheless, the justification for the loss is well explained in the paper.<BRK>This paper presents the tomographic auto encoder (TVAE), which conducts inference over the data space $x$. To resolve the intractability of the entropy $H(q(x|y))$, TVAE identifies the *reduced entropy condition* and transforms the intractable ELBO maximization into a constrained optimization problem. The resulting model demonstrates superior performances compared to baselines, in terms of generating diverse samples. I think this work makes an important contribution. Conditional neural process (Garnelo et.al., 2018a) and neural process (Garnelo et.al., 2018b) are another stream of models for missing data imputation, which are missed by the paper. Regarding the reduced entropy condition, though it ensures that the entropy is decomposed into more tractable forms, will enforcing this condition limit the expressiveness of the encoder $q(x|  y)$ ?<BRK>This paper proposes a Tomographic auto encoder (TAE) for unsupervised recovery of corrupted data. More specifically, TAE takes a Bayesian approach to recover the posterior distribution of a clean image conditioned on an observed corrupted image and thus effectively modeling uncertainty in data recovery. The paper argues that a naive application of VAE is not effective due to the latent variable collapse, and proposes an alternative model where hierarchical latent variable models are used for both prior and variational posterior. The authors state that the problem of a vanilla VAE with hierarchical latent structure (latent code $z$   clean image $x$) is that it is prone to latent variable collapse. This is true, but there are plenty of existing works (partially) resolving this problem. I recommend giving a more detailed description of the baseline at least in the appendix.
Reject. rating score: 4. rating score: 5. rating score: 6. rating score: 6. <BRK>The probe classified is trained on top of the pre trained contextual presentation models, such as non contextualized word embeddings (e.g., Glove), contextualized word embeddings (e.g., BERT), and generative language models (e.g., GPT 2), and tried to reconstruct the structure of the knowledge graphs. This paper is well written. While it was not sufficiently clear why this paper adopts this approach, I think the idea of the probing classifiers and knowledge graph construction is a reasonably good idea to reveal how well the pre training models encode the underlying knowledge graph. The cons of this paper are the lack of a profound analysis of the experimental results. While I think this approach is also good, readers will need more detailed information about analysis results to inspire new ideas to improve semantic learning abilities. For example, the number of samples in each concept depth and wordnet distance between concepts changes. Second, it is better to analyze which semantic category the pre trained methods work well or not well. Thirdly, it is better if this paper shed light on how readers can improve the semantic learning abilities based on these results.<BRK>The paper 1) introduces a method to use three types of text embedding methods (non contextual, contextual, LM  based) to predict  word relatedness (as a binary classification problem) for pairs of words in wordnet 2) uses these relatedness scores to build proxies of the Wordnet graph 3) carry out experiments based on the two bullet points to compare semantic understanding abilities of the aforementioned embedding methods. Major comments:* The paper carries out quite a few experiments with weakly connected goals. * The technique to probe models is quite restricted in that it is centered around single word concepts. Given that contextualized models are utilized, it seems like a rather handicapped investigation of very powerful models. * In section 4.3, authors try to make a point about correlations on visuals. This is a dangerous approach, and it would be much better to rely on numerical summaries of correlations. * It s hard to understand how the proposed probing classifier is different than concatenating $M(x)$ and $M(y)$ and directly applying MLP on it. * There is definitely truth to the title, but I d suggest not conflating the term "knowledge graph", which traditionally represents actual world knowledge, and not lexical databases.<BRK>This classifier also aids in recreating a sampled knowledge sub graph from WordNet. The main findings of the study can be summarized as follows:  The authors show experimentally that models coming from the same family are strongly correlated  The authors show the experimental outcomes of the tasks mentioned above  The authors show how the individual layers of the language models contribute to the underlying knowledge  The authors show for all models how they are affected by different semantic factors (9 different factors, including number of senses, graph depth etc.) The paper is clearly written and understandable and includes enough details to understand the implementation of the semantic probing classifier. The appendix contains detailed outcomes of the different experiments which, together with the result section, give a good overview of the experimental results. My recommendation is towards acceptance of the paper, because the authors contribute to a more detailed understanding on how language models incorporate semantic knowledge and where this knowledge might be located within the models.<BRK>SummaryThis work addresses the question about how pre trained language models encode semantic information. The paper analyzes how embedding models encode suitable information to recreate the structure of WordNet. The study also shows evidence about the limitations of current pre trained language models, demonstrating that all of them have difficulties to encode specific concepts. The contribution of the paper is not clear. What can we learn from the experiment of the paper? Originality        The analysis includes recent models such as ALBERT and T5. The fact that models in the same family have similar results is not discussed  It is not explained why only the Princeton WordNet Gloss Corpus has been used and not larger datasets annotated with WordNet senses such as SemCor. Usually the models are evaluated at each layer, here all the layers are concatenated making it more difficult to understand where semantic information is stored.
Accept (Poster). rating score: 7. rating score: 7. rating score: 7. rating score: 3. <BRK>Summary: The authors present a way of framing contrastive self supervised learning techniques that enables them to use MixUp. Pros:+ The proposed approach for adapting the effective Mix Up strategy to contrastive learning is simple. There have been very few such general purpose innovations in self supervised learning since the original instance discrimination work that started the current trend. + I find the experimental evaluation quite careful and detailed. This clearly seems to be a worthwhile extension to the world of self supervised techniques, especially relevant for new domains with limited data.<BRK>In essence, the premise and contribution of the paper is rather simple   contrastive learning and mixup are both very effective methods in their respective domains, and this paper proposes the use of mixup to improve contrastive learning, and the results seem to indicate this. My rating is a 6, though I open to raising the score if my concerns have been addressed. I am also concerned with the potential of vanilla mixup to  underfit  (see [1]), though perhaps this would be mitigated via careful tuning of the mixing distribution parameters  alpha  (again, see (3)). This is lacking in all of your tables.<BRK>**Update: As noted elsewhere in the discussion, the authors have addressed my primary concern. I will therefore increase my score from a 6 to a 7. Though much better than not using any augmentations at all, the results are not typically better than using standard augmentations (Table 3). The proposed method is simple and effective, clearly improving the performance of several contrastive learning techniques in the regime where overfitting is an issue. Weaknesses:The paper proposes a regularization method for contrastive learning and demonstrates that it can help to reduce overfitting. However, it has not been demonstrated that the proposed method is unusual in this regard. Overall:The role of overfitting in contrastive self supervised learning is an interesting and timely topic, and the proposed method is simple, effective, and general. What does "with the largest mixing coefficient $\lambda$" mean?<BRK>The problem the authors are trying to tackle is meaningful (+1) and relevant. Results have a few sections:1. 3.They are attempting 3 domains with the same method. ## Weak pointsTheir general results are not significant enough. Most results look like this (82.5 vs 82.7). Since some of these claims come down to implementation details, it s important to see the code as well.
Reject. rating score: 5. rating score: 6. rating score: 6. rating score: 7. <BRK>This work proposes a Bayesian approach to meta learning from sequential data. Two algorithms are proposed. The major drawback of the paper is a lack of description of the precise problem setting. Furthermore, certain motivations and comments appear to be unsubstantiated, and it would be great for the authors to provide rationale for them. However, I am willing to reconsider based on author response to the questions. **The paper over emphasizes the sequential nature of the setting, but does not provide clear problem setup beyond this. The goal is to compete with the best (meta) learner in hindsight. It is unclear which setting the paper studies. My understanding is that setting 2 is more relevant to the paper, but the authors should clarify and explain the setting of the paper explicitly. **The paper in Section 2 and 3 sets up an objective that aims to find the MAP estimate, i.e.$\theta^*   \arg \max_\theta P(\theta | \tilde{D}_{1:t+1})$. However it is not clear why this should be the objective. In a streaming setting, doing well on the past need not imply doing well on future streams of data unless some assumptions are made about the data distribution.<BRK>The authors suggest 3 approximation schemes, a Laplace approximation, a Hessian approximation, and a variational approximation. The authors state that MAML requires all base classes to be available for sampling at each iteration. However, this does not seem to be the case for the online extension in Finn et al., 2019. The empirical results to not compare to some of the SOTA methods in the increasingly populated domain of continual meta learning (e.g., Finn et al., 2019), and indeed to previous methods to which the present paper is an extension (e.g., Ritter et al.2018a).Error bars are missing from the figures, or should, at least be mentioned. Specific issues:The authors use the terminology Laplace approximation for the Taylor expansion of the posterior around its maximum. As far as I am aware, the term Laplace approximation is used when this is done within an intractable Bayesian integral, in order to yield  a tractable integration. It would be good to discuss this issue in more detail. The title online learning is a little misleading, as the learning within tasks is done in batch. This should be compared to recently proposed fully online methods (e.g., Finn et al 2019, Khodak et al, Adaptive Gradient Based Meta Learning Methods, NeurIPS 2019)The loop over i on line 4 of Algorithm 1 of the appendix has no termination point.<BRK>This paper presents a Bayesian meta learning framework for sequential data. Two approximate Bayesian inference techniques, Laplace approximation and variational inference, are proposed for estimating parameters. The method is sound though the majority of the paper is based on other algorithms (Opper et al, Fin et al., Ngyuen et al., etc.). This is framed as a typical Bayesian model selection problem. Although meta learning is nothing more than model selection, this is apparent in section 2.2. I encourage explaining "why" performing Bayesian inference helps to mitigate the catastrophic forgetting problem well ahead in the paper. How does the proposed approach would be different from explicitly keeping some representative samples or sufficient statistics in a memory buffer [1]? Alternatively, we can maintain an ensemble. It would perhaps be better than Laplace or mean field approximations for this kind of complex distributions. Is it possible to find some meaningful online datasets such as those used in robotics or signal processing? [2] seems to have such experiments.<BRK>The paper proposes an Bayesian approach to online meta learning. In general, the novelty is somewhat limited, as this extension of LA/VCL to meta learning is similar in principle to the Grant et al formulation as a hierarchical Bayesian model. The main difference  is that the inner loop is done using the standard fast adaptation method from MAML rather than using LA/VI. If you’re only using a point estimate (“We are interested in a MAP estimate $\theta^*$”) then this is not truly Bayesian online learning. I assume that the choice of doing it this way is to sidestep the difficulty of integrating over the parameters to form the posterior predictive, but loses the advantages of having a Bayesian posterior. It also makes use of amortisation networks so that at test time only forward passes through NNs are required. They don’t provide an online formulation. This paper also uses a hierarchical Bayesian formulation, but takes a non parametric mixture approach using Dirichlet processes. They handle distribution shift in the online meta learning setting by modelling latent task structure. The two methods ML PIP and BMAML mentioned above, along with the Grant et al hierarchical Bayes method, could all be applied here, but since they weren’t originally formulated for the setting it would require extra work to do so. Edit post reviewer responses:  I had some misunderstandings in the review above, which have mostly been cleared up. I’ve raised my score accordingly
Reject. rating score: 5. rating score: 6. rating score: 7. <BRK>At the final stage, all the groups would be concatenated and input to a fully connect layer to get the final output. But in terms of a tree, it would be interesting to have some experiments to show the effect of the way combining feature groups. The experiments on the simulated Bayesian network dataset supported the claim that this architecture can utilize the fact that some of the features are not related and do not need to interact with each other. However, the architecture the authors used corresponds to the model that generated data, which is almost impossible in many real life problems. It can be helpful to have some results on simulated data with mismatched architectures from the model to help better understand the performance. One of the most important ideas in this paper is limiting the group wise interactions. It would be nice to have some analyses on the chosen groups and selected features, e.g.the existence of a set of features that always come into one group, and/or comparison of derived groups by GMLP and randomly chosen groups. A more detailed explanation of the dataset would be needed. For example, the authors used MIT BIH dataset to compare the accuracy of GMLP and MLP with different sizes without introducing the dataset. If the complexity analysis of GMLP, equation (7), is only for inference, please also include the training complexity. Another concern is that the results suggest that the number of feature groups may need to be quite large (compared to the number of original features).<BRK>The experiments on various datasets, and important ablations such as the effect of number and size of groups, types of pooling were done. The paper s main motivation is to propose a method that works on domains beyond image, voice, and graphs. However, all ablations are done on an image dataset naturally that has spatial relations. $\bullet$ *Incremental performance gain:* Looking into the performance comparison with vanilla MLP and other methods (SNN, SET, and FGR), the only considerable improvement from the MLP baseline is on the CIFAR 10 dataset. The results on all other datasets, accuracy, and area under curve scores are incremental. In Table 2, the number of parameters should be written or standardized in each method. Fig.5 and 6 show the performance margin could be 10 15%. $\bullet$ How does the group routing evolve during training? (Are the members of the same group more relevant to each other in vice versa?) In the introduction, it may be useful to include that the expression is not related to group ina mathematical sense, and only represents a subset of feature dimensions.<BRK>The paper describes an MLP architectures for problems in which the features do not have a known structure (eg, tabular data). A "differentiable routing matrix" partitions the data into K blocks. Then, standard MLPs are applied to each block and the results are recursively aggregated by moving forward in the model. On the plus side, the paper is well written, the topic is significant (as evidenced by any Kaggle competition on tabular data), and the model is simple enough to be understandable even on a quick reading. [Addendum after review: most of the concerns have been addressed by the authors with a large set of additional experiments.] [Addendum after review: the authors have added a random forest to the experiments. No time comparison is given, and some accuracy deviations appear well within the standard deviations. Still, this is a very good addition to the paper.] Summarizing, point (3) is the most important. Showing that the proposed Group MLP is superior to a classical alternative should be the critical aim of the comparisons. [Addendum after review: This point has been partially addressed.]
Accept (Poster). rating score: 6. rating score: 6. rating score: 6. <BRK>This paper is not bad in the sense that:1. important problem   how to leverage compressed video such a natural and efficient format is very important while under explored by the community2. extensive experiments   clear gains on standard benchmarks demonstrate the proposed method s gains. good ablation studies in like table 3 demonstrate the effectiveness of each proposed pretext task. Thus, I lean to accept this paper.<BRK>This paper proposes an approach to self supervised learning from videos. The approach takes advantage of compressed videos, using the encoded residuals and motion vectors within the video codec. Overall, the paper is interesting. This would help show the benefit of the proposed tasks.<BRK>3.Why not compare with the contrastive learning based approaches for self supervised learning. I would like to see more discussion about the intuition of the proposed method. Self supervised compressed video understanding is an exciting topic. We can still get the positive pairs from the same video, though it is difficult to augment the  videos give the compressed videos.
Accept (Poster). rating score: 7. rating score: 7. rating score: 6. rating score: 4. <BRK>The authors are focusing in this paper on the development of LookAhead mechanism. Their contributions are the following:1. They provide an improvement upon an already existing convergence rate for the minimization case. 3.These results are extensively justified by various numerical experiments. The paper is well written and easy to follow. I fully understand that the paper is primarily experimentally oriented. However some more theoretical analysis would be useful. More precisely, the theoretical justification for the improved convergence part for the minimization part seems to be more like a sketch rather than a rigorous mathematical proof. Overall, I suggest that it would be very interesting to see some provable theoretical guarantees starting with the  paper s motivation example that of the bilinear game.<BRK>Overall, I think this paper is very strong on the empirical results. Summary: This work extends the recently proposed lookahead optimizer (which was designed for single objective optimization) to minimax optimization, particularly GAN training. The empirical results of the introduced lookahead minimax algorithm are quite impressive given its simplicity. However, the lookahead minimax dynamics is not well understood and no theoretical justification is provided in the paper. Actually, this phenomenon occurs even for the simplest quadratic minimax games, see for example [1, 2, 3]. Moreover, Nash equilibrium might not even exist in general, Stackelberg equilibrium [2, 4] is probably a better solution notion. 2.The authors provide some theoretical results of the lookahead optimizer, but for single objective optimization. In particular, the authors argue many times in the paper that the lookahead algorithm is able to handle the rotational dynamics well, I think the authors should be able to verify that theoretically on quadratic minimax games if the argument is right. 4.In the paper, the authors chose Extragradient as one of their baselines. I understand the authors would like to keep the total gradient queries the same over different algorithms, but there is a similar algorithm (Optimistic gradient descent ascent [5, 6]) with only one gradient query per step and perform similarly with Extragradient. 6.In the original lookahead paper, it was mentioned that the slow weights can be understood as the exponential moving average (EMA) of the fast weights. I think it is probably worth testing EMA on the toy experiments and mentioning it in the paper. I notice that the authors have already included EMA in section 5 and EMA improves the performance a lot.<BRK>Figure 1 clearly demonstrates how lookahead minmax can address the rotational nature in GAN’s training, which is further confirmed in Figure 5. 2.The advantages of the proposed lookahead minmax is demonstrated with a bilinear example. The contributions are a little bit confusing. The first contribution listed in the introduction (and Section 3) is an improved convergence guarantee, which seems an independent contribution. What is the relation between this theoretical result and the lookahead minmax algorithm for GANs? Does the following analysis and conclusions rely on this improvement? After reading the paper, I found the rest of the paper does not necessarily build on the new convergence guarantee. CLC GAN [Xu et al.arXiv:1909.13188] proposes to stabilize GANs’ training dynamics by interpreting the gradient flow as a dynamic system and manage it with closed loop control, which is also related to this paper. 3.The experiments are conducted on 4 representative datasets. It would be more convincing if the authors can provide results on larger datasets. (SB G): It seems that $b$ and $c$ are of dimension $d \times n$. 2” are used.<BRK>The algorithm proposed in this paper shows improvements over existing methods, in terms of convergence rate. There seems to me some inconsistency in the numerical results in Tab. 1 and Fig.6.Are the same hyper parameters used for LA Alt GAN on ImageNet? In terms of writing, I would recommend to focus on the minmax problem. In this sense, the section 3 is a bit distracting as it is about the optimization problem. Also what is LA ExtraGrad algorithm? Also is the number of passes the same as the number of iterations? To make the results in Fig 5. more conclusive, I would recommend to zoom into the eigenvalues of LA AltGAN, to see how small the imagery parts of the eigenvalues are. This is an interesting observation that it seems not very conclusive to show that LA GAN shows no rotations. Overall, I think both the results and the writing need to be improved.
Reject. rating score: 3. rating score: 6. rating score: 6. rating score: 6. <BRK>Summary: randomized smoothing is a method to construct provably robust classifiers via additive Gaussian noises on the input. The authors propose to learn score functions as a means to denoise the randomized image prior to a trained classification model. As the denoising + pre trained classifier architecture is already proposed, the contribution is only limited to the choice of using a score function. The justification and realization of the method is limited for two main reasons. 1.Efficiency: one of the most critical bottleneck of randomized smoothing methods is the slow prediction time. The score function based generative / denoising models are known for their slow sampling time, so the proposed method undermines randomized smoothing in efficiency. 2.Many design choices in this paper is not well justified. 2 3) Clearly using the same score function for multiple $\sigma$ is suboptimal. Although the authors mentioned this part as an advantage, but it is not clearly compared to existing methods.<BRK>This paper proposes a method based on denoising to protect classifiers from adversarial attacks. Unlike existing methods based on randomized smoothing with various noise distributions to retrain several classifiers, the proposed one uses denoising as the preprocess of the classifier. The experimental results demonstrate the proposed method has good performance.<BRK>This paper presents a denoising based method for randomized smoothing that converts a base classifier into a smoothed one with p robustness to adversarial examples. The major novelty of this work lies at the proposed denoising method using learned score function. The new denoising method only requires training one score network and is readily applicable to defend various $l_p$ adversaries, which is a key feature not available in [1]. The experiments show the proposed method outperforms the previous denoising based approach, and is sometimes on par with the white box approach [2] that manipulates the classifier. Basically, this is an incremental work over [1] but the contributions claimed are perceived myself (though I have to admit I m an expert on image denoising, instead of adversarial defense) However, I do have some concerns about the method and the experiments, listed as follows:  The major advantage of the score function based denoiser is the flexibility to handle various noise types and levels. As it is the case on Table 1/2, I m wondering what s the benefit of  the proposed denoiser over the state of the art Gaussian denoisers (as used in [1]) under Gaussian noise setting? In Table 2, I suggest the authors to add comparisons to [1] with denoiser trained on Gaussian noise setting, as well as ones trained with noise type aligned with the test setting. [1] Denoised Smoothing: A Provable Defense for Pretrained Classifiers, Arxiv 2020  [2] Certified Adversarial Robustness via Randomized Smoothing, ICML 2019<BRK>Summary & contribution: in this paper the authors propose an improved method for randomized smoothing (for provable defense) which performs better and is less computationally expensive than previous work. More specifically the authors propose two image denoising algorithms (based on score estimation) that can be applied regardless of noise level/type. Strengths:  strong quantitative results. The gap with white box smoothing is small on cifar. The method outperforms Salman et al.Only need to train one score network to handle various types of noise type/level  Denoiser doesn t need access to the pretrained classifier. More specifically I did not understand the motivation for using score based denoisers rather than a more "standard" algorithm for blind denoising. Method seems to be effective for low resolution images only. The gap with white box increases on Imagenet. What is the advantage of one step denoiser over multistep, does it perform better for gaussian noise?
Reject. rating score: 3. rating score: 5. rating score: 5. rating score: 6. <BRK>The paper proposes to train machine learning models with communication efficient differentially private approaches. I have several concerns about the work including the experimental setup. Experimental setup:Experiments use a privacy budget of epsilon   400 for LeNet Models and 2000 for ResNet models. Other:a. R_s is not defined in Section 1.2.b. PrivQuant seems to be a very interesting algorithm, however very little intuition is provided.<BRK>This paper studies FL under local differential privacy constraints. But in this paper, the comparison is only about large epsilon. For the experiments, the epsilon is too large for privacy protection as 400 and 2000,. For this level, the privacy is not well protected.<BRK>The paper proposed a differentially private training algorithm for federated learning. However, there are quite a few works on communication efficient privacy preserving distributed training. Overall the paper is well organized and easy to follow. My main concern is that the paper seems incremental and the comparison with existing works is not sufficient.<BRK>  Overview This paper studies a low communication algorithm for multivariate mean estimation in the federated learning setting with differentially private communication. The paper does a good job of placing itself in the context of prior work. The authors use this paper as justification for the large local epsilon values used in the experiments. The experiments are well designed.
Reject. rating score: 2. rating score: 3. rating score: 3. rating score: 4. rating score: 4. <BRK>Summary:This paper proposes a new approach for meta RL. The paper claims that the proposed method reduces variance and bias of the meta gradient estimation by only a few samples. In addition, this paper claims that their method is more interpretable. Hence, I d recommend this paper to be rejected.<BRK>Authors introduce a new meta RL algorithm based on SAC. Authors claim their method reduces variance and bias of the meta gradient estimation, is closer to human learning, encourages the agent to learn to explore, is more data efficient in test time and has competitive performance among gradient based algorithms. I found many of the claims of the paper are unjustified:1. Derive a new policy **which** maximizes3. The experiment results suggest that4.<BRK>The authors test of very simple benchmark problems compared to recent work in meta RL. The approach is based on combining gradient based policy updating with recurrence based updating of the value function. The authors evaluate the method of simple standard meta RL benchmarks. The authors should also improve the presentation of the paper to improve clarity.<BRK>### **Summary and Contributions of Paper** This paper proposes to improve the K shot RL meta learning problem by using an LSTM, whose repeated inputs are (s,a,s ) state action transitions, and whose step wise outputs are context vectors. The writing of the paper can be significantly improved. This is in contrast with previous works which used manual methods for the policy exploration.<BRK>Suggestions to improve the quality of the paper:  The title states "interpretable" but there is no interpretability discussed anywhere in the paper. It is not clear how the proposed method is different from these. Prior work has shown that SAC exploration can be improved with dual Q function and upper confidence bounds, but that is not being used here: https://papers.nips.cc/paper/8455 better exploration with optimistic actor critic   It is unclear how the K shot version of the meta RL algorithm is different from other meta RL algorithm. The proposed algorithm also samples from the tasks uniformly, same as prior algorithms. The quality of these graphs need to be improved for readability.
Reject. rating score: 3. rating score: 5. rating score: 6. rating score: 6. <BRK>The proposed framework is demonstrated with two datasets from the transportation sensornetwork. See 3 for details. This islargely ignored by the proposed methods. ordemonstrate it with experimental design. In Section 4.3, all the experimental results are notconvincing or counterintuitive:First, I am surprised that the authors have a result to say that split learning performs worse thancentralized training. Without such a comparison, the novelty is notconvincing. ***2.No technical contribution***The focus of this work is diverse (both model and training), but lack of contribution in eachaspect. The model used in this paper isa simplified version of these two GCN+RNN based models in transportation. Note that FedAvg usedin the proposed method is a centralized training with decentralized datasets. Without the code release, it’s hard to check the correctnessof the implementation. However, the authors do not discuss this.<BRK>The model consists of four major steps. Cons     Privacy concern: This paper introduces some privacy preserving methods in Related Work, but the corresponding protection methods do not present in the paper. Or the author regards the extracted feature vectors and corresponding gradients are not sensitive. In addition, I am curious about where communication costs mainly come from. Experiments:  An important baseline is missing, a model that aggregates all the data and directly combines GRU with GNN to predict the traffic flow without a federated setting (like DCRNN).<BRK>The methodology seems to be clearly explained while it relies on many existing methods in the literature which limits the novelty of the paper. The model is applied to two real data sets and outperforms some existing methods. Overall, I find the methodology interesting. It would be appropriate to show the robustness of the method with respect to changes in the spatial weight selection. (2) Removing trend and seasonality is a common practice in time series forecasting, but it is not clear whether this step is addressed properly or not. What about the seasonality in the data?<BRK>This papers is apparently one of the first few attempts to combine them for spatio temporal data modeling. The time series data in the local nodes is modelled by an Encoder Decoder architecture and spatial locality property of various nodes is captured by the server. To ensure that all the nodes encode their temporal data in a common space, the encoders are shared by the clients. Following are some doubts / concerns I have:1. In Eq.3, the specifics of how the edge features and global features are extracted is never mentioned in the paper. All that is mentioned is how they connect two nodes based on their distance of separation. 4.In Table 3, why are we not comparing the computational cost on the server?
Reject. rating score: 4. rating score: 5. rating score: 6. rating score: 7. <BRK>This paper tried to analyze gradient descent with weight decay and momentum for scale invariant networks. I think this paper has several weaknesses and hence do not recommend accepting the paper at the current point. for example, it doesn t rule out the case of converging to a bad local min. The analysis and arguments are very poorly explained. "while the gradient component provided by WD always tends to reduce weight norm." The authors say "one can derive" (12). "Besides, practical implementation suggests training DNN without WD always suffers from poor generalization"   This needs references. 2.The proof is not rigorous and the writing of the proof is very sloppy. I believe the proof in the appendix is not rigorous and I feel unease.<BRK>This paper focuses on the reasonableness of the equilibrium condition. ### CONTRIBUTIONSa) The analysis of the equilibrium condition is important. In Eqn.17, the formula (left) is the MSE form, which is the combination of the bias and the variance. The authors explained it like “the square of weight norm can linearly converge to its theoretical value in equilibrium, and its variance is bounded by the variance of the square of unit gradient norm multiplying the square of learning rate”. To reach the conclusion, one needs to at least claim “V\eta^2/l” is small. As shown in Figure2 (a), I do not think V is such small. Also, the claims in the main text that “the learning rate \eta is small” is not satisfying. I am not sure if I have skipped some important claims in the main text. I would like to raise my score if the authors can explain it better. It is still unclear to me why one needs to use an angular update. b) There are several typos (and also something like log >\log, min >\min) in the appendix.<BRK>Unfortunately, I am not qualified enough in the literature of learning dynamics of the neural network to comment on the novelty and contribution of this paper. Therefore I may take the author’s word and would like to rely on the comment on other reviewers. The main contribution comes from  Theorem 1 and Theorem 2. This paper proves that weight norm can converge at the linear rate under mild assumption . It also defines another index called angular update to measure the change of normalized neural network in a single iteration and prove its convergence with linear rate.<BRK>The main goal of the paper is to establish theoretically some previous known results that for scale invariant networks the weight norm has a fixed point with ||w||^4 eta/lambda ||\tilde{g}|| . The theorems are the main results and I don t think they are very powerful given what is known in the literature (the van Laarhoven paper but also the more recent Li Arora and Lewkowycz Gurari where the relevant eta lambda time scale has been shown to be characterize the convergence rate). The experiments do not scale invariant networks (even if they have batch norm, the last layer for example is clearly not scale invariant) so it is not that clear why the theory applies to that case, and the authors should probably discuss this. In the learning rate decay setup, the experiments don t include the equilibrium value for the weights and it would be good to include them, and it does not seem like the equilibrium analysis is valid there as the authors discuss.
Reject. rating score: 3. rating score: 5. rating score: 6. rating score: 6. <BRK>This paper presents an argument that model capacity differences are not necessarily the root reason for the performance gap between the student and the teacher, and the distillation data matters when the student capacity is greater than a threshold. Based on this, the authors develop KD+ to reduce the performance gap between them and enable students to match or outperform their teachers. In addition, this paper designs experiments to confirm the proposed arguments. However, this paper should be rejected because:(1)the results of Table 1 cannot confirm the authors’ argument that the widely used students are CSTs. For example, the paper states“This suggests that these students have well captured the knowledge on sparse training data points but have not well captured the local shapes of the teachers within the data distribution.” in Definition 4.2.<BRK>This paper investigates why conventional student network is not outperformed to teacher network. Based on several experiments design, this paper argues this is due to the student only fit the local, in distribution shapes of the network rather than model compacity. Besides, this paper design a go beyond in distribution distillation approach to overcome this issue. The motivation and the organization of the paper are easy to follow. 2）The second concern is about the novelty of KD+, the selection of Out of distribution sample is similar to the technique applied in the current state of art semi supervised learning methods: 1) mixmatch[1] 2) remixmatch[2]. 4) The last one is the student can outperform to teacher has been studied some literature? I remember some of the paper point out this.<BRK>The paper studies knowledge distillation. In particular, it tries to disentangle the effect of student model capacity and distillation dataset on the performance of the student. The study is very interesting in how it tries to characterize the impact of  student capacity and distillation data on the performance of  the distilled student. There are some important questions though that arise:  Is the effect of improved student performance coming from having out of distribution data or simply more data for distillation? Similar observations were made in self training where there has been a lot of work on leveraging how hard an example is, or how  confident a model is to select samples for self training? The gains seem to vary given the distillation algorithm, with bigger gain when using basic KD. Could other distillation algorithms that try to align the representations of the student and teacher by using different criteria be accomplishing similar generalization effect to the proposed approach<BRK>This paper explores why the large gap existing from the viewpoint of data distribution rather than student model capacity. The exploratory experiments on out of distribution data is interesting, which is meaningful to discuss why the gap between the student and teacher model exists from a new viewpoint of data distribution rather than the student capacity. This paper fails to compare their results with [3] or discuss the difference by using a GAN style to learn the output distribution of the teacher. The paper fails to discuss training time changes on more training data. 4.In Table 2, the simulation results of students (CSTs) are better than the teacher models. I want to know what if use the teacher model to teach itself on the same simulation experiments, as the work in Born Again Network [1] or Tf_self [2].
Reject. rating score: 3. rating score: 5. rating score: 6. rating score: 6. <BRK>Summary:The paper proposes a parameterized learnable aggregation function (LAF) that can aggregate a multi set of numbers (i.e.map them to a single real valued number). This is different to prior works such as Deep Sets that use fixed aggregation functions such as max, mean, etc. The presented idea is rather simple, which I think is a strong point of the paper. I also like the evaluation presented in Figure 4. Weak Points:  The setup of the experiments with scalars is unclear. And how are they composed into a single architecture? However, a Deep Set should have one function mapping the input to an intermediate representation, a sum aggregation, and a function that maps the aggregation to the output. I don t see why the model needs three max, sum, and mean units. It remains unclear to me why it can make sense to map scalars with a 3 layer network. Also, the architecture of the output mapping is not described. While the problem investigated in the paper is permutation invariant, several methods have been proposed to approximate permutation invariant problems with recurrent architectures such as LSTMs and GRUs. However, no comparison to these kinds of methods is performed. A comparison would be interesting since they also learn an aggregation function. An analysis of this question is missing.<BRK>The paper proposes a rational approximation approach for learning aggregation functions. In general the paper is well written and technically sound, although there are some questions that could help the readers better understand the paper. One of the problems with learnable rational approximations is the potential of finding a pole, e.g., x/0, you do not mention how you avoid/use? Could you mention something about this? Regarding the use of the sigmoid to transform the values of x to the range [0,1], it is not clear to me, how you can recover the mean with such low error, i.e., how can you achieve $\mu(x) ~   (\sum sigmoid(x)^b)^a / N$. But with the sigmoid transformation is not as straightforward to see, although you show very good results in Fig.2.Furthermore the sigmoid transformation destroys the linearity needed when the values of x are larger. Could you also give some intuition on why using the sigmoid is superior compared to a minmax scaling process? You mention that you use 9 LAF(x) aggregation functions in the experiments, could you explain more what do you mean exactly? are they independent of each other and train for the different target functions? In general, I find the paper interesting and the results promising, just a bit more explanation could help the reader understand the benefits and intuition behind the decisions.<BRK>The idea proposed by the authors is novel and elegant. For this reason the authors propose a new parametric family of aggregation functions, called LAF (for learning aggregation functions). It can be seen as a smooth version of the class of functions that are shown in DeepSets. LAF aggregator could learn all standard aggregation functions. Moreover experiments shows that the proposed model is superior to the other models with whom it has been compared. My major concern is about the clarity of the paper. Hopefully the authors can address my concern in the rebuttal period. In the study the goal is to learn a different types of target aggregation. 3.The authors provide an extensive set of experiments on a wide range of datasets, including point clouds, set expansion and graph properties. On most of the given tasks LAF is superior to other methods. The authors should consider rewriting a section 2 (with a description of the aggregation), considering the points I list below. However hey do not show how to achieve this condition. Whether they use exponent, non linearity or do it in another way. 3.In the definition of LAF aggregator, the authors states that x should be a real number, however looking at the experiments, it seems to me that it could also be applied to vectors. 4.The more detailed description of builded networks should be included (even in the appendix). Why section 5 (Multi task graph properties) is not the sub section of section 4 (Experiments)?<BRK>This paper addresses the problem of finding appropriate aggregation functions that can be used for instance in deep neural network architectures. The authors investigate the possibility to learn aggregation functions from data. They however restrict these functions to lists of reals that are in the unit interval. The theoretical part of the paper is short. The limited amount of theory is compensated by extensive numerical experiments. They also show that their model can be plugged to more conventional neural net layers and is backprop friendly. The main issue of this paper is, in my opinion, that the impact on the ML community will be limited if their LAF aggregation model does not become a gold standard like convolutional layers have become. To achieve such goal, one misses an application in which the LAF allows to bring disruptive results by leveraging features that cannot be derived through more usual architectures. In conjunction with this remark, the practical usability of LAF would be much wider if a generalization to sets of vectors would be proposed which could learn other features such as covariance. I think these should be limiting cases too. It seems that have 9 dimensional outputs. Sec 4.2 : Do you use convolutional layers before the aggregation layer ?
Accept (Poster). rating score: 7. rating score: 7. rating score: 6. <BRK>Convergence results to a fixed point are proven for the new rule, and it is shown that for the case of random patterns, the Hopfield network can memorize exponentially many patterns (with high probability). Finally several implementations are given showing how incorporating the new Hopfield net in classification tasks can improve classification accuracy in regimes where data is scarce and where neural networks do not fare well. One of the biggest advantages of HN was its simplicity and elegance. The authors might consider breaking their long paper to two different sections, one presenting the theoretical advantages of their new model and the other focusing on practical benefits. Some recent work about the complexity of finding fixed points of continuous functions may be relevant here:A converse to Banach s fixed point theorem and its CLS completeness. I would recommend outlining the contributions of this paper earlier on.<BRK>This work extends the binary Hopfield network (Demircigil et al., 2017) to continuous patterns and states. However, there is just not enough detail in this paper for me to understand how the models are implemented or why the model works better than other approaches. The experiment section compares performances with existing models, but lacks any analysis of why the proposed models work better. ## After author feedback ##Thanks for the paper update, and now I have a better understanding of the proposed approach. "Transformer and BERT models can be implemented by the layer Hopfield."<BRK>The paper introduces a new Hopfield network which have continuous states and propose update rules for optimizing it. It also draws connections between the new model and attention mechanism used in transformers. Overall I like the technical contribution of the work but feel the paper could be revised to improve clarity about the optimization in the new proposed variant of hopfield networks. Can the authors clarify this? Maybe present different scenarios to make it clear. Empirical study is limited in my opinion and can be improved.
Reject. rating score: 5. rating score: 5. rating score: 5. rating score: 6. <BRK>This paper contributes by a new multi task RL algorithm that addresses the negative transfer issue arising when one tries to apply multi task learning strategies across partially unrelated tasks. Enhancing the practical relevance of the proposed algorithm via an extensive empirical study (involving multiple state of the art approaches and challenging real world problems) could help. I think the experiments can be improved by including comparisons with more, state of the art algorithms including these discussed in the last two paragraphs of Section 2. ATARI experiments (Figure 6) seem to suggest that when the individual tasks are not related, (hence multi task learning is not beneficial), applying the proposed algorithm can degrade the performance from the PPT, still suffering from the negative transfer. Thank the authors for their responses. I read through the responses from the authors and comments from the other reviewers.<BRK>This paper proposes a multi task RL algorithm that leverages unsupervised task clustering. The authors propose to initialize a number of policies, cluster each task based on its performance on different policies, and train each policy with data coming from tasks within the cluster. Moreover, the empirical results seem to suggest that clustering can indeed recover the natural clustering or help performance when natural clustering is not obvious to humans, which could make the method valuable. Moreover, the idea of learning separate policies seems a bit contradictory to the goal of the MTRL, which is learning a single policy that can tackle all tasks. It would much more appealing if the authors can somehow distill the multiple policies into a single policy that can tackle all tasks, as in Distral [3]. I think it would be fairer if the total number of parameters is the same for all three methods.<BRK>The authors propose to approach multi task RL problems in which tasks may differ considerably in transition functions/dynamics and reward functions as well as in the action space through task clustering. All policies are evaluated on a single task and the relative cumulative discounted rewards over some iterations determines the assignment of tasks to policies. Simulations show the advantage in terms of number of training iterations on several tasks compared to a selection to other related algorithms. As the assignments to clusters are not soft/probabilistic, this seems almost more inspired by k means than by EM. The algorithm is a straightforward extension of previous approaches. It would be desirable to at least comment on how to select this parameter. The authors may wish to comment on the complexity of their algorithm. This is to be expected by design of the considered learning problems. The authors also provide learning results in which the advantage is not given, as in the ALE tasks. Overall, this is a quite informal presentation of an algorithm that is closely related to several previously published algorithms. The empirical evaluations are promising for specific multitask settings.<BRK>The clustering is achieved in an unsupervised way inspired by the EM algorithm, by iteratively evaluating the set of policies on all tasks, assigning tasks to policies based on their current task performance, and then training policies on their assigned tasks. It would be informative to evaluate the sensitivity of the EM algorithm to different numbers of initial policies. The proposed approach is evaluated on several continuous control and Atari multi task RL problems, and compared to single policy and per task policy baselines, as well as with a multi head baseline inspired by an approach from the recent literature. ### Review:The paper proposes a simple and novel approach to task selection and policy learning in the multi task learning problem setting. For Atari experiments it is explicitly mentioned that n 4 and that the same is done for Bipedal tasks. This explanation is missing. 4) It would be useful to provide some discussion about the stability of this approach, as in the beginning the policies could have tasks from different clusters assigned to them, but also the clusters change. 5) The results on Atari for EM are not outperforming other baselines.
Reject. rating score: 3. rating score: 5. rating score: 5. rating score: 6. <BRK>This paper focuses on the problem of augmenting the user provided knowledge into Neural networks. The core premise of the work is that the existing approaches tend to replace the True and False with values in [0,1] and then use maximum/minimum operators which have vanishing gradients. The key idea is to use logit representation and therefore, the authors hope to avoid the vanishing gradients problem. I think its an interesting paper but somehow the comparison with prior work is not properly contrasted, which makes me believe that the paper has probably reinvented quite a bit of aspects.<BRK>After rebuttal: The long discussion with the authors to clarify the questions in my review and further related questions actually shows that the paper is not really clear and would still benefit from a substantially improved presentation. So, I slightly downgraded my overall rating of the paper.<BRK>Other comments:* What is the state of the art for the VRD task? * The way references are used in the text should be made consistent. This resembles ideas coming from the neuro symbolic community, such as the pioneering work by Towel and Shavlik on KBANNs, and more recent approaches such as Neural Tensor Networks, DeepProblog, Lyrics. Although the work is interesting, and perfectly in scope with the conference, focusing on a very hot topic in AI, I have to raise some concerns regarding the experimental evaluation, which basically does not compare against similar approaches, nor it considers similar tasks in the literature.<BRK>In summary, this seems like a nice paper but a little fuzzy on the details of the experiments which can be clarified better. Neuro symbolic models seem very promising and therefore the paper addresses a very significant problem. The approach seems similar to Rockstachel et al.2017’s approach for theorem proving in first order logic.
Reject. rating score: 4. rating score: 4. rating score: 5. rating score: 6. <BRK>This paper proposes an improved sample wise randomized smoothing technique, where the noise level is tuned for different samples, for certification of robustness. Further, it also proposes a pretrain to finetune methodology for training networks which are then certified via sample wise randomized smoothing. However, the paper does have shortcomings in its clarity and organization. Explaining it clearly, and showing an ablation study that compares using region allocation and not using region allocation would provide good motivation for its use. I noticed that the authors usually chose noise from 0.12 up to the amount that they compare to with SmoothAdv, but the reasons for this are not discussed. Overall, I feel that the paper has a well motivated idea (sample wise randomized smoothing) and shows some minor improvements in terms of results, but that clarity for all other parts of the paper must be improved significantly. Additionally, baselines such as "allocating a region for every single test point" should be compared to in a clear way (as opposed to being in the appendix), as such baselines seem natural to compare to.<BRK>This paper suggests an extension of randomized smoothing, wherein the degree of smoothing is optimized both at training and test time on each individual sample. At training time, the model is first "pre trained" using a range of smoothing parameters (variance of the Gaussian perturbations), and then "fine tuned" by selecting the variance on each sample which maximizes the verified radius. (as opposed to an adversarially perturbed image). If not, could you explain how the adversarial image is selected here? Minor points:  It would be interesting to see an ablation of whether the fine tuning phase helps. The pseudocode is very helpful. I hope some of this feedback will be useful to the authors. One thing regarding point 1 in particular: the transductive setting seems contrived for adversarial robustness as it does not seem to correspond to a plausible threat model.<BRK>The paper propose a method to improve the randomized smoothing algorithm for certified robustness against adversarial attacks. The idea is that, instead of adding the same Gaussian noise to every data points, it uses a different standard deviation for each data points. Pros:  Certified robustness is an important problem in adversarial ML, and randomized smoothing is one of most promising methods. The proposed method is intuitive and seems to be a practical way to improve the original randomized smoothing algorithm  Experiments show that, the certified accuracy on CIFAR 10 really increasesCons:  It seems to me that the proposed method is a relatively straightforward extension from the original randomized smoothing algorithm, so the technical contribution is limited.<BRK>This paper considers the problem of provably defense to adversarial perturbations using randomized smoothing. The authors propose sample wise randomized smoothing   assigning different noise levels to different samples. They also propose to first pretrain a model and then adjust the noise for higher performance based on the model’s outputs. 3)	Issues with assigning arbitrary noise level to test points is well described/thought and solutions (online and batchwise) are proposed to make it compatible. Cons:1)	My main concern with this work is that it is not clear to me that these ACR gains are being achieved at what cost? 3)	Similar to adv smooth, it will be useful to see how much gain can be achieved with: 1) pre training, and 2) semi supervised learning. 4)	Results in Sec 5.2 is for online or batch setting?
Accept (Poster). rating score: 7. rating score: 6. rating score: 5. rating score: 4. <BRK>Abstract: The paper proposed a way to learn unbiased (debiased) concept based explainable models in the presence of unobserved confounders by the use of labels as instrumental variables. The proposed algorithm has 3 main steps: (1) regresses concept labels from the final labels (2) replace the original concepts with the learnt concepts and learn a model of debiased concepts as a function of features (3) predict label as a function of debiased concepts. Pros:   Clarity: The paper is quite well written and the explanations are clear. Significance/Impact: The ability to explain modeling prediction is quite important and furthermore it can lead to better generalization and robustness  Experimental design: I really liked the experimental section, very clear and well doneCons:  The causal graph assumption   Motivation: I would have spent a bit more time on explaining the motivation of the work and the scope that it adresses. For example, the causal graph assumption that makes labels to be the causal root of the graph, works well in image classification, but not in learning to control applications, such as recommender systems where the label is the reward and is the final consequence of the causal chain.<BRK>The approach proposes a causal graph, and then notes based on the proposed graph that sample labels satisfy conditions required of an instrumental variable. Primarily, I don t think the model labels can be used as instruments in such setting, but perhaps the framework is still appropriate? If this is the case, then that is a substantial positive for the approach. The ROAR approach used in this work is not an appropriate form of evaluation. al.paper and I am generally familiar with instrumental variables. A more damning issue is that the paper does not explicitly set out the question/problem that it seeks to address. I have worked with the CUB birds dataset before, so I am familiar with the concepts and data. I document some of the other questions I had in the latter parts of this review. Overall, it is important to obtain explanations from models that are not confounded with other variables. Take fig.2a., the causal graph says that the labels cause the concepts, which then cause the features. However, could this also be the other way?<BRK>The authors consider the issue of concepts being correlated with confounding information in the features. They propose a causal graph for representing the system and use instrumental variable methods to remove the impact of unobserved confounders. The proposed method is evaluated on synthetic and real data. I have the following comments:(1) The authors provide a graphical modelings for the setup of the problem. This includes the conditional independencies above as well as the way it is assumed that variable c is generated. (2) The model in this work is different from an IV model: One of the main requirements of the IV framework is exclusion restriction which requires that the effect of the instrument variable on the outcome should be only through the treatment variable. In the proposed model, variable y is also directly connected to variable x. Therefore, the model in this work does not represent the IV model, and y is not a valid IV, although it seems that that was actually not used in the approach. (3) It is not quite clear what exactly the variable d represents compared to variable c, and what is its interpretation. It is important to see the performance for larger values for the variance of the noises.<BRK>Summary: The paper studies the causal nature of concept explanations. Specifically, the authors treat labels as instrumental variables to then debias explanations and improve predictive performance as well. Strengths  Figure 2 was helpful for understanding the contributions of this work. However, it is not clear if \hat_d captures the same concepts that c alone would. If you could motivate using \hat_d with a pictorial example of where c, the concepts alone, fail that would be helpful. Weaknesses  While both experiments (synthetic and BIRDs) show the method s utility, it would have been nice to see experiments on other datasets. OAI perhaps like in Koh et al.The connection to Yeh et al.is not clear to me. What is the notion of completeness in the proposed method? It seems as though using \hat_d for the concepts simply recovers the independent concept case from Koh et al.
Accept (Poster). rating score: 7. rating score: 7. rating score: 6. rating score: 4. <BRK>##########################################################################Summary: The paper proposes a new self supervised technique, Bootstrap Your Own Robust Latents (BYORL), based on an existing technique, BYOL. As the % of labeled training data increases, there is not a significant improvement in robust test accuracy for CIFAR 10, that increases for CIFAR 100. 3.While robustness evaluations are good, the transfer to unseen and without adversarial training are much more encouraging. BYORL proposes to provide adversarially robust representations for low label regimes.<BRK>This paper introduces a new algorithm for learning adversarially robust models in the semi supervised setting, where a small amount of labeled data is available together with a sizeable unlabeled dataset. The proposed approach BYORL adapts an existing self supervised learning method BYOL by introducing a new adversarial augmentation technique based on maximizing the cosine similarity between representations. Finally, robust representations are shown to be more important than learning a robust linear classifier on top. Overall, the robust accuracy improvements achieved in the low label regime by BYORL are significant and the method is somewhat novel.<BRK>In this paper, adversarial self supervised learning is proposed to render robust data representations for down stream fine tuning tasks. The core idea is to integrate BYOL with adversarial training. The paper is well written in general. Thus, it is important to provide additional explanations and comparisons for the achieved results.<BRK>This paper proposes some modifications to BYOL (Bootstrap Your Own Latents) in an attempt to address adversarial robustness in low label high data regimes. Although the author s claim their approach to be novel, as is, the main contribution of the paper is adding adversarial training to BYOL, which already does not require labels. The authors also claim that their approach is better than pseudo labeling training for downstream tasks. The authors also mention that their approach reaches optimal performance with 5% of labels (and seems to deteriorate with more labelled data). There is also a lack of comparison against contemporary SOTA self supervised approaches.
Reject. rating score: 5. rating score: 6. rating score: 6. <BRK>This paper claims two contributions: 1. Proposes a connection between beta VAE with fixed variance Gaussian decoder and VAE with variable variance Gaussian decoder 2. Pro: The connection between beta VAE and variable variance Gaussian decoder is somewhat interesting. The proposal that the variance can be adjusted for each individual image seems novel, but currently there does not seem to be a good theoretical/empirical justification for this. The writing is quite good. Easy to follow and clear. Con: I think the main claimed contribution has limited novelty. The main difference between the current work and the prior common practice is to learn a single variance shared by all the pixels (instead of learning a variance per pixel). This by itself is fine, the lack of novelty can be made up by very convincing experiments and good practical guidance. My major concern is that the paper does not sufficiently justify its choice with experiments:The baselines are surprisingly bad. For example, the paper says it uses the model in Maaloe et al, 2019 for celebA; comparing the samples in the current paper and the original paper, there is a very big difference. In addition, the FID score of baselines also seem to be much worse than typically reported in the literature. Following up on the above point, typically some kind of annealing of the beta parameter is the standard practice to avoid converging to a poor local minimum (which seems to be what is happening to the baselines here). It would be nice to have some discussion / comparison. In fact, I think it is completely fine if the argument is to show that reasoning effort to choose an annealing schedule do not lead to optimal results. However, I think this point does not come out from the experiments. The criticized alternative (learning a variance per pixel) has been widely used in the literature and the samples look quite reasonable (e.g.see http://ruishu.io/2018/03/14/vae/) unlike the baselines in the paper. Granted there are a few implementation tricks (such as bounding the output to avoid numerical instability), but these are quite mild. It is difficult to conclude that the proposed approach uses less practical implementation techniques to achieve these results. For example, when people say a probability forecaster is calibrated, there is a specific property the forecaster must satisfy. The title is not very informative. In particular, with the current interpretation of "calibration" which is to "learn better probabilities", I think any VAE paper could have used that title. Thank you for the detailed reply and revisions.<BRK>This paper discusses a well known problem of VAE training that decoder produces blurry reconstruction with constant variance. This also could be easily connected to the well known $\beta$ VAE works. The experiment results in Tables 2 and 3 show the proposed model obtains a better FID score than the existing works on multiple datasets. In general, I am not surprised that shared variance would work well in practice as it is somewhat obvious as lots of works with beta VAE to get better performance. However, it is surprising that no one aligns beta VAE with the variance of observation before. The most valuable knowledge I learned from this paper is written in one small section about variance implementation details where a new variable $lambda$ is introduced to avoid numeric problems. It is more helpful than the main claim of the paper somewhat in practice. Pros 1.The paper is well written, and the authors provide sufficient evidence to show the proposed model works well in practice. 2.Mutual information analysis is great for reading. Cons 1.The proposed approach is too simple to validate its novelty since it seems a rewritten of the existing formula by assigning the beta hyper parameter with another meaning. 2.Figure 8 is hard to believe as Gaussian VAE on CIFAR 10 should perform much better than it is shown here. Again, it is quite surprising that the method is not introduced before the year 2020. I have an impression where some of the VAE implementations did include things like proposed in this paper (but fail to find them now).<BRK>The authors follow up on other papers in the literature and try to answer the following research question:  If a Gaussian decoder is used, is it better to fix the variance (e.g., \sigma 1), or learn it? In general, the paper is fine, however, it lacks novelty and it does not highlight that the VAE framework is a probabilistic framework, and a Gaussian decoder is not appropriate for modeling images. **Strengths:**S1: An in depth analysis of using a shared variance across dimensions in Gaussian decoders. S2: Comparing both NLL and FID is a good indication of showing importance of learning variance (or, more generally speaking, modeling uncertainty) in decoders in VAEs. As a result, using Gaussian decoders is inappropriate. It is possible to use dequantization as it is typically done in flow based models, see:  Theis, L., van den Oord, A., and Bethge, M. A note on the evaluation of generative models. arXiv preprint, 2019  Hoogeboom, E., Cohen, T. S., and Tomczak, J. M. Learning Discrete Distributions by Dequantization. arXiv preprint, 2020However, this is not the case in this paper. VAEs have a strong probabilistic foundation, and it should be treated as a probabilistic model. I would highly appreciate if the authors would make it very clear in the paper, and start with a remark about choosing appropriate distribution for observed data. Afterwards, it could be explained that we need some sort of dequantization to utilize Gaussian decoders. R2: The authors indicate a connection between a VAE with a Gaussian decoder and a \beta VAE framework. The connection is very clear from the optimization perspective, but it is not the case from the modeling standpoint. If we take a look at the objective and consider the optimization process, then indeed, there is no difference in potential optima, because multiplying by \sigma or \beta results in the same objective. However, this has different consequences from the modeling perspective. However, the authors propose a valid VAE, i.e., the objective is a valid lower bound to the log likelihood function. I find it very confusing to indicate the connection without being very precise in what sense these two approaches are related to each other. R3: As indicated by the authors, a similar approach was already discussed in the literature. However, I must admit, not in depth as here. R4: It would be beneficial to discuss the following paper:  Ghosh, P., Sajjadi, M. S., Vergari, A., Black, M., & Schölkopf, B. From variational to deterministic autoencoders.
Reject. rating score: 4. rating score: 4. rating score: 4. rating score: 5. <BRK>The idea is to use the compressed DNN when possible and to fall back on the original DNN when the confidence in the prediction is low. This is an interesting idea and area, as the authors outline there is considerable scope to improve throughput/latency (or save power) with such an approach. The authors report they can reduce latency by nearly 2x at a low cost. Experiments are performed using an FPGA. Interesting work has also been undertaken to provide improved confidence metrics for such approaches. It would be useful to indicate the size of each network too (rather than just DSP/LUT counts), again to help comparisons to previous work. I still believe novelty is limited and will leave my score unchanged.<BRK>The approach of the authors consists of estimating the uncertainty of the highly compressed network by thresholding the scores. 4.Overall, the paper does not explain the specialized hardware used to implement the proposed approach. This makes the paper hard to understand for readers that are not familiar with FPGAs. In my opinion the paper needs a rewrite before publication. Perhaps, the interpretabilty of this plots could be improved by adding color and a colorbar? After reading the updated version I still would recommend to reject the paper. After reading the authors  response, I still think that the novelty of this paper is very limited.<BRK>Many terms are not well explained in the paper, such as FPGA, IPs (page 4), LUT(table 1). Figure 2 seems to be confusing. Why are there two inputs to the original network if the input of the original network is determined by the compressed network? In addition, I recommend making the makers bigger. Overall, the presentation of the paper needs to be improved and further clarifications are needed for some parts.<BRK>cons 	  Some of the points that may be questioned are not explained. The core idea of this method is to take the output confidence of the compressed network as the criterion. Otherwise they are likely to be unreliable. It used to be a discrete choice, but now it s continuously adjustable. In addition, such a network also makes the quantization compression technology more simple and of more application value.
Reject. rating score: 3. rating score: 5. rating score: 6. rating score: 7. rating score: 8. <BRK>This is all trivial and well known, yet the authors do precisely that. Can it be saved by proposing gradients also of w.r.t.constraints? No.These results are (slightly) less trivial but   as authors admit   are known since 1975. Moreover, the gradient with respect to $c$ is the only one used in experiments, as far as I understand. The claims of better performance compared to cvxpy are also absolutely non surprising   cvxpy currently uses a slightly suboptimal   and a very expensive   solver for linear programs.<BRK>Your method is promising, and practitioners that do not have a background in combinatorial optimization may want to use it. significance The paper s proposed method is practical for very common ML setups in NLP and computer vision that are used day to day. Experiments build the method on top of out dated models and do not demonstrate that the method could be used with modern models (e.g.attention based decoders).<BRK>This paper shows how to differentiate through combinatoriallosses by differentiating through the ideal formulation LP. The experimental settings considered in this paper compare tobaselines that *don t* differentiate through the combinatorialaspect of the problem. While this is a great step of validatingthe power of these approaches, I think that it would be significantlymore convincing to empirically compare to approaches thatdifferentiate through the combinatorial losses. I also think it s important to discuss the comparisons to therelated approaches for differentiating through parameterizedcombinatorial optimization. # Other questions and commentsHow should the gradients of the continuous baselines (with CVXPY)compare to the method being proposed in the experiments?<BRK>The authors propose two example applications, that are described precisely, and validated against generic LP solving approaches. Review The paper is well written and well organized. The theoretical aspects are well documented, and the examples are introduced precisely and pedagogically. On the other hand, the novelty of the method may be a little overstated. In this example, we only require the gradient with respect to the cost (P is"primal pdfiff efficient"). Arguably, this manuscript also enable us tobackpropagate through parametrized constraints, using the formula proposed inTheorem 1, which is little known in this community.<BRK>The authors present a technique to integrate combinatorial optimization sub problems into a gradient descent based application. The approach they describe relies only on differentiation of the value of the combinatorial program (instead of the solution vector), and can be done with relatively low overhead (compared to techniques that involve modifying combinatorial algorithms to differentiable elements, or the use of differentiable linear/quadratic programming layers)They motivate and show the advantages of their approach using two natural and useful examples. The experimental results show promise, and the paper is well written, and motivated.
Accept (Poster). rating score: 8. rating score: 7. rating score: 7. rating score: 6. <BRK>I think this a very good contribution to ICLR given the topic and the quality of the submission (originality, contribution to the stare of the art, experimental evidence, etc)  Some of the strong points of the submission are summarized as follows:1. The paper tackles a very interesting subject, questioning the conventional wisdom the CE is superior to MSE loss in a wide range of machine learning problems. 4.I have read people making similar claims in other forums and articles, but the authors here provide a very thorough and careful experimental design, which helps to validate their hypothesis. However, it would be interesting to see how these experiments generalize to problems (in particular in computer vision) where datasets are noisier or where the image quality/resolution are lower. 5.The experimental design is good, showing a careful analysis to validate the proposal and several ablation studies to confirm that the hypothesis holds for various machine learning domains, as well as several datasets.<BRK>This paper questions the omnipresence of cross entropy loss for classification tasks while showing that square loss also yields competitive results. The authors find that the model performance is more stable w.r.t.the random initialization of parameters when trained using the square loss. For NLP and ASR datasets, the authors find that the square loss yields slightly better results. The paper does not make an effort to provide well grounded explanations for the experimental observations. An explanation in the paper would be helpful.<BRK>Summary: This paper compares the more popular cross entropy loss function to the squared loss function for classification tasks. The authors look at NLP, ASR and and CV tasks, keeping the architecture the same (as much as possible) and varying the loss functions. A minor comment is that the average accuracies are reported but not standard deviation to get a sense of the variation across performance for both loss functions. Statistical significance calculations would also be helpful to interpret results. I recommend accepting this work.<BRK>The paper shows that squared loss is better or competitive with cross entropy loss in most cases. Most of the experiments and comparisons seem to be well done; parameters, setups etc. are well explained. I believe a few additional tasks and settings would have helped put a better picture of the comparison. Also, it is perhaps worth showing results for squared loss with softmax outputs.
Reject. rating score: 4. rating score: 4. rating score: 5. rating score: 6. <BRK>This paper proposes to study auto induced distribution shift (ADS), a phenomenon where the machine learning model itself may change its own data distribution via its decisions or actions. The paper seeks to understand, in situations where ADS is undesirable, whether learning algorithms are privy to, and pursue, incentives for learning models which take advantage of ADS. Pros:+ The paper proposes an interesting and promising direction for study. + Simple examples serve to illustrate the concepts outlined in the paper. Weighing these pros and cons, I am inclined to reject this paper. It is not difficult to imagine that ADS may indeed be a problem that must be dealt with in certain applications, but it is also unsatisfying to leave this to the imagination. The content recommendation experiment seems promising and could perhaps be built out into a larger scale experiment. In general, the context swapping approach seems rather ad hoc, and might there be potential downsides regarding tradeoffs in model performance? Further investigation into this or other mitigation strategies would also significantly strengthen the work. Clarity The paper is generally well organized and well written. I appreciate the hierarchy of definitions as well as the discussion of how certain frameworks such as meta learning can exacerbate the problem being studied. Further discussion into some of the prior works mentioned in the related work section would help in this regard. Might it even be possible to design (or borrow) an experiment based on this prior work? In my opinion, the current exposition is not enough to convince the reader that the problem is important. The running content recommendation example seems hypothetical, and a real world system would likely, perhaps even obviously, disincentivize such behavior from a model that drives users away. This indeed may be a hand designed solution, which as the authors correctly point out may not scale to cover all instances of this problem, but it is up to the authors to argue this more convincingly.<BRK>This paper discusses a phenomenon where machine learned models may influence user behaviors in future iterations, creating self selection effects such as filter bubbles or propagation of fake news. The paper calls these effects auto induced distribution shift (ADS) and argues that a specific meta learning algorithm PBT manipulates users instead of maximizing rewards. Despite an interesting motivation, the paper is quite vague in the proposed methods. It can be corrected by any learners that focus on the detection of distribution shifts, e.g., via inverse propensity scoring, or under ignorability assumptions. On the other hand, PBT is just an algorithm with automated hyperparameter search. What can we learn by connecting ADS to PBT? 2.Why are reinforcement learning / supervised learning free from incentive / ADS issues? For the reasons above, I believe these claims are unrelated and wrong. If offline, how would the learner foresee the shifts in user behaviors after online feedback loop? If online and unobservable, what are the belief states? While Experiment 1 makes sense that by sacrificing an immediate reward, the user gets "manipulated" into a state of more predictability, I am not seeing how Experiment 2 makes practical sense. In this example, the environment is set as a modified prisoner s dilemma. The paper assumes "defect" to be a better solution, but in this specific case, it seems that cooperation seems to be the true user utility, despite the user gets "manipulated" by non myopic RL algorithms. The paper also lacks general clarity. I am not sure how content swapping works in Section 4.2. The details about Q learning should be reorganized. Overall, the authors should think about how to eliminate the unrelated claims and simplify the paper to include just one proposed method, which solves a wide class of interesting problems at the same time.<BRK>This paper introduces the concept of auto induced distributional shift (ADS), and argues that some meta learning and reinforcement learning algorithms have the incentives to change the distribution so that the problem is easier to solve. The paper presents unit tests to detect hidden incentives for auto induced distributional shift (HI ADS) and also proposes context swapping to reduce the distributional shift. Pros: 1) The concept of ADS is interesting. Distributional shift is not a new phenomenon especially for reinforcement learning. And if distributional shift is harmful then algorithm could use a better reward function to mitigate the effects. But this concept is indeed not formally defined and studied in previous works. I have the following concerns and questions:1) How to measure HI ADS is unclear. In the unit test, the incentivized behaviors are clearly defined for the two toy environments, but the paper does not discuss how to find / define incentivized behaviors for a general environment. 2) How to generalize the unit test? The unit test only tells whether an algorithm reveals ADS in the two toy environments. Even if a meta learning algorithm passes the unit test, it is not guaranteed that such algorithm won t exhibits ADS in other application scenarios. Can we get more from the unit test? In real world applications if we do not have a perfect understanding of the environment that what kind of behaviors are encouraged by incentives, then how to evaluate HI ADS? 3) It is not clear whether PBT algorithm itself or all meta learning algorithms would exhibit HI ADS. The authors use PBT as an example to illustrate HI ADS, but it is unclear whether this observation suggests other meta learning algorithms are also vulnerable to ADS or that we should test them individually using the unit tests. My rough understanding is after switching the environment, the learners are using misspecified models and won t fully exploit current estimation. Does context swapping help with content recommendation task? Is this a general approach for other environments instead of the toy unit test setting? 2) Appendix is missing, which is supposed to contain additional details on experiments according to the main content.<BRK>Abstract: The paper highlights an interesting problem of learning dynamics, where a learning system has the incentives to change it’s future input in order to increase its performance. This is not always problematic, but it can, at times, lead to perverse incentives for the system, such as under performing on users with complex profiles in the present, in order to remove them from the future testing sets. The authors propose a set of unit tests to detect this issue and a way to solve the problem for the PBS metalearning frameworks. The experiments on toy/simulated datasets show the relevance of the approach. Pros:   Clarity: The paper is quite well written and the explanations are clear. I think the problem that the paper aims to address is real and of practical value. Experimental design: The experimental section, is very clear and supports the claims of the paperCons:  Significance/Impact: I find the scope of the paper to be limited to Population Based Training, which as far as I can tell, its not a well established training method in real world applications, especially recommendation. I would like to see the expansion of the same ideas to general batch learning from bandit feedback which is as far as I can tell the most established setup for RW ML decision systems.
Reject. rating score: 3. rating score: 4. rating score: 4. <BRK>* The lack of experimentation is the second most important concern and also the reason for the current score. * Large parts of the introduction are overlapping or repeated in related work, it is not necessary to discuss relevant literature in the introduction, this part should be moved to related work completely.<BRK>The approach is well described and supported by experiments on simulated and real world data. eq.8 is not correct "the phenomenon is called the curse of dimensionality"  > maybe better: "the underlying phenomenon"  eq.(6): argmax should be formatted as \text{argmax).<BRK>The description of the method is at times a bit hard to follow. From the text it is not clear to me how this challenge is addressed by the proposed method.
Reject. rating score: 3. rating score: 4. rating score: 6. <BRK>This paper proposes a generalized additive model to learn joint intensity functions for multiple Poisson processes. However, its experimental result is weak and the model construction is of a question to me. After carefully reading the setup in section 2.2, I feel that the entire method is based on a crude approximation of Poisson processes. The main contribution is to consider the partial order of high order interactions when parameterizing the model.<BRK>The authors develop a method that (a) is theoretically principled and (b) scales to large datasets. In that case, the likelihood would be different from (1). However, in its current form, the paper is excessively unclear, to the point where I am not sure that I understand the precise problem the authors are trying to address (despite my familiarity with the area).<BRK>The paper under review proposes a new model for multi dimensional temporal Point processes, allowing efficient estimation of high order interactions. Note that correlation does not mean simultaneity of the events. Overall the contribution is original and promising, but in its current state suffers from imprecision making difficult to assess its quality. This is particularly unclear.
Reject. rating score: 5. rating score: 5. rating score: 6. rating score: 7. <BRK>### SummaryThis work proposes a model to generate scene layouts by treating the scene as a composition of primitives, such as instance class, coordinates or scales. The model is a Transformer architecture, that attends on all previously predicted or given instance primitives. + Providing results on different data domain datasets helps supporting the work. Unclear comparison with baselines: Given that some metrics/datasets were not used in the original papers, I have some concerns regarding fairness of comparisons; (1) were the methods re trained for this paper or taken as is from the code of original authors? ### Reason for scoreIt is a simple approach based on existing work, just slightly adapted for this layout generation task. are these parameters tuned for each dataset? If baselines are allowed to tune parameters for each specific dataset, do they provide consistently worse results than the proposed method? ### After authors responseAlthough authors have addressed one of my main concerns and some minor ones, I still have doubts regarding the fairness of comparison with the baselines (lack of proper hyper parameter optimization) and therefore cannot trust the results.<BRK>The model performs competitively with state of the art methods with respect to appropriate metrics. **Strengths**+ The paper is written clearly and the implementation details are appropriately described+ The idea of using transformers is interesting+ The evaluation of the method is thorough**Weaknesses**While the evaluation conducted by the authors is thorough, my main critique of this paper is that the results are not convincing enough to show the value of the proposed model. The quantitative results do not outperform the state of the art models consistently across all metric. Is the quality of the layout dependent on the number of entities in the scenes?<BRK>I do not share the concerns of R2 & R4 regarding the quality of results and fairness of comparison. I still would recommend adding the following evaluations:  more diverse initial state for Figure 10.  a more interpretable dataset for Figure 15: I think ShapeNet would demonstrate this point better. I like the idea of separating different attributes a lot   probably my favorite part of the paper. Following previous point: I think the novelty in this work is quite limited in general, using existing architectures and relying on ideas that have been explored (albeit in slightly different settings) before. The reason is two fold. First, it is unclear to me whether the proposed method is flexible enough to handle all the other tasks that those domain specific method could, as only generation results are shown here. The empirical performance is also quite decent.<BRK>The architecture is not dramatically new, but it is well justified and analyzed, and there are some interesting tweaks. The results are strongest in that they show good performance of essentially the same architecture and hyperpameters across quite different domains: to my knowledge such variety has not really been demonstrated for any of the assembly based generative models I m familiar with. How is the projection to d_model dimensions then performed, if d_model is 512 in one domain and 128 in another? Also, would it be possible to do an ablation study where no discretization is done at all? This is another paper that presents a fairly general framework for an autoregressive layout generator. (Various flavours of layout generation have seen a huge amount of research in the last few years, so I am not totally confident of my assessment especially vis a vis prior work.)
Reject. rating score: 4. rating score: 6. rating score: 6. <BRK>This work also designs a style encoding framework by introducing transformation before style encoder. The results show good fidelity performance. The concepts of class and content are borrowed from the LORD algorithm. This paper introduces an additional disentanglement dimension called style, which seems to be learned unsupervised. The learning method for this style is similar to learning general attributes that are robust to transformations. My major concern is also about this  style . If it is, please verify it experimentally. If the authors are able to address the above questions, then I am happy to raise my score.<BRK>However, the reliance on the nature of transformations applied is still an important factor. Nonetheless, I do see value in the approach presented in this paper. Weaknesses:   The main contributions of this work seems to be handling the style and content disentangling, and the quality of image generation. Consequently, the paper seems like a "natural" (or straightforward) extension of LORD, giving an impression of limited novelty. LORD was ostensibly non adversarial, but OverLORD is not. However, it is not clear what is the key factor for disentangling the style and content, both of which are image specific. image to the content of lion image is successfully transferred, despite both having the same class (wildlife). How were the results for competing techniques generated? Or were they retrained by the authors?<BRK>To tackle the problem of previous methods that the learned content and class are often entangled, the authors propose to disentangle image representations to class and attributes, and further disentangle attributes to common attributes among all classes (content), and class specific attributes (style). In this way, it is able to transfer the common attributes while preserving the class and class specific attributes. Experiments are conducted on animal faces and human faces datasets, and the proposed approach is able to preserve the class specific attributes (e.g., shape or identity of the face) better than previous image to image translation methods that only disentangle style and content. The disentanglement space in this paper gives clearer definitions and better control for image translation. (2) What s the shape of the content embedding? What if using a generator similar to the generator in FUNIT that uses AdaIN to combine the class and style information with the content information? This restricts the approach from generalizing to other various datasets with different types of class specific attributes. I partially agree with reviewer #3 and reviewer #5 that this work is an incremental extension of LORD. In addition, like I mentioned in the "Cons", the styles are decided by the pre defined transformations that are dataset specific. And the results of this paper seems good compared with previous approaches.
Accept (Poster). rating score: 7. rating score: 7. rating score: 7. rating score: 7. <BRK>This paper studies the asymptotic convergence properties of (population level) policy gradient methods with two layer neural networks, softmax parametrization, and entropic regularization, in the mean field regime. On the other hand, it appears to me that one major weakness of the result is that the theorem holds true only when the dynamics converges to a stationary point. The paper also presents results for finite time convergence of the training dynamics for neural networks to the mean field limit. It would be helpful if the authors could provide more discussions on this condition.<BRK>This paper provides a mean field characterization of entropy regularized policy gradient dynamics for wide single hidden layer neural networks. It would be nicer to spend more space to discuss the major differences of the theoretical analysis in this paper compared to earlier results on the mean field limit in the supervised learning setting. And the convergence properties of the dynamics are established. The evolution of neural network parameters is described by a transport partial differential equation.<BRK>This paper extends previous work in the parameter dynamics of simple neural networks to reinforcement learning framework in continuous state and action spaces with nonlinear function approximation and overcomes the challenge of lack of convexity. The main concern of mine would be that the theorems in the paper are not powerful enough to help us fully understand the experiments. 2.Under mild conditions, it demonstrates interesting convergence properties of the particle dynamics to the mean field counterpart and further, the mean field dynamics to the global optima. Hence there is lack of quantitative results under the setting of finite number of samples and finite gradient step size.<BRK>This paper studies the mean field limit of the policy gradient method (with entropy regularized) and proves that any stationary point under this setting is a global minimizer. The result seems promising and well complements several theory results in RL in the past year, e.g.the optimality of policy gradient under NTK regime and the TD algorithm in the mean field regime. i.e.mean field limit, seems standard, as the authors claim it is very similar to the case of supervised learning.
Reject. rating score: 3. rating score: 3. rating score: 4. <BRK>I think that the only problem is that the venue chosen by the authors does not fit its purpose. The paper presents an interesting study on the dynamics of the epidemic. In my opinion, the development of a tool is relevant for medical and decision making studies but is not enough novel or significant for the ML field. After rebuttal: the paper seems interesting but, as I already mentioned and as other reviewers pointed out, the main concerns about this paper are novelty and relevance to the ML community.<BRK>**Strengths**The paper addresses an important and timely topic. **Weaknesses**The main weakness of the paper is the strength of the contributions. The authors’ main contributions are a gym environment for a specific epidemiological model. Unfortunately, the novelty is somewhat lacking.<BRK>This paper introduces OpenAI Gym environment for RL optimization of epidemic containment policies. On the positive side, connecting the epidemiological and ML communities is definitely an important goal. I believe that there are technical issues in the development of a platform for health policy optimization which are likely to result in research contributions, for example dealing with uncertainty in models/parameters, developing more efficient methods for multiobjective optimization, or providing explanations of policies (particularly since experts are unlikely to implement a RL policy verbatim, but rather try to synthesize its recommendations with other considerations/sources of information).
Reject. rating score: 3. rating score: 4. rating score: 4. rating score: 6. <BRK>The setup is online in the sense of evaluation: they evaluate on the new sentences and then train over them (unlike image datasets), and catastrophic forgetting is hence characterised as having higher error than was in the past when there is a switch between the domains/languages. They compare a mixture of expert baselines with gating by different gating methods on this setup. There are few  sentences and terms that are hard to understand and to me they seem imprecise. Do you have any reference to support this hypothesis? Too much adaptivity does not ensure less forgetting. (1.5) Section 3, loss after switch: what do you mean by a switch? In practice the loss curve is not smooth. 2.Regarding experiments, is it not possible to design much simpler methods which work for this problem? Why isn t it possible to use a baseline which consists of experts for one domain/language where the character sequence decides which expert to use instead of these weaker gating based methods? 3.Why is it not possible to apply traditional continual methods like Experience replay to this setting  you simply store intelligently selected past sentences in memory (when say error shoots up) and replay using them. There are many other continual learning approaches that potentially could be applied here. Any particular reason for not using them? al., On the Relation between Linguistic Typology and (Limitations of) Multilingual Language Modeling (edited)<BRK>In particular, the dataset along with the proposed evaluation metrics captures the three stated objectives of the benchmark. It’s nice when potential questions are anticipated and answered, for example, “why weren’t continual learning SOTA models evaluated?” and “why weren’t transformers considered as baselines?” The authors answer these questions candidly. A component of this is showing where existing models fail (and why this dataset will help improve them). #############################################################################################I also have some questions that I hope the author can help address:1. Note that the answer to this question should also be empirically demonstrated. This benchmark could very well be a valuable contribution that fills a hole in the existing body of work, but the paper in its current form does not adequately establish this. The rebuttal should better address how this benchmark fits into existing work by comparing it to existing datasets and more relevant baselines.<BRK>Strengths:This paper proposes a new evaluation framework and gives two available evaluation datasetsWeakness：  the paper needs a major rewrite to improve fluency and to better state motivation and contribution  the empirical validation is weak. Reasons for accept:The advantages of this paper are: 1)	this paper proposed a new evaluation benchmark and dataset to promote the related research of online continual learning; 2)	the proposed plastic gate allows it to distribute different distributions among different experts, which has certain effects from the experimental results. The abstract mentioned that "it is hard to demarcate task boundaries in actual tasks", and then said that a new benchmark, new metrics, and gating technique are proposed. Therefore, please explain its advantages in detail (including the advantages of this evaluation framework compared with the evaluation framework of related literature, and verify it)4. The baseline uses LSTM and does not use CNN, Transformer, .etc, which shows that its generalization is limited. Is it possible to evaluate some of the latest online continual learning systems? This will have a good evaluation effect on measuring the versatility of your evaluation framework.<BRK>The key downside of the paper is that no standard continual learning baselines are trained on the proposed datasets; I would be inclined to increase my score if results were shown for 1 or 2 algorithms specifically designed for continual learning with neural networks (as discussed in more detail below). While a range of composition of experts baselines are used for evaluation, it would have been much better to also include other methods specifically designed for online continual learning, such as those cited in the paper [1, 2] or, though not strictly online, a replay based method such as CLEAR, which works in the task agnostic setting. It is claimed in the paper that including state of the art online continual learning methods would have involved “non trivial adaptations significantly departing from the original models, which would limit any possible conclusions we could draw” as they are designed for image based datasets. I don’t fully understand the basis of this claim; perhaps the authors could elaborate   as far as I am aware, for example, [1] is not restricted for use on image based datasets. •	Overall, not evaluating the datasets with any standard continual learning baselines is an important weakness.
Reject. rating score: 4. rating score: 4. rating score: 4. rating score: 5. rating score: 6. <BRK>Compared with the previous works of two layer neural networks, this paper considered deeper neural networks. However, I had several concerns about the conclusion and the techniques. b) This paper considers deeper neural networks instead of two layer neural networks, which is an extension of the existing works. 2.If the bound is tight, one needs wider neural networks in deeper neural networks. I do not think that is the reality. ### COMMENTSOverall, this is an interesting paper. I tend to reject this paper without a convincing response.<BRK>The major concern of this work is that the setting is far from practice. Since $n$, the number of training examples, and $l$, the number of layers of a network, are large most of the times in the field today, the theorem is not very practical and not applicable in real networks. Therefore, it is not very meaningful to find the global solutions, since the network is highly overparameterized and not every global solution generalizes the same. 3.Third, the result is far from practice in deep network training. In practice, people, in general, believe that there are spurious local minimizers in deep networks (see Itay et al.“Spurious local minima are common in two layer relu neural networks”, “Visualizing the Loss Landscape of Neural Nets”), but the data has structures and the deep network is enforcing implicit regularizations to avoid those bad local minimizers.<BRK>[Pros] This paper provides interesting results on the optimization landscape of deep learning. These results could provide a better understanding of the training problems in deep learning and why the neural networks can be successfully trained by local search algorithms like SGD. Indeed, I feel like the definition of *spurious local minima* used in this paper is similar to the so called *set wise strict local minimum* in the above mentioned reference [A]. [A] provides similar results but is missing. The authors should compare the results with [A]. So in this sense, the result that the training problem in this regime has a nice optimization landscape is not that surprising.<BRK>SummaryThis paper studies the optimization landscape of the training loss of deep neural networks. Let me start with the second one. There is a large body of literature that refers to non optimal local minima as spurious/bad local minima, and investigates existence/nonexistence of such local minima in the context of neural networks [1, 2, 3, 4, and many more]. In light of these existing results, I fear that the width requirement in this paper is too big, and it grows exponentially with the depth $l$. Although I liked the construction illustrated in this paper, this significant weakness of the main result makes me hesitate to recommend acceptance.<BRK>This greatly extends previous result and understanding. 2.The proof is heavily based on a new concept "path equivalence" and the algebraic manipulation. The paper is hard to read for the various calligraphic notations. This requirement is even larger than the NTK regime under which the gradient descent is guaranteed to find global minima. Thus it is doubtable of the theoretical/practical value of the work: use an even larger width and describe an optimization landscape that is not related with optimization methods. Despite of the above weakness, I will recommend its acceptance to foster diverse techniques.
Reject. rating score: 6. rating score: 7. rating score: 7. rating score: 7. <BRK>Namely, doing model selection based on English (as the typical source language) dev data often displays suboptimal transfer performance in a wide range of tasks and for a wide range of target languages. Is this a reasonable assumption? For instance, instead of running LTR, why not using development data of a language which is quite similar to the test language, and optimise performance on that  neighbouring/pivot  language? Given that the gains with the LTR approach are still quite small compared to selection based on EN dev, I wonder how this baseline approach would work. 3.Unfortunately, all the experiments are run only on languages which could be considered high resource languages (e.g., see the work of Joshi et al., ACL 2020 or Lauscher et al.. EMNLP 2020). However, this work in particular should pay particular attention to truly low resource scenarios, and not limit the evaluations only to a small number of high resource languages. However, the paper does not offer any experiments in those setups. The work is not strictly  meta learning .<BRK>Referring languages other than target language as pivot language seems misleading, as this paper does not assume any similarity between target language and “pivot language”. This paper conducts experiments on 5 NLP tasks and cross lingual transfer from English to 5 target languages, and shows outperforming model selection using the English dev set. given a product of seed and hyperparameter. However, it is not clearly stated. While training the scoring function might be fast (not discussed in the paper), this paper should discuss this assumption in more detail. 3.Get back to the original research problem, recall that cross lingual transfer has high variance. In appendix A.3., is a new scoring function trained for XLM R or is it the same scoring function of mBERT? If the former is true, more results on other tasks are beneficial as the results so far on XLM R are mixed.<BRK>The target languages are represented by using lang2vec as a reference point. Strong results in comparison to the English dev data baseline. While CoNLL NER data is well established, there is data for 250+ languages by Pan et al.for NER as well. I kindly suggest that the final version revision includes more languages and typological analysis in the experiments. How biased is the usage of English development data with respect to language (dis)similarity to target language in these tasks?<BRK>The empirical results show that the LMS technique is effective in predicting when a multilingual model’s representations are a good match for the target language, as opposed to a baseline in which the model selection is done using the performance on En dev data. This is important because it validates that the degree to which this meta learning approach works depends on the family of languages for which the task is attempted (the fit between the training data language family and the target language family). The paper is well written, with the experimental section well executed.
Reject. rating score: 3. rating score: 4. rating score: 5. <BRK>The main difference from DARTS is that rather than jointly selecting operator and topology, first the topology and then the operator is selected. It s roughly a form of coordinate descent. The paper is empirical in nature and claims good results. Cons:The presentation is far from top tier conference standard. Also figures 2 and 3 are unreadable on printout.<BRK>The experiments show that the searching time is reduced significantly compared to DARTS and the results on CIFAR 10 and ImageNet are very competitive. ** Table 3 is not clear, some codes are not described in its caption. The information of this comparison is all scattered in the manuscript.<BRK>2.This paper has described several interesting observations. It will be better if they are further explained and discussed. Does this imply that the proposed method is not effective enough to deal with images of normal size? After reading the response and the comments of peer reviewers, the rating is altered as follows.
Reject. rating score: 2. rating score: 3. rating score: 6. rating score: 7. <BRK>Why transformations like this have anything to do with mode colapse or training stability is not at all at the level of this conference. The only quantification for mode dropping is on mnist, a dataset that s widely accepted as useless in the current state of generative modelling. Furthermore, important baselines like wgangp are missing. Furthermore, the language and the overclaiming of the paper is very strong: "our approach is able to stabilize GAN s training and improve the quality and diversity of generated samples as well". It is simply unacceptable given the limited experiments (tiny datasets, small improvements, far from the current state of generative modelling). Honestly, I feel very dissapointed about how this paper presents itself.<BRK>Being a critical element of the proposed method, this should be discussed in more detail (rather than just focusing on related GAN work). There are dozens, if not hundreds, of GAN training papers which introduce small changes to the (now 6 year old) vanilla GAN or VAE GAN objective, and there are no ideas in this paper which I would consider espeically new or novel. The change to the vanilla training setup is effectively quite small, but introduces a 100% compute overhead relative to its baselines. My primary complaint with this paper is that for such a small change to be of interest to the community (especially given its tremendous compute cost and any added implementation complexity) there must be substantial evidence that doing so is beneficial, and the empirical results in this paper are quite weak.<BRK>In other words, the authors propose to stabilize GANs by intervening with the discriminator. Throughout the paper, the authors use a \textbf{block substitution} transformation as the intervening transformation. Furthermore, looking at Figure 7, the gain appears marginal. C. The authors introduce a high order extension of JS divergence, denoted Multi distribution JS divergence, and show that standard JS divergence is a special case for $k 2$ and $\pi_1 \pi_2 0.5$. The authors then use this quantity in Theorem 1 to relate the intervention loss to JS divergence. I think it s acceptable to use the high order JS divergence as intuition about the intervention loss, but problematic to use it as proof of anything.<BRK>The authors describe how this intervention loss is equivalent to a multi JSD. This was a rather intricate algorithm and I think it’s generally well explained. This paper demonstrates clear empirical improvements of the method on many benchmarks of interest. I therefore recommend acceptance. * Does this intervention loss have applicability outside of GANs? Does a mismatch with the latent distribution used to produce fake images impact performance? Notes:* Regarding the comment “Training GAN has been challenging, especially when the generated distribution and the data distribution are far away from each other. In practice, we never use a *perfect* discriminator.
Reject. rating score: 5. rating score: 6. rating score: 6. rating score: 6. rating score: 6. <BRK>The experimental results are extensive and compelling. Introducing a GP to model the correlation between configuration rung tuples in the existing ASHA work offers minimal technical merit though. This question does not seem to be adequately addressed in the paper. If so, can the authors support this claim? In ICML 2019. The authors say that "For each method we report mean and the standard error of the mean across multiple runs with different seeds of the random number generator." Exactly how many runs would that be? Minor issuesThe title, which includes neural architecture search, may not be appropriate, considering that the methodology in Section 3 is not about NAS. A more general title ought to be considered since the proposed methodology is applicable to general BO problems, as seen in the experiments section. Page 2. substantially behaviours?<BRK>After carefull discussion with AC and other reviewers, I would, however, have to decrease my score to 6 due the lack of significant technical novelty. Overview: This paper proposes to combine GP to SH/random sampling for asynchronous hyper parameter optimization, and it proposes to use scheduling schemes like in ASHA for example (promotion scheduling) to handle the idle workers. The benefits of the new algorithm is then proved by extensive experiments to compare the proposed algorithm MobSter against the state of the art algorithms. But I do think the paper is an useful work for the AutoML community. I think it would probably be better that the authors put a bit more effort on explaining the notion of rung and bracket (in Section 2). I m not sure if it is that clear for those who are not familiar with the litterature. Have the authors considered of comparing MobSter to other NAS specific algorithms? Section 4, paragraph 1: which is why omitted them here  > I would rather say  which is why they are omitted here  probably?<BRK>* On page 3, K is not defined. The paper presents a simple strategy to speed up hyperparameter optimization / NAS that gives good empirical results. 2.The paper is well written and motivated. 3.The empirical results are convincing and support the claims made in the paper. * Labels in figures are small and hard to read (especially figure 5). The paper does not bring additional insights. The reviewers clarified some of my misunderstandings. The method proposed in the paper looks more like engineering work. How does this affect the model? 4.In figure 4, the results don t show how ASHA changes as a function of the number of workers, but the main text describes it. It would be helpful to have a sensitivity study. Minor notes:* In figure 1 caption, x, y  > $x$, $y$. * On page 2, "they lead to substantially behaviours in practice"  > "they lead to substantially different behaviours in practice"?<BRK>However, can the    results in the paper be directly compared to results of other    papers? Originality The way the work is presented suggests that the proposed approach is notvery novel. It is simpler to    start training from scratch whenever a configuration is promoted. Quality and Clarity The paper is generally well written. "This is done in our implementation" suggests that    you do in fact start training from scratch whenever a configuration    is promoted. Some of the experiments on the larger    models (LSTM, ResNet) seem inconclusive, but it s good to include    them. Perhaps it is not immediately straightforward to use a GP in    combination with successive halving, but the challenges are not    clear in Section 3. Figure 2: I find the dashed lines fairly difficult to see. I    appreciate the effort to label the difference between stopping and    promotion, but I think the figure is actually not informative at all    of what is going on. All of my understanding from this figure is    from the caption.<BRK>This paper may be better placed in a journal venue as a consolidation of techniques and through experimental evidence. However, from a practitioner s perspective, it is interesting to see confirmation that this works well as expected. Is it a distributed setting where the workers are independent instances (VMs) and they are assigned a computation by a master instance? Contribution 1 in the text (end of the first section) is not a contribution, I would suggest that the authors remove it from the contribution list. It is just a background section. The text should be reworked and made clearer, with smoother transitions between the subtopics, and paying a great deal of attention to giving enough background on the combination of techniques used. But aren t the authors using simple regret in all the result plots? Page 4: in figure 2 the authors can add a legend for the 3 configurations (and the 3 different colors).
Reject. rating score: 4. rating score: 5. rating score: 5. rating score: 5. rating score: 6. <BRK>This paper proposes two algorithms to solve Wasserstein distributionally robust optimization (DRO), posed as a min max max problem, with three players taking actions at each round. Unfortunately, the algorithms in this paper are quite standard and not scalable. In terms of performance, the results are within statistical error of the existing baselines. Here are some specific comments for the authors to improve the quality of the paper. **Q2.** The proposed MCMC method for the non convex problem setup is not scalable.<BRK>This paper proposes a "three player framework" for distributionally robust optimization (DRO), where the authors considered lifting the constraint of the ambiguity set of using the Lagrange multiplier method. In my opinion, this paper contains a lot of strong and implicit assumptions that the authors did not address:*****************Major issues, theoretical*****************(i) Theorem 2 assumes that we can "update" the distribution in Algorithm 2 (for \pi_t), which is in general an impossible task. After reading the rebuttal and other reviewers  comments, I feel that the consensus is the impracticality of the proposed method. Instead, the authors might want to try out gradient sampling methods such as Langevin dynamics https://arxiv.org/abs/1611.01838.<BRK>*Detailed Comments: The primal dual formulation and the use of primal dual method to finding an equilibrium is quite standard. Translating the hard computational problem into a transportation problem is nice. The paper is clearly written and clearly covers prior works on this topic. Most of the presented derivations and ideas are quite standard, there is however one nice idea which tries to handle the hardness of the problem by translating it into a  transportation map. Post rebuttal: I have read the authors responses and I keep my score<BRK>This paper consider the distributionally robust optimization (DRO) problem with Wasserstein ambiguity set. The paper proposes iterative algorithms to solve the three player game, along with providing guarantees (both numerical complexity and statistical). Is pi_1 a discrete, continuous or mixed distribution? ii) What is the nature of the distribution pi_{t+1} in each iteration? In that case, why is the output \hat{\pi_t} a discrete measure? The authors did not resolve this difficult but "leave this problem to future work". Is it possible to pose a min max min formulation first and arrive at problem (7) as an equivalence?<BRK>This work presents a three player game framework for solving Wasserstein DRO problems, and it can handle general loss functions with new algorithms. I find this work interesting and potentially applicable to many problems. 1.In Lemma 1, the authors state that the strong duality holds. Is there any requirement on the l function? 2.For Theorem 1 and 2, can the authors change alpha and beta/eta to a range that the results still hold? For example, if the problem is a high dimensional estimation problem, such rate doesn t hold.
Accept (Poster). rating score: 8. rating score: 7. rating score: 6. rating score: 6. <BRK>In this work, the authors illustrate an approach for learning logical rules starting from knowledge graphs. The approach seems interesting and the tackled problem could interest a wide audience. The paper is well written and self contained. Moreover, the experimental results show that the proposed approach has competitive performance comparing to other systems (even compared with systems that do not learn rules but perform only link prediction). For all these reasons I think the paper should be accepted for publication.<BRK>mode?Overall, I think this work indeed has some novelties but it is currently prone to unjustified claims and confusing writing. With that being said, I would recommend weak rejection at this point, but I m happy to raise the scores if these concerns are addressed. Candidates are evaluated with a separate evaluation module that computes the scores. I think the idea of separating rule generation and evaluation is interesting, and using EM to jointly train the modules is also novel. However, I think the current paper writing is convoluted and prone to notations issues. Psi_w(rule) is not defined in the main text. Experiment:  Many baseline scores on the FB15K and WN18 are cited from the original paper. mode, I assume no embeddings are learned in the reasoning module, so the score should be comparable to the vanilla score used in NeuralLP.<BRK>The idea in the paper is that E step would try to generate rules while M step would update the parameters. The empirical comparisons are interesting and show that the paper improves on prior work. There are several aspects that make it very hard for me to understand the paper s contributions and evaluation. I am listing below and hoping that other reviewers or authors would be able to clarify:1. It is hard to understand what MLN tool is being employed here and why MLN based technique would return a suboptimal answer (the combinatorial solvers may return suboptimal answer due to timeout but that should be clarified in this case). It may perhaps be the case that sampling rules from a good distribution allows us to search only over a small space but that needs to contrasted with weakness of MLN based methods. I am failing to understanding how is the ranking computed for rules and why such a metric is a good approach.<BRK>There is a lot of recent work on link prediction in knowledge graphs. The numbers in the current paper for TransE, DistMult, RotatE, ConvE and ComplEx seem to be taken from the RotatE paper. So overall, the provenance of the numbers in Table 1 and 2 is in serious doubt. I realize that the authors do not have a chance to respond and modify the paper. The origin of many of the results in these tables is in doubt. But when I look at these papers, I see different (or no) numbers. The high level ideas are easy to follow, but the precise way the algorithm is put together is hard to follow. RecommendationI recommend accepting this paper.
Accept (Poster). rating score: 6. rating score: 6. rating score: 6. rating score: 5. <BRK>I like the main ideas articulated in the paper, but find the writing lacks some clarity: Summary of paper: The paper  takes as a starting point the study from Barbu et al where the robustness of object recognition pipelines to be able to handle distribution shifts are studied by testing ImageNet trained architectures against ObjectNet. The main point in the current paper is that the performance degradation seen in Barbu et al is due to the fact that the CNNs were processing the image with entire image as context  and when one only provides a sub window around the objects of interest the resulting performance improves significantly. Analysis of properties of ObjectNet and its challenges. Cons:  It has incremental insights. The structure of the paper could be simplified with a table or diagram that illustrates the logic behind the experimentation and conclusion.<BRK>Real world applications frequently provide challenges that are not seen in common computer vision datasets like Imagenet, where images are blurry, dark, corrupted, objects are highly occluded, test objects may be out of distribution due to natural distribution shifts, etc. The authors describe the image distortions they consider to be “natural”, but are applying them synthetically. This is a useful reference point for future work.<BRK>This paper revisits ObjectNet dataset closely and found applying classifiers on object bounding box significantly reduces the gap between ImageNet and ObjectNet. "the matter" means context for robustness? Figure 5: it is clear that seg mask actually isn t robust to adversarial attacks (accuracy dropped significantly), which contradicts with the claim from authors. Would the segmentation mask itself be indicative for object categories? I am still a bit concerned about the definition of "robustness", but the paper overall does look good for ICLR publication. This is not necessarily true.<BRK>If you argue that only 10 categories are common between COCO and ObjectNet, how many are common between LVIS and ObjectNet? I would strongly encourage the authors to leverage these pre trained models and sharpen their message & contributions. This work reduces the performance gap by cropping out the object using bounding box or mask information and running the recognition model on top of it. ObjectNet is a better snapshot of the real world and models trained on ImageNet suffer in ObjectNet. However, the proposed approach to crop out the object using bounding box information or mask information is moving the data distribution from the real world setting of ObjectNet closer to the ideal setting of ImageNet. I would encourage the authors to even consider an object detection model trained on LVIS which has a larger number of object categories. However, the experimental setup is very limited (700 train + 300 test).
Accept (Poster). rating score: 7. rating score: 7. rating score: 6. rating score: 6. <BRK>Overall, the paper is well written and has a clear layout. One shortfall of this approach could be the available data itself as you d rely on the policy to provide you with good samples for RCRL.<BRK>The primary novel component is using a return based auxiliary task. **Quality**Overall I found the approach and results to be interesting and moderately compelling.<BRK>## Return Based Contrastive Representation Learning for Reinforcement Learning### SummaryThe authors propose Return based Contrastive Representation Learning (RCRL), a contrastive auxiliary learning task that guides the feature network to encode representation relevant to the task rewards. Overall, the paper is well written, the topic is relevant to the field and the approach is novel. The improvement in continuous control tasks seem to be really marginal.<BRK>This is an interesting paper that proposes abstractions based on return distribution similarities to be used as auxiliary tasks to aid in learning. The theoretical results seem interesting, but I have some questions on their validity and clarity (see point 7 and 8 below). Why is this third state necessary? There are no page limits for the appendix. If so, are you really learning a distribution?
Reject. rating score: 3. rating score: 6. rating score: 7. rating score: 8. <BRK>While the paper is easy to follow, I found all the results in this paper trivial and already known. Pros:1.The paper is well written and easy to follow. What if a given graph does not contain any automorphism? 3.The proposed model is not permutation equivariant after adding Gaussian noise. It is trivial that the model becomes permutation equivariant when the Gaussian noise is ignored (because the model just reduces to an ordinary GNN). 5.The idea of using node identifiers (essentially equivalent to the Gaussian noise) to make GNNs position aware is not new. In fact, this idea is clearly mentioned in the P GNN paper already (Section 6.2 of [1] “for inductive tasks, augmenting node attributes with one hot identifiers restricts a model’s generalization ability”).<BRK>The proposed model, dubbed as stochastic message passing (SMP), arguments the existing GNNs with stochastic node representations. Additionally, showing some examples of automorphic node pairs and the performance on these nodes could demonstrate the difference between models. The experimental results also suggest that this simple technique is quite effective in many tasks. Although in theory, it would be possible to learn GNN that preserves node proximity, if a given task doesn t need to model proximity aware representations, random resampling may hinder the convergence of the proposed method.<BRK>The authors propose to add random node features to the input of message passing graph neural networks for them to become proximity aware. Strengths:  The paper is well written. Including the experiments in the appendix, the empirical analysis is very exhaustive and seems to be reproducible. It also does not lead to a significant computational overhead but fits nicely into the existing message passing framework with linear time complexity (in number of edges). It is nice to have a formal framework that justifies the application of this trick. In general, GNNs to solve matching tasks are a very fitting application for the proposed method, which the authors do not consider. The paper achieves what it sets out to do in providing an exhaustive theoretical and empirical analysis of a simple but effective idea.<BRK>This paper introduces SMP, a novel stochastic message passing approach that preserves both permutation equivariance (common to GNN models) and node proximities. I thoroughly enjoyed reading this paper, both for the insights and the technical soundness. If the avg running time for an SMP epoch was significantly larger than the one for GAT (for instance), I would have considered SMP yet another specialized model. Instead, given that the GPU consumption (both in terms of computation and memory) is similar to any other GNN model, I believe SMP could be adopted as a more flexible graph ML method (which would avoid having to choose a method given the target task, e.g., node classification vs. link prediction). The current SotA is more than 10 points above the performance achieved by SMP.
Reject. rating score: 3. rating score: 5. rating score: 7. rating score: 9. <BRK>Summary: This paper proposes a new regularizer that can be plugged in gradient based learning algorithms, which aims at solving the problems induced by unobserved confounders. And based on this the algorithm is proposed to deal with the problem of unobserved confounders. Strengths: 1. I read the prove and I think this norm is defined as an integral which has nothing do with beta any more. Could you provide some explanations of this assumption or give some examples of it?<BRK>summary:This paper proposes a new domain generalization (DG) method, which enjoys certain statistical invariance property in the presence of unobserved confounders. The derivation of the last inequality in the proof of theorem 1 is not very straightforward. pros:  this work views the distribution family of training data as generated from manipulating a causal model $M$, which is novel in DG methods and of pracitical significance. If possible, it would  be great to also consider comparing with REx [1].<BRK>The problem is thoroughly motivated and relevant literature reviewed. The paper lacks an investigation of where and how the proposed methodology fails. It would have been helpful to provide more intuition around the formal explanations in section 2 and 3. ## Detailed Comments  "new or related data"  > what does "related data" mean? "Doing so is difficult however, some form of uncertainty about the distribution of new data is unavoidable." Prolly the eqs (1) and (9) have the same latex equation label.<BRK>I found the paper well written and clear, with many intuitive examples. I found the method compelling and the theory appears correct. Additionally, at this point in the paper, it is not really clear what a "causal solution" is or how it differs from the proposed solution. This was then addressed, in part, by the last parts of 3.1 and 3.3, but I would consider including a more explicit contrast between these various objectives earlier in the paper. 6.Links to Equation (9) should be swapped for Equation (1) (e.g., page 1, par 2).
Accept (Poster). rating score: 8. rating score: 7. rating score: 6. rating score: 6. <BRK>The authors provide a clear review of different divergences used in contrastive learning and their relative strengths and weaknesses in terms of training stability, minibatch size dependence, and usefulness on downstream tasks. This motivates the need for a new divergence which they introduce based upon chi squared divergence. The paper is well written, and provides helpful context to not just motivate the value of the new technique, but quantitatively and qualitatively contrast with existing techniques that helps inform the reader about the broader field.<BRK>Authors propose RPC (Relative Predictive Coding), which is supposed to improve training stability (with chi squared distance based regularization), minibatch size sensitivity (avoid sampling large batches), and downstream task performance (show generalization). It is a good direction in self supervised training with convenient training schemes.<BRK>This paper proposes a new objective for self supervised contrastive learning. Compared to other objectives for contrastive learning, the authors illustrate the advantages of the proposed one in training stability (or easiness to train), sensitivity to batch size, and downstream task performance.<BRK>This paper presents a new contrastive representation objective that has good training stability, minibatch size sensitivity, and downstream task performance.
Reject. rating score: 5. rating score: 5. rating score: 6. rating score: 6. <BRK>Clearly, the only difference between the proposed method in section 3.4 and SGC is that the authors used heat diffusion as the spectral filter matrix, while SGC used a polynomial filter (the K th power of the normalized adjacency matrix). Furthermore, the other reviewers raised similar works such as graph ODE, which further reduces the novelty of this work. The main novelty of the proposed HKGCN, therefore, is on a combination of heat kernel and the SGC approach. The HKGCN method is motivated by the oscillation problem of GCN. Ideally, to show how HKGCN is different with the GCN approach. There are not theoretical statements in this paper and I am evaluating it as an algorithmic contribution. Please be more specific. "no one has made a clear connection between GCN and the heat kernel."<BRK>This paper studies semi supervised node classification in graph data. The paper generalizes GCNs into a continuous model via heat kernel, where the proposed model uses continuous layers for information propagation. However, the paper also has several weaknesses:1. Although the idea of developing continuous propagation layers is interesting, the idea has been explored by many recent works. The authors should explain and clarify the difference between this work and existing works. The strongest baseline methods in Table 3 are GCN and GAT, and in Table 4 they are GraphSage and GCN. I wonder how does the proposed model manage to avoid over smoothing in practice? Could the authors elaborate on that? In the propose method, the hidden matrix H_t has the same size as the feature matrix X. Is there a way to deal with the potential problem?<BRK>Can the authors also clarify the difference between this submission and [2]? This submission has done a detailed and thorough evaluation of the proposed method. 1.This submission proposes to replace the propagation in GNNs with heat kernel. The main motivation for this method is that the laplacian filters tend to oscillate, as illustrated by Figure 1. Importantly, I believe the motivation of this method is *not* to prevent oversmoothing but to prevent over oscillation. Note that here the heat diffusion matrix does not need to be computed/stored explicitly.<BRK>In this paper, the authors find out that features propagation based on heat kernel allows to control the oscillation between low and high frequencies. In order to control that, the authors explain that  GCN based heat kernels can act as a low pass filter cutoff. This combination of GCN and heat kernels are empirically validated considering node and graph classification tasks. The settings are clear and the comparison with related works is convincing. However, it’s not clear how the proposed GCN tackles the problem of over smoothness and graph isomorphism. The overall approach is  original, well placed in the litterature and the paper is well written.
Accept (Poster). rating score: 7. rating score: 7. rating score: 6. rating score: 5. <BRK>The contribution of this paper is in proposing an approach to automatically determine which channels to keep, reuse, or skip per layer and per target instance that can result in efficient action recognition. STRENGTHS: The proposed method is model agnostic, making it easy to use as a plugin operation for other network architectures. Reusing history features when necessary to make the network capable for strong temporal modeling.<BRK>#################################Summary:The paper presented an adaptive inference model for efficient action recognition in videos. The key innovation seems to be the perspective of modeling temporal feature fusion for adaptive inference. The proposed model was evaluated on several video action datasets and compared against a number of existing deep models. If that is the case, the contribution of the proposed adaptive fusion scheme is much weakened.<BRK>In this work, the authors introduce an AdaFuse network for efficiency action recognition in videos. Strength 1 The paper is written well, and the organization is OK2 The idea of adaptive temporal fusion is somehow novel and interesting Weakness1 How to save computation. I indeed suggest that, it would be better to perform the proposed method on Kinetics to further show the effectiveness.<BRK>#### GeneralThis paper proposes an adaptive temporal fusion network called AdaFuse for action recognition, which adaptively removes temporal redundancy and reuses past features for accuracy and efficiency. The propsoed method is not compared with some of the recent methods such as [1 3] ([4] is optional because the publication date is very close to the ICLR 2021 submission deadline). 1.Some of the important details are not clear. I would appreciate if the authors could answer the questions I listed below. One is a trick for sampling from a categorical distribution, and the other is a trick for making the opperation differentiable.
Reject. rating score: 5. rating score: 5. rating score: 6. rating score: 7. <BRK>Summary: The paper studies the problem of classifier recalibration under differential privacy constraints. Rigorous experimental results are provided. I ve listed the strengths and weaknesses below. Hopefully, the authors can address my concern in the rebuttal period. 2.The choice of the query function is novel for privacy constraint, as it has lower sensitivity compared to the log likelihood function. 3.They provide extensive experimental results to demonstrate the effectiveness of the proposed method. I don t see how the algorithm addresses the domain shift problem. But they claim that  We also fine tune on the target domain  in section 2.2. The framework doesn t seem to be novel, but it can be a novel use of this setting for recalibration. It would be interesting to see the comparison. Or maybe it would be more clear to remove the subscript.<BRK>This paper studies the problem of privacy preserving calibration under the domain shift. The empirical results seem complete. 1.The proposed algorithm is not described very clearly in section 3 and section 4. 2.The privacy part seems like a plug and play of the Laplace mechanism. Moreover, the calculation of sensitivity seems to be wrong. As the authors claim in Section C.2.1 in the appendix, the sensitivity is technically infinite.<BRK>Summary:This paper studies the problem of recalibrating a classifier under the presence of domain shift and the constraints of differential privacy. Addresses a new, interesting, and practically relevant problem. What the authors propose (Section C.2.1) to address this   i.e., using a sufficiently large value based on the empirical values   is not generally accepted. The reduction to a 1 dimensional minimization problem over T makes sense. Experiments show that the proposed approach actually works as it is expected to. Hopefully authors can address it adequately in the rebuttal.<BRK>The paper tackles the problem of privacy preserving calibration under domain shift, which is an interesting combination of 3 separate problems that may often occur together. This is not really covered Whilst not a startling novelty, it is a nice observation that their method Accuracy Temperature scaling (Acc T) combines so well with differential privacy, and yet does not lose out in terms of utility (and in fact does better under more stringent privacy settings, since fewer DP noise iterations are required.Strong experimental section. There are extensive experiments on recalibration under domain shift (using perturbed image datasets).
Reject. rating score: 3. rating score: 4. rating score: 4. rating score: 5. <BRK>The paper proves that (1) when m 2, any stationary point (of the student network) has to be aligned with the teacher; (2) when m> 3, stationary points satisfying certain additional conditions have to be aligned with the teacher. So the present result seems to imply that there is no saddle point at m 2 and (likely) no saddle point at m> 3, when the teacher is one relu. Thank the authors for the responses.<BRK>Therefore, I don t think that the current version is ready for publication and I highly encourage the authors to improve the current paper (see my suggestions for improvement below). The authors mention d 2 only once in the paper. I understand that for d>2 the analysis might be intractable. Therefore, it is not clear whether this is a realistic conjecture.<BRK>Furthermore, the polar angles satisfy a certain positive definiteness condition in this special case. In particular, these include:(a) The dimension of the neurons being only two. (b) The results for the student network having more than two neurons are still rather preliminary. While this paper does provide several interesting ideas that might help in ultimately resolving the question, as the results stand currently, it is quite unclear to the reviewer whether the conjecture(s) will hold or not.<BRK>2.Why the results only consider the case when d 2? 3.The presentation of the current paper needs to be improved. The problem considered in this paper is interesting.
Reject. rating score: 4. rating score: 5. rating score: 6. rating score: 7. <BRK>The evaluated EDP is fed into the Bayesian optimizer of mapping and hardware separately in sequential. How large is the design space? Pros: + The solution framework for co searching hardware parameters and software mappings is neat. It s nice to see the comparison between the proposed method and TVM; and the ablation study on whether to co search hardware and software. Concerns:   The key concern about the paper is the lack of novelty. There are increasing works on hardware search for neural networks (see below), while this paper doesn t mention or compare to any of those. Device circuit architecture co exploration for computing in memory neural accelerators. Best of both worlds: Automl codesign of a CNN and its hardware accelerator.<BRK>This paper presents a method to optimize the co design of code mapping and hardware configuration for neural accelerators. The method is based on a two level Bayesian optimization (BO), with each BO optimizing the code mapping and hardware configuration, respectively. The design space of both code mapping and hardware configuration contains many complex constraints, some of which remains unknown until running expensive simulation. Strength of the paper: this paper aims at solving a meaningful problem: SW/HW co design of code mapping and neural accelerators. Weaknesses of the paper: 1/ this paper spends a lot of texts explaining the basics of BO, but what s missing to me is the representation of software design points and hardware configurations. What s the meaning of distance in the representation space?<BRK>##########################################################################Summary:The paper presents a method for hardware and software co design using Bayesian optimization method. It is based on previous work of TimeloopThis paper seems a just an application of Bayesian optimization on the Timeloop framework. It searches using set parameters that are tunable by using some optimization method. The paper being first system that co optimizes both the hardware and software for DNN, which is dubious given the pletora of NAS and HAS papers.<BRK>In this paper, the authors propose to co optimize the software and hardware for DNN executions to maximize the energy delay product. There are three main contributions: 1) propose a formal representation of software and hardware that facilitate the search process; 2) propose a Bayesian optimization framework; 3) propose to search the hardware and software space separately and the search process is optimized by leveraging the Gaussian process model. Below are some questions and concerns:1:  the representation of software basically considers loop ordering, loop tiling, and computational parallelism, while the hardware representation is focused on available hardware resource and dataflow configuration. 2: the search goal of this work is to minimize EDP. But I am also curious about the detailed energy and performance comparison.
Reject. rating score: 4. rating score: 6. rating score: 9. <BRK>The paper proposes to first predict a coarse (32^3) voxel grid by aggregating independent predictions from individual views. the boost in the results seems impressive compared to P2M++There are however several things I don t like or that worry me about this paper:  the pipeline presented in this paper is extremely complicated, and has many different parts. It uses voxels, mesh and depth maps, Graph convolution networks, attention based architecture, SVR and deepMVS, the training loss has 5 balancing hyperparameters, between things as different as cross entropy and chamfer distance. Given these results, it is completely unclear to me how the proposed approach can lead to a ~14% improvement over pixel2Mesh. I thus think the approach should be strongly simplified (maybe loosing 1% in final performance), but the paper should provide a clear ablation that actually explain why their framework is so much better than P2M++ and this is interesting. Some additional notes on presentation:    I am not sure “contrastive depth” is a good choice of name since contrastive feature learning is a popular but unrelated research direction. I could only do it with the help of fig. 1 which is itself hard to parse and does not represent e.g.how the attention based pooling happens<BRK>The major contribution is in the refinement stage upon the coarse reconstruction obtained from voxel predictions, typically for the introduction of the Attention based Multi View Feature Pooling. Method Novelty According to the paper and the attached code, it seems like the authors mostly utilized existing networks to build a system. The author introduces their Attention based Multi View Feature Pooling mechanism which is new. Despite the results, the system is rather bulky and ad hoc. ResultsThe paper achieves plausible state of the arts quantitate results on standard evaluation sets and metrics. Their results struggles to getting clean surface especially when compared to implicit based methods, such as DeepSDF. ClarityThis paper is well written and easy to understand. The attached code is well documented and can be deployed. ConclusionOverall, this is a well written paper with plausible outcomes. The reviewer is marginally positive towards its acceptance due to the pleasing results, but is holding a conservative attitude towards its contribution significances. The reviewer would like to see the questions addressed in the rebuttal period, while also refer to other s reviews. For each single view voxel prediction, the paper did not clarify which coordinate system those voxel are in. When aggregating multi view voxel grid, how is the coordinate transformation handled between different viewpoints? If voxel from different coordinate systems should undertake transformation, how is interpolation handled when merging to a single 32x32x32 grid? How would this method overcome this, especially when the cubified mesh is in wrong topology? Why would we need contrastive depth feature extraction?<BRK>**Quality:**Overall the quality of this work is high. The quantitative and qualitative results are impressive relative to the SoA. I would like to see the qualitative results for the Best model as opposed to just the Pretty model, and I m curious why the best qualitative mode was not the same as the best quantitative model. I would think analyzing this difference could give the authors insight into how to improve the model. **Clarity:**Overall the paper is written clearly, explaining and justifying the different components of the model clearly. It would also be good to shed some intuition on why this criterion is the right one. **Originality:**The paper generally uses a mix of SoA techniques creatively woven together in a fairly sophisticated model. Oher novel aspects such as using the neural renderer to create the contrastive depth module was interesting.
Reject. rating score: 5. rating score: 5. rating score: 6. <BRK>Could this restrain the applications, compare to what we see in a lot a (random) QNNs with the mixture of the three types of Rotations? Using their methods, that include a specific encoding circuit and a specific loss function, the gradient of each parameter vanishes less badly compared to the usual Random QNNs. The authors demonstrate a lower bound on the (expectation value of the) norm of the overall gradient, which is also observed numerically. This could ensure a better trainability of these quantum circuits, which is of great interest. The authors propose a new analysis of a certain type of trainable quantum circuits, namely Tree Tensor Quantum Neural Networks (TT QNNs), that gives new and positive guarantees of its learning abilities. The article is well written and quantum computing concepts are well introduced, at least for an accustomed audience. However, some remarks could cloud the main results. It is hard to follow. Is this procedure realistic? Is the input state created close to the amplitude encoding objective? Finally, if the encoding circuit proposed in this work was not to be used, and if one would use a random encoding instead, what could be said on the bounds of alpha(ro_in)? Could this kill the benefit claimed and lead to a similar vanishing gradient as Random QNNs? Other Remarks :   In the complexity analysis of Algorithm 2, it would have been more informative to provide the runtime O(n_gate n_para n_train T) in terms of n, m, L, s, and other circuit parameters. Similarly, the authors should explain in the main paper the choice of using only RY rotations in the main circuit. It seems from reading the Appendix that it is for avoiding to have a unitary 2 design, is it?<BRK>Traditional Quantum Neural Networks suffer from poor trainability and one biggest reason is that the gradient vanishes exponentially with the input qubit number. It proved a training guaranteed lower bound of $\mathcal{O}(1/n)$  on the expectation of the gradient norm on TT and further proved a lower bound for  the expectation of the gradient norm that is independent from the input state. Clarity: The paper is well written and easy to be extended. This paper proposed a framework which can be employed for analyzing QNNs with other different structures in the future. Experiment: Although theoretical analysis is sufficient and convincing, the experiment does not convince me very much: 1. 3.Only 0 1 classification experiment was conducted, the results of some other pairs should also be provided. 4.I suggest that other tensor network Quantum circuits could also be compared. B.Limitation: I believe that TT QNN has better trainability and can solve the gradient vanishing. This paper only discussed an example structure and it is doubtable to generalize to other structures.<BRK>##########################################################################Summary: The design of a useful generalization of neural networks on quantum computers has been challenging because the gradient signal will decay exponentially with respect to the depth of the quantum circuit (saturating to exponentially small in system size after the depth is linear in system size). This work provides a detailed analysis of quantum neural networks with a tree structure that uses only a depth logarithmic in the system size. The authors show that the gradient signal will only be polynomially small with respect to the system size. However, the improvement in prediction accuracy (under early stopping) when using tree structure quantum neural networks is not very significant. This is likely because the considered system size (8 qubits) is too small to fully demonstrate the exponential decay and the inability to train random quantum neural networks. It provides a rigorous result for training a promising class of quantum neural networks. This is because a quantum neural network with a tree structure only has a log(n) depth, so the gradient norm would be 2^{ log(n)}   1/n. However, I don t think this intuition that gradient signal decays exponentially in circuit depth is widely known, so this work still provides a novel contribution from a theoretical aspect. The barren plateau problem has been a challenge that the quantum machine learning community has to overcome. 3.A good set of numerical experiments supplement their theoretical analysis. 4(a), we can stop at iteration 50~60 and the prediction error of the random quantum neural network would be fairly small. 2.The proposed TT QNN will have a limited expressibility due to the logarithmic depth. Experiments with 8 qubits are too small and a gradient norm of 0.2 (for random QNN) is quite large. Numerical experiments with a large system size will likely make the paper much stronger from an empirical perspective (if we can see a larger improvement in prediction error).
Accept (Spotlight). rating score: 8. rating score: 7. rating score: 7. rating score: 7. <BRK>The authors studied the behavior of adversarial examples from the channel view of activations, which is very novel. This provided a novel perspective for us to understand why state of the art adversarial training method works to a certain extent but not so good. Pros:1.The authors studied adversarial examples from a new perspective of channels in activations. From two aspects of activation magnitude and frequency, the authors found two novel characteristics of adversarial examples: adversarial examples have higher activation magnitude and more uniformly activated channels compared to natural examples. The authors found that the activated channels are still uniform under adversarial training, that is, some redundant and low contributing channels are still activated. To suppress the redundantly activated channels, the authors proposed Channel wise Activation Suppressing (CAS) training strategy.<BRK>This paper investigates the adversarial robustness from the activation perspective. Specifically, the authors analyzed the difference in the magnitude and distribution of activation between adversarial examples and clean examples: the activation magnitudes of adversarial examples are higher and the activation channels are more uniform by adversarial examples. Based on the above interesting findings, the authors claim that different channels of intermediate layers contribute differently to the class prediction and propose a Channel wise Activation Suppressing (CAS) method to suppress redundant activations, which can improve the DNN robustness. Combining CAS with the existing adversarial training methods leads to better DNN robustness.<BRK>To force the behaviors (in this paper, channels activations) of adversarial data to be similar to those of natural data, the authors explicitly suppress the redundant channels by reweighing the channel activations. Overall, I vote for accepting. Technically, this paper proposed effective training strategies (i.e., channel wise activation suppressing (CSA)) to enhance adversarial training. ##########################################################################Pros: 1 This paper provides the understanding of adversarial training from the channel activation perspective, showing that adversarial training can reduce the magnitude of the activation of the adversarial data, but fail to break the uniform activations by the adversarial data. 3 The experiment evaluations are comprehensive, showing CSA strategies  efficacy across various adversarial training methods, network structures, and attack methods.<BRK>The authors make a convincing illustration in section 3 on how adversarial examples tend to activate more channels compared to natural examples, and adversarial training is not effective in reducing them. This provides a convincing motivation to their design of the Channel wise Activation Suppression (CAS) module. How important is it to have the correct y available for the mask, as oppose to \hat{y} from the channel predictions? 2.Just to confirm, are both losses (CE and CAS) in Eq 5 taken into account in the generation of adversarial perturbations with FGSM and PGD? I am leaning towards acceptance of this paper if the authors can address the above questions sufficiently.
Reject. rating score: 5. rating score: 5. rating score: 6. <BRK>In this paper, the authors propose a model for integrating news representations for stock predictions. The authors claim that their proposed model outperforms the previous baselines. Presentation needs improvement. Without knowing this, it is difficult to evaluate the contributions of this paper. However, I do not understand the modeling noise component. However, I find the benchmark dataset used in this paper is out of the date.<BRK>The authors claim that separately representing the news and equity states is a novel part of this paper. Quality: The paper is somewhat unclearly written and the performance improvement compared with conventional methods is marginal. Only careful readers would find out the "At test time, future news events are not accessible". Experiments     I recommend the authors try other evaluation metrics such as the Sharpe ratio, to evaluate the realistic wealth curve. Sec.3.2 The long term articles may include short term articles, and it seems redundant. Is the $v_t [a_t, b_t, c_t]$ for training and $v_t [\hat{a}_t, b_t, c_t]$ for test? Significance: I barely see significant modules proposed in this paper.<BRK>Summary: This work extends the line of research for the problem of news driven stock price movement prediction. In doing so,  the authors presents a Noisy Equity State (NES) representation model, where they explicitly model equity state sequences by a recurrent state transition in an LSTM model. sound evaluation and analysis, including ablation and case studies   extensive analysis to show the importance of past, present, future news and noise  reports gains with the proposed approach Weaknesses:  the paper lacks novelty in terms of methodological contribution and is incremental. The work combines existing approaches and introduces additional signal/information    unclear experimental setup of baselines: Do all the baselines models also use the same (additional) information as the proposed approach does.
Reject. rating score: 3. rating score: 4. rating score: 5. rating score: 5. <BRK>The paper studies the performance of ensembles of deep networks on small data tasks taken from subsets of Cifar10 and Cifar100, with either the cross entropy loss or the cosine loss. The approach seems promising, and the extensive experiments provide a comprehensive picture of the performance of various choices of model and ensemble sizes on the Cifar datasets.<BRK>Paper strengths:+ The paper is well written and organized. If it is an empirical paper, the experiments are also weak. There are also several typos in this paper. Paper weaknesses:  The novelty of this paper is low.<BRK>The novelty of the paper is limited. Weaknesses:  The paper is basically a compilation of experiments with no explanations of the observed phenomena.<BRK>The paper seems well written and the experiments are well constructed with means and standard deviations reported for every experiment.
Accept (Poster). rating score: 9. rating score: 7. rating score: 5. rating score: 2. <BRK>The authors introduce and formalize the concept of Invariant Learning Consistency (ICL), which is motivated by the idea that "good explanations are hard to vary" in the context of deep learning. This paper is well written and clearly presented. The exploration of using geometric mean with a logical AND masking to pool gradients in deep learning is very interesting and novel. It would be great if the authors can provide further insights on what kind of data distributions (or applications) could potentially benefit from the proposed method versus using the arithmetic mean. In addition, it would be interesting to explore hybrid methods that combine the advantages of pooling gradients using arithmetic mean and geometric mean (i.e., AND mask).<BRK>This paper investigates one possible pitfall of current gradient descent method that averaging gradients over different examples failed to capture the invariance between different examples, through it can learn quickly by memorizing data. The authors claim that this is one important reason that machine learning with GD can t generalize well to out of domain dataset. To solve this issue, this paper focus on developing a method to learning the invariant explanations. Overall, the work is important and the paper is well presented. The paper studied the important problem of model generalization. The paper is well motivated and the knowledge presented can benefit the community. 3.The authors validate the value of capturing invariance through the implementation of a practical AND mask, and run experiment on a set of tasks. The baselines are reasonably chosen. It is very helpful for the readers. However, recently work (e.g https://arxiv.org/pdf/2001.10528.pdf) found that noisiness exists in many popular datasets. It would be interesting to show that ILC can also handle such naturally introduced noises.<BRK>The authors attempt to recover invariant solutions, defined as those with a similar loss neighbourhood across environments. This concept is interesting, especially since it can be incorporated with a clever understanding of loss gradients and hessians which underlies the generality of the proposed approach. The objective for all intents and purposes is to recover the causal solution from data in multiple environments, but this is never explicitly mentioned even though causality is said to be a "key element" in the Appendix. The introduction is therefore also vague, what do we mean by generalizing out of sample? More details here would be helpful. With unobserved confounding, it is known that the causal solution does not have zero loss gradient (for example in a linear model, least squares returns biased parameter estimates), what can we expect in that case?<BRK>a machine learning algorithm that is trained using gradient descent usually averages gradients across examples. baselines: sum vs geometric mean is a well known effect in products of experts vs sums of experts work. do not introduce more unnecessary lingo, because it leads to sentences like  the shortcuts are not shared across environments, but provide a simple way to classify the data, even when pooling all the environments together . for example, consistency is overloaded here (it might confuse some readers from a statistics background). this is used in RL (3) what are shortcuts, and how do they relate to environments? learning invariant mechanisms with geometric mean gradient descent  is an example of a title that would be easier to understand: (1) mechanisms relates to prior work such as the pearl work that is cited (indeed, in the appendix the authors state  causality... is a key element informing our exposition ). a causal mechanism is a well known concept to the ICLR community.
Reject. rating score: 3. rating score: 4. rating score: 4. rating score: 7. <BRK>Each agent sequentially selects data to include in the batch to be labelled according to specific heuristics. Finally, the various agents are weighted according to parameters found by a gradient less approach. While meta learning of active learning is a very interesting and useful problem, I find the contribution of this paper too weak for a conference as ICLR. The approach is rather straightforward (only a linear combination of different heuristic agents), the experimental results are not fully convincing (there is no real gap between the ensemble and the best individual approach, so using the best agent at training time is maybe a strong alternative) and the paper lacks clarity and details. From my point of view the related work section should shortened to focus on mainly important aspects related to the presented work, a better detailed view (more formalized, less algorithmic) of the approach should be given, and some theoretical insights should be given before it could be considered for publication in a top machine learning conference.<BRK>In the proposed method, the ensemble weight vector (\beta) is learnt from data. AL is simulated on a set of training tasks where performance for various choices of \beta are estimated using a Monte Carlo approach. In summary, the paper aims to propose ensembling as a simple alternative to computationally costly RL techniques for active learning. While the method is well motivated, it is missing some key experiments (especially on the significance of the contribution) and analysis, and has a weak discussion/analysis section. While some of my concerns have been addressed, a few key questions haven t been answered. In Sec 3.1, the reward is chosen as the improvement in current model’s accuracy after a step i.e., choosing a sample or a batch of samples. * The general author response mentions that reward shaping is not used in the proposed method. * The contribution is limited in novelty. Also, the authors respond to R1 that the gap between the ensembles  performance and each single classifier s performance is small since they chose SOTA models for the individual classifiers. This is perhaps even more reason to show how ensembling works when the base models are not SOTA. * I also agree with R3 that similar ideas have been explored before (papers cited in R3 s review), and it is important to compare with those methods as baselines in the experiments. These may be important, especially considering there seem to be different choices of classifiers for different datasets.<BRK>Summary of the paper:The paper proposes an algorithm for batch mode active learning using an ensemble of 4 active learning heuristics. The basic idea is to use an ensemble of heuristics/agents as the utility function to select a batch of samples. The authors perform experiments on various datasets. This is disappointing and also misleading. What will the performance be if we use RL algorithms? 2.The experiments are only comparing to the baseline heuristics, and are missing comparison with other previously proposed learning AL methods as in Section 2.2. 3.The idea of ensembling different heuristics is not new either. Last paragraph on page 4   it should be $\binom{n}{b}$ options to choose the batch (the orders do not matter for a batch).<BRK>The authors introduce a novel, batch mode ensemble approach to Learning to Active Learn that combines the best of both worlds: heuristic  and learning  based active learning. This is a well written, easy to follow paper on an important topic. The work appears to be original, and the findings are significant. The basic idea of the paper is to combine the best of both worlds of heuristic  and learning  based active learning. OTHERS:  please run a spell checker to avoid errors such as "acitve learning" (page 2) or "lineracombination" (page 8)
Accept (Poster). rating score: 6. rating score: 6. rating score: 6. <BRK>Summary: The authors analyze consistency based models in specific settings where analytically tractable results can be obtained. They establish that leveraging more sophisticated data augmentation schemes is crucial to obtain huge gains when using consistency based models. It tries to explain the why consistency based method achieve good performance compared to other semi supervised models. I find this relevant and helpful in understanding what makes these models work the way they do. Cons:The experiments are a bit weak only using simple synthetic settings. Comments:More experiments should be provided to establish some of the claims in the paper. One of the claims of the paper is that mean teacher and simple Π model approach share the same solutions in the regime where the data augmentations are small but advanced data augmentations performs better.<BRK>+The paper analytically discusses that the type of data augmentation plays a significant role in the performance  of the SSL models based on consistency regularization. However, I still think that the theoretical part of the work is not strong enough and can be improved. The quality of the paper will be  improved if it uses MTC and provides some new results which do not exist in the SSL literature. This is because  MTC may not be the only approach to analyze these points. Generally,  I think this  paper provides a good direction for understanding the consistency based SSL methods.<BRK>It indeed shows that (a) the method is relatively insensitive to the parameter λ in the context of the toy task but it doesn’t show that (b) the method is sensitive to other types of regularization such as weight decay. # SummaryThis paper proposes a theoretical framework for understanding consistency based semi supervised learning. To me, this is the most important result of the paper. It would be more convincing to take the GitHub code of one or more, possibly recent, SSL methods and vary $\lambda$ on real datasets. 2.I fail to see any other takeaway from the theoretical framework than the predicted insensitivity to $\lambda$. 7.“These results are achieved by leveraging more sophisticated data augmentation schemes such as ... Mixup”. What non linearity was used? I didn’t see it.
Reject. rating score: 4. rating score: 4. rating score: 6. rating score: 7. <BRK>Sections 2.2 and onwards seem more clear. Cons’:There seems to be a misnomer on the definition of what local and global means. Authors present good empirical analysis but there is no potential explanation of their conclusion that suggests that global features can be approximated better with shallow networks, and local features with deeper networks.<BRK>Based on the experimental results, the authors conclude that deeper networks perform better on certain datasets whose labels are more “local”, while shallower networks are better at more “global” labels. Particularly, it is discussed in Cao and Gu, (arXiv:1905.13210) that different definitions of NTK may differ in a 2^L factor for ReLU networks.<BRK>It devised well designed experiments, but all the contributions are empirical observations. It would definitely gain more value if a rigorous theoretical discussion of the results may be offered, or, if the same behavior may be observed on other, possibly real world datasets. The paper is well written, and the experiments properly designed and explained.<BRK>The design of the synthetic dataset is very clever, and the results on depth and finite vs. infinite width are very compelling.
Reject. rating score: 4. rating score: 6. rating score: 6. rating score: 6. <BRK>In this work the authors propose a two stage, adversarial training technique to calibrate a semantic segmentation model when faced with conflicting labels in the train set. Their approach is to first train a segmentation model. Pros:1) The technique is conceptually simple as training a segmentation model with cross entropy loss as well as training a GAN are both well understood. This seems like the multi modal behavior the authors argue for. It seems the calibration network is the base segmentation model and the refinement network is calibrating the model. 5) The paper would benefit from an algorithm sketch to explicitly show the two stages of training. The text seems to imply that this shows that their technique does not calibrate their model well.<BRK>* The paper includes experiments on toy data   which highlight the contributions of the paper. This seems to be a weak baseline, as recent works [3] address the model collapse issues of conditional GANs. * Experiments on CityScapes   the paper does not show results using metrics used by prior work [2]   in particular Precision Recall curves and calibration plots which shows the frequency of correctly predicted labels for each bin of predicted probability values. These metrics are also used in [1]. These metrics would better illustrate the calibration of predictions of the proposed approach. * Several unclarities   What is the contribution of the two components   Calibration network and Refinement network on the calibration of the final output?<BRK>** Summary: This work addresses the context of semantic segmentation where a single input image could be associated with multiple valid labels, as a result of natural ambiguities. Starting from a pretrained deterministic segmentation network F, this work proposes to use an additional conditional generative model G, named as *refinement network*, to generate multiple segmentation predictions; the model G is conditioned on the segmentation probabilistic output of F and the input image. Can the authors please provide experimental evidence of how the cross entropy loss and adversarial loss are not well aligned in the presence of noisy data? ** Preliminary evaluation: this work targets an interesting task of stochastic semantic segmentation. The major problem is the lack of evidence to support the claim on output calibration.<BRK>Summary:This paper presents an approach to stochastic semantic segmentation. The proposed strategy involves a simple extension of neural network architectures for semantic segmentation. The experiments on a toy dataset and two segmentation datasets demonstrate the superiority of the proposed approach compared to using standard segmentation loss and a few related works. In the preliminaries section, the authors discuss several simpler alternatives that are not tested in the experiments. I think these settings would serve as good additional baselines in the experimental evaluation. Moreover, on the Cityscapes dataset, they only compare to one single baseline. Considering the concerns of the other reviewers and the authors  answers and additional experiments, I think that the paper provides a sufficient contribution to an important research topic. Therefore, I retain my initial rating.
Reject. rating score: 1. rating score: 4. rating score: 6. rating score: 6. <BRK>This paper analyzes the convergence of SGD with a biased gradient, where the bias comes from label smoothing. However, I did not see how to make it happen, and authors completely ignored such analysis in their response. My another concern is that the role of $\delta$ is unclear. Yet, there are still two issues here. Updates after author response The authors basically posted their response at the last minute of the window which eliminates the possibility for further discussion.<BRK>This is the first work to theoretically understand the effect of label smoothing by analyzing the convergence speed. 2.Based on some reasonable assumption, this paper proves that SGD with LSR can have faster convergence speed than SGD without LSR when delta is small, and it will converge to a worse level of O(delta) when \delta is large. Experiment is not enough. It is better to show some results on the other optimizers frequently used in deep learning, such as Adam or Adagrad. 3.Lack of experiment setup.<BRK>(This is just a comment on the story/motivation of the paper.) The experiments show how the proposed TSLA performs better than the label smoothing baseline for 3 image datasets. It compares the convergence properties of SGD without label smoothing and SGD with label smoothing. It further proposes an original method that first uses label smoothing but later turns it off, which is shown to be empirically better with some convergence properties.<BRK>The overall readability of the paper is decent. There are numerous minor issues (see below), they are all trivial to fix. **Page 5** *it runs a stochastic algorithm A(e.g., SGD) with LSR in T1 iterations*Runs … **for** T1 iterations? > Page 7,8: the value of theta**[resolved]** As far as i understand, you select theta by maximizing the performance of LSR baseline and then use the same theta for TSLA (if not, please clarify). Please elaborate on how exactly you tuned theta. Is there some intuition why optimal theta for LSR would generalize to TSLA? *Equation 4: {\hat y} vs {y^{LS}}:***[i was wrong]** As I understood, \hat y is the common smoothing vector that is used to construct y^LS.
Reject. rating score: 5. rating score: 5. rating score: 5. rating score: 6. rating score: 6. <BRK>However, only in  and post processing are compared. Theorem 4.2: in Hardt et al.(2016), they show that the resampling probabilities can be calculated by solving an LP, need to justify the novelty of the result / emphasize the increment. It would be interesting to include also the pre processing to the discussion. The authors give a condition under which EO is attainable for classification.<BRK>This paper studies under what conditions a classifier can satisfy the condition of equalized odds. Weaknesses:Even though the results in this paper are interesting, the main weakness of the paper is that  the techniques are not novel. Additionally, I think a more interesting question to consider would have been the trade off between EO fairness and accuracy. I didn’t find the result about deterministic classifier to be surprising.<BRK>What this paper doesn t answer, and what would in my mind be a far more interesting result, is the converse: if the conditions specified in the paper are not met, is it still possible to get an epsilon approximation of equalized odds? This paper examines the conditions under which equalized odds can be achieved from a theoretical perspective. Overall, I don t really see the motivation for this paper. Section 2.1 appears to be an attempt to specify a qualitative relationship between ideas about fairness, but I find it difficult to understand what the purpose is.<BRK>The paper studies the attainability of the equalized odd fairness criteria introduced by Hardt et al 16 in classification and regression tasks. In particular, the paper claims that under certain conditions EQ is not even attainable. They proved the claim for the regression task but I could not find exactly where they discuss the classification attainability. In fact, the (non)attainability claim about EQ is confusing to me since by definition, a *perfect* predictor (which is non trivial) satisfies EQ. But as we consider classification or non linear regression it becomes less believable. The results on classification with deterministic prediction seems restrictive as condition (i) seems very strong. Lastly, Theorem 4.3 seems very interesting.<BRK>When the prediction function is deterministic, it shows that Equalized Odds may not be attainable under certain conditions. The conditions under which perfect Equalized Odds can be attained are identified under various settings, including both regression and classification, and cases when prediction mapping is deterministic and randomized. Are they the best attainable level? Because of the tradeoff between accuracy and fairness, it is possible to improve accuracy by adjusting the fairness guarantee. Moreover, the reduction approach returns a randomized classifier, rather than the deterministic function of features as mentioned in the paper.
Reject. rating score: 4. rating score: 6. rating score: 6. rating score: 7. <BRK>Since QTRAN is a significant improvement for value based multi agent reinforcement learning after QMIX, the practical implementation of QTRAN is expected for a long time in the community. QTRAN++ relies heavily on the true joint action value function. After a careful re evaluation of the paper, I have many concerns about the performance of baselines on the StarCraft II benchmark tasks. The reported performance is not consistent with those reported in the SMAC benchmark paper (see Figure 4,5,6 in [1]) and QPLEX paper (Figure 5,8,19 in [2]). However, it is difficult to tell the contribution of the monotonic part. Why should this part be included?<BRK>### Summary and claimsThis work proposes a MARL (multi agent reinforcement learning) algorithm. This seems pretty arbitrary. The loss function is modified to impose two additional constraints on the transforming value function. Gradients are now also backpropagated from the "tracking loss" into the "true" action value estimator, which makes it somewhat unclear what it is actually representing. ### Relation to prior workThe paper is positioned sufficiently with respect to prior work. I m not sure what the purpose of moving the related work into the appendix is, especially if some of the papers mentioned there are not actually related to the work presented in this paper.<BRK>### SummaryThis paper presents an improved version of QTRAN [1]. The design is based on new loss function design, as well as new action value estimator designs. ### Weaknesses  Only one environment is evaluated, which might not be that convincing. In Figure 3, some results seem to be not converged yet. This ..."Considering all the aspects, I tend to accept the paper in the current stage. Qtran: Learning to factorize with transformation for cooperative multi agent reinforcement learning.<BRK>Specifically, it improves on the QTRAN algorithm, a theoretically justified algorithm which previously had not produced strong learning performance. ## PositivesThe problem addressed   cooperative multiagent environments with CTDE   is a widely studied and important one. The improvements made to the algorithm are clear and well motivated; section 3.1 in particular explains clearly the difference the modified loss is intended to make. The empirical studies in the paper are strong. Since the empirical performance of the algorithm is central to the paper, this is important.
Reject. rating score: 5. rating score: 6. rating score: 6. rating score: 6. <BRK>The 22nd International Conference on Artificial Intelligence and Statistics. "Universal statistics of Fisher information in deep neural networks: Mean field approach." A series experiments are conducted to verify the understanding. On the other hand, though written as "trace of Fisher" in the paper, in experiments they compute a different regularizer   norm of expected gradient. Abstract, "We highlight that in the absence of implicit or explicit regularization...". How do you get rid of the implicit regularization??? Trace of Fisher is the expected gradient norm, but in Eq.(2) you compute norm of expected gradient. Statistically the latter has nothing to do with the former: one is the second moment, and the other is the squared first moment. Gradient confusion is highly related to the trace Fisher. See this one and its follow ups. Therefore I am not at all convinced by your arguments. 4.From the above discussion, at least you need to normalize the "trace of fisher" by gradient norm   which then becomes a measurement of gradient confusion. 6.Finally, let us take about the practical role of FP. # Missing RefsTons of theory paper should be discussed. A few of them come to my brain are listed in below. Please do a more complete literature investigation.<BRK>The experiments section of this paper is based on the empirical Fisher matrix (true Fisher matrix is expensive to compute). This makes the paper more convincing. Therefore, I keep the weak acceptance recommendation. The paper should have a discussion on the difference between true Fisher and empirical Fisher. By tracking the trace of FIM in the initial training process, the authors show that it can be strongly connected to a model’s generalization performance. Pros:This paper takes one step forward to the question of why some hyper parameter choices (small batch size or large learning rate) lead to better generalization. However, there is still a gap between the anisotropic noise and implicit regularization, i.e.why does anisotropic noise lead to implicit regularization hence better generalization. FIM is also studied in the context of SGD regularization [2]. How it relates to the generalization performance still remains unknown. Larger scale dataset such as Tiny Imagenet is included in the experiments. Cons:Most of the empirical verifications in this paper are comparing large and small learning rates. The method which reduces memorization should be more robust on out of distribution datasets.<BRK>This paper empirically investigates the effect of the trace of the Fisher Information Matrix (FIM) early in training has on the generalization of SGD. Furthermore, they experimentally show that the early low value of the trace of FIM may bias the optimization towards a flat optimum which has been observed to correlate well with good generalization. *Positives:*+ The paper is well written and easy to follow. Authors convey their message clearly. + The paper s motivation is definitely relevant. *Negatives:*  The exact mechanism how FP influences learning with noisy labels could be investigated a bit more rigorously. The discussion provided in the paper is not completely clear. Furthermore, the experiments are comprehensive and in depth. Therefore I would recommend acceptance if a couple of issues are addressed. Can the authors comment on this discrepancy? Minor comment: there is a typo in the caption of Fig.6.<BRK>Summary: This paper explores the relationship between the trace of the Fisher Information Matrix (FIM) and generalization performance of deep learning models. While it is probably not surprising to deep learning optimization/generalization researchers that the trace of FIM is closely related to generalization performance, this paper does a good job from the empirical standpoint, the experiments are done in a clean and systematic way to verify the hypotheses. Some comments/remarks/questions:  The manuscript is overall well written; I have no problems following the logic and the experimental setups. A downside is that there is completely no theoretical arguments given in the paper, I am wondering if it s possible even in the case of a convex quadratic/extremely simple networks, the authors can show how a small Tr(F) leads to a tighter/better generalization bound? In Section 3, the authors compared FP with GP which is a fair comparison. I am wondering if the authors compared their approach to even more closely related methods such as input output Jacobian regularization [2], regularizing by the Frobenius norm of Fisher; Tr(F^2) instead of Tr(F), and the Fisher Rao norm which has a tight relationship with capacity/generalization of neural networks as shown in [3].
Reject. rating score: 3. rating score: 3. rating score: 5. rating score: 7. <BRK>The authors claim that their method will perfectly reconstruct signals of finite innovation rate, however there appear to be mathematical errors in the proof. Yet these signals can still have finite innovation rate (for example if band limited to 2f). In summary, this paper describes a potentially interesting biologically inspired algorithm, but at least some of the claims appear to be incorrect.<BRK>This manuscript explores a mathematical framework and theory for a signal encoding/decoding scheme that shares many similarities to sparse deconvolution algorithms. I am not confident in all of the theoretical analysis. I have some concerns about the theory. It is unsurprising that the system can decompose signals produced by it. The relationship to existing work needs to be more clearly defined, so that the contributions can be considered in context.<BRK>In this paper, the authors describe a framework for stimulus encoding and reconstruction using spike trains. Please make it larger. Post feedback edit:The authors did not address the (admittedly, small) terminology issue I raised in my review. More specifically, I argued that their method in general requires transmission of the value of the threshold function, to which they replied that this is not true if the threshold function has a particular form (not required by their general theory), and then added some discussion about removal of spikes which was not what I was getting at. Moreover, I was not convinced by the authors  reply to the points raised by Rev 4. Moreover, the smaller terminology issue I raised in my own comment (about the<BRK>PROS:* new* both theoretical and experimental results* an alternative to matching pursuit, more accurate in certain regimes, which could have a broad range of applicationsCONS:* fully deterministic (see below)The authors propose a new way to encode a broad class of continuous signals (those with finite rate of innovation, which include bandlimited signals) into spike trains, and to reconstruct those signals from the spikes. The reconstruction is exact under certain hypotheses: the signal should be a weighted sum of the kernels used by the neurons with some temporal shifts. In my opinion these results are new and worth sharing. So the subthreshold signal can be estimated from the spike rates   something that would be impossible in the absence of noise. This is called stochastic resonance, and I wonder if the theory presented here could shed light on it.
Accept (Poster). rating score: 7. rating score: 7. rating score: 6. rating score: 6. <BRK>The shared embedding space is an interesting idea and can be extended to other time series models as well. The key contribution of their work is the use of a shared embedding space, that allows them to employ a mechanism to modulate spatial and temporal interactions between the RNNs. Since the functions used to map the observations and locations, and the embeddings vectors associated with each RNN, are learned, the joint learning objecting corresponds to learning a topological structure on the points in this shared metric space.<BRK>A set of RNNs, each with a learned embedding that corresponds to a vector in the embedding space, is used to process observations that are close in the embedding space. It s difficult for me to judge whether the model will also work on other domains based on these experiments. I m curious whether some of the components of the model are actually needed (e.g.attention between RNN modules). Another aspect of the model that could be investigated is the dependence of the performance on the kernel that modulates information exchange by distance in the embedding space. ### Presentation and clarityThe paper is generally well written. This could also leave more space to discuss the experiments in more detail or add an ablation study. The added ablation experiments demonstrate that each of the different attention modules proposed in the paper improve results, which I think really improves the paper.<BRK>This paper models noisy observations from complex dynamical systems, consisting of multiple interacting subsystems, by a set of sparsely interacting recurrent networks. The interactions between the recurrent networks (or modules) are constrained to be spatially localized, this is done by embedding the position of each module in a metric space and scaling the strength of the interactions between modules using this metric. How does performance vary as you change these? At the very least, the main text should state how these were chosen. Fourth, are there cases where the spatial structure is *not* a good inductive bias?<BRK>* Please include a discussion of limitations in the conclusion. Concretely, this paper studies video prediction from glimpses and world modeling in a multi agent setting. Arguably, on this task these methods provide the best trade off between locally interacting sub systems and more global modeling since each RNN specializes to a specific object, which is evidently not achieved by S2RMs. This positional information can then be used to limit the interactions between modules, to distribute inputs (that come with an associated location) to the modules, and to query a particular subpart of the global state at a future point in time.
Reject. rating score: 3. rating score: 4. rating score: 6. rating score: 7. <BRK>Until now, the ISBI 2012 challenge (http://brainiac2.mit.edu/isbi_challenge/) dominates the evaluation of cell membrane segmentation in EM data, even though the performance is nearly saturated. The evaluated "state of the art" methods are not "state of the art". Additionally, no parameters of the methods were adapted. The segmentation results on high resolution EM data presented in this paper display many "unclosed" edges, which lead to severe problems, when using the segmentation as a basis for connectivity analysis. The dataset is not published in the format of a challenge, which would allow benchmarking on a private test set. The segmentation of thin cell boundaries imposes different challenges and includes different priors, than in other domains of instance segmentation. A metric is proposed, that is (more) consistent with "human perception". This is an interesting aspect, but its contribution to the successful analysis of neuronal connectivity from EM data remains unclear.<BRK>Given the uniqueness of the proposed dataset, it is likely to contribute to the future EM based research, such as membrane segmentation. (2) For membrane segmentation in EM images, the authors develop a human perception based evaluation criterion, called Perceptual Hausdorff Distance (PHD). Based on the experiments on a small scale dataset. the proposed PHD metric is better consistency with human perception than the traditional ones. This will further highlight the novelty of the PHD metric proposed in this paper. The overall contributions of this paper have two parts: establishing a new dataset, and proposing a new PHD metric for the EM membrane segmentation tasks. The reviewer agrees that this manuscript has made some contributions on biomedical image analysis.<BRK>This supports the value of the database. The new results support that for the specific problem of cell segmentaiton, PHD agrees more with human perception than other metrics on this task, including the very related HD and ASSD. My primary remaining concerns have to do with the actual paper being of interest primarily to the audience interested in the specific task of cell segmentation, and that the technical value of the PHD metric is relatively limited (as per my initial comments). But, on the other hand, I don’t think the article adequately describes the dataset or compare it adequately with existing databases (which makes less of a “database article”). These improvements make me increase my score from a 3 (Clear Reject) to a 6 (Above acceptance threshold). 2.Interesting human based evaluation of the usefulness of IoU/Dice/F1 scores for the cell segmentation problem (by 20 humans, this is nice). If they are the same, then perhaps one of them should be removed. I think this point could be sufficiently addressed in the rebuttal.<BRK>This paper presents a large high resolution cell membrane segmentation dataset and also proposes a new evaluation metric that is more consistent with human perception. Overall, I think this is a good paper that addresses how to correctly evaluate cell membrane segmentation, which is essential for evaluation at a fair standard but has not been studied extensively. The pixel number and the number of images are presented to demonstrate the advantages over ISBI 2012 and SNEMI3D. Is it possible to make the criterion more generalized? Is it possible to use PHD not only for evaluation purposes but also for improving standard training?
Reject. rating score: 4. rating score: 4. rating score: 6. rating score: 6. <BRK>The authors propose a framework in which a separate sub task policy is learnt to accomplish each such sub task proposition. However, some of the technical aspects in the paper are difficult to follow concretely. This raised several questions from me on the general applicability of the framework (as it is presented in this paper) beyond the presented experimental tasks, and on some of the specifics of Theorem 3.1. This idea seems very similar to learning with Reward Machines. This is not true for RMs. Pros> The general idea of the paper is good. However, I am concerned that the significance of the work might be limited to the specific examples of the paper.<BRK>Summary: The paper proposes a hiearchical RL framework augmented with linear temporal logic. My second major comment is on technical exposition. The paper s writing needs a lot of work, especially the method section. For example, how exactly is LTL used in LOF? I understand that this was already introduced in Icarte et al., but it is worth reviewing it in the main paper. Comments on experiments:  For compositionality experiment, why might RM and Flat not be applicable here (related to my first comment)? The paper has been using FSA through except first introduced FSA.<BRK>This paper is on a new RL framework that leverages logical reasoning to improve the learning performance of RL agents. In particular, the knowledge is encoded using LTL, and includes both safety knowledge (used for reward function definition) and liveness knowledge (used for constructing FSA). The framework was compared with baselines including another LTL based RL methods (Reward Machines). The main concern is that the developed framework looks quite incremental in comparison to the methods from the literature. 2020In general, it s not surprising to see human knowledge is useful in RL. However, human knowledge is not always correct. The reviewer appreciates the response in detail. Overall, the reviewer still feels positive on this work.<BRK>In this paper, the author proposes logic option framework for learning policies that satisfy the logic  constraints. The LOF is tested on a gridworld and an OpenAI Gym benchmark and is compared against baselines that do not use rules. However, I m not familiar with the RL and option literature and it s difficult for me to tell its original work from the existing ones on the RL side. I m also concerned about the scalability of this method. It seems to me that the options need to be evaluated at each possible state in the environment.
Reject. rating score: 2. rating score: 4. rating score: 4. rating score: 4. <BRK>It would be helpful if the authors had a concrete application to showcase their results. This appears as an unnecessary padding. I suggest authors put that kind of definitions in the appendix and/or cite relevant literature, e.g.the paper by Patty and Cuturi.<BRK>The paper introduces the notion of the generalized projected Wasserstein metric (GPW) as a pseudo metric, associated with a set of mappings, between two probability measures. I recommend replacing "we need to" by "we sggest to". c) There is no sufficient justification in this paper for claiming that the pseudo metric defined by d_S is computationally efficient and robust. Furthermore, I would very much appreciate if the authors could elaborate on how the computational efficiency and the robustness should be understood within this work.<BRK>The paper is not well written and is sometimes simply wrong in my understanding. Comments on these points are of interest. In other words, how hard is the optimization problem introduced by the authors? Experiments lack a clear motivation and the synthetic experiments are not particularly illuminating. — page 8: « projects » projections— page 5: « Both problems already have solutions which are going to re use » we are going?<BRK>The paper lists a general approach to compare probability distributions. All of these questions would help strengthen the claim of superiority of the metric mentioned. I would encourage the authors to resubmit a more complete draft at a future submission.
Reject. rating score: 5. rating score: 5. rating score: 5. rating score: 7. <BRK>I appreciate the clarifications and improvements made to the paper and have increased my score 5. My concerns about the generality of the framework (as also pointed out by Rev1) still hold, however, as an evaluation on non image data is still missing. I encourage the authors to extend their work further into this direction, but as is, I would keep my recommendation to reject. #####**Summary**This work presents a generic approach for out of distribution (OOD) detection or anomaly detection (AD) called GenAD. Classification based anomaly detection for general data. Deep anomaly detection using geometric transformations. I could imagine the hardness of an in distribution classification task can be due to a complex in distribution, for which the OOD detection problem is also more difficult. Include other types of data in the experimental evaluation, which would strengthen the generality claim of the proposed approach. The title of the paper is very generic. 3.The batch sizes reported in the experiments are uncommonly large (1024, 2048). A unifying review of deep and shallow anomaly detection. Understanding anomaly detection with deep invertible networks through hierarchies of distributions and features.<BRK>The paper proposes an OOD detector algorithm that first learns a function to reduce the data dimensionality followed by learning a classifier discrimination model to separate in distribution data from OOD. Pro:1.The paper compares many baseline algorithms2. The paper title is  A General Framework... , however, the few datasets selected for experiments represent a very narrow domain. 2.There are gaps in the intuitions such as why would two instances in the same neighborhood in the reduced dimension not be expected to have similar labels. The overall approach is that of reducing the dimensionality of the data by projecting it onto a lower dimensional manifold (surface of hyper sphere) and then using a discriminator. 2.While the paper claims that this is a general technique, it depends on the concept of  semantic neighborhood  for which it only provides CIFAR variants as evidence. I suggest the paper remove  semantic neighborhood  terminology.<BRK>Summary  Presents GenAD as a general framework for anomaly detection  Method builds on top of contrastive training and proposes to learn a discriminator to distinguish between semantically similar and dissimilar pair of examples  Results are SOTA but need verification through code and methods clarificationClarity/Quality:Paper is overall written OK but several typos/grammatical errors as highlighted below:  “For visual data we show new state of the OOD classification accuracies for standard benchmark data sets”  > new state of the   “art” OOD classification  “The contrastive objective aligns feature vectors h   h(x)”  > consider using different symbols for the vector output and the    encoder function  “A statically meaningful score”  > statistically? If (and it is a big if) the results are verified, this could be a very important paper in the field of OOD detection. Is it jointly or separately? The only difference seems network sizes. Not sure how these results came   about? If yes, then how is the network pruned to smaller width? Why was ADAM used instead of LARS as in Chen et al? Would encourage authors to share code to help verify the methods/results.<BRK>Then, they classify neighbouring pairs of test examples as in  or out of  distribution based on the amount of the shared semantic information. ##########################################################################Reasons for score: I recommend to accept the paper since the authors deal with an important problem and they propose a clear and well written method that outperforms in their empirical applications, at least, several existing approaches. In that section the authors should list the most relevant, existing, anomaly detection methods and briefly explain them. 2) The authors identify that the main limitation of the proposed approach is the definition of a semantic similarity which in some applications can be very difficult. I propose to add one or two tables similar to Table 1 in which they will compare versions of their method resulting from using different/misspecified transformations with the competing methods. ##########################################################################Minor comments: 1) Define d in  d dimensional  in page 2.
Accept (Spotlight). rating score: 7. rating score: 7. rating score: 7. rating score: 7. <BRK>This paper proposed a new method that prevent the teacher model from being learned by other models. The paper  introduces and investigates a concept called Nasty Teacher: a specially trained teacher network that yields nearly the same performance as a normal one, but would significantly degrade the performance of student models learned by imitating it. Pros:  This is overall a meaningful direction of knowledge distillation, that makes a neural network  undistillable . The method is simple yet effective by disturbing the output distribution of the teacher network. The experimental results show that the proposed method can succesfully prevent student networks from learning knowledge from teacher network both with original KD method and with data free KD method. Cons:  Since this is a paper that focus on the real world application problem, I would wonder whether the results are acceptable on ImageNet dataset. I believe that there will be a larger gap between nasty teacher and original teacher on ImageNet in order to achieve the same undistillable results. Experiments should be conducted for further explaination.<BRK>Summary:This paper reveals and studies a new problem, that KD is posing a potential risk of intruding the intellectual property (IP) of released ML models or their training data. Even trained ML models are released only in “black boxes” (e.g., as executable or APIs, no open sourcing codes), their functionalities can be largely replicated by KD through imitating input output behaviors. I find this problem and idea very interesting, and of both social and legal importance. Pros:  The self undermining training method seems to be a variant of adversarial training. It maximizes the K L divergence between the nasty teacher logits and those produced by another adversarial network. The authors used a similar idea to born again NNs and self training KD, to generate adversarial attacks from another pretrained network of the same architecture; thus no extra model is necessary. The training method is conceptually simple but effective. Cons:  Even the authors presented an ablation study, it remains unclear to me why this self attack is a good choice?<BRK>A nasty teacher model is a specially trained network that yields nearly the same performance itself, while significantly degrading the performance of student models learned by imitating it. The introduction section motivated the study clearly and nicely. Specifically, my initial expectation after reading the abstract and introduction parts was quite high (those seem to have been well revised and mature). Section 3.1, “a αclose to 1” – “a: should be “an”. “varies as much as…” means “correlates well with”, but your method should expect them to differ from each other. Another issue is while the authors stressed their training algorithm called self undermining KD in the abstract and introduction, that algorithm is only very briefly introduced in the last paragraph of section 3.2   which I nearly missed. From the draft, it is also unclear whether the authors will release their codes and models for reproducibilityOverall, this paper is technically novel and interesting, and can potentially generate a positive impact. But the draft quality is currently unsatisfactory.<BRK>Summary: This paper explores an interesting and novel research problem: how to make a teacher model undistillable. The proposed Nasty Teacher approach is a two stage method, which first trains a good teacher network, then utilizes a self undermining KD strategy to further distill the good teacher network to a bad one. Although the performance decrease is not huge, the proposed approach is promising and can be a very useful baseline for this new research direction. 2.The proposed idea is well motivated and described. 3.The effectiveness of the proposed approach is substantiated by thorough experiments. It is ok to train the nasty teacher, but how about the student network? In the intro the authors argued that protecting “backbox” APIs is one of the main motivations of the proposed method, but this was not tested in the experiments. I believe the effectiveness will be even better if the student can only access probabilities. 4.It seems like the distillation experiments were all done on 100% training data? This setting is somewhat less suitable from model stealing since the attacker may not have 100% training data. Prior works in this field should also be reviewed, if there are any.
Reject. rating score: 2. rating score: 2. rating score: 3. rating score: 3. rating score: 4. <BRK>Summary The paper describes a method for inverse reinforcement learning called stochastic IRL that learns a distribution over reward functions. In that sense, the method is similar to Bayesian approaches, however, the learned distribution doesn t seem to approximate the posterior for any given prior. The experiments compare two versions of the algorithm called SIRL and DSIRL. There are several reasons for this lack of clarity. The results on the objectworld are also not convincing.<BRK>There are lots of such things in the submission. ### ClarityThe readability of the submission is poor and needs to be improved. Experiment settings are unclear, and the results are not confident and seem irreproducible with given information. I think the probabilistic view was originated from Bayesian IRL (e.g., uniform prior on rewards may cover the idea of this work). (p.4, `Initialization`) `~in each learning task`  Do we care about multi task learning or multiple reward weights only? The submission only sets MaxEntIRL as its baseline, but I think Bayes IRL should have been considered. (p.1, `Abstract`) `we generalize the IRL problem to a well posed expectation optimization problem stochastic inverse reinforcement learning (SIRL) to recover the probability distribution over reward functions.`  SIRL tries to solve the inherent issue of IRL problem, not **generalize** IRL. (p.8, `Robustness Experiments`)  I couldn’t understand how the robustness of reward is related to the proposed experiments.<BRK>The experiments are unacceptably small scale and inconclusive. There were confusing parts in the exposition that I will outline below. I agree then with the 3rd point in 2.2.1, this could be a different way to model expert sub optimality than BIRL. You seem to set \Theta_2   W by the end. Then this seems to be very complicated and redundant. In particular, it is very confusing how the dimensionality of the parameter space can depend on the number of monte carlo samples and changes at each iteration ! 3.1.3:   This seems like a straightforward EM derivation. Previous papers have used IRL for real world problems like robotics, vehicle routing etc.<BRK>The paper proposes a novel method for inverse reinforcement learning: inferring a (distribution over) reward functions from a set of expert demonstrations. In particular, I would suggest the following modifications:  + Significant edits to improve clarity. This paper performs maximum likelihood estimation of a parameter for a *generative model* over probability distributions, using a Monte Carlo expectation maximization (MCEM) method. The motivation of the work is good: you identify important problems on IRL in the first introductory paragraph.<BRK>The authors propose an approach to model based inverse reinforcement learning which estimates a Gaussian mixture model over reward function parameters. The authors evaluate the approach on objectworld. It would also be helpful to disentangle the machinery of MCEM from the novel algorithmic contributions of this paper. * DSIRL is used as an acronym for a variant of the method but is not defined in the paper as far as I can tell. The motivation for the use of O_i is that it may correspond to different modes in the expert set, e.g.demonstrations by different experts; however, the assignment of weights to object sets is not consistent between iterations so it is unclear to me how this would be able to handle different experts. * In the beginning it is mentioned that IRL methods require knowledge of the transition model.
Accept (Poster). rating score: 7. rating score: 7. rating score: 6. rating score: 4. <BRK>The method is depending upon a number of hyper parameters (such as the threshold $\varepsilon$ for the visual/semantic neighbor graphs or the softmax temperature $\gamma$). It is not fully clear to which extent the proposed variants of tiered ImageNet, adapted for zero shot learning, would improve upon the existing ImageNet specs. *Pre Rebuttal Evaluation* I think the method is novel and original, and it is adopting a very original direction which is dissimilar for mainstream approaches in zero shot learning. My only questions relate to the computational sensitivity of the proposed method, which needs some further experimental shreds of evidence. *Post Rebuttal Evaluation [FINAL]*I have thoroughly inspected the revised manuscript and read the response provided by authors.<BRK>Final rating  I am satisfied with the author s response and updating my ratings. This paper focuses on improving zero shot classification by reducing the bias of the classifier towards seen classes. Also, did the authors try using the prototypes in an embedding based classifier  Why is the classifier in Eq (8) referred to as a KNN classifier. The mode without any propagation achieves H 70.7 in table2(b).<BRK>Page 8, Sect 6, first sentence:  each space ans across two   > and**Conclusion**This paper proposes a novel method to learn semantic and visual prototypes for zero shot classification, which uses episodic learning. flaws and unclarities in the paper yields my rating of marginally below the acceptance threshold. Note that is unclear why the visual prototypes should be mapped to the semantic prototypes. The reverse is equally explainable. * Eq 2 (and below): a^s and c^s are not defined.<BRK>This paper proposes an Isometric Propagation Network (IPN) for ZSL. Some generative or discriminative ZSL methods proposed in CVPR’20 or ECCV’20 are not compared. 3.More experimental results besides ZSL results and H value should be given to show the advantages of IPN. The explanation of the objective function and the main formula is clear.
Accept (Poster). rating score: 7. rating score: 7. rating score: 7. rating score: 6. <BRK>Summary:The paper considers the problem of Bayesian inference with partially conditioned variational posterior. The paper s main theoretical finding is that the partially conditioned variational posterior behaves like a product of experts, resulting in a degenerate solution. Clearly, the product of densities hardly depicts features of the mixture since a near zero value of a single member is enough for zeroing out the product s density. I find this paper to be a useful piece, both theoretically and practically. My only suggestion would be to include several works into consideration. The problem of partial observability is also important for generative image models [1,2]. Or how one can possibly escape difficulties when full conditioning is not possible on the test stage.<BRK>Specifically, this paper addresses the issue of conditioning in using variational inference to fit sequential latent variable models to data. This seems like an  obvious  insight, but I think that is a strength of this paper. Further, empirically the work demonstrates how to correct for the conditioning gap. This will benefit the research community as a whole, and lead to higher quality variational approximations and papers. For example, there are other divergence measures that do not suffer from the issues presented here  (c.f.https://dataspace.princeton.edu/handle/88435/dsp01pr76f608w).<BRK>[1] Cramer et al.inference suboptimality in variational autoencoders. Through an example with discrete observations the authors derive that when the true posterior is conditioned on the full data, and the approximate posterior is only partially conditioned, the optimal approximate posterior is something akin to a product of true posteriors over the unconditioned information, and not a mixture where the left out information is marginalized out. As the authors also state, several studies have shown that full conditioning on future observations results in negligible performance gains. **Cons**  The authors argue that the conditioning gap is a distinct gap from the amortization gap that was discussed by Cramer et al.[1].It is not clear to me why these two gaps are distinct/independent, I would say the conditioning gap is part of the amortization gap introduced by Cramer et al.since the way that conditioning is handled in amortized inference is essential to the gap between the amortized and non amortized approximate posterior. The effect of the narrow posteriors for partially conditioned approximate posteriors due to it being a product of distributions and not a mixture is not clear in the experiments, even though the authors do hint that this is observed in the qualitative prefix sampling experiments.<BRK>The paper reviews the issue of partial conditioning of the amortized posterior in sequential latent variable models, typically state space models trained with a VAE style loss, but where the posterior used is the filtering rather than smoothing posterior. Equation 5: Technically the left hand side should be the function w, not the particular value w(z) (note z is a bound variable on one side of the equation and bound on the other side). The derivation of the shared approximate posterior is standard variational inference derivations, but it is nice to see it explicitly written, and contrasted with the optimal posterior.
Reject. rating score: 3. rating score: 5. rating score: 5. rating score: 6. <BRK>This paper employs the linear value factorization proposed by VDN and extends universal successor features with GPI to multi agent reinforcement learning. QTRAN and QPLEX can potentially address this issue, but they may not perform well in some tasks that require more efficient exploration. I don’t think it is fair to compare UNEVEN with most of the baselines in this paper.<BRK>But, my impression is that the contribution is not very large. The novel idea is combining these ideas to create multi agent universal successor features. would go a long way toward characterizing the space of domains where this approach is appropriate. The paper is quite good for what it claims to do.<BRK>One of the effect this paper focuses on is that the monotonically factorization lacks the representational capacity to distinguish the values of coordinated and uncoordinated joint actions during exploration. However, these approaches still rely on inefficient greedy exploration which may fail on harder tasks e.g., again, the predator prey task above with higher value of p.This paper applies universal successor features to the multi agent setting (Multi Agent Universal Successor Features MAUSFs). 2.It clearly stated the current research gap in value function factorization. This paper might not fill the research gap it mentioned very well. 2.The author can elaborate more on the experiments from an intuitive view, rather than just stating the experiment data.<BRK>So can this approach really lead to more generic extensions that would work on unknown and general reward functions? However, regarding the general applicability and its practical usefulness, I m not fully convinced yet. Their approach is to extend Value Decomposition Networks (VDN) by combining it with a multi agent version of USFs (Successor Features + reward function based Universal Value Functions), called MAUSFs. ##########################################################################Reasons for score:Overall, I think this is, in general, a sound paper and so would like to see it accepted.
Reject. rating score: 5. rating score: 5. rating score: 6. rating score: 6. rating score: 7. <BRK>Summary The paper proposes a method for imitation learning for goal directed tasks that uses a learned proximity function for computing rewards. Strong theoretical results or a thorough empirical evaluation (with suitable baselines) would both be fine for me. The agent is improved using PPO where the gain in proximity serves as reward function with an additional cost based on the uncertainty about the proximity (estimated by the standard deviation of the ensemble). Strong points   Predicting time from states is an interesting idea and assigning higher reward to later states is sensible for goal directed tasks. RelevanceThe approach is limited to goal directed tasks and thus much less applicable than comparable methods. It would be easy to provide this information also to the adversarial methods by weighting the discriminator samples during training dependent on the time step (in the extrem case by only using the samples form the final time steps). Questions 1. This is not really relevant to the algorithm but still I m wondering whether this claim is correct.<BRK>To accelerate and improve imitation learning for goal driven tasks, the authors introduce a goal proximity function that is learned from the observation of expert demonstrations and online agent experience. The method is simple and looks effective, as shown in the experiment. It is better to clarify the theoretical foundation. The relationship with GAIL is mentioned several times. However, the explicit comparison between the proposed method and GAIL is not given. )Describing the comparison (as an appendix) may help readers to understand the key idea. If we consider "goal proximity function" as a goal related reward function, the method is regarded as the integration of an imitation learning, e.g., GAIL, and a goal driven reinforcement learning. From this view, this work looks related to the following paper.<BRK>**Paper summary**This paper proposes a method for imitation learning from observations based on learning a goal proximity function from expert demonstrations and using it as a dense reward for training an imitator. The authors show that this method improves generalization to unseen states in comparison to several baselines. **Pros:**  The idea is simple, well motivated and the paper is easy to read, clear and well written. Figure 5 and 9 demonstrate that a key component of the method is the adversarial training of the proximity function. In comparison, not using the uncertainty part, or not training offline, makes less of a difference. This seems to indicate that the adversarial part is more important than the temporal aspect of it. 2.Following from point 1, I wonder why are the GAIL results so different (e.g GAIfO). One concern I have is the final reward. In fact, as shown in Fig 4d, the value of $f$ doesn’t end up corresponding to the time (or number of actions) that would be required to reach the goal in the general sense.<BRK>## Paper and Review Summary This paper introduces a goal proximity approach to learning rewards from observations captured during demonstrations in goal oriented tasks. This metric allocates rewards based on the temporal distance to a goal, relying on a model trained to predict this distance. The paper also proposes an adversarial learning approach to policy optimisation with this reward, which helps to improve policies in regions where demonstrations were not captured. Although I do see potential novelty and additions going beyond the work in [Angelov et al.], [Burke et al.] ## Pros   The paper is well written, with detailed experiments and nice ablations  I like the idea of combining a goal progress metric with adversarial learning  The inclusion of the uncertainty metric is interesting, potentially allowing for risk based policy optimisation### ConsLimited novelty:   A linear goal progress metric has already proposed by Angelov et al.in the paper [Composing Diverse Policies for Temporally Extended Tasks](https://arxiv.org/pdf/1907.08199.pdf). Here, the goal progress reward is used for policy optimisation (value iteration for grid world experiments) and online learning. But, in our experiments, linearly discounted proximity consistently performed better than alternatives*"   I am curious that linear proximity models performed better as experiments in Brown et al.and Burke et al.show that non linear temporal progress metrics are more effective. In Equation (2), I see the progress metric is not used directly, instead a derived reward was used.<BRK>SUMMARY:The authors propose a new method for imitation learning from observation that attempts to estimate and leverage a notion of goal proximity in order to help the learning process. The paper proposes a technique with two major components: (a) an estimated proximity function, and (b) a method to exploit uncertainty information in that estimate. This is a fundamental question that must be addressed. (W2) As written, some of the details of the proposed method are not clear to where it would likely be difficult to reproduce. RECOMMENDATION STATEMENT:The proposed method describes a novel approach to a good problem and seems to hold promise. However, I feel that important experimental results need to be added before the paper should be published. It seems as though they should be less proximal.
Reject. rating score: 4. rating score: 4. rating score: 6. rating score: 6. <BRK>The authors examine the commonly used paradigm of not discounting in the policy gradient objective. (1) discounting the critic improves representation learning. These hypotheses are studied through a series of empirical tests in the MuJoCo domain with PPO. Strengths:  I believe this paper is asking the right type of questions about common setups. Overall, there a lot of experiments both in the paper and the appendix, which is detailed. 4.Why does PPO FHTD with H 1024 produce different results for the different parametrizations? This is problematic as many of the claims in the paper are supported by empirical tests where the performance is not strikingly different. 9.I wonder if the result from Figure 9 is reproducible if the flipping was done in a different way. Conclusion:I think the authors present a lot of interesting ideas and experimental approaches to answer their underlying questions. However, I felt that the experimentation was not sufficiently robust to justify their conclusions and I cannot recommend acceptance.<BRK>Specifically, the paper considers the case in which the actor and the critic employ different values of the discount factor. Two scenarios are considered. A quite large suite of experimental results is reported. Can the authors elaborate more on this point? However, I have some concerns about the application of the clipping technique independently for the two terms. The experiments are carried out on Mujoco tasks that are characterized by continuous state action spaces. ***Overall***I think that the paper addresses a relevant problem that is surely important to bridge the gap between theory and practice.<BRK>In summary, while I think the approach is quite interesting, there are concerns in some of the claims made in the text. I appreciate the effort that went into the current set of results and the experimental setup. Strengths:  The paper investigates and draws further attention to an important open problem that does not seem to widely known. However the text indicates that this is still done even in this case. I also think the representation learning experiments in Scenario 1 using FHTD are an interesting approach to study the effect of learnt representations. My second concern is with the method used to choose hyperparameters for the experiments. a large scale empirical study. Addressing function approximation error in actor critic methods. Could the authors clarify why they think this happens?<BRK>In this paper, the authors focus on the discounting mismatch in the Actor Critic algorithm. From comprehensive experiments, the authors claim that this mismatch is either a bias variance representation tradeoff or an auxiliary task for the actor update. Since the discounting mismatch problem is a well known gap between the theoretical analysis and the application, their work, especially the experiments, might have some impact on how to understand this gap. However, since it does not provide any new analysis technique or practical model to improve the performance of the AC algorithm. and Meanwhile, since in the first scenario, the mismatching of $\gamma$ is considered to reduce the variance, it would be interesting if the authors could compare this kind of variance reduction with the stochastic variance reduction on the policy gradient algorithms [1] [2] [3].
Reject. rating score: 4. rating score: 5. rating score: 5. rating score: 6. <BRK>Was cosine selected arbitrarily or is it motivated by some theoretical insight? In its current form, the paper appears to formulate rather simplistic observations as theoretical results with a rather dense presentation. After discussion with authors The reviewer thanks the authors for clarifications. With the confusion from the typos resolved, the paper is easier to follow. The technical correctness of the paper is not in doubt any more. The diversity enforcement has been reported before, and more comprehensive discussion of related work would be useful. x,y are being used as values and the variable. Questions and suggestions to the authors:1.<BRK>Unfortunately, the presented methods for practical applications are two simple regularization terms (of questionable form, see below), and, more importantly the authors are not able to conclusively show any benefits empirically   only MNIST and CIFAR10 show improvements vs baselines, ImageNet is evaluated as the only large scale dataset, but with negative results. This should be evaluated. * The paper reads like two papers glued together (one would be chapter 2 + chapter 4 + appendix E about the regularization term DRT, the other is chapter 3 + appendix C + D about the theoretical certified robustness of the ensemble types with smoothness). I would like to see some results on that    * By increasing the confidence scores further, any improvement in regarding robustness may come at the cost of an increased expected calibration error   this would be interesting to evaluate* Ablation study: an additional evaluation of how beneficial each loss term is for the robustness is missing* Theorem 4 and onwards: The random variable “epsilon” cannot by element of R^d, since a random variable is a function.<BRK>I find the work valuable in the sense that it nicely combines ideas of ensembling and certified robustness. However, it was difficult to follow all the theoretical results and how they motivated the main finding (DRT) was not clear. Below are my comments/questions:  I want to thank the authors for nice summary in Appendix B1 and B2 on certified robustness and their map to ensembles. I wonder how realistic this assumption is. It would be nice to include a discussion on this.<BRK>I wish more papers in the community are written in this way. However, my current evaluation to this paper is a weak accept   it is a bit conservative, but I think it s based on some valid concerns, detailed below. The experimental results, while showed non trivial improvements over single model baselines, may not be very strong. To summarize, despite of the limitations mentioned above, I think this work is overall good enough for recommending acceptance and thank the authors for their effort.
Accept (Poster). rating score: 8. rating score: 7. rating score: 6. rating score: 6. <BRK>The paper analyses the generalization properties of deep neural networks for classification tasks. In particular, the authors study the convergence of gradient descent (and its stochastic version). In this paper, the results are generalized for deep neural networks. I appreciate it. The paper is fluid and nice to read. Here are some remarks that might improve the presentation of the paper:  It would be nice to recall the function $\sigma$ in Section 2  In my opinion the remark: " Moreover, while Ji and Telgarsky (2020)essentially required all training data to be separable by a function in the NTRF function class with aconstant margin, our result does not require such data separation assumptions, and allows the NTRFfunction class to misclassify a small proportion of the training data points",is a big advantage of your analysis. Otherwise it is not very clear.<BRK>  OverviewThis paper greatly relaxed the rate of over parametrization for deep neural networks. Comment.The results are impactful, showing that global convergence can be said to be possible with over parametrization, even in deep neural networks with smaller number of nodes. The paper is carefully written and the differences from existing research and the place of novelty are easy to see. My question is what difference does this make to the smooth activation function case? Is it possible to give an additional analysis of the theoretical limitations of the rates obtained here to see if they can be further improved?<BRK>The paper studies optimization and generalization properties of deep relu networks trained with (stochastic) gradient descent on the logistic loss in the neural tangent kernel (NTK) regime. (it seems like it may compensate the m^{ 1/2} from the initialization of the first layer, which is often not present in other works, but this should be further discussed)  in prop 4.4, it would be interesting to compare the obtained results to a simple linear model in the class NTRF, to see the cost of ensuring good linear approximation: does linear approximation require a larger m than what is needed just for good approximation of the infinite width kernel? more discussions on this comparisons at the end of sections 3.1 and 3.2 would be welcome  the gap in sample complexity between GD and SGD is somewhat surprising (both the 1/eps^2 and the exponential dependency on L), is this just an artefact of the GD analysis in the generalization bound?<BRK>The paper extends an existing proof for the sufficiency of polylogarithmic width for sharp learning guarantees of ReLU networks trained by (stochastic) gradient descent from shallow networks to deep networks. The theoretical analysis links the convergence of GD and SGD to the width of the network. The analysis highly relies on the radius R of the NTRF function class and the authors provide a discussion showing the connection of the requirements on data separability to the values that R can take. Also, since the paper extends a result on shallow networks to deep networks it should be discussed how depth affects the results. The generalization guarantees provided in this paper are vacuous without strict assumptions on data separability, as the authors state. Could the authors argue whether the bound is non vacuous for separable data?
Reject. rating score: 3. rating score: 4. rating score: 5. rating score: 6. <BRK>The paper would greatly benefit from a more formal explanation of how the models are scored, and how the consensus is computed. Why is it necessary to have the models trained from scratch? Points on which the paper can be improved:1. Clarity of description of the method   there is no clear formal definition of how the committee voting works; adding formulas would improve readability.<BRK>The latter improves on the clarity of the approach in terms of the formal presentation of the approach, but a concise problem definition is still missing in my opinion. Here, it is important to establish when the method is actually (guaranteed to be) sufficiently concise, such that it can be used in practice. However, I am left with several open questions for the approach, which I summarize below. In 2019 IEEE Global Conference on Signal and Information Processing (GlobalSIP) (pp.1 5).IEEE.+++ Updates after author response +++I want to thank the authors for their answers as well as their attempts to improve the manuscript. I have read the other reviewers  comments and the updated version of the paper.<BRK>The explanation method is use to produce explanations for every image/sample on a target dataset, and the aggregated explanation from every model in the committee is considered as "approximated/quasi ground truth". limited number of model explanation methods. I would suggest very early on the manuscript defining these terms so that their meaning is clear much later in the body of the paper. Moreover, this seems to actually point to the definition of one of the ambiguous terms mentioned in the previous point. It seems that "interpretation result"  actually refers to the explanation produced by a given sample. This still sounds like a expensive requirement. In the comparison against the Network Dissection method (Sec.3.3), it is stated that the rankings produced by the proposed method are quite similar to those of Network Dissection, but with differences between the ResNet152 and DenseNet161 models.<BRK>The paper seems to introduce a very important model to evaluate intepretatbility of neural networks. However, the paper is not extremely clear and intepretable. The procedure and the model is described in a single section, that is, section 2. It is extremely obscure the way Consensus operates. Can you use it to explain your model?
Reject. rating score: 4. rating score: 4. rating score: 5. rating score: 5. <BRK>The authors propose a discriminator based approach to inverse reinforcement learning (IRL). The discriminator function is trained to attain large values ("energy") on trajectories from the current policy and small values on trajectories from an expert policy. They follow up with demonstrating better performance of their approach compared to certain baselines when tested on a number of tasks on Physics simulators. On reading this, I first wondered what happened for negative values of $D$, but equation (4) seems to suggest $D$ is positive at all times? This should be spelled out more properly, and $D$ should be introduced before it is first used in equation (2). That said, I believe the proposed method is likely to be a good one and will make a worthwhile contribution to scientific progress on IRL.<BRK>This paper proposes a learning framework for imitation learning (IL) that uses an energy based objective for generative adversarial imitation learning (GAIL). On the empirical end, the baselines are very weak in the context of the current work. The paper is easy to understand and follow.<BRK>EB GAIL is an architecture for training generative adversarial imitation learning, where the discriminator is an autoencoder with the energy being the reconstruction error. WeaknessThe novelty of the article is unclear. Theoretical results are marginal as they are mostly from previous works.<BRK>The authors may want to explain these questions in more detail. The authors also provide some theoretical analysis to show the proposed measure can match the occupancy measure of the expert policy. Some key baselines are missing. Learning robust rewards with adversarial inverse reinforcement learning.
Accept (Oral). rating score: 7. rating score: 7. rating score: 10. <BRK>In contrast with previous literature that consider the case where the patterns / data features to memorize were uncorrelated, the authors extend the mapping to arbitrarily correlated patterns, which allows to consider much more realistic settings. This paper should be considered as an applied one, as there is no real analytic theory of why this mapping helps the learning, but the experiments are well carried: the boost in learning is demonstrated through experiments in MNIST data, and the results are well explained and convincing. Overall the paper is well written (the paper can be used by non specialists also as introduction to Hopfield NNs and RBMs), the results are interesting and relevant to the ML community, the paper can be read without much effort. The paper is overall very well written. I recommend publication after slight corrections, see below.<BRK>update  I thank the authors for providing such detailed response. ## Strong points:+ I am not familiar with the literature, but the results seem new to me. I hope to see this paper accepted. and the results raise several questions to be addressed. # RecommendationI m in favour of rejection, but some concerns can be addressed fairly easily (with experiments) so I m open to raising my score if questions are well addressed. How about increasing the batch size so that there is little SGD noise? This also applies to Figure 5. # Detailed suggestions (not to affect decision)* Reference to RBMs should include more historic ones from Hinton (e.g.2006)* I do not see the purpose of (7) and (8), and they are only referred to in the Appendix (the review content in the Appendix is informative by itself though).<BRK>The paper demonstrates a mathematical equivalence between Hopfield nets and RBMs, and it shows how this connection can be leveraged for better training of RBMs. What a great paper   well written, an enlightening mathematical connection between two well known models that to my knowledge was not previously known. I really enjoyed reading the paper, I learned something new, and I think others will too! It is an important advance in our understanding of Hopfield nets and RBMs.
Reject. rating score: 6. rating score: 6. rating score: 6. rating score: 6. <BRK> Summary : Disentangled representation (DR) of data is useful in downstream tasks. However, VAE based DR fundamentally suffers from a trade off between high quality reconstruction images and disentangling. The widely used techniques in style transfer (FILM and AdaIN) are used to adopt  Z  into  Y  for a high quality reconstructed image. I suggest the authors provide comparison experiments with this kind of sequential learning in representation. Also, an ablation study on the proposed method without AdaIN and FILM would increase the persuasion of readers and provide justification for the proposed method. At a high level, the overall method is to learn the representation of an original image which helps to transform a low quality reconstructed image, which is trained in VAE, to a high quality reconstructed image. Also, the experiment results in figure 4 show that disentanglement was not improved and only the quality of the image was improved. Finally, if the ultimate motive of this paper is to learn the representation which can reconstruct the original image fairly well, the comparison with the VAE based Method([2], [3])  which generated a high quality image should be added. For the disentangled representation which is used for future downstream tasks, there is no big difference with \beta TC VAE in the best hyper parameter setting as shown in Figure 6.<BRK>This paper proposes a multi stage approach to learning disentangled representation that refines the low quality reconstructions of a state of the art architecture using a second deep generative model that learns how to modify these reconstructions with additional correlated latent variables. Despite its simplicity, this method makes a noticeable difference in the experiments, so it could be very useful trick for other researchers working in the area of learning disentangled representations. My main concern for this paper is the motivation of the proposed approach. The need for a multi stage approach is justified using d separation, saying that in this way independent and correlated latent variables (c and z) are completely separated during training. An interesting ablation study that would allow to better understand the mechanics of this technique would be for example using the same split of c and z and the same FiLM + AdaIN decoder, but training end to end placing the beta factor as in the beta VAE only on the c latent variables, with for example a schedule for beta in which independent latent variables are more important at the beginning of training, or some alternating updates.<BRK>This article introduces a method for learning high quality generative model with disentangled latent representation by splitting the learning process into two steps. The first step consists in learning a generative model on the data using a method with strong disentanglement constraints, producing a low quality generation. As a second step, a conditional generative model is trained to turn this low quality sample into an high quality one. However, I have a few questions, and I think these points should be clarified in the text of the article:1. The previously observed issue of models ignoring their disentangled latent and storing everything in the correlated one does not applies as, this is the point of your model, Y is here treated as if it was part of the dataset. 2.As a follow up of the previous point, it would seem natural to expect that an abstract value for Y would potentially be more easily used by the conditional model than a low quality reconstruction. If on the other hand Y0 and Y1 are indeed directly learned from θ, then it should be clarified.<BRK>The paper studies the problem of learning disentangled representations while maintaining good data reconstruction. Then a hierachical generative process is proposed, where the first stage is to reconstruct a preliminary version of the data given the disentangled representation C, and the second step is to reconstruct a full version of the data given C and correlated representation Z. Experiments show the approach can learn disentangled representation C as well as the previous β TCVAE model, but improves in terms of the reconstruction quality. It is also shown that the second stage can properly maintain the conditioning on C (instead of conditioning only on Z). A potential shortcoming is that the two stage components are learned separately instead of jointly, which could results in suboptimial learning.
Reject. rating score: 4. rating score: 4. rating score: 7. rating score: 7. <BRK>There exists a prior work that bridges a gap between the two exploration methods for linear policies, and this paper generalizes the prior work for various deep RL methods: on policy (A2C, PPO) and off policy (SAC). Finally, Deep Coherent Exploration enhances the performance of baseline algorithms and has better performance than prior works (NoisyNet, PNSE) on Mujoco tasks. The paper focuses on exploration, but the experiments only focus on the return performance of simple Mujoco tasks. In order to show the superiority of the proposed method, additional experiments on pure exploration or sparse rewarded tasks are needed. Minor concerns:* In background, there is no explanation about step based and trajectory based exploration.<BRK>Summary: This paper focuses on undirected exploration strategies in reinforcement learning. *The proposed method significantly outperforms the baselines when investigating the on policy methods A2C and PPO. Cons:*In section 4.2, "we maintain and adapt a single magnitude σ for the parameter noise". *In section 5, why the advantage of the proposed method is poor with SAC? *In section 5, apart from the comparison of the performance of the learned policy, the comparison of the complexity (which might be measured by wall time to learn the policy?) It will be better if there are experiments on other more complicated domains.<BRK>This paper presents a method to combine step based exploration with trajectory based exploration (in the form of action space noise and parameters space noise) in continuous MDPs, which is scalable to deep RL methods. The Introduction and Related work sections are good and clear. I think it would be unhelpful for the reader not already familiar with the discussed algorithms, and on the other hand redundant for those familiar with them.<BRK>Summary of the paper The basis of this work is van Hoof et al., 2017; there, “Generalized Exploration” views policy parameters as being drawn from a per trajectory Markov chain. The authors of this work introduce “Deep Coherent Exploration”, which scales to deep reinforcement learning methods. 3.Detailed recipes for incorporating the method into on policy methods (A2C and PPO) as well as an off policy method (SAC). Assessment This work explores an important problem in RL and proposes a promising method that would be of interest to many in the community, and I think it would be a valuable contribution to ICLR. Additionally, the ablation experiments were very insightful. I tentatively score this paper as accept, and looking forward to the rebuttal to calibrate with the other reviewers.
Reject. rating score: 4. rating score: 5. rating score: 5. rating score: 6. <BRK> Summary The paper proposes a method for the sequential meta learning problem. To this end, the meta learn model finds appropriate model parameters and adaptive learning rate vectors that capture task general information. Overall experiments are performed on few shot meta learning settings with sequential domains (datasets). Cons   The approach is too simple (Adding learnable strength vector weights for gradient update of conv.blocks) and heuristic. There is a lack of concrete insight into how does adaptive learning rate weights mitigating forgetting. The majority of recent impressing CL works considers further realistic and applicable to the broader areas, called class incremental learning problem that task oracle isn t given during training/inference of the model.<BRK>The authors propose a new instantiation of Continual Few Shot learning (CFSL) with multiple domains coined sequential domain meta learning. Mainly, the setting needs further motivation and clarifications, and the method needs some justification as well as stronger performance. The authors claim that these are aligned with the newly proposed setting. I suggest the authors remove it or move it to the Appendix. However, this is not true for [2] which operates in multiple domains within one experiment, e.g.the Omniglot / MNIST / Fashion MNIST experiment. Also, [3] is a Meta Continual learning method and not an incremental few shot learning one. I suggest they use them to improve the paper. 2.2 The authors should acknowledge that learning the learning rate (or double adaptation) is a well known trick now. The gains are however relatively small.<BRK>This work focuses on sequential adaptation of a model without forgetting. To that end, they introduce a problem setup where the model receives a sequence of few shot tasks from different domains. I am not sure I entirely understand that the motivation and the proposed algorithm in this work. How does adaptation of learning rate address catastrophic interference? Maybe, it is helpful to provide a real task for this setup. Having that said, I appreciate the authors  effort put into evaluating multiple baselines. Comments and questions: What is the value of N (i.e how many tasks were shown to the model?)? Recently, it has been shown that catastrophic forgetting often occurs when transferring a meta learning model to a new context. Some missing references: “Schmidhuber, J.<BRK>There is no need to make the motivation more intricate than that. The first is a new online meta learning problem setting where the meta learner acts on a sequence of few shot learning *domains*, as opposed to tasks within a single domain. The second is a method for meta learning with this form of domain shift. SummaryAt the heart of this paper are two separate contributions. Cons:  Unclear claims to novelty. Typos:  catastrophe forgetting  > catastrophic forgetting  scarifying  >degrading(?) Main concerns:  *Contributions:* The authors list three contributions. Of these, the first relates to both adapting parameters and learning rates and the second relates to using layer wise meta learned learning rates. This is not described in the algorithm, where it appears as if all model parameters are being tuned to the current task and all layer learning rates are being meta learned on replays of previous tasks. At the end of the day, the paper proposes to meta learn over a sequence of few shot learning domains, as opposed to a sequence of tasks from the same domain.
Accept (Spotlight). rating score: 7. rating score: 7. rating score: 6. <BRK>Summary:The paper considers several types of CNN and proposes convex reformulations for non convex problems of training these networks. As a result a polynomial complexity is shown for the training problem. The results are also interpreted as implicit regularization induced by the choice of the architecture. Finally, numerical experiments are made to support the theoretical findings and show that in the predicted regime, SGD for the original problem converges to the global minimizer given by the convex reformulation. Pros:1.Good to know that many CNN architectures can be trained in a polynomial time and which implicit regularization they introduce. 2.In comparison to (Pilanci & Ergen, 2020) the exponent of the polynomial in the complexity bound could be much smaller than the dimension. In this sense, as far as I understood, the convolution operation helps to get better complexity estimate. Why the first reparametrization is equivalent? Why strong duality holds?<BRK>The paper studies the non convex optimization problem of training CNNs with ReLU activations under different choices for the CNN architecture, and shows how these can be framed as convex problems with a poly time complexity w.r.t.relevant variables. The derived convex problems provide valuable insights on how the CNN architecture induces different weight regularizers by giving them in explicit form   these show a rich connection between the architecture and regularizer. I believe the contributions are significant and two fold. First, the convex problems are solvable with reasonable time complexities (polynomial in all relevant parameters) even for networks with two non linear layers   this is in contrast with the cost of optimizing ReLU fully connected networks, where an exponential dependency on the input dimension if the data matrix is full rank, hence global optimization of CNNs seem more likely to have practical implications than that of fully connected networks. The condition for strong duality to hold seems quite restrictive, though, and further discussion on it would be valuable. Second, it further expands what is currently known about how network architectures induce different regularizers on the parameters, showing that precise characterizations for CNNs with ReLU activations. The fact that the theoretical results hold for classification losses is also valuable, and the experimental results are very appreciated. I think the CIFAR experiments in Appendix A.2 are more interesting and relevant than the MNIST one in the main body (esp.considering the time cost of convex opt. I still believe that the paper provides valuable contributions and insights.<BRK>[Summary] This paper focuses on training convolutional neural networks (CNNs) by using convex optimization techniques. By taking the dual of the nonconvex training problems, (and the dual of its dual), the main contribution of the paper is to show the strong duality between the convex problem and its original nonconvex training problems. This result has been proved for multi layer CNNs with one ReLU layer and three layer CNNs with two ReLU layers. [Pros] This paper presents interesting results on the convex equivalent formulation for the training problems in deep learning. This result could potentially lead to new algorithms for training the network. The convex formulation also reveals the hidden regularization property of the original nonconvex training problems, such as the group L_1 norm regularizer. [Cons] 1.This paper is an extension of previous work (Pilanci & Ergen, 2020) that provides an exact convex formulation for training two layer fully connected ReLU networks. This paper extends this work to CNNs, multi layers, but still has a very strong limitation on the number of ReLU layers: only one ReLU layer for multi layer CNNs and two ReLU layers for a three layer CNN. 2.Since the modern neural network is often over parameterized, which is also true in Theorem 2.1 as the number of filters $m$ needs to be relatively large compared to the number of data points, the nonconvex training problems can usually be solved to a global solution, either in practice or in theory under certain conditions. For example, only n   99 training samples are used in the MNIST experiments, which makes the results not that convincing. Why only $m^*$ filters can be constructed from the solution of the convex problems in Theorem 2.1?
Accept (Poster). rating score: 7. rating score: 7. rating score: 7. rating score: 6. <BRK>The effectiveness of optimizing f divergence with label noise is also verified by a set of numerical experiments. I think that it would be better if the authors include additional explanations (in the abstract and introduction) that the f divergence measure can be understood as the f mutual information in this problem formulation. ##########################################################################Reasons for score:  I vote for accepting. The manuscript provides theoretical analyses of the f divergence optimization with noisy labels.<BRK>The authors study optimising f divergence for supervised learning in the context of label noise. It is unclear what h is used for experiments and what variational function was used exactly.<BRK>The authors derive that the f divergence can be decoupled into variational differences defined on distributions without noise and a bias term introduced by the noise. For example, analyze the input of the g function to verify the first conjecture. If the authors could explain the assumptions on the label noise that the work is based on early in the introduction and Section 2, the presentation would be improved.<BRK>Overall, I found the paper interesting with solid experimental results, though I still have some questions of the theoretical principles of the work. I see in Section 2.2 special cases for when this is true, but can f mutual information return a very bad solution when these conditions are not true? The authors use the variational form of f divergence to optimize their loss function. They consider an approach where they optimize the mutual information between the predicted label and the noisy label (or more generally, a variant of mutual information defined by an f divergence).
Accept (Poster). rating score: 6. rating score: 5. rating score: 5. <BRK>In this paper, the authors propose a new loss function to learn feature representations for image datasets that are class imbalanced. The loss function is a simple yet effective tweak on an existing supervised contrastive loss work. A number of empirical tests are performed on long tailed datasets showing the benefits of the proposed loss in beating state of the art methods. Is Figure 1 hypothetical ("desired outcome") or real (based on actual observations)? 2.Minor style comment: please don t call your own contributions "important" : )   that is for others to decide. 3.Eqn.(3) is not clear at all   please provide an intuitive explanation and motivation. It seems to appear out of thin air. 4.Is there a tradeoff between accuracy and balancedness? Figure 2 seems to suggest so. For example, if we did not train CE loss to maximal accuracy, would be automatically get the balancedness property? And similarly, how about CE + KCL? 7.What is the ImageNet accuracy of KCL?<BRK>**Overview:** The paper presents experiments showing that the contrastive learning losses produce better embeddings or feature spaces than those produced by using binary cross entropy losses. The experiments show that embeddings learned using contrastive learning losses seem to favor long tailed learning tasks, out of distribution tasks, and object detection. The paper also presents an extension of the contrastive loss to improve the embeddings. The experiments in the paper  use common and recent long tail datasets as well as datasets for object detection and out of distribution tasks. Overall, I think that learning a feature space improving the learning from these imbalanced datasets is an interesting idea. However, I think the clarity in the experiments is insufficient, see below. I have several concerns with the experiments:1. The *balancedness* metric in Eq.(3) may not be a robust metric for measure performance. The reason I am not convinced about this metric is that if the accuracies of the classifier are low but equal, then the metric will say that the *balancedness* is good. I think a good metric for a classifier learning from an imbalanced dataset is one that indicates if the overall accuracy is high, maintains the many shot the classification accuracy high, and increases the accuracy of the classes in the tail. I think if the experiments would ve been stronger if they included results of a trained linear classifier on the learned embedding and still showing good results, then I would be more convinced about the impact of a contrastive loss. In practice, it is challenging to have a balanced dataset as the paper used. The main question is about the performance when training a linear classifier from a long tailed dataset using the learned representation. While I value the goal of using different datasets varying the imbalance in a dataset, I am not convinced that ImageNet LT is the dataset to use. The reason is that ImageNet LT is a synthetic long tailed dataset. In fact, while the dataset shows imbalance, it does not necessarily follow a power law distribution. I think the generation of these datasets in the experiments should be done using a power law distribution. From the text, it is unclear how the datasets from ImageNet LT were generated for the experiments. Minor concerns:*Plots lack information*.<BRK>Summary:  This paper made a key observation that the self supervised contrastive learning methods perform stably well even when the datasets are heavily imbalanced. As for the method this paper proposes a classed balanced version of supervised contrastive loss (Khosla et al., 2020). Extensive experiments demonstrate its superiority on multiple recognition tasks. Pros:  The paper is well written with nice figures. The paper conducted experiments on extensions other than image classification, which provides some useful insights. Cons:  The advantage of using self supervised learning to learn a representation to combat label bias issue in imbalanced problems has been discovered and validated by a previous work [1]. I understand that [1] was accepted just a few days before the DDL of ICLR. Unfortunately I still have to devalue the contribution of this paper as it seems to me that the major contribution of this paper relies on the finding that SSL can help to alleviate the issue of label bias. In terms of large scale experiments on ImageNet LT and iNatualist, it seems that this paper has no advantage over [1]. Additional Questions and Concerns:  Why does the author use cosine lr schedular? 4?Why is there only 10 classes? NeurIPS 2020 post rebuttal updateI appreciate the discussions between the authors. I plan to keep my original score, for the reason that, at least in my point of view, the difference of the two methods is subtle and it is not clear whether the subtle difference results in drastic improvement.
Reject. rating score: 4. rating score: 5. rating score: 6. rating score: 6. <BRK>This paper extends on the existing network slimming approach, computing a heterogenous width for each layer. The layer width is computed by solving a multi objective optimization problem based on pareto distribution. Pros:The paper is well motivated and well written. The experiments are run across mutliple datasets and models. And also compared with existing approaches in the literature. Cons:The main aspect lacking in the paper is the novelty. So, I find lack of novelty and do not learn much or take away much reading this paper. The results in Mobilenet is slightly better as compared to NS   ~1 to 1.5% improvement, which is not significant, in my opinion.<BRK>This paper proposes a new multi objective optimization method for slimmable neural networks and jointly optimizes network architecture and weights. However, the use of multi objective optimization methods for networks is not new. The advantage of this paper over existing work in this regard is not clear.<BRK>This paper proposes a multi objective optimization approach to jointly train both channel configurations and shared weights of slimmable networks. The results experimentally show that the channel optimization can improve the performance and efficiency of slimmable networks. This paper is logically organized and the motivation is based on sufficient theoretical supports. A few constructive criticisms or concerns as follows:1. Detailed theoretical analysis on the discrepancy between approximation and the ideal objective had better to be provided.<BRK>The paper directly extends the “Slimmable network” by using per layer width multipliers to allow more flexible network configurations. PareCO mainly aims to optimize both width multipliers and shared weights while considering Pareto optimal. Compared to previous work (SLIM), PareCO consistently achieves better Pareto optimal performance. Related works are well established. Additional techniques, such as storing query for BO, binary searching, restrict to 1000 configurations, makes the proposed algorithm practical. The experimental result seems quite promising. In my opinion, the novelty of the paper is not heterogenous width multiplier nor supernet like training. Many works state that per layer channel sparsity is important, and the idea of per layer width multiplier is not new. Several works in NAS target Pareto Optimal architecture. Some use Bayesian optimization. Especially for a big dataset like ImageNet.
Accept (Poster). rating score: 9. rating score: 7. rating score: 6. rating score: 5. <BRK>The main contribution of the paper is the development of an *induced compressor* that takes as input, a possibly biased compressor, and outputs an unbiased compressor with similar error variance as compared with the original compressor. Cons: > I would like a more detailed discussion on the effect of the unbiased compressor, that is used in the development of the induced compressor. It shows that there are good unbiased compressors and provides a recipe for constructing them. On reading the author s responses to my (and other) review comments, my recommendation remains unchanged.<BRK>The problem is clearly described and the solution is well motivated. 2.The flexibility of the proposed approach which makes it amenable to the extensions  in Section 4 is not illustrated in the experiments which are limited to a direct comparison of the proposed approach with EF. **Comments after Author Response**I thank the authors for their response. My opinion of the potential of this paper to encourage further discussion in this area is unchanged. I can already see from the response on choice of biased and unbiased compressors to combine that there is plenty of scope for future work that builds on this idea.<BRK>Cons:  The experimental evaluation is not convincing. Specifically, the authors proposed a technique that transforms any biased compressor (e.g., Top k) into an unbiased one, referred to as induced compressor, with better convergence guarantees and properties than the former. Pros:  The contribution of the paper is very nice. The dimension "transferred bits" is missing in the experiments.<BRK>The paper provides a new theoretical analysis for DCSGD, with weaker technical assumptions and tighter bounds. 2.The induced compressor proposed in the paper can transform any biased compressor into an unbiased one, which can then be used in various existing methods designed for unbiased compressors. How do those analysis methods used for finding convergence rates in two algorithms compared to each other? How can we make sure that the improvement is caused by the algorithm itself, but not the theoretical bounds? I suggest the authors citing these papers and highlighting the differences. Is it because of the additional unbiased "error feedback" the induced compressor can provide?
Reject. rating score: 5. rating score: 5. rating score: 6. rating score: 7. <BRK>The authors propose a theoretical setting for context agnostic few shot learning. Also, I notice the authors mainly use two simple types datasets, traffic signs and hand written characters, while I am expecting to see the performance on more realistic datasets, such as ImageNet. Delta encoder: an effective sample synthesismethod for few shot object recognition. I do agree with the AC s comment that *I don t see a drawback the use of only one sample and no info about the target, rather I d like to know if the proposed method could use more than one sample in order to make the comparison with SoA methods fair and still be competitive or outperforming them. * Also,  I would recommend the authors to have more experimental validation and resubmission.<BRK>The paper defines the task of context agnostic learning and proposes an algorithm to solve the problem while assuming the ability to sample objects and contexts independently. They propose to decompose factors contributing to the risk into two, context bias and object error. The proposed method achieves promising performance on two synthetic classification tasks compared to other existing methods for few shot learning and domain adaptation, which requires more labeled or unlabeled data during training. ) However, if stochastic gradient descent is used for optimization, I think it is unlikely because the model changes continuously. Relating to the above point, I also have a concern about the experimental validation. It is not guaranteed that the proposed heuristic sampling strategy generalizes to other gamma functions. ) I think evaluation on additional datasets with different characteristics (such as CIFAR 100, Caltech 256, CUB 200) would be necessary. )<BRK>This paper studies a theoretical setting for learning models whose predictions are independent of background signals. 2.The proposed approach is very simple, yet effective. This method is able to learn a context agnostic model by minimizing a formally defined notion of context bias. 3.On the theorical side, the authors try to explain their goal of classification as learning to extract reliable signals. The proposed method has a strict restriction: The training sets must have a single synthetic image for each object class with no additional information about the target domain. 2.The quality of the paper will be upgraded if the authors further investigate more data augmentation or style transfer methods in related work and ablation study. Since the context and object concepts are similar with the style and content in some GANs and transferring works. 3.The experiments seem a little bit weak. The experiments are only conducted on two simple datasets, i.e.GTSRB and MINST. 4.The comparison with existing methods is insufficient. Though this setting is novel, we can conduct task by slighting changing existing methods.<BRK>This paper proposed a context agnostic learning approach that combines an object area and a context image (s.t.background image) to generate input synthetic image and train the model as context independent. When applying this method on the task of traffic sign recognition, character recognition, it provides enhanced performance in a setting where model is trained using synthetic data and evaluated on real world dataset. My concern is the presentation. However, it is very difficult to understand how to achieve "context agnostic" in algorithm 1 and 2. If it is correct, how can the bias be corrected just by training with other objects? In addition, the effectiveness of the proposed method can be verified with such simple tasks, but it is advisable to add a discussion about more complex tasks such as object detection that requires more diverse contexts. This will be more interesting for many. I can recognize some technical contributions that can appeal to many ICLR researchers. As a minor correction,In Section 3.2, the object error o^ is defined but not used anywhere. I have read the revised manuscript and the comments of all reviewers. For concerns that the experiments are not sufficient to validate the proposed method, I am leaning to the author s rebuttals that the datasets have been chosen to meet the heretical assumptions of context agnostic learning. The given datasets seem to be sufficient to demonstrate the effectiveness of the proposed learning method. Therefore, I will not change the rating.
Reject. rating score: 4. rating score: 6. rating score: 6. rating score: 6. rating score: 6. <BRK>Summary:The authors apply reinforcement learning to automated theorem proving to eliminate the need for human written proofs as training data. They show that it enables the prover to outperform a supervised learning baseline without using human proofs. The proposed method outperforms the baseline on the HOList benchmark. The paper is well written overall, though some sentences are difficult to parse, e.g.,  "comparison OF the effect OF availability OF human proofs" in the intro. Weaknesses:The method is not new. There exists a body of work leveraging reinforcement learning to learn theorem provers [A, B, C, etc.]. Besides, using TF IDF for premise selection is not new. It would be great if the authors can further clarify the difference with Gauthier et al.<BRK>This paper aims at improving high order theorem proving using deep reinforcement learning. The key idea of this study lies in the choice of premises for helping the prover. The sublist of exploration premises is generated using a standard TF IDF metric. Overall, this approach is conceptually simple and the experimental results look promising. Yet, I am not entirely convinced that this study provides a novel contribution since it is essentially using well known techniques. As a minor comment, it would be nice to give more details about the (higher order) language used to define expressions. Online learning to rank with top k feedback.<BRK>The authors propose an RL approach to theorem proving focusing on improving the exploration aspect and demonstrates empirically that the proposed approach can perform well without requiring human proofs. The paper is easy to read and the presentation is clear. The authors address a very challenging issue of exploration in RL. The idea is itself not new but the way it is used in the RL setup, together with the presented empirical results, are relevant contributions. With respect to the results in Figure 4, I wonder whether the authors can provide additional details in terms of the computational cost involved in each approach. One would also imagine that with enough time resources, even zero reference can eventually match zero explore in validation score, but at what price?<BRK>In this paper, they tackle the challenge of learning to automated theorem (ATM) proving without any human imitation. Specifically the problem they focus on is premise selection. They find that while a vanilla RL scheme for ATM without any imitation learning of existing proofs frequently gets stuck and is not able to prove very many theorems, when a portion of the premises are selected via a simple term frequency inverse document frequency (tf idf) rule,  it dramatically increases the fraction of theorems proved — approaching the fraction of theorems proved with imitation learning of existing proofs. While the approach is simple, the authors convincingly demonstrate that it is effective and it’s certainly interesting to learn that it is effective in this novel and highly important context. The authors also do several ablation experiments to thoroughly evaluate the components of the system. (2) Is the allowed maximum runtime the same for the systems compared with and the new system?<BRK>The computational complexity and cost of the proposed method is not discussed in detail. Since the method is proposed to reason in large theories, the training efficiency should be important. 2.The paper provides a side by side comparison of the effect of the availability of human proofs on the final theorem proving performance. 3.Experimental results show that the theorem prover trained without human proofs outperforms provers that are trained only on human proofs.
Reject. rating score: 5. rating score: 5. rating score: 5. rating score: 6. <BRK>  Summary  The paper concerns model based meta RL. The paper bounds the gap between the expected reward of a policy in the actual POMDP and the estimated model and then theoretically shows that this gap can be reduced when using dyna style / branched rollouts instead of full rollouts under the learned model. When to trust your model: Model based policy optimization. Both the theoretical analysis and the proposed algorithm are sound. Both the theoretical and the algorithmic contribution of the paper are small. Overall, the paper is a straightforward extension of [1] to POMDPs with meta RL experiments.<BRK>For one, the authors propose a meta model based RL algorithm working with a meta policy and a meta model. Nevertheless, I am concerned about the amount of novelty, this is an extension to POMDPs with a correction to the theory. What separates this paper is the extension to POMDPs and meta learning formalism. The writeup is dense, although the idea behind is simple (branched model rollouts in POMDPs). Any explanation for that?<BRK>So I feel the novelty in the proposed algorithm is limited. It provides a theoretical relation between true environment returns and the returns from learned models in a POMDP setting. And the idea of using branched rollout to control model errors is not first proposed by this paper either. When to trust your model: Model based policy optimization.<BRK>### **Summary and Contributions of Paper** This paper modifies the work "When to Trust Your Model: Model Based Policy Optimization" (Janner, 2019) to handle the meta learning case. The main contribution seems to be this modification, but all other techniques of the paper (e.g.defining the correct terms such as a _branched rollout_, generating the concepts such as C(eps, eps ), etc.) ### **Strengths**  Contributes to the literature in model based techniques in the meta learning setting, which is generally lacking. Paper shows that a small modification of (Janner, 2019) using an RNN can solve meta learning problems in RL.
Accept (Poster). rating score: 9. rating score: 7. rating score: 6. rating score: 6. <BRK>## Summary of the PaperThe paper provides a potential theoretical explanation of the known empirical observation that cold (or tempered) posteriors improve predictive performance of deep Bayesian neural networks. The empirical results agree with the predictions. To my knowledge, it was found out only recently that Bayesian neural networks systematically outperform their point estimated counterparts if the prior is made artificially sharper than what probability theory would predict (i.e., by "lowering the temperature"). This empirical result has been puzzling from a theoretical perspective, but the present paper provides a simple potential explanation for this effect. I believe the findings in this paper go beyond a theoretical justification of an empirically known fact. Validations and tests sets are typically curated in the same way as the training set in the machine learning community. However, when models are deployed in the field, they typically see uncurated data points. I would be curious to know if explicitly modeling the curation process, as the authors do in this paper, would also address this issue. Specifically, what changes for a model trained on curated data when it is either (a) tested on an equally curated test set or (b) applied to uncurated data in the wild? Would the optimal $\lambda$ during training differ between cases (a) and (b) or would the posterior have to be changed after training? I think both would be fine but I would be very curious to know how the figure looks with real data.<BRK>This paper addresses the perplexing issue of cold posterior having better predictive performance than the ideal Bayesian posterior in Bayesian deep learning (Wenzel et al., 2020), and offers a possible explanation in terms of a mis specified likelihood function that deviates from the true generative process of the data. 1.The paper is well written and motivated, the method appears sound (but see questions below), and concepts are explained in a clear and pedagogical manner. Cons:This might be due to my limited understanding of the paper, but I think there are still some limitations to the paper s proposed theory, e.g., it doesn t explain the observation that extremely cold posterior (λ  > 0) doesn t seem to hurt the performance of BNN (which should, according to the proposed theory, as there is only one optimal temperature λ   1 / S, where S is the true number of underlying labelers), and more below. My biggest confusion is this: the paper argues that it s incorrect to assume a simple categorical likelihood p(y|x) as it doesn t take into account the data curation process; however, under the extended likelihood model as proposed, when conditioning on the event that y! 2.Since point estimation with SGD optimizes the same likelihood function, why don t we observe the tempering effect in SGD? Is there an explanation for this?<BRK>The authors propose the idea that cold posteriors in Bayesian neural networks could be caused by the likelihood instead of the prior. They argue theoretically that the curation process of popular benchmark data sets would lead to a different weighting of the likelihood in the posterior. They show in some experiments that the cold posterior effect can be reduced when accounting for this. The theory suggests that the optimal posterior performance should be achieved at lambda S, that is, cooling down beyond that point should deteriorate performance again. This is an interesting prediction, since it does not seem to fit the observations in [1]. Moreover, for a paper that is proposing a statistical theory of tempered posteriors, works such as [2] and [3] should probably be mentioned.<BRK>The work propose a theory suggesting that the cold posterior phenomena arises solely due the the curated nature of image benchmarks. A generative model is proposed where multiple annotators label datapoints, and only unanimously labeled datapoints are accepted into a dataset. This theory is studied under a toy problem using VI and a relabelled version of the CIFAR 10 test set with SGLD. However, many questions remain unanswered and the proposed theory is not sufficiently studied. Q: The cold posterior problem was highlighted in the SGMCMC case, but this work s main toy problem only explores tempered posteriors as prevalent in VI. It would be beneficial if the work highlights why these results should extend to the cold posterior, or better yet, run experiments in this scenario. It should be an easy addition to study this connection for a range of values for S to explore if this holds. Q: The main experiment presented in figure 5 is missing some important ablations: what happens when the CIFAR 10 baseline is trained under the same conditions (learning rate) as CIFAR 10H? This seems like a problematic shift in data distribution. Q: the theory of dataset curation is interesting but makes a broad claim. The sole focus here on CIFAR 10 provides too little evidence. At this temperature the baseline performs just as well. The core idea proposed in this work is thought provoking and contributes to the discussion on this topic.
Accept (Poster). rating score: 7. rating score: 7. rating score: 6. <BRK>Summary: The paper introduces a defence for adversarial attack based on minimising a self supervised loss on the test examples. Authors work under the assumption that minimising the self supervised loss would be equivalent to minimising the supervised loss (to which they don t have access at test time). Strengths:  The paper address the important topic of adversarial defence. Given the numerous adversarial attacks that have appeared in the last years and the deployment of novel classification methods into real world tools, I believe the field of adversarial defence is relevant. Authors use in a smart way the self supervised loss which is traditionally used for learning good representation as pretrainining networks. Authors are also able to compute its own budget using the self supervised loss, which I believe it s additional evidence that the usage of self supervised learning for adversarial defence is interesting and useful. Although I consider the proposed attack a baseline attack (maybe other alternatives can be used), I believe it s a relevant result. Weaknesses:  I am missing evaluation of the method in larger scale datasets, or more natural images dataset. I think for this methods to be applicable and useful, authors should demonstrate its usefulness into real data. I am missing some images of the CIFAR dataset similar to Figure 2. Given a new dataset, which methods should I select? Conclusion: I believe the paper presents an interesting method with strong experimental results.<BRK>[Summary]Online defenses of adversarial examples is an old topic: Given an input x (potentially adversarially perturbed) at test time, we want to sanitize x to get x , on which the trained classifier $g \circ f$ gives the correct answer. This paper proposes a new architecture for online defenses via self supervision. There are two new things in the proposal:1. And the auxiliary self supervised component h works on the same representation. This architecture for online defense seems new (as far as I know). 2.The paper leverages an interesting hypothesis that for a common f, a large classification loss happens if and only if a large self supervision loss happens. 3.For the experiments   the paper trained f, g, and h under Gaussian corruptions, and indeed found that this online purification strategy provides robustness under adversarial perturbations, even for auxiliary aware attacks, which is interesting. My first worry is that the performance of the defense is still much worse than the performance from direct adversarial training (for example, check the MNIST numbers). This is especially the case if we consider auxiliary aware attacks. 2.Following (1), what worries me more is that online purification still needs to be aware of the attack type. Namely if one looks into equation (4), the objective has encoded norm based attacks within it. Why do we need to know the results for FCN (fully connected networks)?<BRK>###################Summary:This paper studies adversarial defense by combing purification and self supervised loss. During inference, the authors propose an online purification method based on (clipped) iterative gradient ascent. The loss used by purification is from some pre defined self supervised tasks. The proposed method is well motivated and reasonable. 2.The paper is clear and easy to follow. Large T will significantly slow down the test time efficiency. My understanding is this should be very small value, since multi step PGD attack is pretty strong. 4.Seems like self supervised tasks are pretty ad hoc. Is there a principled way of selecting a good self supervised task? 5.The two datasets used in the paper represents limited visual patterns. Two things for future:  Make it work on bigger and more realistic images, imagenet, pascal, coco, etc. Now the adversarial community and deep learning community in general, highly relies on experiments, because theoretical guarantee is still mysterious.
Reject. rating score: 4. rating score: 5. rating score: 6. rating score: 6. <BRK>This paper proposes SALR, a new optimization algorithm which adapts the learning rate to avoid sharp local minimas. Empirical tests are performed on MNIST, and CIFAR 10, showing that the proposed method works better as compared to naive SGD, and entropy SGD. Strong Points:+ The problem being studied is very important and can have high impact for cases such as large batch training in which getting "trapped" in sharp minima is a problem+ The paper does a good job of intuitively explaining their algorithm which adds noise in sharp regions by increasing learning rate. Major Comments:While the proposed algorithm is interesting but the theoretical and empirical results provided need to be strengthened further to support the claims. In particular, please note the following:1  The empirical results are very limited. Given that the theoretical results are only for simple convex settings, a much more thorough empirical analysis would be needed to evaluate the proposed method. It is not clear how these results would extend to non convex settings. 5  How does the proposed approach compare with existing methods such as cyclical learning rate? I will look for the rebuttal and will adjust my score accordingly. Suggestions for improvement:There needs to be a more thorough evaluation of the paper for more challenging learning tasks such as ImageNet classification, and language modeling with transformers.<BRK>The paper proposes a simple method (SALR) to encourage the SGD to converge to flatter minima for better generalization. The basic idea is that it increases the learning rate when the sharpness is high, vice versa, such that SGD can escape sharp regions quickly. The paper is well written and easy to follow, and shows some interesting observations, such as:1. It will be interesting to see if increasing the epochs can achieve higher accuracy or not;2. However, there are following issues in the paper:1. discussion on connections to previous works  1.1. the sharpness in "Definition 1" is highly correlated to gradient magnitude. SmoothOut [1] also injects noise to escape sharp regions, by averaging over the same neural network under different small perturbations of parameters. For LeNet, any SGD can achieve above 99% easily. This is not observed in Figure 4 Right. [1] https://arxiv.org/abs/1805.07898[2] https://benchmarks.ai/cifar 10<BRK>This work proposes an algorithm that aims at finding a flat minimizer. The authors claim that by increasing the learning rate, the iterate can get out of the undesired region. The proposed algorithm has a promising result empirically, compared to entropy SGD (Chaudhari et al.) Strength:(1) Paper is written well. The motivation is clearly explained. (2) The algorithm is easy to implement. Weakness:(1) Compared to the empirical results shown in the paper, the theoretical result is relatively weak. Specifically, it could be the case that the conditions of the theorem are satisfied and that the iterate generated by the proposed algorithm escapes flat minima eventually. The paper will be in a much better shape if the theoretical result shows that the iterate escapes sharp local minima while attracts to flat minima. I hope the authors can discuss this issue. (2) (Remark 3 and normalization in Definition 1) I agree with Remark 3, but the argument made in Remark 3 also implies that the normalization will reduce the sharpness value when the gradient is large, which might be an undesired effect. after rebuttal  I thank the authors for the feedback. Regarding the proposed theorem, I was hoping to see if the iterate can stay in the flat minima instead of leaving the region eventually, as the constant of the local strong convexity is inside the log s, which is not very sensitive to the value (the landscape) and might be tricky to provide useful guidance to differentiate flat and sharp minima. I decided to maintain my score, but I still recommend a weak accept of this paper.<BRK>This paper proposes a method called SALR, which adeptly updates the learning rate based on the local sharpness of the loss function to escape the sharp local minimum. By doing this, the author shows that their method can enhance the generalization and converge speed during the training procedure by encouraging the network to converge to a flat minimum. The paper provides theoretical analysis as well as empirical results to support their claim. For the theory part, the paper gives the evidence to prove that SALR can get a better converge rate while in the empirical part, the authors test SALR on MNIST and CIFAR10 using several different network structures. Pros:1.The analysis of the converge rate part looks good for me. Though I do not check the proof one by one. (2).For the statement of faster converge speed, I can not actually see the obvious change from the empirical result from figure 4. Could you add more experiments on other benchmarks like CIFAR100 and even ImageNet? Overall, I think this paper gives interesting points with the theoretical analysis. However, I still think the algorithm is hard to convince me in the empirical parts. Update after author s response  The additional experiments look good to me, it solves most of my concern, I would like to lift the score to 6.
Accept (Spotlight). rating score: 7. rating score: 7. rating score: 7. rating score: 6. <BRK>Strengths:+ The problem studied is an important one for analyzing large datasets. Computing eigenvectors of a p by n X matrix can be done by solving p by p linear systems involving the matrix XX^T instead, and for many reasonable notions of errors, this matrix can be sketched. However, I do feel the analysis done is more broadly applicable. Overall, I feel this paper takes an interesting step toward problem specific, and algorithmic specific sparsification.<BRK>The paper rigorously studies the effect of sparsification and quantisation on the eigen spectrum of kernel matrices, and subsequently on downstream tasks, such as spectral clustering. It is shown both theoretically and numerically that significant reduction in computation can be introducing entry wise non linear operations (sparsification etc) on the kernel matrix with significant reduction in performance. Moreover, there is hardly any discussion on the limitations of the analysis.<BRK>This work considers the effect of sparsification, quantization and non linear transormations on the spectrum of a random matrix with respect to performance in downstream applications like clustering. The results are very nice and gives a firm theoretical understanding of a well studied practical problem in machine learning. I would like to see further research with non asymptotic rates for misclassification.<BRK>This is a nice paper that shows that one can perturb a kernel matrix (or pass it through a non linear transformation) without necessarily modifying the underlying eigenspectrum significantly, and as such, without hurting the performance of spectral clustering applied on the matrix. How is p coming into play? Why is it important? For example, how would it be reduced for f which is just a sparsification, I would be curious? Again, I strongly urge the authors to instantiate some of their results to simple cases, where many of the variables and traces of variables disappear, so that the strength of the results can be thought about.
Accept (Poster). rating score: 8. rating score: 6. rating score: 6. rating score: 6. <BRK>This paper studies the effect of training BN parameters on training deep neural networks. To stress the important role of BN parameters, the same number of parameters are chosen randomly and trained. Yet, it is observed BN parameters can obtain far better accuracy. Furthermore, an interesting observation is conducted on the distribution of BN parameters: when training only these parameters, a sparsity pattern is observed on the optimal parameters. The sparsity pattern indicates that an efficient network only needs to have a particular ground truth connection between different units and the choice of weights is not important. I have a question to understand the results betters: When authors are talking about optimizing only BN parameters, do they mean that they did not optimize parameters of the last linear layer in the network (the one that is connected to outputs). This result will provide more intuition about the coupling of the weights and BN parameters. * I have read the authors  response and also other reviewers. In my view, this paper provides novel insights into batch normalization.<BRK>Pros:  The topic is interesting. The paper is well written and organized. Cons:  The paper is more like a technical report. If not, some theoretical analyses may need to be provided. Some observations are not novel. The role of gamma has also been proved by Luo et al.[2]In conclusion, this paper provides some insights into understanding the role of BatchNorm parameters. However, some observations are not novel and it is unclear how these findings help the community, which prevents this work to be accepted. [1] On implicit filter level sparsity in convolutional neural networks. It clearly clarifies their contributions compared with the previous work. In particular, an interesting observation about the random feature and affine parameters is made.<BRK>This paper studies the expressive power of batchnorm parameters by training only these parameters while fixing other randomly initialized parameters. With experiments on different datasets and models, the authors show that batchnorm parameters are consistently more expressive than other parameters. The authors also try to explain such phenomenon by examining the values of parameters and activations, showing that training BN only can lead to sparse values. The following are the pros and cons of this paper. The paper is well written, and it is easy to follow. 2.Understanding the mechanism behind batchnorm(BN) is a very important problem. 4.To explain the effect, the paper provides some interesting results from training BN parameters only: there is sparsity in the BN parameters, and sparsity in ReLU activations. K for the Price of 1: Parameter efficient Multi task and Transfer Learning, ICLR 2019. In NIPS, 2017. I think the paper is interesting, but the novelty and significance is limited by previous works along the direction.<BRK>The main conclusions of this work are that BatchNorm coefficients have a greater discriminative power than the rest of network parameters. On the other hand, it is known that $\gamma$ and $\beta$ are highly expressive [1,2] and that they produce sparsity [3]. Strengths * Understanding BatchNorm is an important and interesting topic. * The paper is well organized and easy to read. For instance, FILM [1] layers modulate neural networks by changing these coefficients. * It is also known that BatchNorm coefficients can produce sparsity [3]. * Besides the main experiment of training BatchNorm only vs different alternatives, it would have been interesting if the authors had provided the results for alternative transformations. For instance, what would happen if only the PReLU parameters were trained? In this case, it may be normal that an affine transformation works better since the number of random parameters of the convolution will overwhelm the number of non random ones. Overall, I think this work is interesting and I think studies like this are necessary (although more in depth).
Reject. rating score: 5. rating score: 5. rating score: 5. <BRK>Authors extend the information bottleneck by Tishby to also include an additional auxiliary variable that represents either domain index or private information. Using this generalined IB the authors study tradeoffs/fundamental limtis on achievalbe accuracy in predicting labels while maintaining invariance against the auxialiary variable. The proposed approach seems promising but much more work is needed:   it is unclear how the proposed frameworl relates precisely to plain vanilla IB. can one use these characterizations to verify sub optimiality of existing methods for privacy preserving ML or algorithmic fairness   how can the proposed model be turned into practical algorithms that (nearly) achieve the fundamental limits/tradeoffs.<BRK>This work studies a fundamental and important problem in representation learning, the tradeoff between accuracy and invariance of the learned representations. For classification and regression problems, the paper analyzed the inherent tradeoffs by providing a geometric characterization of the feasible region in the information plane, where it connects the geometric properties of this feasible region to the fundamental limitations of the tradeoff problem. It is not how stringent the assumptions are in practice. 4.Although the result is theoretical, it could be better if the authors provide some experimental justifications, which is completely missing.<BRK>The paper formulates the problems of learning in invariant representations as a min max game, exploring tradeoffs between accuracy and invariance of these representations via a geometric plane analysis. This is an interesting theoretical take on the issue of exploring tradeoffs between accuracy and invariance of representations, but I am not sure to what extend this theoretical analysis provides actionable insights about real world problems (e.g.what doe convexity of the feasible set imply for algorithmic fairness?).
Accept (Spotlight). rating score: 8. rating score: 7. rating score: 7. rating score: 6. <BRK>This paper presents an interesting idea of using random graphs to represent relational structures amongst contextual samples and between contextual samples and target samples.<BRK>Summary:The paper proposes a novel way of using random graphs to improve task free continual learning method. The idea is well formulated, and carried out in a sound way. Overall, I find the paper quite strong.<BRK>This paper presented a memory based continual learning model where relationships between training samples are represented with a random graph that is defined from the non linear embedding of the input data. I tend to rate this paper as a good paper for acceptance because the presentation is clear, the idea is novel and the experiment is convincing.<BRK>2.Based on the use of replay to solve catastrophic forgetting, the current popular graph structure is introduced to capture the similarities between samples. The latest papers on the three types of methods (regularization, expansion, and rehearsal) for solving catastrophic forgetting are included. The promotion of current tasks is also crucial in continual learning. 3.The experimental results given in the paper can basically show that the proposed method is effective.
Accept (Poster). rating score: 7. rating score: 7. rating score: 6. rating score: 6. <BRK>  Summary  This paper improves upon state of the art GANs by incorporating recent advances of contrastive representation learning into the training of discriminator. While each of these terms alone is not entirely new, the author proposes several tricks to make the training of GANs together with the contrastive loss works. Empirically, the proposed method outperforms other GAN methods trained with auxiliary data augmentation techniques, and demonstrates good representations under the linear classifier probing setup. Q2: I feel like there’s one ablation setting missing in Table 1: using “HFlip, Trans” data augmentation for the ContraD. This way, we can see the true benefit of combining those three losses.<BRK>**Summary**The manuscript proposes ContraD   a method that incorporates the recent SimCLR self supervised learning method for images into the GAN training framework. Experimental results show that the proposed method consistently improves on strong baselines in terms of FID scores. **Score justification**The paper is well written, the introduction carefully synthesizes a lot of recent work on self supervised representation learning and GANs; results are convincing (albeit mostly obtained on smaller scale datasets) and ablations support paper claims. * It would be interesting to see linear separation and transfer to other datasets (as in the SimCLR paper) for the ImageNet models. Strong results there would support the authors  narrative of the synergy GANs and SimCLR training. In particular it seems to be that GAN training benefits from contrastive learning (it seems to be a good auxiliary loss in presence of strong augmentation), but the claims of benefit to SimCLR don t seem fully substantiated. Making it clear that this is not the case (e.g.by including the original SimCLR results and/or supervised performance) would be helpful to the readers.<BRK>In this paper, the authors suggest using the contrastive loss to improve the training of the discriminator and further stabilize the GAN training process. Strengths:* The idea of using self supervised learning for improving the training dynamics of the discriminator makes sense and is an interesting exploration area. weaknesses:* The proposed method is rather a careful ensembling of existing components, e.g.simCLR self supervised or supervised contrastive loss in the right context, rather than a radically novel methodological contribution. It would have been interesting to also demonstrate the contributions on the more challenging higher resolution datasets. * An ablation study I was missing was having the proposed method without the extensive simCLR augmentations and see how it compares to the other methods. * Typo: "approaches that handles"<BRK>The authors propose to improve GAN training by incorporating augmentations from contrastive learning. Specifically, a new contrastive discriminator, named ContraD, is proposed for GANs; with ContraD, the encoder part of the discriminator is trained with (two) contrastive learning losses, while the left discriminator head and the GAN generator are trained as usual. The authors argue that this specific fusion of GAN and contrastive learning signiﬁcantly stabilizes GAN training and, moreover, the fused two research fields could benefit each other. The paper is well written overall. However, I am not fully convinced because of the concerns listed below. Although the approach is proposed to stabilize GAN training, this aspect was not highlighted in the experiments. ” So why would that happen? Please elaborate on that.
Accept (Poster). rating score: 7. rating score: 7. rating score: 7. rating score: 6. <BRK>The paper proposes to define the uncertainty set in the DRO problem as a family of parametric generative models, which is to allow more flexibility in the choice of the uncertainty set architecture. To realize this idea, the paper first proposes a new relaxation of the DRO game s inner maximization problem (with KL constraints) so as to improve the training stability. It then develops a principled approach to select the hyper parameters of the proposed method. Strengths:+ The paper is well written. + Experiments with real world problems are conducted to evaluate the effectiveness of the proposed method. I particularly like the experimental analysis the authors conducted to understand the behavior of their proposed method. Weaknesses:  The experiments are only on NLP tasks. I have few questions to the authors:1) How good the adversary model needs to be for the proposed method to perform well? In the experiments, an auto regressive transformer model based on the GPT 2 language model is employed. Will the proposed method performance be too sensitive to the accuracy of the adversary model? 2) In the experiment (last paragraph of Section 5.1), the temperature \tau and the normalizing window k are fixed whilst the adversary learning rate \lambda is searched by grid search. So how \tau and k are selected in practice? What is the performance of the proposed method when \tau and k vary?<BRK>The paper is well written and easy to follow. Bad points   I don t see just how this model is "parametric". In statistics, "parametric" the adversarialdistribution is modeled as a gaussian, etc. with sought for parameters (mean, covariance, etc.). In the absence of that, I would have expected "parametric" to mean parametrizing the adversarialdistribution as the (softmax) output of a neural network. Neither of the above is the case in this paper. So, what are the "parameters" in the proposed DRO adversary ? All I can see is thatthe authors do  a full search over all distributions, subject to a KL constraint (see sections2 and 3.2). The authors say "In particular, direct gradient descent on the uncertainty set suffers frominstability due to the large variance of the gradients (Greensmith et al., 2004), andhyper parameter selection is not straightforward." I m not sure about this claim (whichis one of the main premises of the manuscript.What do the authors make of this paperfor example Faury et al.(AAAI 2020) "Distributionally Robust Counterfactual Risk Minimization" ? The authors of that paper demonstrate how to efficiently formulate and solve KL based DROproblems. That paper also contains both theoretical and practical insights. The arguments in the paper very heuristic. Since the paper is supposed to be empirical (see previous points), I would have expectedexperiments on real datasets. Errors   Change "solve the inner max efficient" to "solve the inner maximization problem efficiently"  Change "$x, y ~$ " to "$(x,y) ~ $" all through the manuscript  Eqn (5): why not take $p$ and $q_{\psi_0}$ to equal the empirical distribution (as is usuallydone) in DRO ?<BRK>This paper considers distributionally robust optimization (DRO) and uses the neural generative models to characterize the uncertainty sets. The proposed robust method is validated on NLP tasks. This paper is well written and of a good structure. Although the main idea is simple, the authors make several modifications to the algorithm to make it tractable and with performance guaranteed heuristically. To summarize, the main contribution of this paper is a new algorithm that combines standard techniques, such as Lagrangian relaxation and KL reverse, into the DRO problem with KL uncertainty sets. And this algorithm was shown to perform well under synthetic and real data NLP tasks. Since there is no novel techniques proposed in this paper and there is no performance guarantee for the proposed framework, overall, I think this is a borderline paper due to its limitations in theoretical development and technical novelty. For example, I am wondering is it applicable to compare with Wasserstein DRO or Huber s classical work of Total variation based DRO, or some other DRO works in the literature, so that it will be more convincing on the performance of the proposed method. A minor typo in the paper: in section 6, there is a duplicated "produce" in the sentence: "In such cases where good quality generative models are unavailable, or such model cannot produce produce densities efficiently".<BRK>TL;DR: The paper makes an interesting contribution from a practical point of view, but two important theoretical concerns need to be addressed in the rebuttal for acceptance. The paper proposes the use of ideas taken from the literature on distributionally robust optimization within a parametric framework. More precisely, the main idea is to consider only a (parameterised) subset of the traditional KLD uncertainty sets. As this avoids the need for elegant analytic solutions (at the expense of a more brute force computation), it has the flavour of a more ‘black box’ approach towards the deployment of DRO. Overall, I really enjoyed the way this paper was written. I also liked the contribution and believe that the paper demonstrated its ideas to be useful. There are however two points of major concern from a more theoretical side. In my mind, these are rather substantial, and I will list them below. This means that the KLD between the two distributions is not defined/infinity for any value of \psi (Mismatch of support problem). These kind of problems are the precise reasons why other quasi distances (like the Wasserstein distances) have become increasingly interesting for ML. The KLD is not symmetric and in general will not even have the same minimum. In fact, generally speaking the only time the minimum will be the same in either direction is when the KLD’s global minimum is such that $q_{\psi}   q_{\tau, \theta}$ (i.e.we can drop the KLD term for the loss in (7) completely, so that it simply equals $C$). Given the definition of $q_{\tau, \theta}$, it is unreasonable to assume that this global minimum is attained. Calling the outcome an ‘approximation’ is then grossly inaccurate. (See e.g.the visualisations here: https://wiseodd.github.io/techblog/2016/12/21/forward reverse kl/)Lastly, since the chief concern of the paper is the construction of new uncertainty sets, I would have liked to see two additional recent references discussed which have produced uncertainty sets purely based on moments (https://arxiv.org/abs/2007.04458, ICML 2020) as well as on general IPMs (https://arxiv.org/abs/2006.04349, NeurIPs 2020). Both these types of uncertainty sets do *not* suffer from the mismatch of support problem, and—like the famous f divergence based uncertainty sets—have elegant dual forms. POST DISCUSSION: The authors promised to clarify the two issues I pointed out in ways that are satisfactory for a paper whose main concern is practicality (as opposed to theoretical rigour).
Reject. rating score: 4. rating score: 5. rating score: 5. rating score: 6. <BRK>The paper proposes SANE   an architecture and a training algorithm for continual learning. Novelty:Different parts of the method did have appearance in the prior literature. Even though, EWC does rely on task boundaries information, it would at the very least define the gap that exists to the methods that are agnoistic to this kind of priveledged information. Other comments: I think what authors mean by "discounted reward" and $r_{disc}$ is usually denoted as "discounted return", this should be clarified.<BRK>It is hard to judge how good the method is without extending the range of baselines (I appreciate the addition of the and task settings.As a result, I am only increasing my score from a 4 to a 5, but I do think SANE is an interesting method and I encourage the authors to keep working at it to prove its effectiveness. •	The problem that SANE tackles, i.e.task agnostic continual learning, is an important one for which there are not a large number of approaches in the literature.<BRK>* The architecture is quite interesting. ## Objective* Addressing the problem of continual learning in the context of reinforcement learning. I think of the tree structure as a way of inducing a prior for the learning agent. * Ablations: The paper introduces several hyper parameters but does not consider any sort of ablations with them. ## Some other questionsPlease note that these are some general questions, and I did not consider any of these design choices as "wrong" or "against the paper." Is there a reason to not work in the more general any task at any time setting? SANE.My understanding is, training SANE takes a lot more time/steps.<BRK>The approach is sound with decent empirical evaluations. The paper could benefit from some clarity in the explanation of the results and the assumptions made in the method. It is an interesting approach for modular learning, even within the same environment. The approach is clearly explained. Figure 4 is a bit hard to understand. The results in this environment are not so convincing.
Accept (Poster). rating score: 7. rating score: 7. rating score: 6. rating score: 6. <BRK>Summary:This paper considers the estimation of the concentration of measures, which possibly causes of the vulnerability of machine learning models to adversarial attacks. Empirical studies are conducted to show the efficiency as well as the efficacy of the proposed method when compared to [Mahloujifar et al.(2019b)].Comment:The paper is overall well written and the presentation is clear.<BRK>Mahloujifar et al.(2019a) have shown that estimating intrinsic robustness is closely related to the problem of estimating the concentration of measure. In this work, the authors provide an efficient algorithm for measuring concentration empirically, which yields a more accurate estimation of the intrinsic robustness compared to the previous results. The paper seems well written.<BRK>The paper is very well written and it was a pleasure to go through it. Since I am not an expert in this research area, I have a low confidence in my decision. At least, I believe that there might be tools that could make the proof of Theorem 3.3 shorter ( I had a quick look at it). Please make the argument stronger in the text on why "concentration of measure is not the main reason for the adversarial vulnerability of classifiers". It is important since it compares with Theorem 4.2.<BRK>It basically builds upon a series of recent  works, mainly by Mahloujifar et al.(2019a and 2019b), which establish a fundamental relation between the  concentration function of input distribution, denoted by  $h^{\\left(\\ell_p\\right)}_{\\mu}\\left(\\epsilon,\\alpha\\right)$, and its intrinsic robustness, i.e., maximum possible adversarial robustness for any model class in a robust learning problem. The paper s main contribution is to propose a new way for approximating the empirical value of $h^\{\\left(\\ell_p\\right)\}_\{\\mu\}\\left(\\epsilon,\\alpha\\right)$ for a given input dataset of $m$ samples. Also, paper claims that the new proposed method converges to the optimal solution faster than previous works, however, no theoretical guarantees are given for this claim. $\{\\bf \{Comments\}\}$:Paper is very well motivated and fairly well written. Mathematical notations are carefully crafted and results seem to be solid and understandable (I have not checked all of the proofs in Appendix). Still, I think the paper is publishable even in its current form.
Accept (Poster). rating score: 7. rating score: 7. rating score: 6. rating score: 6. <BRK>I believe this paper should be accepted. The proposed approach also obtains competitive results on VQA (CLEVR and GQA datasets). ***************************************************************************WEAKNESSES AND REMARKS  I have some concerns regarding the novelty of the proposed approach. The paper is revisiting an existing idea (i.e.using modulated convolutions for visual reasoning, which has already been proposed in the FiLM paper) with a new formulation, which gives better results, and thus is applied to natural images (in addition to synthetical ones, like in the FiLM paper). Knowing that there is no such questions in GQA is crucial to understand that the proposed approach can serve as a general mechanism to improve visual reasoning, beyond counting tasks.<BRK>By providing motivation of modulation for visual counting, this paper presents the MoVie module which consists of four modulated convolutional bottlenecks. Strength:  This paper is well motivated with well justified compelling results. Extensive interesting ablation studies have been provided   number of modulated bottlenecks, modulation design, scale robustness, etc. Extensive analysis has been provided with interesting visualization maps. Weakness:  One might argue that anonymity is not properly maintained by mentioning “we won first place in the VQA challenge 2020”. Was a similar situation observed with MoVie module? Post Rebuttal update:Thanks to the authors for providing relevant details and fellow reviewers for nice discussions.<BRK>Summary:The paper addresses the counting based VQA scenarios, as well as general object counting problems. It is not clear to me besides improving the FiLM formula, and employing it for VQA counting related and counting tasks, what are the other contributions of the paper? However in appendix, it is presented that the model doesn’t have competitive results on the PASCAL VOC dataset not only compared to [4] but also to some other papers in literature. The reason that is mentioned in the paper is related to the scale of the dataset [5].<BRK>This paper proposes a method to count general objects for a query. The authors report the experimental results for object counting, visual question answering (VQA), and GQA. **Cons**  One of the highlighted result is the generalization beyond counting. However, Table 5 only shows the performance with the performance of MoVie in comparison to the other methods. Table 2 shows that the combination of X 101 and MoVie achieves the best performance; as shown, the backbone network is different from the existing methods. The significance is also unclear on this table. **Overall rating**The reviewer is leaning toward acceptance because the motivation is clear and because the proposed method is effective in several experiments.
Reject. rating score: 5. rating score: 5. rating score: 6. rating score: 6. <BRK>Other, more specific suggestions that may improve the quality of the paper and make the results more convincing:1. A more rigorous investigation into explaining the phenomenon, or a more comprehensive empirical exploration to identify what does and what doesn t influence the best depth, would make this a great paper at a later conference.<BRK>Specifically, the authors present an empirical analysis of the impact of the depth on the generalization in CNNs. What is the difference from these works? Could you please give more explanations? 20202.This paper analyzes the linear neural networks and demonstrates that increasing depth leads to poor generalization.<BRK>and provided in depth findings via experiments. *********Weakness Of The Manuscript: *********Overall, apart from the contribution of the paper I have some concerns regarding the paper which are listed below. Even outside of the contribution of this paper, I would recommend this paper to people getting started with CNNs as it provides a thorough description of the part of the pipelines it deals with.<BRK>Since I was familiar with this work, I am not especially surprised by the findings in this paper and hence the reason for my somewhat lower score and suggestion for incorporating a bit more (richer) content, either empirically or theoretically. I had several questions about the empirical results: Some of the trends seem to fluctuate a bit rather than behave smoothly after an inflection point (e.g.Fig.1(b)).Is there a reason for this?
Reject. rating score: 3. rating score: 4. rating score: 6. rating score: 6. rating score: 6. <BRK>The paper proposes a new metric for measuring example difficulty: variance of gradients. Illustrative examples, observed correlation with error rate on test data, and successful identification of out of distribution examples indicate that the proposed metric is useful. The primary shortcoming of the paper is that it does not compare to any other measure of example difficulty. It is unclear whether the proposed metric provides a significant amount of additional information. Data augmentation is used in the experiments but the paper does not explain how this is accounted for in the calculation of the metric.<BRK>Hence it is not a 100% reliable indicator and needs to be more firmly placed in context. It can be used as a trustworthiness indicator (correlates with accuracy), but also seems to correlate with out of distribution examples and example memorization. Cons:Overall, while VOG clearly shows some useful properties, there could be more clear benchmarking and comparison to other methods for detection of example difficulty. The paper does cite and describe relevant prior research, but more quantitative comparison is called for. A few proof reading issues:*    In "Methodology", the authors reference $A^l_n$ without defining $A$. However, the authors should present some baselines here:*    The actual probability of the highest confidence class. The authors state that "DNNs produce output probabilities that are uncalibrated and thus cannot be interpreted as a measure of certainty". To trust or not to trust a classifier. Predictive uncertainty estimation via prior networks.<BRK>The authors propose to use a scalar measures called Variance of Gradients to discover challenging to learn examples on which the model is more likely to make an error. In terms of dataset interpretability and analysis, I think this work provides interesting empirical insights, especially using such a simple method, which is always good. 1.The authors do not actually provide a clear definition of VoG   it is not clear WHAT we are taking the gradient off with respect to an input pixel   the predicted class probability? The notation is opaque and needs further clarification. In practice, judging from the picture, this seems to be also the case in practice on C10/C100/Imagenet. However, the authors do not answer *WHY* examples on decision boundaries should have a high VoG score. While I understand that this is an empirical work, I think it requires discussion of the relationships of the proposed measure to other uncertainty/difficulty/confidence estimation approaches is necessary.<BRK>##########################################################################Summary: The authors propose to use the variance of the gradient (VOG) values measured during neural network training as an estimate of the difficulty of examples. VOG also shows that easy example (with lower error rate) are leant first during training. ##########################################################################Reasons for score:  The idea proposed in this paper is simple and seems to be effective. However, the authors should demonstrate that in practice VOG can be used to increase the accuracy : * is it possible to use VOG as a confidence score and classify samples only when their VOG is above a given threshold ? This should lead to higher accuracy. On the other hand, could VOG be used to guide the data augmentation process by selecting the "difficult" samples ?<BRK>Summary:The authors propose Variance of Gradients (VoG) as a quantifiable metric to identify examples that are difficult to classify. This is motivated by the intuition that examples that are easy to classify do not contribute much to the loss beyond early stages of training, hence don t contribute much to the gradient. There is a high variance in VoG values for shuffled examples. Conclusion:Overall, my decision is to accept the paper because this is a powerful proposal that deserves to be investigated further. However, I have some reservations about the empirical results as described above. If authors can explain/clarify these aspects it would be a much stronger submission. In addition to those listed above, the authors should address the questions below in future work:  How come not all or even a majority of examples that are misclassified by a network have high VoG? Strengths:  A new metric, VoG, is proposed that is easy to calculate and shown to be associated with examples that are difficult to classify. The metric is easy to compute as compared to competing methods.
Reject. rating score: 4. rating score: 4. rating score: 4. rating score: 6. <BRK>This paper proposed a novel adaptive data augmentation algorithm that produces random perturbations on the training dataset to train an imitation learning based self driving network. It starts with a sensitivity analysis of network performance under different types and levels of perturbations. I think the author did not get my major concern on competing algorithms. Validation has been conducted over simulated data with both seen and unseen perturbation types. Pros:* Very practical task. These include but are not limited to:  Learning to reweight different training samples. The baselines used in the experiment are a bit too weak. This is to evaluate whether this proposed method could improve performance when there is no domain gap in perturbation. Plus, it would be preferable to evaluate perception performance as well, such as detection/segmentation, not only an end to end IL driving model, which is less practical at this moment for real world self driving. Instead, the subset improves overall performance.<BRK>Summary:This paper presents an algorithm to improve the model generalization of the task of "learning to steer". Will the proposed method have a consistent superiority on the two groups over the common data augmentation technique? Pros:1) The idea of evaluating the sensitivity of a model to different degradation factors in the same metric space is interesting, providing empirical insights for preparing datasets with different degradation levels. 2) Inspired by adversarial learning, choosing the most difficult datasets for training at each iteration may be informative and potentially improve the model generalization. 3) Empirical study on a base dataset for the "learning to steer" task is carried out. Cons:1) The choice of FID as the unified metric for evaluating the quality of different degraded images caused by different factors is not convincing. A careful inspection and more discussions are encouraged. 2) It seems that the sensitivity analysis is only used as an empirical guideline to discretize the parameters of degradation factors into their corresponding levels. However, the effectiveness of such a guideline has not been validated. The authors can compare it with other discretization methods such as uniformly sampling, unevenly sampling towards heavy degradation, or light degradation. Moreover, if the abovementioned user study is carried out, how is the performance using the discretization method based on the human visual experience levels? 3) The proposed method seems to be not limited to the "learning to steer" task. Considering that some well established benchmark datasets such as ImageNet C and different methods have been proposed for studying the impact of degradation factors, it is recommended to carry out experiments on these datasets and included more representative state of the art methods into the comparison. 6) In the Experiments Part, both the baseline method and Scenario 1 are called the baseline, which may be confusing in the following description and discussions. Does the vanishing point serve as a cheap feature for "steering"? If so, the structural information of the road should be more useful.<BRK>It has two fold contributions. First, the paper analyzes the sensitivity of a learning algorithm w.r.t.different image transformation in a realistic scenario. Next, they also propose a technique to learn from the generated images. The paper addresses a very important problem. However, there are some major concern about this work:1. For example, a.	Tian, Yuchi, et al."Deeptest: Automated testing of deep neural network driven autonomous cars." The above two papers work on the same premise that the self driving car learning model is susceptible to many real world conditions and propose methods to analyze to the sensitivity of the method. The paper should compare with these baselines. 2.The re training method using evolutionary optimization is interesting. However, a similar technique is proposed by Gao, Xiang, et al."Fuzz testing based data augmentation to improve robustness of deep neural networks." 4.Only tested for one learning model.<BRK>### **Summary**: This work proposes a new method to improve the generalization of ML models for the task of vehicle steering using a hybrid of data augmentation and adversarial examples. ### **Strengths**: * This paper is after an interesting and relevant problem, fundamental for the progress of the autonomous driving field. * The paper is well written and simple to reproduce ### **Weaknesses**:* As mentioned above, my main concern is that there are possible flaws in the evaluation protocol. During training, the method chooses among the K transformed datasets those who minimize the mean validation accuracy and based on this selection the steering model is retrained. I thank the authors for including other datasets and considering additional baselines. I still think that Chen s dataset should not be the main dataset to use in the paper, but I am happy to see strong datasets. After reading the experiments I think that the narrative of the paper is not as consistent as it could (should?) However, section 4 does not contain these elements. All experiments are carried out using Chen’s dataset, and one of the main questions should be, why this dataset? In short, this doesn’t seem to be the right setup t study the problem of generalization. Instead, I would like to recommend a controlled environment, such as the [CARLA simulator](https://github.com/carla simulator/carla).
Accept (Poster). rating score: 6. rating score: 6. rating score: 5. rating score: 5. <BRK>This paper proposes a method for finding the axes of largest variation in the latent space of a generative model. ##########################################################################Reasons for score: The analyses are interesting and the method seems novel. This method could prove useful in analysis of latent space properties. Especially, the claim that interpretable axes are found seems central but it is not much supported in the experiments. Could the authors summarize, in practise and plain English, how their method should be used to find the most intepretable axes of variation for a new generative model, and how to confirm and measure that we have indeed found them? Typo: on page 7 around "(Fig 5.)" ##########################################################################Update after rebuttal discussions:  In the light of the considerable overlap with [Chiu et al., 2020] pointed out by the other reviewers, I decreased my score. However, given remaining differences, I do not find it unreasonable to consider this paper as "complementary" to [Chiu et al., 2020], *provided that the authors explicitly address the similarities in the final version*.<BRK>The paper performs the analysis of the GAN latent spaces from the geometric perspective, inducing a metric tensor in the latent space from the LPIPS distance in the image space. The main authors  finding is that under such metric, the latent spaces of typical GANs are highly anisotropic, which can be exploited for more effective GAN inversion. Pros:1) The paper is exceptionally well written and provides a very interesting read. While the performed analysis is simple and natural, it does reveal several interesting findings about typical latent spaces: LPIPS anisotropy, global consistency of the metric tensor. Cons:1) Missing work on interpretable GAN directions:[A] The Hessian Penalty: A Weak Prior for Unsupervised Disentanglement, ECCV 20202) In my opinion, the authors do not provide enough support for their claim "This finding unifies previous unsupervised methods that discover interpretable axes in the GAN space". While the proposed method does seem to generalize both Ha ̈rko ̈nen et al., 2020 and Shen & Zhou, 2020, I do not see, how it captures Voynov & Babenko, 2020 and Pebbles et al (see above [A]). Furthermore, I believe that such claims should be supported by the experiments. Could the authors experimentally confirm that their method results in the same set of directions as the existing methods? Overall, I am positive about this submission, since the main analysis is both interesting and practically useful. AFTER REBUTTAL  I appreciate the authors  efforts on additional thorough comparison to existing works on interpretable axes discovery. Overall, I am still on the positive side since the observed findings deliver a clear profit for GAN inversion.<BRK>They use the Riemannian manifold analysis to investigate image manifold, which leads to a simple algorithm with eigen decomposition of the Hessian matrix of a local point. But still, there are interesting ideas contained in this paper: 1) the latent space is homogeneous (in a sense that the major directions obtained by a latent position are shared at different positions with similar semantic meanings), and 2) extensive eigenvalue analyses of Hessian. Based on the findings, they show interesting applications of efficient GAN Inversion, gradient free search in Image space, and unsupervised discovery of interpretable axes. [C1] Human in the Loop Differential Subspace Search in High DimensionalLatent Space, SIGGRAPH 2020. Reasons for score: I like the idea that leverages the findings of the homogeneous property of Hessian and improves the efficiency of GAN inversion with Hessian preconditioning. Thus, with these remaining contributions, this reviewer could not vote for the acceptance of this work at this point. The paper is written very clearly and readily. The authors validate the ideas with enough effort in the experiments. Detail comments:  While [C1] presents mainly about the SVD of Jacobian (but they also show Hessian interpretation (metric tensor) in Sec.4.1 as well), it is essentially the same because of Eq.(3) in this paper. Also, [C1] suggested much more diverse exploration methods with diverse search methods.<BRK>_Summary_:This work intends to explore the geometry of the latent space and proposes to define the distance in latent space as the distance between the corresponding generated images and use the Hessian of that squared distance as metric tensor to define the manifold. Can you elaborate more on how you performed exploration? Further, the authors discuss three areas of possible application (gradient based GAN inversion, gradient free image search, interpretable axes discovery). _Strengths_:  The paper addresses an interesting topic (understanding the latent space of GANs) and proposes a straightforward method. In this paper, only LPIPS was considered as metric. However, after also reading through [Chiu et al.,  SIGGRAPH 2020] I do find that this paper has a large overlap with the one mentioned. [1] Lipton, Z.C. I do think this is a very interesting proposal, however, with only showing empirical observations, I believe the scope of this paper is too little. ICLR 2017 workshop. Requirement for understanding the latent space (abstract): The abstract mentions inversion and interpretability as requirement for understanding the latent space. Further, the abstract also claims "This geometric understanding unifies previous results of GAN inversion and interpretation.". Can you clarify these claims? Appendix A.2 Methods for computing the hessian: Parts of how the Hessian is computed should be in the main paper as this is the main method for this paper.
Reject. rating score: 5. rating score: 5. rating score: 6. rating score: 7. <BRK>The paper proposes a practical asynchronous stochastic gradient descent for Byzantine distributed learning where some of transmitted gradients are likely to be replaced by arbitrary vectors. The paper is well written and easy to follow. However, there are some issues:1. Compared to other asynchronous SGD methods, one advantage of BASGD is it doesn’t have the need of storing any samples on the server. The authors declare that it helps BASGD take less risk of privacy leakage. By contrast, ZENO++, a robust fully asynchronous SGD, could ensure convergence to a stationary point. This strikes me that the theorem is quite weak. 4.Some parts of the algorithm are not well explored. The authors have addressed most of my concerns, but I still think the motivation is a little farfetched.<BRK>The main contribution in this paper is to introduce a new approach to do asynchronous Byzantine learning without storing training samples on the server like zeno++. Pros:+ The problem of Byzantine learning in the asynchronous environment is interesting and has rarely been studied, especially under the assumption that the server has no training data. Concerns:  BASGD is not fully asynchronous as the paper claims to be. The server only updates the model when all of the buffers are non zero. That is, the whole system will be slowed down by the slowest buffer. In this case, BASGD resembles SBL with larger batch size but at the cost of tolerating less Byzantine workers. This paper does not explicitly state how to choose the number of buffers $B$ in order to achieve both asynchrony and Byzantine robustness. However, when $B$ is small, there is no robustness; when $B$ is large, there is no asynchrony. The results in the theorems are not clearly presented.<BRK>They provide theoretical guarantees and some experiments on small scale tasks. When updates are asynchronous, such a procedure cannot be employed because gradients are not necessarily communicated to the master node at the same time. This work proposes instead to use gradient buffers on the server to eliminate the need to store training instances on the master node; this is the main selling point of the algorithm, and deems the method sufficiently novel in my opinion. However, the motivation for avoiding storing instances on the master node is not well fleshed out in my opinion. It is unclear exactly what the privacy concerns are, but I would suggest reworking the motivation exposition a little. *On the theory*:Note that the bounded gradient assumption is very strong! $N^t_b$ is used in the main paper (e.g., page 5), but is only defined in the appendix on page 14. Under asynchrony or non iid data, theorem 1 does not guarantee convergence to a stationary point… even with a diminishing step size. I do not mean to criticize the results; only to point out this fact. With L smoothness and the assumption of $D$ bounded gradients, as I mentioned above, all iterates of any objective (regardless of the algorithm), will remain with a ball of a stationary point, the size of which is proportional to $D$. (To the authors credit, I have seen similar bounds in previous byzantine learning methods).<BRK>Its main contributions include generalization of the existing literature on Byzantine fault tolerance in distributed learning to incorporate the case of asynchronous learning. This generalization involves an algorithm, convergence analysis for the algorithm, and experimental results. 1.In Section 2.2, the definition of Byzantine worker, it is not clear why the worker is being indexed with $k_t$? What is the meaning of $t$ in this usage of the worker index? Some of the sentences are hard to parse on first read, while some other sentences suffer from grammatical errors. 4.A number of aggregation functions have been proposed in prior works (see e.g.Adversary resilient distributed and decentralized statistical inference and machine learning: An overview of recent advances under the Byzantine threat model). However this speed up does not seem to be coming up in the analysis or the discussion. In the absence of such a speed up, it is not clear if the authors are really providing guarantees that are useful for distributed learning. While the paper could always be improved, I believe it is now above the threshold of acceptance and it should be accepted into the program, if possible.
Reject. rating score: 4. rating score: 4. rating score: 4. rating score: 5. <BRK>This paper proposes an extension of the RL as Inference framework, and demonstrates how to use it to express an object centric RL model and train it on simple environments. Their approach is also interesting, theoretically guided and sound. I do not feel like the way the abstract and introduction argue for a general framework “Joint Perception and Control as Inference” is productive. As mentioned during the introduction, this isn’t a particularly novel framework (considering RL as inference, all the work done in temporal generative model for RL, or the rather new formulation by Hafner 2020 [4]) and it doesn’t appear like it provides a very interesting fit to the object centric RL model proposed. 2.I understand that performing them for POMDP would be valuable, but then it would be more useful to address what is missing from existing methods and just show the required changes. 3.I would argue that the paper would be stronger if you only expressed the object centric RL derivation, and put it into context with NEM and OP3, to express what is different about it. 3.Does the proposed model also inherit the limitations of NEM?<BRK>This paper features two complementary contributions:1. The perception and control as inference (PCI) framework, which describes the graphical model of a POMDP with auxiliary optimality variables, allowing for the derivation of an objective for optimizing both a perception model and policy jointly to maximize $\log p(\mathcal{O}, \mathbf{x} \mid \mathbf{a})$. As the name suggests, this is similar to the control as inference formulation with a focus on partial observability and the addition of a perception model $q^w$. 2.Object based perception control (OPC), which is an instance of PCI using an object factorized model. (A discussion of some of these, such as [[Farahmand et al, 2017]](http://proceedings.mlr.press/v54/farahmand17a.html) or [[Oh et al, 2017]](https://arxiv.org/abs/1707.03497) could serve to better highlight the difference between PCI and existing hybrid approaches, but these references are not "missing" in the sense that their absence is an issue.) Aside from experiments, the presentation of the core ideas could also be improved somewhat.<BRK>The authors propose a framework for joint perception and control as inference (PCI) to combine perception and control for the case of POMDPs. One of the major contributions in the paper is a well worked out joint inference derivation, which performs amortized inference in this shared model utilizing an RNN. Experimentally, the authors verify their joint model on a waterworld environment, which consists of simple object shapes with semantics. If they use one of these perceptual grouping models as feature extractors without joint inference, would A2C work well? This is not demonstrated convincingly and is a huge missed opportunity. I am left without a firm understanding if the highlighted contribution of joint inference benefits this model in any way.<BRK>In my opinion, it should be stated more clearly in the paper that the main contribution is the application of the object based perception model within the control as inference framework, and not an introduction of a novel framework. The method is evaluated on a version of Waterworld environment. Would it generalize to other environments? Below are the main reasons for my decision.
Accept (Poster). rating score: 7. rating score: 6. rating score: 6. rating score: 3. <BRK>This paper looks at LSTMs with the intention of understanding their functional connectivity. I am not sure exactly what the relationship between the brain and LSTMs is being assumed or proposed herein — however I understand the need to understand complex neural networks regardless of their relationship to biological systems. I would have liked to have a discussion with respect to what the hierarchical organisation is due to. Is this merely a repercussion of the connectivity, for example? In terms of work that looks at ablation (i.e., damage), it might be useful to bear in mind limitations of such work if various (seemingly, perhaps) extraneous factors are not taken into account, see: https://doi.org/10.1007/s42113 020 00081 zI think this paper can be polished to the level of a solidly good paper if the authors can sketch out a bit more their rationale and syllogisms with respect to my above questions. * In LaTeX to open double quotes you need to use two backticks. Also the \cite and \citep commands should be used appropriately in terms of places where \citep is needed as well as use of optional arguments to avoid double parentheses.<BRK>This paper explores the application of innovative methods to track the flow of linguistic information in LSTM language models. In particular, the overarching question is how contextual information might be encoded in the network at the level of single units, and how context disruption might alter the LSTM dynamics and thus impact its predictive ability. The paper is clear and it tackles an interesting question. The approach is well motivated, and the authors give a brief survey of the most recent applications of this kind of methodology in linguistics and cognitive neuroscience studies. Also, the analysis could be improved by applying statistical testing in order to better quantify the strength of the observed effects. Moreover, I think that further analyses are required in order to better clarify some important aspects. In particular, I think that ablation studies should be performed in order to better identify the functional role of the “controller” and “integrator” units, whose actual functional role remains a bit speculative (and mostly based on structural / connectivity information). Indeed, as also noted by the authors almost “all the long timescale units are of unknown function”. Other comments:  Why did the author choose to test the model on a different corpus (Anna Karenina novel) rather than considering a test set from the same corpus from which the training set was derived? In relation to the results shown in Fig.3A, I did not understand how the thresholds and parameters for the k core analysis were chosen. In order to improve reproducibility, it would be very helpful to share the source code used for these analyses.<BRK>This paper applies tools from neuroscience to understand how language models integrate across time. The authors then measure how long it takes for the unit activations to become similar for the two different contexts, which provides a measure for how long the context impacts the representation. They find that (1) timescales increase at later layers of the language model (2) that only a small fraction of units exhibit long timescales (3) that long/medium timescale units appear to come in two forms which they try and characterize using graph style analyses. Pros:How language models integrate across time is clearly important, and this paper describes interesting first steps in characterizing the analysis of time using relevant tools from the neuroscience literature. The method presented is simple and broadly applicable. I also think that the sparsity of the long timescale units is cool and interesting. It’s not clear to me if the notion of time is a meaningful one in a language model. For example, the duration of contextual effects on a unit that codes syntactic number will presumably be highly variable and depend upon the details of the particular sentence being encoded. How robust are these results if one examines a different point in a sentence? Update: the authors have repeated their analysis for a different sentence point (after the 10th word) and report similar results. This analysis is helpful, though of course the 10th word is not a very principled break point, and there presumably is a lot of variation in timescales that are being averaged across. I continue to wonder how meaningful the notion of an absolute timescale is. The authors have motivated some of their analyses by discussing brain research reporting that longer timescale regions are more densely connected. Of course, the relationship between connectivity between large scale brain regions and the units in a LSTM remains highly speculative. 3.It would be interesting to know how dependent these findings are on the model’s architecture. The authors have attempted to address this point, but with limited time were not able to train a network to a high level of performance. Minor points:In Figure 4, it would be helpful if the absolute timescale was labeled in all plots rather than the rank of the unit or the “normalized timescale”.<BRK>_**Update after author response**_: I think this is a very promising paper, and I am really excited about seeing techniques from neuroscience employed to answer questions about neural network models. (for other suggestions and comments, see below). Lastly, there are a few unsupported claims, the most important of which that their method recovers the previously discovered units of Lakretz et al, while (as far as I understand), they actually only *use* their method to analyse those neurons, but did not find them independently. More specifically, the authors test the timescale of individual units in a word  and character level LSTM by comparing the units  activations values on the same sentence, but with different contexts. Lastly, the authors analyse the connectivity between the longer time scale units and find that the units with longer processing timescales make a larger number of strong projections. Within these units, the authors identify two sets of units in the word level LSTM: "controller units", that play a role in how the connectivity of the network is updated, and "integrator units", that instead integrate information. I think these methods can be useful for other researchers as well  Time scale analysis of LSTMs is a very relevant and interesting topic, that deserves more attention than it is currently getting*Concerns*  My main concern is that there seems to be a mismatch between the "language time scales" on which the authors operate: their experiment is designed to investigate the impact of extra sentential context, but the Lakretz et al results they keep coming back to concern syntactic phenomena that are only relevant *within* a sentence, which is a different scale. In other words, the units found by the authors of this paper are long distance when it comes to integrating context, but the syntax and number units found by Lakretz et al are not really related to that: they model relationships *within* sentences. Lakretz at all actually identified several syntax units, but only one of them was interpretable. I would suggest to rephrase this a bit. Since this is not addressed at all in the paper, it makes the results in general a bit difficult to interpret. That the authors do find a difference points to a potential flaw in methodology._  Relatedly, the authors say that  their result that the syntax unit is a long distance unit, while the number units are not. This is not consistent with what they say in the related work of the section, but also not with the results reported by Lakretz et al, who hypothesise that the syntax units represent the depth of the syntactic dependency. The results are a bit underexplained, and understanding them requires many back and forths to the appendix. How do assessing activation differences and correlations differ in terms of conclusions? These things should, in my opinion, all be worked out a bit better.
Accept (Poster). rating score: 7. rating score: 5. rating score: 4. <BRK>Questions during rebuttal period: 1. The paper has convincing evidence showing the reduction in percent of accuracy drop when applying projective clustering to the embedding weight vectors. What are some alternatives?<BRK>This work proposes a new approach, based on projective clustering, for compressing the embedding layers of DNNs for natural language modeling tasks. Rating: The paper presents interesting ideas for compressing embedding layers.<BRK>This paper extends the idea of using subspace clustering to compress the neural nets by considering multiple subspaces and projecting each point to its closest subspace. The paper needs more investigation on the related works. Basically, the idea and the technique is not novel.
Reject. rating score: 4. rating score: 5. rating score: 6. rating score: 6. <BRK>2."Conditional on the shot number" is a reasonable idea. Cons:1.The motivation for conducting such a work is not new, and more importantly, the proposed method is too incremental compared to many existing works. But I don’t think it is significant. 2.The experiments lack comparisons to the sota few shot learning methods (in the same setting of using varying shot tasks in episodic training). The paper does not prove that using the proposed conditional meta training can achieve better performance of varying shot learning over the sota few shot learning methods.<BRK>It proposes a shot conditional form of episodic fine tuning. The idea of shot specific feature extractors is not new, and it is a common trick in amortized variational inference to reduce the number of model parameters. I am not familiar with this area or related research papers, but was assigned to review this paper.<BRK>who propose a solution to shot overfitting in the context of prototypical networks. While I understand that the proposed approach is more general than that of Cao et.al. the experiments which still focus on the prototypical networks, which is not really an issue, but then warrants comparison to Cao et.al. I think the idea is simple and interesting, and now the paper has enoughexperimental comparisons to be a valuable addition to the conference.<BRK>I would suggest explaining in detail the smooth shot procedure in the paper. The proposed approach is simple but effective, it trains neural networks that conditioned on a support set of a different number of shots K during the episodic fine tuning stage, with FiLM as the conditioning mechanism. The content of the experiments is not self contained.
Reject. rating score: 5. rating score: 5. rating score: 6. rating score: 6. <BRK>Hopefully the weaknesses and comments will be addressed by the authors during rebuttal, currently, I view the paper as marginally below acceptance. While a possible strategy was proposed, it has not been evaluated empirically, see comments below for more details. I am thus maintaining my score. However, the results show that their approach is highly sensitive to the choice of the shape hyperparameter alpha.<BRK>The performance gain of proposed method comparing to other baseline methods which can also stable the training of RBF network is marginal (espeically in Table.1). The formulation of eqn.3 is interesting and naturally connect RBF and RELU neurons with a parameter $\alpha$. cons:  The essential idea of this compact support network is based on the assumption that $L_2$ distance of the representation space is meaningful.<BRK>The paper presents an approach that supports better performance when out of distribution cases occur. It does so by letting neurons be of only compact support and thus if the input is out of distribution (OOD) it is expected to be outside that support and therefore the output will be zero.<BRK>In this paper, authors propose compact support neurons to prevent high confidence responses from examples that are away from the training data. The design and training of such a neuron seem novel. In (3), what is R2? 3.From experiments, the proposed method shows effective for the out of distribution (OOD) sample detection task.
Accept (Poster). rating score: 7. rating score: 6. rating score: 6. rating score: 6. <BRK>The paper introduces Latent Convergent Cross Mapping (Latent CCM) that combines Neural ODEs with ideas from dynamical systems to detect causation between time series. The idea of using the latent space of a neural ODE to replace the delay embeddings in CCM is very nice, but I wonder if we can say that there is a one on one mapping between the time series from Neural ODE and the latent space?<BRK>Will you run into model misspecification problem of the latent process as both time series are short? In the context of this paper, more justification is needed to convince readers that you can replace X and Y with their (estimated) latent process counterparts. ##################################################Pros:The Neural ODE method is interesting and novel. It solves the problem of having irregular time series in the applications where the dynamic systems have been extensively studied and represented by ODEs.<BRK>This work proposes latent CCM, a causal discovery method for short, noisy time series with missing values. The method checks whether there exists CCM between latent processes of the time series without computing delay embeddings. Empirical results show the proposed method is more accurate in finding the right causal direction in various datasets. It would be interesting to report the computational complexity (time and space) of the proposed method. In terms of writing and presentation, It would be better to make the content self contained and make the use of terminologies consistent.<BRK>Thus, the authors propose to fit a latent neural ODE and theoretically argue that they can use the Neural ODE embeddings in place of the delay maps. The authors provide two sets of experiments, both on simulation data. * The authors argue that their method works for short time series. The ODE described in the paper is not an appropriate model for it. Please use another ODE for this case.
Reject. rating score: 3. rating score: 4. rating score: 5. <BRK>The authors investigate different tokenization methods for the translation between French and Fon (an African low resource language). However, I am unsure if this work is suitable for a machine learning conference.<BRK>Instead of just using subwords or statistical phrase identification, the authors propose to use the intuition of native speakers for translating AfricanFon languages into French (and vice versa). According to their experiments, BLEU and other indexes significantly improvedover standard IBM 1 phrase based machine translation. The property of African Fon languages, such asdiacritics and affixation, are not used here. If w is a word, what is the meaning of "w \subseteq v"? That being said, I strongly agree with the authors that neural machinetranslation of African low resourced language is important.<BRK>Also, I think putting the english translation in a different font or color would be greatly helpful to our eyes. I think this paper can reasonably be rejected, but I d like to give actionable of constructive criticism, since I do think the work on this low resource language is important for the NLP community. Its not that I/readers dont believe you when we are *told*, but being *shown* makes it much more interesting and give people an appreciation for Fon tokenization challenges! Contribution: The authors contribute a new tokenization method, code, and a dataset.
Accept (Spotlight). rating score: 8. rating score: 7. rating score: 6. rating score: 4. <BRK>SUMMARY: The authors present an elegant Markov Chain Monte Carlo (MCMC) method to carry out the task of generating molecular structures that satisfy several objectives. PROS:   The work is well written, concise and easy to follow  The methodology is competitive with other approaches that are state of the art in the optimization of single properties (such as GA D) and show that they outperform them in most cases. The references that it cites are balanced. The multiobjective optimization is based on biological objectivesCONS:  I see no major cons with this work.<BRK>The idea of MARS is based on generating the chemical candidates by iterative editing fragments of molecular graphs. The results reported in the following paper are very promising and show that this could be a good direction in the area of multi objective molecules optimization. 2.The proposed model is a new state of the art in the area of multi objective molecules optimization. In the experimental section the authors shows that molecules generated by MARS have the highest desired objectives in 5 out of 6 proposed tasks. 3.The molecules generated by MARS are evenly distributed in the space with a range of novel regions covered, as showed in Figure 3. This is a desirable behavior, better than in the other generative models, where we can see clusters of generated molecules. More precisely, the authors states that hidden state for fragment graphs is given by h^{graph}   MaxPooling({h^{node}_{u}}), however they do not state whether the nodes taken for aggregation are from the fragment itself or from combined fragment and molecule x or whether from the molecule x only. 2.How fast is the convergence of the proposed method? 3.The authors did not state what was the number of train steps used to converge MARS in their experiments. What number of time steps did you use? How long does the MARS training takes, compared to other methods? The problem of multi objective molecules optimization is hard and really important in cheminformatics. The idea proposed by the authors is novel and confirmed experimentally.<BRK>##########################################################################Summary:This paper proposes a method to generate molecular graphs with multiple optimized properties. Molecular graphs are constructed/edited by the iterative addition and removal of molecular fragments. The paper presents an interesting method for multi objective molecule optimization that shows good performance in the evaluation. However, I have some concerns about the benchmark tasks and baselines that hopefully could be addressed##########################################################################Strengths:*Paper is written in a clear way, and is well structured*Proposed model shows very good performance in the evaluation, compared to other baselines*Ablation studies that provide useful insight about the modelWeaknesses:*Some concerns about the benchmark tasks and baselines (see below)##########################################################################Questions and other comments:*I think that the summary product score (success rate x novelty x diversity) could be a bit misleading for the casual reader. Looking at Table 2, it is clear to me that different models have different performance characteristics in relation to the success rate, novelty and diversity model performance metrics. Eg based on Table 2, we would pick the MARS model for high success rate, GA+D for high novelty, RationaleRL for high diversity. *The abstract claim: "[The method] outperforms the best prior methods by 100% in terms of success rate, novelty, and diversity of generated molecules." *Any thoughts about the applicability of some public molecule generation/optimization benchmarks for this work, eg https://github.com/BenevolentAI/guacamol?<BRK>Summary: The paper proposes a sampling based approach for multiple property optimization in molecule generation. Also multi objective molecule generation is an important task. Weakness: I have several concerns regarding this paper. (1) Section 3.2 is unclear, with lots of details missing. In [1], it requires a large amount of existing drug molecules to pretrain a good MPNN model. However, baselines such as JTVAE directly output the generated molecules, all of which are kept. Therefore, in performance comparison if you compare the best molecules in 5000 molecules with a single molecule generated by JTVAE, this is unfair. (3) complexity issue: How many times do the authors need to query the oracles when performing optimization using the proposed sampling algorithm? How does this compare to the baselines? The authors also argue they adaptively train the MPNN, so the transition kernel will change during sampling. (5) There are also issues with experimental setting and results.
Reject. rating score: 4. rating score: 4. rating score: 4. rating score: 4. <BRK>This is done by training the classifier with the **adaptive label smoothing**, where each CNN input is assessed according to the actual proportion of the foreground object and the target vector is label smoothed to reflect the proportion. This paper has great potential to guide the researcher in such a direction. It is not fully convincing yet that the proposed solution is working, given the width and depth of experiments. Below are more specific comments on the weaknesses. It is not only the main task where ALS is falling short. The paper s answer seems to be no. My question is: does ALS really improve the quality of uncertainty? While this is true and it is a good signal, object removal is only a particular case for introducing uncertainty in an image. 3.**Structure the paper better.<BRK>Summary:This paper introduces the concept of adaptive label smoothing (ALS). The proposed method does not significantly improve over the baselines on object detection. While the idea is certainly very interesting and the problem is important, the authors should give a more elaborate experimental evaluation of their approach on a variety of datasets and with a larger set of network architectures and for different visual recognition tasks to highlight that their strategy indeed is of general relevance. After reading the other reviewers  comments and your responses, I think the paper is not yet ready for publication. All reviewers are concerned by the lack of a technical contribution and the limited benefit of the paper.<BRK>The idea of computing the smoothing parameter per example is interesting but it is not enough. I like the idea to compute the smoothing parameter per example where the value is the proportion of the object in the image. The paper is easy to read. **Cons:**Overall, I think the technical contribution of this paper is not enough for ICLR. The method section is very small and is based on the label smoothing mechanism. The main novelty is that the smoothing parameter is computed per example and its value is the proportion of the object in the image. The authors should improve the presentation of these tables.<BRK>The idea is to adjust the entropy of the labels in proportion to the amount of objectness of an image i.e.the amount of pixels related to the object. Strengths:+ the paper is easy to read and presented well, beside some small issues that can be fixed easily. Weaknesses:  experiments and results show limited benefit from the method. There is mention that 54k more images are derived but it is not clear how. Discussion in 4.2 does not discuss the other metrics ECE, U.conf, MCE enough to give some insights about the results. In particular the U.conf gets higher with the method.
Reject. rating score: 4. rating score: 6. rating score: 6. rating score: 8. <BRK>The Geometric Scattering Transform is a fixed feature extractor with a GNN structure and fixed wavelet filters that can extract both high and low frequency features. The authors then show that two useful properties of the original transform (stability, equivariance) are still valid The authors present results of their method on graph classification and regression tasks. The approach is principled: (1) Take a good fixed extractor (2) Relax it to better represent the data (3) show that the good properties of the original transform still hold (4) compare to the original transform. The relaxation of the laziness parameter is not actually used throughout the paper since it did not work well. The evaluation is rather limited, especially when considering the relatively minor novelty of the model. Section 2   it might be useful to exemplify/illustrate the transform on simple graphs.<BRK>Summary:This paper proposes a parameterization of the scatter transform on graphs and builds graph neural networks based on this parameterization. Lanczosnet: Multi scale deep graph convolutional networks. This part is not discussed in the paper. If the theoretical guarantees on the stability do not hold anymore, then the contribution of the proposed method will be degraded significantly. If the performances of your proposed model are on par with or superior to other GNNs on many datasets, then having fewer parameters would be a merit. It seems that the proposed methods perform comparable or better than other baselines on biochemical datasets but worse on social science datasets (as shown in the appendix).<BRK>This paper extends geometric scattering network by relaxing its scattering construction to enable training / data driven learning. Why doesn t this advantage manifest on performances on other types of graph data (e.g.social networks)? If LEGS only performs well on graphs that exhibit certain properties, showing results on synthetic datasets would help. * What are the advantages of the proposed method when compared with scattering GCN [1]? They also address the problem of oversmoothing and scattering GCN is also learned in a data driven fashion. Why is it not included in the baseline? Scattering gcn: Overcoming oversmoothness in graph convolutional networks.<BRK>Summary:This paper proposes a novel graph neural network based architecture. Building upon the theoretical success of graph scattering transforms, the authors propose to learn some aspects of it providing them with more flexibility to adapt to data (recall that graph scattering transforms are built on pre designed graph wavelet filter banks and do not learn from data). As a matter of fact, even a graph convolution with an order one polynomial can be a high pass. To see this, assume we adopt the graph Laplacian as the matrix description of the graph. This is an example of a high pass filter.
Reject. rating score: 4. rating score: 5. rating score: 5. rating score: 6. <BRK>MUSE: Modularizing unsupervised sense embeddings. The paper argues that it is unsupervised, "Unlike these methods, our language models learn word senses in a fully self supervised way". What are the relevant tasks? It needs a *multilingual dictionary* to know how many senses a word has and if it has a translation.<BRK>The correct sense is selected with the contextual representation and the model is trained to predict the correct sense. However, I cannot assess it as support for the claim as it s not an apple to apple comparison, and I cannot evaluate this paper based on future projection. This paper presents both English monolingual model and bilingual model with 4 language pairs. As a result, it’s unclear how well the model learns word sense compared to models with supervision. However, the result presented in this paper does not support this claim.<BRK>**Summary:**This paper proposes the alignment of cross lingual contextual embeddings not just at the word level, but at the sense level. This observation is central to the paper s argument for the need of the proposed approach in BERT, and elaborating on this with more analysis would have been nice. * The linear projection for cross lingual alignment has been shown to not work on zero shot sentiment classification. Do you have insight as to why this might be, why it adds noise to the embedding features? Perhaps by using monolingual data to train models with the sense aware cross entropy loss and dynamic pruning to obtain the different sense embeddings, and then aligning these sense embeddings (Eg: with MUSE). Would this help learn a many many mapping for BLI (where normally 1:many/many:many are difficult to achieve precisely because senses aren t taken into account)?<BRK>This paper proposes to introduce multiple senses into pre trained models. 2) The paper is well written and easy to follow. Is it possible that large scale models have already captured the sense information within the context and the context provides enough information for disambiguations?
Accept (Poster). rating score: 7. rating score: 6. rating score: 5. <BRK>Paper proposes a contrastive learning based approach to combine different data augmentation techniques for NLP tasks. Empirical results on the standard GLUE benchmark leads to an impressive 2.2% average improvement. The proposed framework can be applied with any text data augmentation methods. 2.Paper provides clear motivations and describes their methods, experiments in detail. Weakness: My understanding is that all numbers reported in the paper are from a single experiment. This result is intriguing.<BRK>4.Contrastive training (negative sampling) is one of the crucial contributions of this work. The experimental results on RoBERTa highlight the applicability and importance of this data augmentation approach on the downstream task of text classification (GLUE). This makes it difficult to conclusively prove that this is an "applicable to all" data augmentation scheme. 2.How do you measure the diversity (as mentioned in the paper title) in the generated samples?<BRK>The paper proposes a novel data augmentation framework, which explores different combinations of isolated label preserving transformations to improve the diversity of augmented samples. The authors find that stacking distinct label preserving transformations produces particularly informative samples. In my opinion, the exploration of different combinations of isolated label preserving transformations is the major contribution of this paper, which may inspire future works for data augmentation. Is stacking distinct label preserving transformations the default setting for CoDA in your GLUE experiments? MRPC, and so on.
Accept (Oral). rating score: 9. rating score: 8. rating score: 8. rating score: 7. <BRK>Experiments show that combining the predictor and corrector routines leads to better performance, a nice validation of the theoretical claims. I believe on its own this is a nice contribution.<BRK>I find the paper to be very well written and straightforward. However, this is a very imprecise statement.<BRK>#### StrengthsThis paper makes significant technical and empirical contributions to the emerging area of score based generative models. The empirical evaluation is particularly well done.<BRK>I think the proposed model and sampling algorithms offer substantial conceptualimprovements to the existing models and should be of interest to the community. Thepaper is well written and structured.
Reject. rating score: 4. rating score: 5. rating score: 6. rating score: 6. rating score: 7. <BRK>The two learning rules are not applied together, but rather a boundary is determined where layers prior use SGD, and the ones after use the Hebbian approach. For accuracy reasons, they include weak supervision by using the overall classification loss to control the sign of the update. Several techniques are introduced without ablations to measure their effectiveness and justify the added complexity. This is particularly important as these techniques add additional burden on the practitioner in terms of tuning the new hyperparameters. The work combines existing learning rules (SGD, Hebbian) with some novelty in how they are employed, and with a weak supervisory signal, to achieve reasonable results. These contributions were not foundational improvements, so the paper s main merit is in the potential practical impacts of this method.<BRK>The effectiveness of this approach is shown by the experimental results that report a nice trade off of minimal accuracy loss and good throughput improvement compared to baseline SGD and competing efficient training techniques. In other words, the hyperparameter settings for these techniques appear ad hoc (I suspect that it is not), and so it is not clear to me how much exploration is required. What could have helped is to include a study of the incremental benefits of these techniques in the experiment section. 2.The general approach of computing layers differently over the course of training is quite intuitive. 3.Presents two techniques that appear to make localized learning practical and effective for DNN training. 2.The proposed techniques are not sufficiently explained to help build intuition. 3.Incremental benefits of the techniques is not provided in evaluation.<BRK>This paper try to leverage the benefit of Hebb learning to reduce CNN training time cost. In order to achieve this,  a learning mode selection algorithm is proposed to progressively increase  number of layers using Hebb learning. The writing of this paper is good and the idea is also interesting, however, the experimental part should be improved:1. The criterion used in learning mode selection algorithm is the model update norm of current epoch. Could you just fix these layers to accelerate training? Does the proposed method still outperforms freezing strategy?<BRK>Accelerating DNN Training through Selective Localized LearningIn this paper, the authors proposed a new approach by the name of LoCal + SGD (Localized Updates) to replace the traditional Backpropagation method. The key idea is to selectively update some layers’ weights using localized learning rules. The authors provided some experimental results on common deep learning benchmarks such as ImageNet/ResNet and CIFAR/VGG.<BRK>I see this as a sign that the paper is currently not in a good enough state to really convey its potential impact. As such, I do not view the experimental results as the main contribution, but overall, the papers  main contribution is that it shows a way to include (gradual) localized learning in a neural network while not impacting performance. I think (1) might be better for you. Strong points:  Very creative use of multiple learning rules during training. You note that improves performance but by how much would be an important detail. Another issue is that your work is relevant in many different domains, but you keep it confined to the idea that your method is only useful for faster training. You do not need to elaborate on this, but I would like to see some of these connections in the paper because not everyone has the background to see these connections. This work is very impactful in many different ways, but it only mentions the computational efficiency perspective. It is a creative solution to a significant problem in localized learning. Recommendation (long):This work is impactful for multiple reasons:  Localized learning is difficult. The brain does not use SGD, and it is difficult to think about algorithms that work in the brain and yield good performance. This enables fully asynchronous training for early layers. I am not quite sure if this is the right way to evaluate this paper since I view it as having a broader impact that goes beyond the story in the paper, but other reviewers disagree with my view on its broader impacts.
Reject. rating score: 6. rating score: 6. rating score: 6. rating score: 8. <BRK>A brief summary of the paper. This paper proposed a novel architecture termed VEM GCN to address the over smoothing problem of GNNs in the node classification task. The main idea is to optimize the graph topology by removing the inter class edges as well as adding the intra class edges, and then the noise information would not pass between nodes with different categories. The framework learns with two alternating steps: E step optimizes the topology while M step improves the performance of GCN. 1.This paper proposes a joint learning framework for GNN classification model and graph topology, which leverages variational EM as a learning framework. The proposed method is clearly introduced. 3.Extensive experiments are conducted and results analysis are given. In summary, the novelty of the presented VEM GNN is a little bit minor. In summary, I agree that topology optimization is beneficial to enhance the node classification performance, but its effect on surpassing over smoothing is suspicious.<BRK>This paper proposes a method to alleviate the over smoothing problem of GNNs. The key idea is to generate a latent graph structure via leveraging stochastic block model to approximate the observed graph structure and label information. The whole framework is well designed as an MLE problem, with EBLO solved by an alternate EM style algorithm. Both E step and M step are assumed to enhance each other s performance, but this point is not clearly validated in the experiments. The methodology is designed well as an MLE problem. 2.The idea of joint topology optimization and node classification is not new. From the paper and the appendix, this point seems not discussed in detail. It is better to provide a clearer comparison between the complexity of the proposed method and some other recent topology optimization methods.<BRK>They incorporate a variational approach to GCNs, as a novel architecture that iteratively refines the node labels and the graph. Some comments to the authors follow. Could the authors discuss alternative choices to l2 minimization such as the paper Algorithms for Lipschitz Learning on Graphs by Rasmus Kyng et al.at uses Lp norm minimization as means to avoid over smoothing in a different but super relevant context of label propagation? The authors discuss drop edge. Can you prove that when the input graph is stochastic block model your method provably results in the right classification?<BRK>Summary:The authors present a method for tackling the problem of over smoothing in graph convolutional networks. The authors present an EM variational algorithm for approximating both this latent graph and using it to improve the estimation of a GCN. Strengths:The paper tackles an important question in the GCN literature, which is how to deal with situations in which the graph is unobserved or the observed graph structure is only a fraction of the true graph. The method proposed, modelling and optimising a latent graph, makes sense and is well justified. This includes empirical comparisons to a wide array of existing competing methods. The paper is well written and clearly describes the proposed method. It is a solid, clear, and novel contribution to the literature on GCNs that directly addresses an important consideration in these models that is often overlooked.
Reject. rating score: 4. rating score: 5. rating score: 5. rating score: 7. <BRK>The authors proposed a neighborhood to sequence construction & pre training approach to handling graph representation learning on large graphs. Pre training is a relatively standard technique for representation learning on large graphs, and the schema proposed in this paper has very limited novelty;2. The application of attention mechanism for sequence learning is also a standard practice that has been widely adopted in this domain;3. However, minimal theoretical discussion and empirical ablation study are provided to reveal the guarantees & benefits of the proposed method. Therefore, the limited novelty and contribution of this paper clearly outweigh its merits.<BRK>Overall, the paper propose an interesting approach for computing node embeddings in a scalable way. The idea of this line of works is to embedding node/graphs by random walk sequences. However, none of these works are mentioned in this paper. The authors should compare these papers in the experiments as the baselines. Without such comparison, it is hard to evaluate the performance gain of the proposed Neighbor2Seq.Additionally, the discussion with prior works is very weak. The paper misleads the readers by arguing "However, they have inherent difficulties when applied on large graphs due to their excessive computation and memory requirements" in Section 2.1. For example in the PinSage paper (https://arxiv.org/pdf/1806.01973.pdf), they can scale their GNN model to 3 billion nodes. I assume Neighbor2Seq is theoretically less expressive than WL test.<BRK>The paper proposes a method called neighbor2seq that converts the hierarchical structure of the center node to a sequence during message passing in graph neural networks. But it s surprising to see that Neighbor2Seq+Conv is better than Neighbor2Seq+Attn. I think there could be more information/experiment/insights added to explain why this is the case. 2.The experiment results do not seem to improve a lot compared with existing models for some datasets and tasks. Since people usually see that attention based approach performs better, is it because conv based one has more parameters. Does the proposed method have more parameters than existing models in general? Can you also provide some empirical comparison on the training time of your model compared with baseline models?<BRK># SummaryThis paper proposed a simple graph neural network architecture that is easy to scale up and perform stochastic training. Instead of performing message passing as commonly used GNN, this paper first performs weighted combinations of node features per each hop of the neighbors of a center node, and then performs either CNN or attention mechanism to aggregate the features and obtain center node embedding. Since the feature aggregation can be performed offline, and the computation can easily be decomposed and stochastic training is straightforward, the method can easily scale up to graphs with 10M nodes. Experiments on median size or large size graphs show the comparable or better performance than alternatives. # Cons  some comparisons are incomplete  Not sure how general this approach would be  time/memory cost can be reported and compared# DetailsOverall I lean towards accepting the paper. This paper provides a simple yet efficient and effective approach for graph node embedding calculation. It enjoys similar computation efficiency as the SGC, but is a bit more expressive in the design, where the CNN or attention is introduced on top of the per hop embeddings. I like such a simple design that adds the expressiveness without too much additional cost. It is also good to see the large scale experiments on OGB graphs. The SGC results are not presented in Table 4 or Table 5. I understand that the main purpose of this approach is scalability, but it would also be good to know the potential limitation on its parameterization.
Accept (Poster). rating score: 8. rating score: 8. rating score: 7. rating score: 5. <BRK>(b) The paper is carefully laid out and argued, and is at a nice levelof clarity and precision. (2) “Theory III: Dynamics and Generalization in Deep Networks” by Banburski et al.also considers general deep relu networks and shows that the resulting margins are max margin—requiring only separability, not orthogonal separability. (3) While the paper says that it is not directly applicable to deep nets, it draws motivation from the popularity of that literature. In that spirit, to justify such an evocation, can you show at least one experiment on a non synthetic dataset such as MNIST/CIFAR/etc (perhaps even simplified with hand engineered preprocessed features and subsetted to two classes) that would support the potential connection to deep learning? Is there some featureengineering procedure that tends to produce orthogonal separation?<BRK>This paper characterizes the implicit bias of gradient flow of two layer ReLU networks on orthogonally separable data trained on the logistic loss. The problem of characterizing the implicit bias of gradient descent on neural networks is an important one, and while the authors do make fairly strong assumptions on the data (data corresponding to the different labels lie in separate orthants), the proof is novel, interesting and non trivial. Is it possible to characterize what the outer weights (a s) converge to? If yes, I would suggest that the authors include this either in the main theorem, or as a comment after the theorem. 2.Does a similar result hold if the network also has bias variables? 4.How are the lambda_j s chosen in the near zero initialization?<BRK>This paper studies the inductive bias of two layer ReLU networks trained by gradient flow. Is there any more precision condition on \lambda, which controls the norm of the initial weights? Under a special assumption that the data are orthogonally separable, the paper shows that the dynamics converges to a unique max margin solution. The overall quality is good.<BRK>Note that the inductive bias given in Lyu & Li (2020) is in the form of a maximum margin KKT point of the ReLU network (as a *nonlinear* classifier), however, the result in this paper is more related to the maximum margin solution of linear models, which in general may be much worse than the margin achievable by wide neural networks. This paper studies the inductive bias of gradient flow for two layer ReLU networks for classification problems. Under this assumption, the classification problem can be solved trivially: one can simply randomly pick a training example and use it as the parameters in a linear predictor. While this is to some extent true, I find this comment not very convincing.
Reject. rating score: 3. rating score: 4. rating score: 4. rating score: 5. <BRK>The goal of this work is to learn FOL rules from raw data such as digits presented in image patches in an end to end fashion. My concerns are as follows:Motivation:  Neural symbolic integration usually refers to combining logic reasoning into deep model s decision process for better interpretability or sample efficiency, or to use deep models to help the logic reasoning tasks such as ILP or deduction. With that being said, I find the claim for this hybrid model to be unjustified. Claims: I find many of the claims in the paper to be ambiguous and lack justifications  In section 2, the author claims that differentiable ILP methods rely on fully trained NN for pre processing    this is untrue, for example NeuralLP is an end2end model that can be extended to the MINST benchmark with a perception module that s jointly trainable  The author also claims that most existing NeSy systems only utilize a pre defined knowledge base. I find Figure 2 to be difficult to understand   why is Prolog used here? It seems to suggest the proposed method is using Prolog for solving the constraints?<BRK>Summary.In this paper, the authors have presented a framework that combines meta interpretive learning and the abductive learning of neural networks. The high level idea is to formulate a unified probabilistic interpretation of the entire algorithm so that both the inductive logic programming module and the neural network modules can be trained jointly from data. Comments.The key idea of the paper has been presented clearly. The authors demonstrated two tasks: cumulative sum/product, and sorting. Neural GPU (Kaiser and Sutskever 2015) that can learn to add multi digit numbers without any builtin "add" operations. The authors are encouraged to make comparisons with these methods as well. The authors have made very strong claims in the abstract/intro about "To the best of our knowledge, MetaAbd is the first system that can jointly learn neural networks and recursive first order logic theories with predicate invention." For example, partial ILP and machine apperception can do that, too. Recently, there have also been other trials on using relational neural networks for bridging perception and rule learning, such as,  Graph Neural Networks (https://arxiv.org/abs/1806.01261)  Neural Logic Machines (https://arxiv.org/abs/1904.11694)Overall, I think this paper is not matching the publication standard of ICLR.<BRK>I like the idea of this paper, however the paper seems to be more written to impress rather than inform. I cannot see how it "learns ... simultaneously from raw data." I can t work out: what are the inputs? Surely there is so much noise that you can t perfectly predict the outputs. What is the accuracy of the correct program? Is the correct program in the search space with a non zero prior? The paper needs to be self contained. For example, you need to tell us that #  means equality and what Prolog s permutation  predicate is  (is it related to permute() in Figure 3?) What you did at the top level seems right, but it is not described well enough.<BRK>The paper proposes an EM framework (Meta_abd) for iteratively learning the parameters of a neural network ("perception" component), and inducing the logical rules underlying a domain ("reasoning" component). What re its space and time complexity? Hence, I find its contributions with respect to these two aspects to be incremental at best. Further, EM as a framework for bridging noisy inputs and symbolic reasoning has been explored by prior work (the pLogic, ExpressGNN, and pGAT systems listed below). * Supplementary material, Page 4, Figure 10:How sensitive is Meta_abd to the meta rules? Like Meta abd s logical component, the logical part of these systems are used to infer the probabilities of the triples (neural probabilistic facts), which in turn are used to train an embedding model. In the context of these existing systems, the contribution of Meta_abd appears to lie only in learning rules with the existing MIL system, and thus seems moderately incremental. If they are, how are the probabilities derived (e.g., from conflicting pseudo labels)? This is something that the paper alludes to too on page 7 paragraph 2 ("converges to saddle points or local optima"). How does Meta_abd ameliorate this sensitivity? If so, how does it choose the best model?
Reject. rating score: 4. rating score: 4. rating score: 5. rating score: 5. <BRK># Summary The paper introduces a new function $L(x)$ so that, when optimised under certain objectives defined over continuous observation $x$ and discrete latent $z$, learns the correct clustering probability $p(z|x)$. The authors introduces modifications to the principled objectives in practice and demonstrate performance on toy and real image datasets. # Recommendation:I think this paper is not ready for publication, both in terms of clarity and impact. The comment "This is an attribute of distributed representation which is a fundamental goal in deep learning." update  No response is provided so I am maintaining the score. Mutual information is between two random variables rather than symbols (What is $z^*$?).<BRK>Summary.The paper proposes Neural Bayes, a special parameterization of the posterior p(z|x) in Bayes  theorem with a neural network when the latent variables z are discrete. The relation should be worked out. Hence, I vote for rejection of the paper in its current form. (it s an integral over x), so exact evaluation is not possible in a finite number of operations, and hence it s not closed form.<BRK>This work introduces a parameterization called Neural Bayes that facilitates learning representations from unlabeled data by categorizing them, where each data point x is mapped to a latent discrete variable z such that the distribution p(x) is segmented into a finite number of conditional distributions. The Neural Bayes MIM v1 objective is identical to the IMSAT method as discussed by the authors. Two use cases of the proposed parameterization are studied: disjoint manifold separation and mutual information maximization. What happens if we overcluster the data which is usually the case in practice since we have no idea about the true number of clusters.<BRK>  Novelty is minimal, given previous work in clustering, latent variable models, etc. The generalization to arbitrary manifolds is a strength of the paper (++). Experimental results are not the most impressive either. ": Could you please elaborate a bit on that?
Accept (Poster). rating score: 8. rating score: 7. rating score: 6. rating score: 6. <BRK>**Summary**The paper studies the implicit bias of gradient descent in the problem of matrix factorization. The results are quite clearly stated and explained. Section 5.3 could also perhaps be shortened, by making the results even more informal: the intuition behind the iterative rank growth of the solution of GF is very interesting and should be emphasized more than the technicalities. **Main comments and overall decision**I found the paper well written and pleasant to read.<BRK>The authors finally study deeper factorizations under the light of GLRLMajor comments  The article is easy to read and well organised. Some results could be made more explicit (see minor comments). I am not an expert in this domain, but it seems like the article provides an advance towards understanding the implicit bias of gradient descent.<BRK>The part on the `arbitrary` top eigenvector is unclear. This should be clarified. ## Overall assessment  I find the results and the methodology of for the results in the rank 1 case very interesting.<BRK>This paper analyses the implicit regularization in matrix factorization. They provide certain theoretical evidence for their hypothesis. However, I think their counterexample is invalid: $M_{\text{rank}}$ does not have rank 1 as the authors claim; in particular, it is not even symmetric and, hence, it cannot be represented in the form $UU^T$. I am willing to raise my score, if these issues have been addressed. Moreover, I am not sure whether I find this argument very convincing: The authors claim that one advantage of deeper layers lies in the fact that it keeps the low rank component more separated from the learned component. Furthermore, I wonder whether the proof in Appendix H could be clarified.
Reject. rating score: 3. rating score: 4. rating score: 6. rating score: 9. <BRK>It is a very poorly written paper. Multiple research papers have been published from the extreme of using stale weight to some form of sub network backdrop as a proxy for the full network. Prior work have all suffered with one or both of these two limitations: a) poor experimental framework, or b) not being able to meet the accuracy bar set by backprop. Therefore, I was hoping this submission would go significantly beyond these, which it does not. Each chip incorporates > 1000 processor cores"   This gives me no reason to believe that the authors experimented on any  real hardware .<BRK>2) Experiments on real hardware: The current evaluation on real hardware seems to be weak as the experiments are with different local batch size and backdrop batch size. 3) Novelty: Although this paper provides an analytical model and real hardware evaluation for local training, the novelty of the paper is moderate as it does not offer new solutions or improvements over the existing local training method.<BRK>However, I might be missing something but it seems that the "chunked" strategy is just the same as Greedy with J divided by 2. As the paper rightfully note, some methods might lead to poor generalization. First, the hardware used for this evaluation is clearly niche. Secondly, As the authors empathize it, the drawback of training large networks is communication between machines.<BRK>The paper explores and compares several methods for parallel training of deep nets. Despite the skepticism of the other reviewers, I still think that this is a valuable, thorough paper of high quality. Although I am not an expert in the parallel training, it was easy to understand.
Accept (Poster). rating score: 8. rating score: 8. rating score: 7. rating score: 6. <BRK>The paper leans heavily on SAL, and the change in terms of the overall method seems to be fairly small.<BRK>The approach is very sensible. Algebraic curves that work better. Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition. With that, it might also be nice to include some more qualitative results in the supplementary. Could this method support partial observations?<BRK>## Summary of reviewGeneralizing SAL to include derivative quantities is a natural next step for this line of work. The minimal surface property is well known in the surface reconstruction literature (e.g.[3] cited by the authors in Section 3) and the theorem shown by the authors appears to be for a specific case in 2D unless I am missing something.<BRK>The paper should at least explain the differences of the tasks if it doesn t compare to them. Compared to SAL, this work adds a gradient penalty term, which encourages the derivative consistency. I summarize the pros and con as follows.
Reject. rating score: 3. rating score: 5. rating score: 5. rating score: 6. <BRK>Strong point: the paper addresses an important problem. Three main weaknesses, which justify the score:•	The theoretical developments presented in the paper build on the Rademacher complexity, but ignore the conclusions drawn by Zhang et al.in Section 2.2 of their ICLR 2017 paper (Understanding deep learning requires rethinking generalization). •	The theoretical developments build on the assumption that (i) there exists a lower bound, valid for any input, to the distance between the output of each pair of neurons, and (ii) the proposed diversity loss increases this lower bound. •	Experimental validation are not convincing. Only shallow networks are considered (2 or 3 layers), and the optimization strategy, including the grid search strategy for hyperparameters selection, is not described. Minor issue: positioning with respect to related works is limited.<BRK>This paper proposes adding regularization terms to encourage diversity of the layer outputs in order to improve the generalization performance. The proposed idea is an extension of Cogswell s work with different regularization terms. In addition, the authors performed detailed generalization analysis based on the Rademacher complexity. The appearance of the term related to the layer output diversity in the generalization bound provides theoretical support for the proposed idea. The main weakness of this paper, in my humble opinion, is the lack of important details or rigor in the experiments presented. 2) End of section 2. The authors claim that the proposed diversity term induces "within layer" feedback.<BRK>The paper proposed three ways of diversifying outputs of neurons, and the analysis showed that the generalisation bound becomes tighter when the neurons become more diversified. It is an interesting finding, along with theoretical results and empirical results. However, it is also obvious that there are other factors that one can control to make the bound tighter, and regularising other factors might be simpler in terms of implementation and optimisation. 1.The constant C_4 in the upper bound of the weight vector connecting the hidden layer to the output neuron. \sqrt(J) decays linearly with C_4, and the first term in the generalisation bound for regression tasks decays quadratically w.r.t.C_4.Compared with a linear decay w.r.t.d_min, C_4 seems to be a better option to regularise neural networks. In practice, one can empose an \ell_2 regularisation on the top linear layer to control the overall norm of the weight matrix so that C_4 is controlled. Overall, I think there are other regularisations suggested by the bound that could be put into practice, which might also lead to good generalisation, and also simpler optimisation problem.<BRK>In this paper, the authors propose a technique to encourage the within layeractivation diversity and therefore improve the model performance. They also showed that encouraging the within layer diversity willhelp reduce the generalization error. The paper is well presented and authors provided enough intuition as well astheoretical evidence why the diversity would help. How does that affectthe distribution of the layer output on the unit ball? The experiment seems insufficient to support the argument. Only very simpleneural networks on two toy examples are provided. Overall I think this paper provides some insights to how the generalizationerror is related to the neuron outputs and vote for accept.
Accept (Poster). rating score: 7. rating score: 7. rating score: 6. rating score: 4. <BRK>This work presents a method for learning a slow changing ("low frequency") embedding function, which can be used as a state abstraction function in the context of Hierarchical RL. A high level policy, trained to solve the environments task, acts by selecting abstracted states as targets for the low level policy which is trained as in a goal conditioned fashion (acting in the environment itself attempting to get to the set goal). There is a lot to like about this paper. The work presents strong empirical evidence for the usefulness of the approach. I do have a few questions for which I would appreciate the authors comments:* In principle, the loss for the embedding function (highly) depends on the behavioral policy. In general the relation between $p$ and the policy is not entirely clear. * It seems the underlying assumption here is something like: the agent can independently control/change the state features in an unconstrained way (i.e an "action" is an offset vector s.t the next state is $s_{t+1} s_t+a_t$, but in such a way that the environment somehow  rescales  $a_t$ so that earlier entries are smaller). * Having said that I think the paper presents other good reasons and motivations for the focus on slow features.<BRK>**Summary:**This paper proposes a new method for learning subgoal representations in HRL. This objective allows for efficient exploration, which the paper justifies theoretically, and supports with some empirical experiments on challenging control domains. Thus, any significant progress in this area is useful. The empirical results are conducted on challenging domains and they appear strong. It is not clear if it is actually referring to a proper state in the Markov sense, or some approximation of a state, or even just an observation that might not be Markov at all. How does the slowness objective interact with approximation when you do not have access to the true state (which would be most of the time)?<BRK>The main idea of this paper is very nice:  that we want the features in the representation space for subgoals in HRL to be "slow" in the sense that they don t change much over primitive steps of the policy, but do change significantly over "macro" steps. Using this loss function leads to a sensible algorithm, although I was curious about why the representation learning happens at the same rate as the low level policy learning;  I could imagine that you might want to do that at a slower time scale (more like the time scale for the high level policy learning). Some of the explanation in this section didn t completely make sense to me:  Why do you attribute the success of your method to improved exploration? I don t completely see why that should be, in itself, a goal. Hierarchy works well when the subproblems are, in a sense, "serializable":  that we can achieve subgoal the first any way we want to, without thinking about (and/or making it harder to achieve) the next subgoal we re going to be asked to do. But I d rather have an interesting paper than a perfectly executed boring one. What is a "fact"? (If not, then the process is non stationary). What does it mean for the agent to be able to "move independently and identically in the state space"? It could always move left, in which case these variables would not be anything like IID. What, precisely, do you mean by "the features are all independent"?<BRK>### SummaryThis paper proposes to use slow features as the subgoal representation space in the HRL setting. The proposed approach adopts a slowness objective to learn a representation (high level action space) and use it to train goal conditioned policies (low level). How was this horizon decided for this environment ? What is the extrinsic reward used to train the high level policy exactly ? There is a chicken and egg problem regarding the parallel learning of the representation and its usage to train the low policies. The representation should first be trained on some area before trusting the induced training of the low policy, which requires exploring that area beforehand. It is not clear how the algorithm deals with this challenge. How does the proposed method work when this inductive bias can t be leveraged, and when exploring an area of the state space is required to build a useful representation of it ? Was it learned from the states or the images setting ? It would be interesting to see the oracle training curves along with the compared methods on Figure 3. "the high level policy guides the agent to jump out of the local optimum of moving towards the goal" : local optimum w.r.t which reward function ?
Reject. rating score: 4. rating score: 4. rating score: 6. rating score: 6. <BRK>The first issue is with regards to the motivation. It seems the only new theoretical contribution (i.e.theorem 2) does not provide a converging bound with which a convergence rate can be derived. This is demonstrated by the fact that the authors acknowledge in the theorem that epsilon is chosen, i.e.it cannot be arbitrary. It is also unclear how small a step size eta is required for this bound to hold. Skimming the proof in the appendix, it is clear that the analysis is based on bounding the error between the cont. time solution and the optimum and the error between the cont. However, it is unclear how the discretization error varies as the step size and number of iterations vary. The theorem also does not provide any basis to compare the three discretizations, since the same bound is provided, and the same set of assumptions is used for all discretization considered.<BRK>### SummaryThis paper studied two continuous time methods called  rescaled gradient flow and the signed gradient flow and proposed three efficient discretization, namely the forward Euler, Runge Kutta and Nesterov discretization. The paper demonstrated the finite time convergence of the proposed methods under gradient dominance assumption, and use experiments to show that the proposed methods outperform standard stochastic gradient algorithms in many deep learning tasks. While the paper claims that the study is in the context of deep learning, it is unclear to me whether the theoretical analysis is well suited for deep learning models. It would be more interesting to show  convergence result for stochastic setting where only mini batch of samples are provided each time. In abstract, the paper claims the flows include  non Lipscthiz or discontinuous system, can the paper give a few examples of such cases? By using sufficiently large $\epsilon$, the weak bound is always valid. Can the authors elaborate the intuition of (13) for deep learning models? Is it normal or does it have any underfitting issue? Since the main focus (theory part) is on deterministic setting, to confirm the theoretical finding and  to show the advantage of continuous time methods,  it would be more interesting to compare with Nesterov method or other gradient method for deterministic optimization. "Nesterov s Nesterov accelerated gradient descent" should be "Nesterov s  accelerated gradient descent"<BRK>Summary: This paper studies inertial algorithms motivated from discretization of continuous time systems. The focus of this paper is on rescaled gradient flows and studies three different numerical discretization schemes. The performance of the thus obtained schemes is tested on standard test instances in deep learning. There is no discussion on the efficiency of the method although gradient dominated functions have been studied in the literature to quite some extend. I very much like the idea of using rescaled gradient flows for optimization problems. Furthermore, I am missing a discussion with a comparison between the described method and the other first order methods under the stated gradient dominance condition. Pros: + The analysis of numerical discretization techniques of rescaled gradient flows is a topic that received a lot of attention and definitely is an important subject. Is this a stopping condition in the numerical approach? If so, it must be stated explicitly. I belief that it is more natural to relate the approximation results to these techniques. However, I might miss this point as the authors write on p. 16 that they consider the dynamics as a hybrid system with a possible jump at the optimum. In my opinion this is an imprecise formulation and should be reconsidered. The methods are not really algorithms but rather conceptual computational schemes.<BRK>This paper provides several discretization strategies for three optimization flows. Moreover, I understand that the author may wish to present the results as general as possible. The order should be 1/(1 \alpha). 3.In the context of deep neural networks, transferring the convergence guarantee from the continuous optimization flow to the discrete system is quite important. However, this paper does not utilize any detailed information on DNNs; hence it is more like a general optimization paper. 4.In the experiment, the format and the resolution of the figures are not consistent.
Accept (Poster). rating score: 8. rating score: 7. rating score: 6. rating score: 6. <BRK>The final form of the objective as a modification to the reward is a simple and easy to implement objective with the two binary classifiers. A theoretical derivation based on RL as probabilistic inference is presented that starts with the objective of matching the desired distribution of trajectories in the target domain with the distribution achieved by the policy in the source domain. I hope the authors can clarify this in the future either in the main paper or appendix.<BRK>This paper presents a method to do domain adaptation. The idea is to modify the reward function in the source domain so the learned policy can be optimal in the target domain. This is accomplished by learning classifiers that distinguish transition in the source domain from transition in the target domain.<BRK>## SummaryThe paper introduces DARC, a domain transfer algorithm motivated by maximum entropy RL. By introducing classifiers for the target and source domain, the reward function in the source domain can be modified such that it restricts the behavior of the optimized policy to transitions that reflect the target domain. I take note that data collection from the target domain is nevertheless necessary, in order to fit the classifiers. Might as well improve the simulator(model) in the source domain. Was this done in this way?<BRK>Overall, the paper is well written and the motivation is clear. The paper should be more clear in expressing that domain classification in reinforcement learning is not a new concept. This is important given the claim that DARC is for transfer in environments with different dynamics. But if the obstacle were in the source domain, I would expect the approach to be suboptimal in the target domain since the reward function would encourage it to go around an invisible object. Was this actually used?
Reject. rating score: 2. rating score: 6. rating score: 7. rating score: 8. <BRK>The authors claim that their new federated learning addresses the security and privacy issues of previous methods. In particular, for privacy, the users do not need to send their class embedding vectors to server nor other users. For security, the paper claims that the proposed method is secure against poisoning attacks and evasion attacks. StrengthsI think the major strength of the paper is to design a loss function and a way of modeling embedding vectors for users such that the embedding models can be learnt without sharing the embedded vectors to the server nor other users. 1.For privacy, can you quantify the privacy leakage of sharing embedded vectors with the server? Without a formal quantification, it is hard to claim your method is more private. However, in the proposed method, the server can still poison the model. Also, malicious users can poison the model training, which are more realistic poisoning attacks. But such poisoning attacks are not considered. I don t see how the proposed method can address these poisoning attacks. Some references on poisoning attacks:https://arxiv.org/abs/1807.00459https://arxiv.org/abs/1911.11815https://openreview.net/forum?id rkgyS0VFvr3. The proposed cannot address evasion attack at all. 5.Can you also report AUC to compare different methods, since you already show the true positive rate vs. false positive rate curves?<BRK>**Summary**Federated learning takes advantage of the fact that private user data does not need to be transferred and shared across devices or servers. One crucial hurdle in this scenario is that per device, only positive data are present, potentially turning the device wise training objective ill posed (all embedding are likely to collapse to a single point). As a way to introduce negative examples, FEDAWS has been developed and presented at ICML 2020. This paper recognizes a crucial security risk in the FEDAWS system, that embeddings of user data are transferred to the server, and proposes a more secure training methodology, FEDUV, that involves the error correcting codes. **Pros**The motivation is spot on. FEDUV magically solves this issue by pre defining a unique prototype vector for each user, which are not shared across users and are by design far apart from each other (this is the crucial trick!) Flatten the last part of Section 1 as paragraphs rather than itemize? However, its novelty is also eclipsed by the Yu et al.2020 (FEDAWS) paper. **After rebuttal & discussion**I still tend to think that the paper s scope can be adjusted relatively easily (it is not too difficult to insert more disclaimers and change the title), and we can force apply the adjustment by conferring a conditional acceptance. I had taken this argument as granted, but this is indeed not so obvious, given that there exist many attacks that are applicable in this kind of scenario, as R4 has argued. It would be great if the authors could quantify the improved privacy guarantee. I m okay with rejecting the paper then. I still like the paper quite a lot, but rejecting it will also give the authors a good chance to assimilate more points of views in the paper.<BRK>In this paper, the authors focus on designing a federated user verification solution. Specifically, the authors address two fundamental challenges associated with user verification, i.e., one class data (positive data only), and privacy protection (i.e., the raw data and the embeddings of the users and class). Technically, the authors extend a very recent work called FedAWS by (Yu et al., 2020), and introduce a user specific codewords, which not only protect users  privacy (i.e., not sharing the embedding with other users or the server) but also do not need the negative samples (i.e., the two loss functions in Eq.(5) reduces to one due to equivalence shown in Theorem 1). We can see that the main idea of re writing the Eq.(2) into two loss functions in Eq.(4) and Eq.(5), and introducing codewords are novel and effective, which also address the two challenges well. Empirical studies on three user verification cases show the effectiveness of the proposed solution FedUV. Overall, the technique is novel (and I like this idea) and the paper is well presented. I recommend acceptance.<BRK>The authors propose a method that allows training of UV methods without sharing any user (exemplar or class) embeddings with the server or other uses. Models are trained using gradient averaging on the server, so any leakage through that is not addressed in this work. The authors argue that this is the first work that considers secure training in a federated setup, with neither raw inputs nor exemplar or class embeddings being shared with the server or other users. * The idea appears to be novel and a significant delta compared to the SoTa in terms of security and the novelty of a secure embedding learning protocol in the federated setup were only (one) positive classes are available for training. Bottom of page 5 mentions that increasing the code words and presumably $d_{min}$ increases the performance, but no reasoning is provided. Is the embedding size ~64 in all cases? Is it merely there for the math to work out? In practice, $l_{pos}$ will likely never reach $0$ and a negative loss term could have a significant contribution to the loss surface. Page 6 mentions that increasing $l_r$ reduces the minimum distance of the code for a given code length. Is it because $r_u$ is sampled by the clients and no guarantees can be made? A more detailed discussion would be helpful. This work proposes a new idea that allows training embeddings for verification with only positive classes in a federated setting, while ensuring security. Some areas could be clarified in the paper, especially why it is sufficient to proof the redundancy of the negative loss term only for the global minimum of when $l_{pos} 0$. Assuming the authors can provide a satisfying explanation, I recommend accepting this work.
Reject. rating score: 5. rating score: 5. rating score: 6. rating score: 6. <BRK>This paper presents the TaylorGLO method for learning loss functions for classification. ## Pros1.The paper is well written and quite easy to follow. 3.Networks trained with the learned loss functions typically outperform those trained with cross entropy.<BRK>This paper proposed a method, called TaylorGLO, to learn the loss functions, for training deep neural network, by meta learning. The experiments showed improved performance of the TaylorGLO over cross entropy baseline on several datasets and with different network architectures. My major concern is about the parameterization of the loss function. As is well known, a qualified loss function in machine learning generally contains only one valley concerning its geometric shape.<BRK>So for high enough $k$, the space of such polynomials will be a reasonable approximation of the space of all loss functions we might care about. The intuitive motivation is that any continuous and full differentiable loss function can be written as a $k$ th order multivariate Taylor expansion (i.e.a multivariate polynomial expression). I m also curious to know the extent of prior art in continuous loss function optimization. Rather, the paper optimizes directly in polynomial space.<BRK>Pros: The paper is well written and is easy to follow. It is good to see that different approximates of loss functions are compared. The effectiveness of the proposed approach is also well illustrated numerically.
Reject. rating score: 4. rating score: 5. rating score: 5. rating score: 6. <BRK>The existing approaches to evaluate disentanglement require the supervision of the dataset and it is impossible for the unlabeled data. In this regard, the authors propose the new quantifying metric for disentanglement under the  limited supervision  setting. Also, under this metric, authors provide a VAE based framework that exploits limited supervision for the proposed metric. This part is very important to persuade reviewers and readers. As a baseline, [2] can be used to analyze the performance of $\Delta VAE$. Also, the authors need to emphasize the novelty of their method by comparing it with [2]. I m not sure why the authors didn t perform the experiments on the correlation between the previous factor based disentanglement scores and the proposed disentanglement score in the limited supervised setting.<BRK>Summary: This paper aims to operationalize the symmetry based disentanglement idea proposed in Higgins et al., 2018, and proposes a novel loss function that can be used to both evaluate and learn disentangled representations (including learning from datasets where only a subset of samples are fully labeled). The paper includes some experimental validation of the proposed ideas on synthetic datasets. An implicit comparison to diffusion VAE can be found in the results in Figure 4, when the number of labeled pairs is zero, but a more thorough comparison is required to understand the value of the model and metric. Strengths:* Novel method for evaluating and learning disentangled representations. Clarity* In the experiments, the g’s are given by the data.<BRK>By adding this measure as an additional loss term in a topological VAE, the authors show that the model can learn disentangled representations with a limited amount of supervision on some simple image datasets. Weak points:  some claims in the paper are misleading because they imply that the measure proposed is unsupervised, although it is totally supervised (see below for details). I recommend to reject this paper due to the clarity concerns and concerns about the validity of the claims, unless they can be addressed satisfactorily during the rebuttal period. Main concerns to be addressed:1) It is impossible to understand from the abstract, intro and related work section whether the proposed measure of disentanglement requires supervision about the transformation between pairs of data points or not. In fact, the measure is 100% supervised. Here are examples of misleading claims: Abstract: "Although several works focus on learning LSBD representations, none of them provide a metric to quantify disentanglement. Enigmatic discussion points: "Our LSBD metric and method require a number of assumptions, as explained in Section 4.<BRK>The main contribution of this paper is to provide a metric to measure the quality of disentanglement in the learned representation. This is done by using some data with supervision on the underlying transformation between the data points, and another set of data points where no labeling information is given. The proposed method uses a diffusion variational autoencoder. Pros:1) This paper proposes a metric that can be used to quantify disentangled representations. Cons:1) My first concern is regarding the strong assumptions used in this paper.
Accept (Poster). rating score: 7. rating score: 6. rating score: 6. rating score: 6. <BRK>This paper considers the problem of unsupervised meta learning, where the goal is to generate tasks for meta training without supervision. Specifically, the idea is to first train a generative model on the unlabeled set and then produce training and test sets for meta training by decoding the interpolation of latent space representations of multiple examples from the original unlabeled set. * The big benefit of this method compared to previous work is that it seems to require less tweaking per dataset. Firstly, why are Mini ImageNet experiments not discussed in the main paper but in the supplementary material? Thus, I believe it is a good idea to use whole ImageNet dataset as the unlabeled set, as the authors did, and the results for the method seem favorable compared to previous work. I think a note just needs to be added that the unlabeled set this method uses is much larger than the ones used in previous work for the Mini ImageNet comparison but I don t view this as a big negative because the data required for training is still unlabeled.<BRK>This work proposes LASIUM (LAtent Space Interpolation Unsupervised Meta learning), a novel method of generating meta tasks in an unsupervised way. The method interpolates between samples in latent space, choosing interpolation ratios. Its ability to use any pre trained generative model to perform unsupervised meta learning is promising. However, the paper does not comment on, which is best or what tradeoffs exist between the three methods. RO seems to win on Omniglot, but the paper only shows N for the hardest task (CelebA). We cannot know from the experiments presented here because it only tries one model for each setting.<BRK>This paper presents a novel method for generating few shot learning tasks from unlabeled data. The method is evaluated on common few shot learning datasets with good results. **Pros:**    The concept of generating few shot learning tasks from an unlabeled dataset by performing interpolation in the latent space of a GAN or VAE is innovative and promising. Any reason that those results were not presented? (2) In section 2, it says: “Our proposed approach, LASIUM, takes advantage of generative models trained on the specific domain to create the in class and out of class pairs of meta training data.” However, the goal of a few shot learning system after meta training is that the system should be able to make predictions on unseen input data, potentially from a different domain. Was labeled data also used by the competitive unsupervised approaches (i.e.CACTUs, UMTRA)? Also, the transfer learning results for miniImageNet seem to be missing.<BRK>#### Summary  This paper proposes LASIUM, a framework for constructing tasks for unsupervised meta learning for image classification via the use of a generative model on the unlabeled training data. This results in an unfair comparison, and naturally raises the question of how LASIUM would do if we used a generative model trained only on the meta training split of miniImageNet, which would be more in line with the protocol used for Omniglot and CelebA. The results indicate that the approach is competitive with respect to prior methods for the Omniglot and CelebA datasets, but curiously, results on miniImageNet are not presented. The experiments involve a reasonable variety of generative models, and the protocol seems to closely follow that of prior work. Both methods rely on an unsupervised pre training stage to "organize" the unlabeled dataset into a latent space.
Reject. rating score: 3. rating score: 3. rating score: 5. rating score: 6. <BRK>What is presented in Fig 1 seems to be policy iteration, and Theorem 1 seems to be stating policy iteration (yet $\pi^{n+1}$ has not been defined). Overall Feedback: Adding the KL based penalty is perhaps a useful idea (this may also not be an entirely new one). Is it the value function of the policy $\pi^n$. 2) The experiments are on only 5 domains.<BRK>I think overall the algorithm with the KL constraint is a good idea   as has been explored several times in literature   but I don t think this paper cites that work, nor compares to them along with not a rigorous enough evaluation and poor writing quality. I think the numbers are this unreliable. Analysis of approximation error: In the abstract, it is claimed that the paper performs an *analysis* of this error.<BRK>In this paper, the authors study the error introduced by the estimation of critic function in the Actor Critic algorithm. It is not clear how this double Q help to reduce the estimation error? In practice, the authors always use a one step update, but there is no guarantee that the one step update for both actor and critic can decrease the error or improve the performance. Given the contribution of the newly proposed algorithm, I will suggest accepting this paper. Update after discussionI agree with other reviewers  idea that the contribution of this paper is somehow limited.<BRK>Summary Authors investigated the effect of approximation error for actor critic. They derived an upper bound of approximation showing that minimizing the KL divergence between the two consecutive policies can drive this upper bound down. [+] Aside from minor grammar issues (see details) the paper is easy to follow[+] The trick to calculate alpha and beta automatically is very interesting[+] Empirical results are encouraging [ ] The experimental result section can enjoy more rigor. It is not clear how many seeds were used to generate these results. As Joelle discussed in one of NeurIPS conferences, it is easy use limited number of seeds and deduce very different outcomes (https://media.neurips.cc/Conferences/NIPS2018/Slides/jpineau NeurIPS dec18 fb.pdf). [ ] I think Actor critic has been shown to converge before. > Need space after .
Reject. rating score: 3. rating score: 5. rating score: 5. <BRK>This paper proposes an adaptive neighbor clustering method by estimating normal distribution on the representation space. In addition, the proposed neighbor clustering method can replace the KNN based neighbor clustering in the previous SCAN (Semantic Clustering by Adopting Nearest neighbors) framework for image semantic clustering. Overall, the paper is hard to follow and understand with many typos, incorrect notations, and especially the main term, VAE. Therefore, it seems that the proposed training can be considered as the previous contrastive learning with stochastic regularization. In Table 3, The clustering performance drop of SCAN with AC VAE is not marginal compared to the previous SCAN with KNN, even though the proposed AC VAE seems to produce significantly improved neighbor clustering results in Table 1. However, there is no analysis on this.<BRK>The model takes as a basis the encoder part of a variational auto encoder (VAE) and uses the properties of the distributions induced in the latent space to define cluster boundaries and avoiding the need of setting the parameter K (number of clusters), usually required in methods based on K Means. Questions and Suggestions:Shouldn’t be abs(mu_i   x_j) In Equation (3)? z scores are usually defined as (mu x)/std. Fix typos: “argumentations”  > “augmentations” (section 3.1)“Compression”  > “Comparison” (section 3.2)Pros:The paper is focusing on important problems in the field of representation learning. Cons:The presentation and clarity of the article should be better and it would benefit from a review of the EnglishThe use of terminology is not precise, leaving the reader oftentimes confused about what is meant. This is not covered by clustering accuracy.<BRK>The paper describes the approach  for clustering the image data using z scores for semantic representation. The paper states that the method performs adaptive clustering. Instead, as the reviewer can see from Figure 3, it is a z score based cluster assignment. It is unclear what is the precise meaning behind the ‘adaptive’ quality. 6.For Table 1, the results are shown with specially selected parameters. This is because the sampling process introduced by the proposed network creates uncertainty in the representation, which contributes the decline of accuracy.’* As I understand, the stochastic representation is essential for the z score (but not for KNN). Therefore, is there a way to support this claim by the ablation study, i.e.produce a deterministic (autoencoder based, for example) representation and see what the accuracy would be with the deterministic representation + KNN ?
Accept (Poster). rating score: 7. rating score: 7. rating score: 6. rating score: 4. <BRK>##########################The paper proposes an algorithm and an analysis of its convergence. However from what I have understood of the algorithm proposed, I find the methodological contribution very limited. I leave to other reviewers the evaluation of the convergence analysis. * For a machine learning paper presenting in the end a 3 line simple algorithm, the paper containsa lot of superfluous mathematical notation that crowds the paper and make the reading very tedious. On the other end, a description of the method this paper builds on is left into appendices.<BRK>Summary: This paper introduces Neural Jump Ordinary Differential Equations as a method for learning models of continuous time stochastic processes sampled at random time epochs. I also found the discussion around  justifying “irregular” sampling of the stochastic process to be poorly written. I don’t think this comment is appropriate. I’m quite certain that the sampling process introduces bias into the estimation; for instance, Theorem 1 of ref. [1] below provides sufficient conditions under which an “irregularly” sampled estimator of a functional of an SDE is unbiased. The authors must do a better job of justifying their method. Next, in Prop. 2.1, the authors state that the optimal adapted process approximating process is \hat{X}_t — but \hat{X_t} is only defined pointwise (i.e., at each time ‘t’) and it is not defined as a stochastic process. I believe it is inappropriate to call this a stochastic process.<BRK>Post rebuttal:  I thank the authors for improving the presentation of the paper and including additional experiments comparing to latent ODE. It seems that the model considered in section 3 falls broadly in line with ODE RNN and GRU ODE Bayes. On the other hand, the experiments section also doesn’t compare against latent ODE, which is a strong but relevant baseline. Note my comment here is somewhat handwavy about the precise technicalities, but it should convey the relevant idea.<BRK>This paper does have a contribution. My recommendation is that  the authors show it in a clearer (to the point) manner with an improved experimental validation. Besides the model, the paper also aims to provide a theoretical justification of the convergence of their method. I support my recommendation through the following points:   Following the well known (by now) neural ODE and neural jump SDE, the contribution of the paper seems minor.
Accept (Poster). rating score: 7. rating score: 7. rating score: 6. rating score: 4. <BRK>This paper provides a way to research the effect of soft labels in knowledge distillation from the perspective of sample wise bias variance tradeoff. Based on some observations about regularization samples, the authors propose the weighted soft labels to handle the tradeoff. Experiments on standard datasets show that the proposed method can improve the standard knowledge distillation. The motivation and logic of the article are clear.<BRK>In this paper, the authors studied the soft labels for knowledge distillation from a bias variance tradeoff perspective. Specifically, the authors first provide a mathematically descriptions of the bias variance decomposition in knowledge distillation. Then, based on the theoretically analysis and experiments, the authors proposed an novel weighted soft labels to help the network adaptively handle the sample wise bias variance tradeoff. The paper is well written and easy to follow. 2.The authors provide many mathematically proof in the paper, which could serve as a theoretically foundation of this topic. Maybe the authors can add more experiments for NLP to prove that the solution can be widely adopted.<BRK>Summary:This paper analyzes the distillation from the bias variance perspective. Beyond this, the regularization samples affect the performance. The bias variance analysis, the regularization samples, the weighted soft labels all make sense. +) The analysis is clear and reasonable. +) The experiments are enough to examine the effectiveness of the proposed weighted distillation (see below). What are the results? The main concern is that the grid search is costly in practice. Based on the quality of the paper, I select 6 as the initial score.<BRK>The paper shows a new perspective of tackling the knowledge distillation problem. The author(s) have decomposed the expected student s training error into the bias, variance, and irreducible noise parts. The motivation is clearly explained and the experimental results show that this new approach can improve the model training performance of the student on both CIFAR100 and Imagenet. concerns:  My major concern comes from the definition of variance and its corresponding implementation. But it seems like the author(s) are trying to fade this concept intentionally. First, the expectation s definition for Equation 1 is not clear. Second, after checking on the code, I only find one teacher is utilized in the implementation. Second concern is that the mathematical reasons behind the not too big or not too small weights for these regularization examples are unclear. Also, comparing with the variance definition of [Zitong Yang, 2020], have you tried multiple loss functions and observe the same phenomenon? minor issue: the sample wise error for one example x should be explicitly listed and the notation should be consistent through the whole paper, y(x), and y should not be mixed together.
Reject. rating score: 4. rating score: 4. rating score: 7. rating score: 8. <BRK>## SummaryThe paper proposes a new framework that computes "high quality" prediction intervals (PIs) _and_ an estimate of their conditional coverage. A theoretical justification of the loss is given under some regulatory conditions. The problem of conditional coverage estimation is certainly well motivated. What is unclear to me is whether the $\lambda_1   \lambda_1(\alpha)$ that would achieve a fixed target marginal confidence level $1 \alpha$ is known or have to be estimated via some sort of a tuning procedure. If the latter, doesn t it make the method prone to overfitting? On a related note, in the PI estimation problem, isn t it more interesting to estimate conditional coverage _conditional_ on a particular output of $L$ and $U$, and therefore, estimate $\hat P$ _after_ obtaining $L$ and $U$? ## RecommendationThe problem of estimating conditional coverage is an interesting one. The proposed method is not a convincing solution to the proposed problem. The version of the split conformal learning (SCL) implemented in experiments is somewhat outdated. Of course, this information is unknown, but this at least suggests that the average width may be too crude. Conformalized quantile regression. Adaptive, distribution free prediction intervals for deep networks. In the revised version, the authors have added "(4) Tune $\lambda_1$ such that $CP_{\mathcal{D} } > 1 \alpha$  where $\lambda_2$ and $\lambda_3$ are fixed from (3)." after (3) in Algorithm 4, which substantiates their claim about the marginal coverage guarantee. I still strongly recommend including Algorithm 1 in the main part of the paper, as a prediction interval is rather meaningless unless the associated coverage level is also known. This is great, except that it is hard to see *what* about the method is causing this improvement in performance. Why should there be such a difference in practical performance for the simultaneous training vs a "decoupled" approach, leaving aside the practical concerns such as the computational cost? Now that I have been thinking about this paper for awhile, I suspect that a great deal of the questions that the other reviewers and I have been asking are really about this need for *some* explanation for the improved performance. However, I revised my opinion and switched to (a) after going through the experimental section. The last comment posted by the authors threw me into doubt yet again, however, as it seemed to indicate (c) as the correct conclusion. In my opinion, both these issues need to be addressed before this otherwise interesting paper can be ready for publication.<BRK>So while you can’t compute $E[(A(X)   \hat{P}(X))^2]$, you can compare whether this is higher or lower for a particular $\hat{P}$ by just comparing $E[(Y   \hat{P}(X))^2]$. In classification, Y | X is stochastic, it is 1 with some probability A(X) and 0 with probability 1   A(X). As such, it’s important to compare with standard baselines (e.g.the 2 stage approach). Use the neural network features instead of training the coverage estimation model from scratch in the second stage, and show the MSE and calibration error values. I still think it’s unclear there is much interaction between the “high quality” confidence interval and coverage estimation. As the author response says, setting $\lambda_2   0$ and turning off the Ca module, would not affect the confidence intervals produced. #########################################################################Summary:This paper tackles two problems:1. Providing high quality prediction intervals for regression problems. 2.Estimating the coverage of a prediction interval (conditional coverage estimation). A key missing ingredient is that the paper does not explain why jointly estimating the coverage improves the quality of prediction intervals. I’m unconvinced about the experimental protocol (more details below), the setup and architectures seem different from Pearce, and it’s unclear if results are from a single split which hyperparameters are tuned on. On the plus side, the method seems to have narrower intervals so could be useful for practitioners if some of these concerns are ironed out. #########################################################################Pros:  The idea of outputting not only an interval but also a coverage estimate sounds interesting and potentially useful, e.g.it can allow us to identify cases where the intervals do not have the desired coverage. Measuring calibration of the coverage estimator makes sense (it is weaker than a pointwise guarantee, but stronger than a marginal guarantee). Although if using softmax instead of hinge is being positioned as a major point (I didn’t think it was) there should be a comparison with hinge loss. Taking a step back, does it even make the intervals better? The theory does not address this, and there aren’t any experiments that this L_CA component specifically helps. My judgement (not in the paper) is that the L_CA loss is indeed lower if the prediction intervals have high coverage (if \hat{k}_i is close to 1 and \hat{P} is accurate). Is it just from making the network deeper? The paper oversells a little in claims of “theoretically justified” since it only justified why the coverage assessment is accurate, but not why coverage assessment helps get better intervals   this needs to be edited. I skimmed the proof and it looks correct. I haven’t studied the proof of this theorem. My main questions are 1. The theorem doesn’t seem to explain the advantage of joint training vs the split procedure?, 2.<BRK>Will a constrained optimization instead be faster than this? The theoretical framework about the loss function is laid out clearly, and the performances on benchmark datasets provide accurate results which outperform the other baseline algorithms on high quality prediction intervals generation. ####Minor comments:I think a useful insight that the author can consider is that the calibration problem can be viewed as a classification problem in which the response is the event that the PI covers the Y value (\ind{ Y \in PI}). Then it would be class that the total loss is a PI problem joint with a classification problem. Hopefully the authors can address my concern in the rebuttal period. This paper considers calibration of the coverage incidence, which is important information for real deployment of any prediction interval method. I think the statement appears to be too strong in general therefore can not be achieved in practice. 2.The authors should make it crystal clear that they are not proposing a PI with conditional coverage guarantee; instead, just try to provide an estimate of the conditional coverage guarantee given the feature. 3.The methodology in Section 3 is not naturally motivated from the many discussions and definitions about the coverage estimation error starting from Definition 2.1 to the end of Section 2. It may be better to reshuffle the order of the presentation, or adding some explanation in Section 3 when Loss_CA is introduced. The main gap is that one cannot see how Loss_CA should be defined as in (3.3) after reading all these discussion about CE in Section 2. 4.Is there any advantage to consider the PI problem and the calibration problem in the same network? In this de coupled framework, Loss_IW and Loss_CO will be used in the PI problem and Loss_CA will be used in the calibration problem, and they do not need to share the same network. For the total loss (3.4), since the non coverage probability is incorporated into the loss, we have to manually or adjust these tune parameters until the marginal coverage probability is satisfied.<BRK>Summary: In the submitted paper, the authors study high quality prediction intervals (PIs). The paper proposes a novel design of loss functions to generate PIs and conditional coverage estimates. The theoretical justification for using the conditional coverage error (in Ca module) is presented and the numerical experiments with promising results are provided on multiple benchmark datasets. Pros:  The high quality and reliable PI becomes more critical than ever as machine learning models have been used in the real world decision making process. The paper is well organized and theoretical results are well explained. Numerical experiment results on multiple synthetic and real datasets justify the practical advantages of the proposed algorithm. (Note that the posterior predictive distribution can be directly derived from the posterior distribution). A comparison study with Bayesian methods will help readers understand the advantages (or disadvantages) of the proposed method.
Reject. rating score: 2. rating score: 3. rating score: 4. rating score: 5. rating score: 6. <BRK>The authors proposed FVAE but the associated objective function is not introduced explicitly, which is confusing. But it is hard to see from Fig.3.3) It is claimed that the significance of action is related to the capacity of learned latent information. 1) The authors arranged  three stages for dSprites. 3) How are the curves in Fig.5 derived? There are a lot of papers regarding disentangled representation, and the authors only compared with \beta VAE.<BRK>The authors thus construct a "Fractional VAE" (FVAE), and then construct sequences from Dsprites and Chairs based on their statement of the problem. I would also give the paper another read through for grammar. ", with an update for phrasing. Does it use this information? I think the positive experimental results in Figure 6c mean that there is something here. I think the connection between sequences of images under actions and the proposed method needs to be made, or, if I missed this connection, should be made clear. The initial portion of section 3 concerning dataset construction might also be moved to much later.<BRK>Therefore, I stand with my initial recommendation that this submission is not ready for publication and I endorse rejecting the paper. Nevertheless, I am willing to reconsider my rating if the authors are able to address some of the concerns and questions raised above. I genuinely believe that the proposed approach might pose a relevant contribution but the paper lacks an adequate presentation at the moment, in my opinion.<BRK>Moreover, it would be good to include experimental comparisons to Annealed VAE (for instance in Figure 6) to give more insights on the relevance of the proposed approach. In the latter, it is shown that the disentangled representations are not invariant to orientation of rectangles (A1, A3). Overall, the paper is well structured. While a quantitative analysis has been provided in Figure 6 using the MIG metric to compare FVAE and Beta VAE, it is still insufficient to make clear conclusions on the performance of the proposed method.<BRK>5.There exist some typos in this paper, like “leaned” for “learned”. Moreover, the logic of this paper is a little unclear, and experimental figures are incomplete. With some modifications, this paper could be an excellent paper.. 4.Some descriptions in the paper are confusing, e.g.“We argue that the factors are not the key to disentanglement since the learned representations are changed while the factors are unchanged (A1, A3), and the learned representations do not change while the factors are changed (A1, A2).” This experiment, from my perspective, shows that the learned factors are disentangled in a particular form which is not consistent with the preset ground truth.
Reject. rating score: 4. rating score: 5. rating score: 5. rating score: 6. rating score: 8. <BRK>Through this attention based reading process, DAM retrieves the most suitable information for the current task from distributed representations existing in the multiple memory blocks > As shown in this text, and as used throughout, the term "distributed representation" is overloaded in this work. The authors demonstrate that such a memory network does well in tasks involving relational reasoning. One advantage of memory based models is that they amortize compute across time: they pay an upfront cost to shape and store a memory *online*, but gain an advantages at read time, where they just need to do a basic lookup among the stored memories. A consequence of these  memory based approaches is that the models need to anticipate how they should store a memory given what might come in the future. By "distributing" the memories in such a manner, the model can flexibly retrieve one version of a memory or another, which enables more flexible computations when conditioning on that memory. In contrast, something like a Transformer pays a heavy cost at read time, as it needs to perform a full self attention operation (rather than simple lookup) across all stored memories. In this work, the authors propose the strategy of storing a memory in multiple different ways, which mitigates the risk that a single stored memory will be insufficient for what might come. has recently given way to a more simple approach using memory buffers and self attention. This is not to say that there is no value in developing memory based approaches, as there surely is. Unfortunately, I believe the inclusion of this loss makes the absense of a Transformer baseline even more troublesome. This is because, for this loss to be implemented, we need to keep around a buffer of previous inputs, which is precisely the memory cost associated with using a Transformer! So, given the previous discussion on how memory models can in theory maintain a constant sized memory, in the DAM this is no longer the case. Memory costs grow linearly with time because of the need to preserve inputs for use in the ARL loss. Altogether, the paper is well put together and written well enough to understand the ideas and experiments. There are a few minor points scattered throughout, but I ll just call attention to the following:"insufficient associating performance" > It is unclear what this means. "lossy representation" > This term is used throughout, but I m not sure it s warranted.<BRK>The paper proposes two extensions for existing memory networks (e.g., DNC) to improve associative reasoning: (a) multiple memory blocks instead of just one; and (b) self supervised training as auxiliary tasks. Pros:  Understanding why MANN works and doesn t, and how to improve it are still open problems. The first extension is simply about dividing the external memory in DNC into multiple blocks that connect via an attentive gate. The argument of having multiple memory blocks is to diversify the representation of the same input. A similar idea has been introduced in [3], although the motivation was different. The second extension is a reconstruction loss on the input signals. In terms of experiments, the tasks are too simple and particularly favor the model design in the paper. For example, in the copy task, we need faithful information from the input signals for correctly decoding (copying), which can be enhanced via the reconstruction loss during the encoding phase. The same thing applies to the associative recall task. With such redundancy in storage, it is not very surprising to me that dividing the memory into smaller blocks can improve convergence. The main baseline used for comparison in this paper is DNC, which, in my opinion, is out of date as it can be outperformed easily (e.g., existing works on memory networks [1, 2] show significant improvements over DNC on these toy tasks). Do the authors use L1, L2 or binary cross entropy loss?<BRK>The paper introduces a modification to Differentiable Neural Computer called Distributed Associative Memory (DAM) that comprises of 1) multiple independent memory blocks and 2) association reinforcement loss (ARS). Experimentally DAM improves upon DNC on multiple tasks and is showing comparable performance to some relation aware architectures. While it probably still requires further investigation, the fact that an architecture with factorized memory blocks can match performance of an explicitly relational architecture suggests that  this is indeed a step in the right direction and/or the relational benchmarks currently used in the community are too simple. ARL which is one of the two novel components of DAM is not described clearly. What exactly does "sampled input sequence" mean? Should it be called "subsampled" instead? Is it always valid to simply subsample individual input tokens? * Since the improved performance presumably comes from the multiple individual memory blocks, it is important to understand how exactly information is factorized across them and how each of the blocks is used. One can argue that a wide enough representation can potentially learn the factorization scheme and mimic it using multiple reading/writing heads. * I appreciate that authors do compare memory capacity across different DNC variants, but then it is important to do so for all the baselines and ideally evaluate all the baselines with the same number of floating point numbers reserved for memory.<BRK>The authors propose a distributed memory architecture which shares some interface with the Differentiable Neural Computer however crucially segments memory into a collection of K units. The authors show that by increasing K the model learns to use its memory for algorithmic tasks such as copying and associative recall and learn faster. The authors also propose an auxiliary loss to improve memory representations, which involves reconstructing inputs from the representations in memory. I think the scientific statement is quite clear here and the paper is worth accepting; the only shame is that the authors did not apply this approach to a richer task than bAbI. Also it would have been nice to compare the approach to a multi head attention transformer since these also use distributed representations (across heads). The authors may be interested in the following architecture MERLIN which also uses a reconstruction loss to improve memory representations: https://arxiv.org/abs/1803.10760<BRK>This paper proposes an extension of the Differenciable Neural Computer networks (DNC)In these DNCs, the reading operation on the external memory are done by accessing a single memory block, which represents a single piece of information or knowledge. The architecture proposed in this paper aims instead to give the possibility to access multiple memory blocks at the same time. In this way, the approach of reading memory is more holistic (Chalmers, 1992). This is a desirable feature of distributed representations and, then, distributed memories. Otherwise, these memories are just similar to the classical approach of representing symbols (Fodor and Pylyshyn ,1988). This debate of what is the main charateristic of distributed representations is revitalized in Ferrone and Zanzotto (2020). The paper is well written and results are convincing. However, there is a minor problem. There is not a direct link among equations in Section 2 and equations in Section 3. Are these equations linked only with the M, that is the Memory? Connectionism and cognitive architecture: a critical analysis.
Reject. rating score: 5. rating score: 6. rating score: 6. rating score: 7. <BRK>This paper mainly proposed an evaluation framework for optimizers in machine learning jobs. It points out that existing benchmarking often applies best hyperparameter or random search hyperparameter. Their proposed framework re evaluates the role of hyperparameter tuning in machine learning. E.g.Figure 5 is mentioned in page 5 and located at page 8. The major findings are as follows. The two cases the authors emphasized are practical and common in real life scenarios. 3.The experiments are conducted based on a variety of datasets and optimizer. However, I wonder if there is any way to support the effectiveness of the solution besides running different setups on the model. I wonder the time cost of adopting this evaluation framework, like the time it needs to have a consolidated result on the optimizers  performance. It is better to have an explanation of all the variables mentioned in the algorithm for clear referencing. Variable M, how it is decided and how it will affect the ultimate performance. There are some spelling mistakes in the paper.<BRK>This paper studies the topic of evaluating the performance of optimizers for neural networks. For example, one could ask a question about an algorithm s performance without hyperparameter tuning. In the RL experiments, it is said that the reward is the metric used. Despite what this paper does well, I cannot recommend it for acceptance because there are issues with the paper s arguments and some gaps in the evaluation procedure. In Proceedings of the 37th International Conference on Machine Learning. I think it is ok to evaluate these meta algorithms, but their performance should not be presented as representing the underlying optimization algorithm. Can the authors specify an exact research question this procedure is designed to answer? It should be evident directly from this question what the right way to evaluate the performance is. Shouldn t it be better? How was the study using humans conducted? Did an institutional review board approve it? The primary support for using Hyperband is that it is similar to human performance. The performance of all of these experiments are random. How is randomness accounted for in the results? Quantifying uncertainty is needed at both the per task level and the aggregate measure, similar to that shown by Jordan et al.(2020).The author’s may also be interested in probabilistic performance profiles (Barreto et al., 2010). Quantification of uncertainty is a necessary component for a scientifically rigorous evaluation procedure.<BRK>This work proposes two protocols for evaluating and comparing the quality of different optimizers. It points out that some existing, commonly used ways of comparing optimizers may over  or under represent the amount of time that hyperparameter search can take. I think the paper presents a fairly convincing approach for comparing optimizers, and thus for evaluating new ones against existing ones. I find the argument that Hyperband is a good choice because it more closely resembles human behaviour somewhat weak. I would also like to see some discussion of how others could use the proposed procedures  when deciding which optimizer to choose for their task. Finally, I think the authors could do a better job of explaining why CPE is the right metric to use (i.e., why is considering peak performance not a good choice?) However, I think it could be strengthened by more discussion of how this could be of use to the community in the future. This is especially important given that the no one optimizer seems to be universally best.<BRK>* One of Sivaprasad et al.(2020) s motivations for using random search is that it requires no hyperparameters (except for the search space, which is however assumed to be given by optimizer designers), which could otherwise inject some human bias into the evaluation process. **The paper proposes a new benchmarking protocol for optimizers in deep learning. The main argument is that previous papers have either neglected hyperparameter tuning or have employed hyperparameter search method that is far from how humans tune hyperparameters in practice. To mitigate the latter, the paper proposes to use HyperBand for automatic hyperparameter search. They show through a human study that HyperBand resembles human tuning performance more closely than random search. They then evaluate several optimizers on a multitude of tasks, including many recently proposed methods, under two scenarios: 1. This is a very similar result to what the performance profile shows in your Figure 4. The main result is that the recently proposed optimizers are not substantially better than Adam. On CIFAR10 and CIFAR100, Sivaprasad et al.(2020) also find an SGD variant to perform as well or better than Adam. By independently comparing recently proposed optimizers in two realistic scenarios, the paper would constitute a valuable contribution to the machine learning community. Major questions:* The main argument for replacing random search with HyperBand is that HyperBand resembles human tuning behavior much more closely. You provide the results of a human study as evidence for this central claim. What was the expertise of the participants? * You claim that in the evaluation protocol of Sivaprasad et al.(2020) each bad hyperparameter has to run fully. It is obvious that HyperBand is a more sophisticated solution, but there is no direct evidence that it is so much better that it justifies replacing Sivaprasad et al.(2020) s protocol. Is the cost of HyperBand low enough to allow for a sufficiently large M?
Reject. rating score: 4. rating score: 5. rating score: 5. rating score: 5. <BRK>This paper proposes to use the distance map layers for ensemble based defense. I like this paper, but I have some comments as follows:1. The matrix M in Eq(1) can be represented as M   L^T L. The Mahalanobis distance is equal to ‖x_1 x_2 ‖_M  √((x_1 x_2 )^T LL^T (x_1 x_2))It denotes that the distance map layers are equal to the fully connected layers. I wonder whether the similar modifications on the fully connected layers can achieve similar performance. If not, please give the reason. 2.There are some proposed SOTA methods like [1], Please follow the experimental settings as [1] for a fair comparison. Improving adversarial robustness via promoting ensemble diversity.<BRK>This paper aims to improve the robustness of ensembles of neural networks to adversarial attacks. This has been shown to be true in practice (He et al.). The distance matrices are learned via backprop on equation (3). The authors emphasize that a low Lipschitz constant is important for improving robustness, but I don t see how it matters if the ensemble is aggregating with majority vote over each member s top prediction. Clarity:The paper is clear at a high level, however, I had several questions (see above and below) and there were many typos.<BRK>This paper concerns on developing neural network ensembles that can avoid adversarial attacks. DML is mainly used to improve the diversity of predictions from the ensemble members, and is defined to be the Mahalanobis distance between an input vector and a output center which may correspond to a class. Each member in the ensemble will learn a different Mahalanobis measure, encoded in the inverse covariance matrix (M), as well as the centers. The experiments are extensive and really promising. Formula (1) is also confusing/wrong. It is not supportive when the authors say “we apply the uncertainty of Gaussian process to identify the low confidence regions of input space to the DML layer”. When working work with a binary classification problem, the proposed DML based approach may not provide a good solution. The authors should provide an analysis about this point as it can significantly affect the performance of their method.<BRK>The main concern in robust ML is that many attacks are transferable between models, leading to blackbox attacks. In this paper, the authors derive a new methodology to promote "orthogonality" between different ensembles   which should help reduce the problem of attack transferability. Note that this idea has already been introduced in [1]. The main contribution in this paper is to introduce Distance Map Layers (DML) as an additional layer which maps different class vectors to a class based on their distance from a learnable centroid (each class corresponds to a centroid). Please fix typos:Page 2: "The classification loss function will encourage the distance to the target class to be small while the distances to all other classes to be small, which induces compactness in the embedding space." Page 6: Fig 1 caption: "Distribution lo ..."   Distribution of References:[1] "Improving adversarial robustness via promoting ensemble diversity", Pang et. al., ICML, 2019.
Accept (Poster). rating score: 8. rating score: 7. rating score: 7. rating score: 5. <BRK>The paper proposes a new approach for abstract reasoning and explores it in the context of the RPM task. In contrast to other competing approaches, the authors seek to build into the model as few assumptions as possible to keep the model general and not specific to the specific problem or to particular annotations or supervision signals. The general capability that they seek to incorporate into the model is the ability to effectively compare and contrast candidates in tasks that require choosing the best fit. The task is important, presented carefully and is well motivated and the paper is clear and easy to follow. The related work section covers the necessary backgrounds including both visual reasoning and general and the RPM task in particular. The task is also clearly presented and the authors explain how it consists of two main sub tasks: (1) identifying the rule that links the existing rows or columns, and (2) comparing the candidate answers to choose the best fit. They propose network modules to solve each of these tasks correspondingly: a rule contrast module, and a choice contrast module, to complete each of these sub tasks. Experiments are performed over two datasets of RAVEN and PGM, and achieve 5.77% improvement on average over state of the art, and larger improvement in scarcer data regimes, which the authors particularly focus on. A particular comment that I have is that it seems that the model considers each answer by replicating the board k times for the k candidates, and considering each such completion alternative.<BRK>Summary This paper proposes a new approach that improves performance on the Raven sProgressive Matrices problem without auxiliary annotation by using twocomplementary principles to structure inference. (introduction)The RPM problem is a visual reasoning problem where a 3x3 grid of images is providedwith one empty slot and 8 choices to fill in to that slot. The goal is to chosethe correct image via reference to the logical structure of the grid. 2) Many of the incorrect choices obey some, but not all of the rules. (structure 1) Each of the 2 provided rows and the 8 possible completions of the 3rd roware embedded into a common space, and each choice is represented as a differencebetween that choice s embedding and the embedding of the 2 known rows. This representation is fed through an MLP sigmoid configuation to get a scorefor each choice and perform the final classification. 1.The proposed DCNet outperforms all baselines, without or without auxiliary supervision, on the RAVEN dataset. 2.The proposed DCNet outperforms all baselines, except those with auxiliary supervision, on the PGM dataset. 3.Performance degrades when the amount of training data is decreased, but not as much as CoPINet. The proposed DCNet outperforms relevant baselines on both datasets. (I agree that it isn t.) Or is this bad because of some practical issue? Do these two contrasts just make inference easier, or do both really need to be considered by any system that solves the problem in general? Significance:* The correlation of RPM scores with general intelligence in humans makes the problem interesting on its own; more than a toy problem. But the approach is still very specific. I doubt very much from this approach will generalize to other problems. Maybe there s not an intuitive answer to that question which has been found in investigations so far. Quality   The approach is well motivated and the experiments include lots of ablations and comparisons to baselines that make the conclusion well supported. Significance   This will likely have some small significance in the RPM literature. Improvements are small and the approach is fairly specific to RPM. However, the improvements are meaningul and the approach is interesting. Originality   Previous approaches to RPM problems have taken advantage of the problem structure, but this specific structure in this way. All of the factors above clearly point to acceptance, though general interest in this paper will be limited. Suggestions * The related work specifies that CoPINet also takes advantage of contrast.<BRK>Summary:The paper proposes a neural network based approach called Dual Contrast Network (DCNet) to solve Raven’s Progressive Matrices (RPM). The approach consists of a rule contrast module that compares the latent rules between the unfilled (third) row/column and the filled (first and second) rows/columns, a choice contrast module that helps in picking the correct choice among the given eight choices, and finally uses a 2 layer MLP to predict scores for the choices. Different from previous approaches, the only supervision used in the proposed approach is the ground truth choice. The approach achieves state of the art performance on RAVEN and PGM datasets. S2) The approach requires the least amount of supervision — the ground truth option, unlike previous works that require auxiliary annotations or assumptions. S3) The paper shows the contribution of the approach’s two main modules using ablation studies. The paper should contain qualitative examples showing what options previous methods predict, especially when they fail and the option predicted by the proposed approach. Is the task solved? If not, what obstacles still remain? W5) The paper claims that the ability to solve RPM correlates well with human intelligence. How well do humans perform on these datasets? I believe it should be much lower than that of the previous works as well as the proposed approach. If that’s the case, why do we want to build better and better approaches for this task? ————————————————————————————————————————————————————————————————————————————————————————————————————————————————————————————Update after rebuttal: I thank the authors for their responses to my questions. They satisfactorily answer most of my concerns. Overall, I agree with the concern that the proposed approach is specific to RPM and it s unclear how well it (or parts of it) would generalize to other problems but I think the approach is quite interesting, novel and achieves state of the art results. Hence, I think the contributions of the paper are significant for acceptance.<BRK>The paper proposed an inductive bias contrast module that improves performance on Raven Progressive Matrices datasets. The proposed module makes use of the prior knowledge that rules can only exist in rows and columns of the diagram matrix. While the paper claims to out perform previous SOTA, I have to point out that the authors are missing citations of the more recent SOTA methods on PGM and RAVEN datasets, such as [1] and [2]. If you take into account these more recent results, the proposed model only achieves slightly better results on RAVEN dataset, while not achieving SOTA results on PGM dataset. There is some (but limited) novelty in using the prior knowledge of RPM rules to design inductive bias module that works for this specific tasks. However the module is tailor designed for RPM tasks, which means it cannot be easily adapted to other types of reasoning tasks. While the authors reported scores on PGM and RAVEN datasets, the authors did not report generalisation performances on various generalisation splits of PGM and RAVEN datasets. These missing results are in fact more interesting, as PGM and RAVEN datasets are designed for measuring generalisation capability of neural models. In summary, I vote for reject as this paper is not citing the more recent SOTA results, is tailor designed only for RPM tasks, and lacks experiments on the generalisation split of PGM and RAVEN datasets. I decide not to up my score any further because my other two concerns remain. While the generalization performance is better than MLRN, it does not outperform other baselines such as MXGNet.
Reject. rating score: 4. rating score: 4. rating score: 6. rating score: 7. <BRK>** Summary **In this work, the authors proposed a rewriter evaluator framework for neural machine translation (NMT). The translation is achieved by an iterative way: at the $k$ th iteration, a rewriter generates a translation sequence $z^{(k)}$ based on the source input $x$ and the output of previous iteration $z^{(k 1)}$. ** Clarify **I think the clarify of the current version should be greatly improved. That is, the evaluator is not well defined. Why don’t you evaluate on WMT’14, where the baselines are provided in (Ott et al., 2019)? 4.It is better to give several cases on how your method gradually improves the translation quality. ** About the baseline **I am not satisfied with results of WMT’15 En De as you reported in Table 1. There are some pre trained checkpoints on English >German, which is available at https://github.com/pytorch/fairseq/tree/master/examples/translation. Therefore, I think you should re implement your method following the settings in (Ott et al., 2019) to get more convincing results.<BRK>This paper proposes a rewriter evaluator framework for multi pass decoding, where translation hypothesis at last iteration is refined by the rewriter and further evaluated by the evaluator to decide whether to terminate the iterative decoding process. Compared to previous studies, the evaluator offers this framework the capability of flexibly controlling the termination. Pros:  The idea behind the rewriter evaluator framework is easy to follow. The proposed method achieves significant performance improvement against several multi pass baselines on both Zh En and En De translation tasks. The authors claim that their model can better handle the termination. One important experiment is to testify how many iterations the model uses for translating one sentence, and what factors could affect the iteration number, such as sentence length (would long inputs require more passes?). Particularly, it would be great to have an experiment to compare the termination difference between the proposed model and the Adaptive Multi pass Decoder (Geng et al., 2018), and show evidence how the proposed one outperforms Geng et al., 2018 (i.e.RL based model). 6.The results, in Table 1, for WMT15 En De on newstest2015 are not convincing.<BRK>In this paper, the authors complement iterative refinement for neural machine translation with an evaluator model that controls the termination of the translation process. Their approach is an alternative to the policy network used in [1]. The translation process is terminated when the predicted quality of the current translation is inferior to the previous one by a margin of $\delta$ or more, or after a fixed number of iterations otherwise. The paper is mostly clear and the authors give enough details to reproduce experiments, at least for En De. Comparison to previous work is not that convincing because most use baselines that are significantly worse than Transformer. If so, what are the results? [1] Geng et al.Adaptive multi pass decoder for neural machine translation. EMNLP, 2018.<BRK>### SummaryThis paper proposes a multi pass generation process for NMT. It introduces an evaluator model that learns to score reference translations higher than model outputs, and serves as a policy to determine how many refinements to make during inference, and which samples to prioritize during training. Therefore, I d like to see a comparison to a simple MT baseline where the source is concatenated twice (as described in Sec.4) as input to the encoder. The performance of this rewriter evaluator is evaluated on Chinese to English and English to German benchmark tasks with RNNs and Transformer architectures, and shows superior quality than previous works. Samples that have a low quality score are prioritized, such that the translation (rewriter) module is trained with a focus on low quality inputs where rewriting can still yield improvements. In addition, the relevance of the evaluator during inference is demonstrated in a comparison with an oracle sentence selection. Table 1 hides the number of iterations/rewrites. It would be great to add, to make the advantages of the proposed method shine better. ### Related Work  The prioritized queue is related to works in Active Learning and Curriculum Learning. The experiments on Zh En could have been performed on a public benchmark like WMT 2017 as well. ### Details  Geng et al.2018 do not report any issues with variance in their experiments, so I d refrain from using generic RL statements to argue that their work is inferior. With longer inputs, the rewriter also performs more computation than the normal MT Transformer.
Accept (Poster). rating score: 7. rating score: 7. rating score: 7. rating score: 6. <BRK>1, $\exists$ in the beginning of an english (i.e.non mathematic) sentence is a bit weirdP.3 we define *the/a* invariant mapp.4 Prop. Overall my stance on the paper has not changed and I still recommend acceptance. COMMENTS########The paper is globally well written and understandable (despite notation getting heavier and heavier with the assumptions). The subject tackled is of great interest, and the theoretical angle adopted is novel and insightful. Experiments are eloquent. The same question could apply to $R (\Phi)$ that is the sum instead of the max (but the $\epsilon$ relaxation makes this change more natural). It rather appears now as a trick to use the differential constraint (the $\nabla$ could be replaced by standard derivative since $w$ is one dimensional).<BRK>### OverallThe paper considers interest settings of distributional change and corresponding learning formulations. It makes a lot of assumptions to obtain sample complexities that justify the use of empirical invariant risk minimization, and falls a bit short by not giving a formal converse for the inadequacy of plan empirical risk minimization, despite making the claim. Nevertheless, the contributions are insightful, and the paper may be worth sharing with the community.<BRK>As a reviewer I find it discouraging as the time needed to check correctness would be much more than what I can afford in a short cycle for a conference such as ICLR  result a little over sold in abstract/conclusion: it seems that the theoretical results are somewhat limited (see question on sec.3.3.2) and that is omitted in both abstract and conclusion, maybe giving a false sense of generality to the results. Summary: The paper investigates the choice of learning paradigms to reach out of distribution generalization, namely IRM vs ERM under different scenarios of domain generalization.<BRK>The main conclusion that I understood from the work is: the performance of IRM (provided the assumptions are satisfied) will never be much worse than that of ERM and will be superior to that of ERM in some particular cases. Review:The paper is rather dense and the exposition could be improved. It is difficult to judge the significance of the work. On the one hand, the proofs of most of the results follow fairly standard tools of empirical process theory and I am not sure how much more insights this work provides compared to that of Arjovsky et al, 2019. Pros: The theoretical results on IRM are novel and the authors introduce new variants of colored MNIST. Cons:  The proofs follow the same pattern as that for ERM.
Reject. rating score: 3. rating score: 4. rating score: 6. <BRK>The authors are advised to use state of the art data augmentation and regularization techniques in these experiments. Hessian based measures of sharpness come with caveats, as has been explored in the literature cited in the paper. I do not believe I understand what the paper is arguing here.<BRK>The conclusion of the article is not novel and therefore does not offer any new insight. Finally it is demonstrated that also different optimization techniques lead to results that question the validity of purely flatness based measures. 2.)The proof of the theoretical result (Theorem 1) appears to be incorrect in its current form (see below).<BRK>### 5.SummaryOverall I like the paper. I am a bit unsure about the generality of the derivations and the gedanken model, but the experiments show that at least in some cases flatness does not correlate well with generalization. ### 3.Points of my confusion#### a) The validity of the shift motivation for flatness as a relevant measure.
Reject. rating score: 4. rating score: 4. rating score: 6. rating score: 7. <BRK>The weakness:  The proposed method is a straightforward integration of existing methods on signed graphs and on handling over smoothing in GCNs. The theoretical result on convergence is only on the diffusion convergence at each layer, and it is a straightforward application of the linear algebra.<BRK>3.The writing of the paper is in a good shape and everything is clear to follow. 4.The authors also prove the convergence of the proposed diffusion layer. 2.In addition, authors need to compare with the above paper as well as it s very effective in link prediction even though it s not an embedding based method. The key part of the model (i.e., signed diffusion layer) totally borrows from the existing work.<BRK>Main Idea:In this paper, the author studied the problem of node embedding in signed networks. The network is trained directly with classification loss on edge sign prediction. Since there are no trainable parameters involved in the diffusion step. First, as an important hyperparameter, it is interesting to see how performance of different methods varies with it. Second, the embedding dimension of the proposed method is effective 64 as positive and negative are separated and combined.<BRK>There has been an increasing interest in the last several years on the problem of clustering/link prediction/representation learning of signed graphs, where edge weights are allowed to take either positive or negative values. The main contribution is the end to end pipeline targeted at link sign prediction and the feature diffusion step.
Reject. rating score: 4. rating score: 4. rating score: 4. rating score: 5. <BRK>This paper proposes a multi task Transformer that does speech and machine translation within the same framework. The authors conduct experiments on speech and machine translation and show reasonable performance. The results in the paper don t look terribly good either. Instead, we focus on establishing the importance of task information during MTL and having a single model for all the tasks." I think the authors should compare their model with T5 to have an apples to apples comparison. The number of tasks in this paper is very small (compared to T5) and this is not well ablated in the paper. Overall I recommend rejection. Results are also not very exciting.<BRK>This framework uses a single model with shared representations for learning multiple tasks together. Cons: 1) Seems like the implicit approach does perform more or less the same as the explicit approach. I agree that the proposed framework would be able to learn task specific information without any explicit supervision, but this use case is not well explored in this paper. Overall: The paper is well written and easy to follow with SOTA results. However, I am concerned about the novelty aspect of the proposed framework.<BRK>This paper combines Feature wise Linear Modulation (FiLM) with a single/multi modal transformer for a joint multi task neural network and applies the proposed model to two tasks, which are single modal machine translation and multi modal (speech/text) machine translation, speech translation, and speech recognition. I have the following comments and reservations:+ The paper is well written and easy to follow. There is a plethora of closely related work that was not mentioned or discussed or compared to, e.g., Zhao et al.. 2018; Strezoski et al., 2019; Cheung et al., 2019, to name a few.<BRK>This paper proposes an way to incorporate task information into multi task learning (MTL). The hope is that more explicit knowledge of task information will improve MTL, especially in cases where the model seems to confuse tasks (as shown in the example of speech translation in Table 1). Since the method is relatively straightforward, the paper would look stronger if it has more empirical analyses. For example in introduction the authors write: "However, providing task information is not straightforward for certain modalities such as speech and image. I understand it is harder to integrate the discrete label with the speech signal but it has been done before and the reason needs to be spelled out (i.e.don t assume the reader works in ASR.) It was also be interesting to fix to random weights in the task characterization networks to see if the results are expected.
Reject. rating score: 5. rating score: 5. rating score: 5. rating score: 5. <BRK>This paper aims to analyze 14 commonly used training tricks for weight sharing based NAS algorithms. A new metric   sparse Kendall Tau   for the correlation between architectures. As an emprical analysis paper, the authors do have some interesting finds. However, could these finds be used to improve existing NAS algorithms (which mostly already integrate multiple different training tricks)? A more concrete example is could the authors use the proposed findings to outperform the recent TuNAS? The authors mentioned "requires a smaller lr", whereas 0.005 is worse than 0.01. All the experiments in this paper focus on the DARTS like search space. There are also some other popular search spaces, such as MobileNet like search space. It remains a question on how does the conclusion generalize to other search spaces. In my opinion, the search is to proxy the evaluation task? Regarding the presentation, f_{xxx} and P_{xxx} in Sec 2 take me a lot of time to understand. In Sec.4.4, "The impact of this approach on the super net quality, however, has never been studied." The results of #cells and #channels for DARTS is reported in their rebuttal, and I believe many researchers try these hyperparameters in their experiments but just not mentioned in the paper. In Sec.4.5, the reference before "linearly interpolate" should be "Network Pruning via Transformable Architecture Search" instead of the cited one. I m confused about "disabling dynamic channeling". In addition, it seems this strategy can only be used for NAS Bench 101 search space, instead of others. In Sec.3.2, why does less smoothed loss landscape indicate a smaller learning rate? Would the authors mind adding the results of #channels 32 / #batch size 32/512 / #cells 1,5 in Figure 9? As agreed by the authors  response, this paper lies in a detailed analysis of existing tricks.<BRK>**## Additional NotesIn Section 4.4: It would be helpful to explicitly state how the learning rate is adjusted when you decrease the batch size, since this can significantly affect the final results. In Section 4.5: It would be helpful to update the submission to clarify: if you disable dynamic channels and train a separate model for each possible number of incoming edges, do you include comparisons between candidate architectures with different numbers of incoming edges when you compute the sparse Kendall Tau? (If not, it seems like the sparse Sparse Kendall Tau numbers for different methods might not be directly comparable to each other.) The submission presents several sets of experiments. **Notes on Paper Rating**: I have some concerns about how a few of the experiments are presented  (detailed below) which would need to be addressed before I felt comfortable advocating for the paper s acceptance. If they are adequately addressed, however, I think the paper could be a valuable resource for NAS practitioners. One caveat is that all the conclusions in the paper appear to be drawn based on experiments on the CIFAR 10 image classification dataset. While it s unclear how well the results would generalize to other datasets, I think this is an acceptable limitation, given (i) the broad set of experiments conducted on this dataset, and (ii) most existing NAS Benchmark tasks I m aware of are built around CIFAR 10. At various points in the paper, the quality of the one shot model is measured according to one of three different metrics: (i) "sparse Kendall Tau", which measures the correlation between accuracies measured according to the one shot model and ground truth accuracies obtain by training network architectures from scratch, (ii) the ground truth accuracy of the network architecture which is ranked highest according to the one shot model, and (iii) the probability that a network architecture found using a one shot model will be better than one found using a random search baseline without weight sharing. What this means in practice is that different sets of experiments use different metrics. For example, the experiments in Section 4.4 ("lower fidelity estimates") focus on Sparse Kendall Tau, while the experiments in Section 5 / Figure 11 ("Influence of factors on the final model") focus on ground truth accuracies. The 87.66 accuracy number for "Random NAS" in NASBench 201 was obtained by sampling 100 random architectures from the search space and retaining only the one with the highest one shot model accuracy. It was unclear to me whether "Random NAS (Ours)" used the same evaluation protocol. 2.It was unclear to me how the 91.33 accuracy number for DARTS NDS Random NAS was obtained, since I was unable to find that number when I skimmed Radosavovic s paper. 3.If I understand the table caption correctly, the "NASBench 101" table header is misleading and should be changed to "NASBench 101 (n 7)".<BRK>Summary:The paper analyzes the effect of different components of the supernet training process w.r.t performance of the final NAS models. + The discussion on batch norm is helpful. Weaknesses:  I did not find information on the supernet training process that is used. There are many supernet training method proposed in the literature but it is not clear what is similar or different in the paper training process. I looked for this but could not find it, it is possible I missed it, but this seems to be important information that should not be hard to find. As existing supernet models (e.g., OFA, SPOS, BigNAS) show, sampled subnets can be directly used, with the same level of accuracy compared to training from scratch. This seems to run counter to the main premise of the current paper, which is that the supernet only provides an architecture, which then needs to be trained properly. As such it is not clear how much of the analysis in this paper is applicable to current supernets, and consequently how useful this analysis is. Parts of the paper are very confusing and not easy to understand. As an example, in section 2, $f_{ws}$ is introduced as a mapping but not defined until much later. While parts of the paper are interesting, it does not discuss the training process of some of the latest and most effective supernet models.<BRK>This work analyzes commonly used heuristics for training the supernet in weight sharing NAS. The authors first proposes a new metric, sparse Kendall Tau, to measure the quality of the supernet. Then extensive experiments are conducted on three NAS benchmarks to empirically evaluate the heuristics, and pick the best settings. The supernet training quality is an important factor in weight sharing NAS, which, as argued in this paper, has not received enough attention. This work raises the concern about this issue and empirically shows that it can indeed affect the search result significantly. 2.The systematic benchmarking of different heuristics is valuable and serves as a useful guide to apply weight sharing NAS effectively. The experiments are all conducted on relatively small search spaces. However, the search spaces in more realistic settings are usually much larger and might have different properties. So it would be interesting to see if the conclusions in this work still holds on large search spaces. 3.It is probably expected that the low fidelity estimates would make search results worse since they are trading off search quality with efficiency. It would help to calibrate the comparison based on the search budget. For example, the comparisons in Figure 11 should be done while controlling the search budget for each setting and it would be even better to show multiple comparisons under different search budgets. Since the supernet can be seen as a larger standalone network, it is unintuitive to me why its loss landscape would be so different. It would also help to simplify the elements. For example, the sub figures seem to be also describing the differences in the search algorithm s relationship with the other components, but it is not very clear and there isn t any discussion about it. The use of "the proxy task" seems confusing. An example from (Cai et al., 2018): "they need to utilize~\emph{proxy} tasks, such as training on a smaller dataset, or learning with only a few blocks, or training just for a few epochs. However, this paper seems to use "proxy task" to refer to the target task that the NAS is optimizing for. It would help to clarify the difference in the paper. The term "super net accuracy" and how it is computed should be defined early on since it is used quite frequently and could be ambiguous to the reader. "This is not a reliable metric, as shown in Fig.9 in the main paper." An Investigation With TuNAS."
Reject. rating score: 4. rating score: 5. rating score: 6. rating score: 7. <BRK>##########################################################################Summary:This paper proposes a method to train a neural network by selecting a weight from a set of $K$ randomly generated weights for each edge in the network. The authors say in Sec.3 that the goal is to construct non sparse neural networks with completely random weights that achieve high accuracy. However, the model obtained by the proposed method is no longer a network with completely random weights, because the authors optimize quality scores instead of original weights. However, such a result is not so surprising from the viewpoint that the quality scores are optimized. Also, this paper has few practical implications. I would like to see if the network can still achieve a high accuracy when every edge has a common set of $K$ random weights. If this is the case, the proposed method may lead to a network that is efficiently compressed. ##########################################################################Minor concerns:(p.1) a fixed a set of random weights  > a fixed set of random weights Regarding the author responses, I have updated my rating.<BRK>### SummaryThe paper investigates a type of neural network in which one of K possible fixed weights is chosen in each neuronal connection. ### Quality, clarity, originality and significanceThe paper is well written and easy to follow. The main idea seems interesting at first sight and it is well motivated, but after some consideration of related effects in neural networks, the results do not seem very surprising (see below). The paper seems to not go beyond ad hoc conclusions of the form that these (peculiar) networks "perform competitively on challenging datasets" (which seems to be a bit of an overstatement to me). Why do the results not seem surprising to me: It is possible that I misunderstood the algorithm (which we could clarify in the rebuttal period, of course), but in my understanding of the described approach, the straight through estimation of the scores will lead to preferring the selection of larger (or smaller) weights where standard gradient descent training would lead to larger (or smaller) weights. It would also account for the observation of the similarity in error rates when the network has a sufficient number of weights to choose from. (2)"demonstrates that current networks can model challenging non linear mappings extremely well even with random weights" is not 100% clear because the weights are a *choice* among a set of *initially* random weights, but that choice is a result of training. This seems to happen mostly in the two sentences before 4.3 and there connection *and* statement are not entirely convincing to me "We find a random 6 layer network that performs as well as a 6 layer trained network without any form of pruning." Here it seems to me that the slot machine after training is in fact not random, because it was trained, and that training exploited weight correlations that potentially span multiple layers. The argument could be extended to regular training in the sense that regular training just picks out the random weights among all the random floating point numbers. * Why is K chosen from the set {2, 8, 64, 128}   it seems that some natural values in this sequence are missing or that a more log uniform spread would be more "natural".<BRK>Summary: The work extends an existing algorithm to train a convolutional neural network by selecting a subset of fixed random weights by (1) using multiple random values per weight, (2) using a sampling procedure to select the particular random value for each weight. Strong points:  Interesting results that extend results on the lottery ticket hypothesis and random weight learning. Recommendation (short):While the results are interesting, the paper is poorly motivated and does not conform to standards of scientific work. Furthermore, the paper claims that random initialized networks and trained networks perform the same, but it does not lay out evidence or an argument for this. I do not think you can get this work accepted in this round and instead should try to learn as much as possible from the discussion for a resubmission. Why is your way of doing something similar to quantization more interesting than other forms of quantization? Please also consider if it is true that selecting fixed random weights is training or not. I would say that it should be considered training. Finding subnetworks, like done in Ramanujan et al., 2019, is interesting because you have smaller trained networks, but you do not have subnetworks. It could be interesting if you do (1) a thorough analysis that yields some insights and make this an analysis paper, or (2) try to get better performance by doing both optimizations of weights and selection of weights (but this is very similar to Wortsman et al.2019).Some minor things:   Equation 3 has an additional W (h(*) already contains the W)  Figure 6 has an annotation error; I believe the upper line is supposed to be the PS methodRamanujan et al., 2019: What’s Hidden in a Randomly Weighted Neural Network?, https://arxiv.org/abs/1911.13299Wortsman et al., 2019: Discovering Neural Wirings, https://arxiv.org/abs/1906.00586<BRK>**Correctness**Reasoning in the paper is sound, experiments are well executed and many control experiments and ablations are shown. To further increase impact and significance of the work it would be necessary to really flesh out the advantage of the proposed method over other, similar methods ("it is not too surprising that the method works, but why would I prefer it over other methods?"). To make a clear stance for the reviewer discussion I have therefore increased my score to 7, though I would rate the paper at the lower end of score 7 papers. **Summary**The paper proposes a novel scheme to obtain well performing neural networks without classical adaptation of weights. I don’t see an obvious way for (i), but for (ii) a starting point could be to do more detailed comparison against other methods, in particular Ramanujan and see whether the proposed method compares favorably e.g.in terms of training stability of robustness w.r.t.hyper parameters. The method can be interpreted as finding a high performing sub network within a larger network of random weights. Instead, each connection can have one out of K randomly drawn values, and “training” consists of a backpropagation based procedure to find which value out of the K possible values to select for each weight. My concern is that the proposed method is conceptually very similar to previously known approaches (pruning a larger network which is also discussed in the paper, but also some methods for training low bit weight networks such as [1] and [2]). While the proposed method is an interesting alternative implementation, the advantages compared to the other approaches are fairly limited. I would expect convergence/test time performance of the method to benefit from such a schedule and perhaps even help close the gap to “greedy selection”. 2) A nice and extensive set of ablations and control experiments, as well as repetitions of experiments to establish statistical significance of the results. 3) The paper shows that networks obtained with the proposed scheme can also act as a good initialization for further fine tuning, leading to very well performing classifiers. In terms of impact this is another nice result to add, but probably not strong enough to replace standard initialization anytime soon. I would love to give this paper a very high score because of the great execution and presentation, but the lack of novel insights, or clear methodological advantages makes this hard. The proposed algorithm is a nice idea, but it’s not too surprising that it operates essentially on par with a previously proposed pruning method since the algorithm can even be conceptually recast as pruning in a larger network.
Accept (Poster). rating score: 7. rating score: 7. rating score: 7. rating score: 6. <BRK>## SummaryThe paper proposes a novel approach for third person visual imitation learning from observations, ie for imitating a different agent in a potentially different environment purely from visual demonstrations. Empirically, the paper shows that the proposed regularizations can improve performance across a wide range of third person visual imitation tasks, transferring between simulated agents with different appearances and/or morphologies. However, this baseline does not have access to the additional collected prior datasets used for the introduced "prior regularization" loss. However, it seems that DisentanGAIL w/ domain confusion loss, which applies the MI 0 loss from Stadie et al.2017, works well on many of the tested domains. what differences result in the substantial performance difference between TPIL and DisentanGAIL w/ domain confusion loss? why does the paper report the "max cumulative reward so far" not the expected cumulative reward of the current episode? the latter could give a better idea of the training stability of the different algorithms  out of curiosity: the shown scenarios are visually still pretty similar between source and target (eg background etc)  > how do you think an approach like the proposed one would scale to visually substantially different environments, eg from one kitchen to another? as mentioned in one question above: while the agent appearance and morphology changes between the experiments, the largest part of the observation, the background, is usually constant across source and target environment  > it would be interesting to see experiments with different background appearances to see the robustness of the method## Overall RecommendationThe paper is well written and the experiments are covering a wide array of third person visual imitation problems on which the proposed method shows strong results. The experimental evaluation seems thorough, all design choices are ablated. Therefore I cannot fully support acceptance yet, but if the authors are able to provide the additional evaluations and answer the questions posed above adequately, I am willing to increase my score and vote for acceptance. Particularly the clarifications about the usage of prior data in the baselines were very helpful and the added results with background differences are interesting! I am not sure whether it is standard to show the learning curves with the "max achieved return so far" in the GAIL literature, but if not I still think the true reward per episode should be shown to properly reflect the training stability of the algorithm.<BRK>Summary This paper proposes a method for performing observational imitation learning   an existing task that seeks to enable an agent to learn from visual observations of expert behavior in order to roughly imitate the expert behavior. Experiments illustrate that the method was able to learn and be performant despite visual and embodiment differences in the expert and agent domain on various mujoco environments, exhibiting substantially better performance to a different observational IL method. Experiments also illustrate good performance on a slightly higher dimensional task. Clarity and Correctness The paper has a few somewhat painful clarity issues that make it difficult to understand. The former two terms were never defined in the preliminaries, so it s not clear whether they constitute additional data that was not mentioned in the assumptions. The notion of "prior data" is used in the paper, but I cannot find a clear description of it anywhere. Significance The significance of this paper is that it demonstrates a more performant method for performing observational imitation learning, which is potentially more applicable than standard IL approaches that require state or state action traces. Why was TPIL not used as a method of comparison in Fig 3? The results shown in Fig 2 have rather short x axes, with a maximum of 20,000 steps of training. The comparisons would be more informative if training were run for more steps (e.g.1e6 or 1e7). "High" is subjective, and is perhaps not the most appropriate adjective to describe 7 dimensional tasks. Post rebuttal comments After reading the authors  response and the updated components of the manuscript, I thank the authors for addressing nearly all of my concerns.<BRK>This paper studies observational imitation learning, a problem setting in which the agent wishes to learn from expert observations, but the state, action, and observational spaces of the expert and agent domains can vary. In particular, DisentanGAIL aims to discard domain information in the presentation, while retaining relevant goal completion information. Overall, the problem setting is very practical. The performance gain is greatest in the domains where the camera angle changes, which is promising. I like the intuition provided in Appendix A and think some of it should be included in the main text. Moreover, I would’ve liked to see some analysis of the experiments to support this hypothesis. For example, a version of Appendix D1 would be valuable to include, and perhaps also another visualization demonstrating that other baselines fail to encode goal completion information. Wouldn’t the discriminator then be able to tell the two domains apart (especially if one of the differences is color) and fail to provide a meaningful learning signal to the agent? What’s the gap between DisentanGAIL and the expert policy? The additions in section 4 clarify the motivation much better and also highlight the differences with prior work. I ve increased my score from 6 to 7.<BRK>The authors optimize the standard GAIL objectives along with the mutual information constraints as regularizations jointly. The paper is also well written and easy to understand. However, I have a few concerns about this approach, which I will list as follows. 1.Regarding the novelty of this paper, I feel like the approach is not particularly novel. I can definitely see why the softer regularization works better, but I wonder if the contribution of this work is substantial given such a simple tweak. However, these seem to be more like implementation tricks rather than major contributions and I m unsure if they make the contribution substantial enough for the standard of an ICLR paper. I think these methods could serve as comparisons to DisentanGAIL. 3.Furthermore, since the approach seems to be a bit incremental, it would be nice to have the theoretical analysis that could justify the incremental changes and guarantee the convergence to the optimal policy (e.g.attaining similar results in [3]). 4.The challenging, high dimensional environments used in the paper are all in locomotion tasks in Mujoco. It would be nice to see more realistic environments such as robotic manipulation tasks like ROBEL and  Adroit etc., which would make the domain adaptation more appealing. Post rebuttal updates:After reading the author response and other reviews, I agree that the difference between the paper and prior works is now much more clear and the empirical evidence shows that the new method works well, though I m still concerned about the part where the authors add many components and make the algorithm much more complex and potentially hard to work in practice.
Reject. rating score: 6. rating score: 7. rating score: 7. <BRK>This paper proposes an interesting point process named diffusion modulated cox processes, which generalizes the stochastic intensity to a stochastic differential equation. The variational inference method looks sound. Especially the neural network solution is meaningful and will have an impact on the learning of point processes. Cons:  I strongly recommend the authors to further improve the presentation of the current draft. The definition of some of the notations are not very clear, such as Eq(3), what is the definition of the conditional expectation? The paper claims that $h$ is non negative and thus can be an identity function, which is false.<BRK>The paper under review proposes a variational inference procedure for a specific class of Cox processes whose intensity is derived from a stochastic differential equation. The methodology relies on a restriction of candidate solutions the the subset for which the drift depends on $x_t$, $N_t$ and $t$; the drift is then modelled with a neural network. Could the author(s) elaborate(s) on this point? Would it be possible to have an inference methodology estimating not only the drift but also $h$ (and $\sigma$?) * p.2:  in particular, we show that...  either  that  or  how . * p.2: VBSP has never been defined at this stage of the paper. * p.5: We call $Q^\star$ as the VB ...<BRK>The paper provides a stochastic variational inference method forapproximating posterior path measures for doubly stochastic Poisson processesconditioned on realisations of paths of the Poisson process. The intensityprocess is modelled as the solution to a diffusion stochastic differentialequation. In summary, though my knowledge of point process theory is not sufficient toevaluate all aspects of the paper in detail, I find the paper to be a solidcontribution worthy of publication at ICLR. The method is experimentally validatedon simulated and real data.
Reject. rating score: 5. rating score: 6. rating score: 6. rating score: 8. <BRK>Summary:  The paper proposes a Bayesian inference framework for domain generalization (DG) that deals with both the uncertainty and domain shit. The proposed method treats the uncertainty effectively by applying the variational Bayesian inference to the last two layers of the model (feature representations and classifier layers). Cons:   The technical novelty of this method seems a bit low. This is because this method is a relatively simple combination of Bayesian modeling of neural networks and some existing techniques for DG. In previous works for DG, techniques to bring distributions of model s outputs in the same class closer together across domains have been proposed. For example, CCSA [1] learns the domain invariant representations by matching the representations in the same class across domains. Minor comments:   definition 2.1 (domain invariance) is a little confusing for me although I can see what the method wants to do by seeing Eq.(7).For example, what is the formal definition of domain transform function? Although the loss function for domain invariant learning (7) seems to bring the distributions of the same class across domains closer together, can the method incorporate amechanism to enhance dissimilarity of the different classes across domains? I think that it might improve performance. There are some ambiguous statements.<BRK>The goal is to achieve flexibility in specifying domain invariance as well as modeling uncertainty. The variational approach could also help generalization. In particular, the authors decompose the variational distribution in terms of the domain and classifier parameters, applying the approximation to the last two network layers. The objective function is given as a weighted sum of the KL divergence corresponding the two component distributions. They conclude that the proposed method achieves generally improved accuracy on several image datasets, including PACS, Office Home, as well as MNIST variants. The proposed Bayesian formulation is natural and the resulting learning problem is computationally practical. Weaknesses:   Some parameters such as the weights \lambda_{\phi} and \lambda_{\phi} do not have clear Bayesian interpretations. In the experimental evaluation, different baseline methods are used for each datasets. Could we interpret these parameters from the MLE perspective?<BRK>Hopefully, it would be grateful that the authors could address my concerns during the rebuttal period. (2) The introduced domain invariant principle to establish a domain invariant feature extractor and classifier seems promising, which can lead to an end to end framework with CNN and a Bayesian network. (3) Extensive experimental results on four domain generalization benchmarks show the proposed method obtains a new state of the art performance, which is appealing and convincing. ########################Cons:(1) In the paper, the authors claim that they only apply the domain invariance to the last feature extraction layer. (2) In Table 1, what happens if without Bayesian and with both invariant, i.e., only introducing invariant loss into both classifier and the feature extractor? I notice that such a certain situation is missing in the experiments. Thank you!<BRK>Summary:The paper proposes variational invariant learning (VIL) as a framework for probabilistic inference that jointly models domain invariance and uncertainty. The approach exploits variational Bayesian approximation in both feature encoding and classifier layers in order to facilitate domain generalization and invariance. Comparisons to state of the art methods are well presented. Concerns:The authors state: "the Bayesian approach has not yet been explored in domain generalization" or "this is the first work to adopt variational Bayes to domain generalization". I may not strongly agree on these statements in their current form. I would expect the authors to rephrase their claims in a more specific manner to make this distinction.
Accept (Poster). rating score: 7. rating score: 6. rating score: 6. rating score: 5. <BRK>**GENERAL**The paper proposes to re think the fashion of using label information in the VAE framework. The authors propose to disentangle information about the label (or, more generally, the context) in a "hard coded" manner, namely, by using a separate set of variables for the label (context). The paper is written in a lucid manner, and the presented results are sound. **Strengths:**S1: The presented VAE framework (CCVAE) is interesting. S4: I highly appreciate that the authors used a dataset outside of standard benchmarks, and they focused on a medical application (the Chexpert dataset). Nevertheless, it does not affect my overall good assessment of the paper. **Remarks:**R1: In Eq.2, there is a fudge factor, \alpha. I would like to ask the authors whether they thought about adding this fudge factor, or it is simply unnecessary.<BRK>This paper introduces the characteristic capturing VAE, an architecture that extends VAEs by modelling label dependent characteristics in generative and classification tasks. In a multi label setting, the CCVAE learns representations of individual characteristics, which are disentangled by design to the one of other labels. The paper is well written in general, and is a good contribution to the important research direction on deep generative models focusing on using some labelled data to learn a disentangled latent space. The experiments in the paper are well thought and show convincing performances in terms of label dependent generations and interventions. This formulation is novel as far as I know, but leads to some challenges mentioned by the authors in terms of conditional generation/intervention with this model. * Is y in a CCVAE still an auxiliary variable? This discussion is not however considering hierarchical VAE architectures for semi supervised learning, such as the one presented in "Semi Supervised Generation with Cluster aware Generative Models" by Maaløe et al, 2017.<BRK>This paper focuses on latent representations learning when some labels are provided. It is not clear to me why this paper focuses on a VAE model rather than a GAN model. There have been GAN based methods [1, 2] also based on an auto encoding framework. In addition, in the VAE literature, [3] proposes a semi supervised method. I suggest the authors better clarify the novelty of the proposed method compared to [3]. Minor:In the experiments, the paper reports the quantitative measures for classification and disentanglement.<BRK>In this paper the authors have propsed a method to incorporate label information in variational autoencoders (VAEs) to captures the characteristics associated with those labels. Experiments show that using the label information helps them to betterintervation and conditional generation of images. The paper is well written. However I have few concern regarding the model. International Conference on Machine Learning. The authors should provide a discussion related to this method or if possible should compare their performance by extending the previous model for multiple features. There is no analysis regarding the same. If the first kl is not significantly low, there will be information loss which might affect the quality of the overall generation?
Reject. rating score: 4. rating score: 5. rating score: 5. rating score: 5. <BRK>Iteratively, it trains a classifier using provided successful states as positive and on policy samples as negative and use its predictions as the reward function to learn RL policy. Compared to previous work, it seems a major difference in this paper is to change the event classification model, replacing the neural network classifier with CNML classifier. Other comments are written below. Theorem 4.1 is defined, but it seems not used in the following sections. Similarly Section 4.1 also try to show CNML gives reward that can improve exploration and becomes goal oriented using the same example. It may be interesting to give some insights, as good reward may speed the learning process. On page 7, it may need explanation on how to find that BayCRL outputperforms in terms of sample comcplexity as shown in Figure 4. Is there any reason for line 7 in Algorithm 1 that skips meta learning.<BRK>This is also related to the comments below: it is not clear why the algorithm should work. I agree with other reviewers that a more careful revision of the paper, and a further analysis on the algorithm will be beneficial. How is such a reward model still useful?<BRK>Why is a prior strictly necessary for reward shaping? I have some issues withunclear statements in the motivation and method that should beaddressed. The authors reply, and the updated manuscript, helped my understanding of the paper. In addition, there is a simple ablation but I    would prefer more work on this. I think your paper is very interesting, and I hope that the authors are able to use this feedback to improve their paper. Perhaps some of these baselines which are not competitive can be    excluded, or perhaps different linestyles should be used.<BRK>This paper studies how to solve RL problems with a set of success states instead of a standard reward function. In Figure 4, I didn’t see the lines for VICE+count bonus. is not surprising since the former has prior knowledge. I think this needs to be further demonstrated in a different setting such as the one in the next point. For example, if there are 10 success ground truth states and only 5 are provided in the example set and the uncovered states are quite remote from the provided ones (but a reward function is available, i.e., if we reach these hidden success states, a positive reward will be received), then how would this degenerate the performance of the proposed approach comparing with other methods?
Accept (Poster). rating score: 8. rating score: 7. rating score: 6. rating score: 4. <BRK>The tactic for speaker modelling is that the speaker conditions only the scale and bias terms in the decoder. There is also a phoneme level acoustic embedding which is used in the same way, which at inference is taken from random sentences (why not in training?), and, I guess, is supposed to cover phoneme level idiosyncrasies of the speaker, although this isn t clear to me. However notice that, if I m not mistaken, these acoustic embeddings are used zero shot; it is *only* the speaker embedding that is the input to fine tuning, and this only via the normalization parameters. (Both the normalization parameters *and* the speaker embedding itself are fine tuned.) I am not sure what the speaker embedding is left to do with all this acoustic level input, but OK.<BRK>In the first stage modeling, the authors proposed a new phoneme level acoustic condition modeling in addition to the speaker and utterance level approaches. In the second stage modeling, they employ conditional layer normalization for efficient adaptation. The idea of TTS adaptation for customization of new voice is appealing to me. The proposed approach seems new and technically decent. In paper, it is said that the phoneme level acoustic encoder uses phoneme level Mel features as its input. In the inference, the phoneme level acoustic predictor uses phoneme hiddens as its input to predict phoneme level vectors. I think the Mel features used in phoneme level acoustic encoder contain personal voice information. On the contrary, the phoneme hiddens used in phoneme level acoustic predictor do not seem to contain any personal voice information because they are resulted from the phoneme encoder that uses text information only as its input in Fig 1. Please clarify this issue. The addition of three, that is, speaker, utterance, and phoneme level vectors to the output of phoneme encoder can be done either in element by element or by concatenation.<BRK>In this paper, the authors present AdaSpeech, a TTS system that can adapt to a custom voice with a high quality output and a low number of additional parameters. The model is based on the TTS model in FastSpeech 2, with several additional components. They also provide an interesting ablation study. Overall, the model architecture is interesting and results seem to show its validity. However, I was wondering why the authors didn t compare their results to other known multi speaker systems (e.g.multispeech or deepvoice 2 which were mentioned in the paper).<BRK>The novel piece that enables this is the conditioning of layernorm in the model on the speaker embedding. The grammar reads slightly awkwardly in places, but the paper is understandable and well structured. Descriptions of the model, experiments, and analysis of results are well done. There is little discussion on the theoretical side of the acoustic condition modelling, such as how the authors are able to determine that the utterance level and phoneme level vectors are modelling things like room condition. What is the loss used to train the phoneme level acoustic predictor? My intuition would be that your phoneme level predictor is trained only with phoneme hiddens (textual information only) (do these phoneme hiddens include speaker embedding information?), so at most it models some pitch or prosody information. 1.Similarly, I highly doubt the utterance level acoustic condition modelling does not also capture speaker information. What happens when using a speaker embedding with the utterance level vector extracted from a reference speech for a different speaker? I understand this is a big topic with ongoing research which is why it would be a big bonus if this paper can make any kind of progress in that area. 1.I am curious how your phoneme level predictor would compare with a VAE based setup, although I understand this can be difficult to set up so no action is required here. **4.2 Method Analysis**: What does it mean to remove conditional layer normalization?
Reject. rating score: 4. rating score: 5. rating score: 6. rating score: 6. <BRK>Based on the view of the Taylor method integrator the higher order derivatives with respect to time are an error estimate of the current time step and also reflect on the cost of computing the solution up to a certain accuracy. The idea in the above paper very similar to the idea of the regularization of the error estimate of an adaptive step size solver such as Dormand Prince. The authors say that in the Dormand Prince method "the error is estimated by the difference between the fourth order and the fifth order Runge Kutta methods". In general the work would benefit from a clearer exposition about adaptive step size solvers and the smoothness of the ODE at hand. I do not recommend to accept the paper since the described large changes are required for the paper to become a serious contribution. Further recommendations:  Give references to the claims made in the abstract already in the abstract even if the references follow in the text later. Define the term "procrastinated" in the context of neural ODEs. Section 2.1: The statement "It had been reported that approximating neural networks with differential equations can be done by many researchs" can be read in two ways. Maybe find a different formulation.<BRK>The proposed approach includes two main components. Particularly, the following points need to be clarified to understand the fairness of the provided comparisons. Without knowing the standard deviation, it s not clear if there is a significant improvement of one method over another. 3.How many steps of RK4 and Euler are done during forward? Recommendation (accept or reject) For the current stage of the review, I tend to reject the paper. However, I find the topic of the paper important to the neural ODE community and will make the final score decision after the authors  clarification on crucial experimental setups. Does the DISE strategy to choose an appropriate solver outperforms the strategy when we randomly sample solver for the next input during inference?<BRK>### SummaryThis study proposes a method to accelerate the forward pass in Neural ODEs, known to be a significant time bottleneck. The study is technically sound, the empirical results convincing, but the clarity could be substantially improved. ### QualityThe paper is technically sound and the claims are for the most part appropriately backed by empirical evaluation. In that regard, choosing the Euler method is a sensible decision." This claim is not really illustrated anywhere in the manuscript and it would be good if the authors show this, even if in a supplement. instead of "the auxiliary integrator selection network v is stabilized and we can deploy them", "the auxiliary integrator selection network [...] we can deploy it"; confusing sentence "which is our main experimental stage". ### OriginalityThe novelty of the study is two fold:(1) it proposes a regulariser to speed up the DOPRI ODE numerical solver;(2) it trains an auxiliary neural network to choose the most appropriate numerical solver for the Neural ODE between DOPRI, fourth order Runge Kutta RK4 and forward Euler.<BRK>Summary: This paper addresses the complexity of the forward pass inference in neural ODEs. The paper proposes to augment training of the neural ODE  with an auxiliary neural network that dynamically selects the best numerical integrator for a given input sample. The paper is well written and addresses an impediment to utilizing neural ODEs in practice. For example, it is not clear where the regularizer in Eq.(2) is derived from. Of course, there is perhaps no Markov structure to the data in this setting, but presumably the inputs in the set T could be viewed as i.i.d.samples?Could the authors comment on such alternate formulations?
Accept (Poster). rating score: 8. rating score: 7. rating score: 6. rating score: 6. <BRK>Would this further improve the numerics? A.Strong points  The simplified framework proposed by the authors seems to indeed distill the key aspects of the problem and the results on soft thresholding are very promising. B.Weak points:  There is a bit of a discontinuity in the paper when the scattering transform is introduced. C. Recommendation:I recommend that the paper be accepted. Thus it does not seem completely true that this network does not have any learnable bias terms. Similar remarks hold throughout the manuscript.<BRK>The paper is well written, well motivated, interesting and, to the best of my knowledge, novel. It combines theoretical insights with practical results that are only slightly worse than ResNet 18 on ImageNet. I think it is worth rephrasing this everywhere (even in the title) to avoid confusion. In summary, I think this is an interesting paper that does merit a publication. After the rebuttal: I d like to thank the authors for their answers, particularly for resolving the confusion about the term "contraction".<BRK>The proposed mechanism consists of iterating over tight frame contractions. ############Overall inclined for accepting the paper although I am a bit hesitant due to the lack of experimental details and/or code. Overall, the  main idea is interesting and novel and the paper is well written. ############Cons * Details on the experiments are largely missing, i.e.parameters etc are not listed anywhere in the experimental section. * No code provided or even mentioned in the paper.<BRK>Yet, the main focus of the paper in the first part is to construct the tight frame based networks and then in the second part to train scattering transforms based networks. While the ideas are interesting I have several concerns:1. The idea of encouraging tight frame structure is not new and appeared already in several works in the literature. Given these two concerns, I don t think the current novelty is sufficient for publication. The previous version was indeed confusing to me. I have raised my score although I think some points still need to be addressed in the revision following my previous comments as they were not fully addressed nor in the response neither in the revision: 1. 2.This is the work the authors should look at by Mallathttps://arxiv.org/pdf/1809.06367.pdfThey get similar performance to ResNet with a scattering transform based network. Indeed, also here it is not exactly the same network that the authors here are using but there are remarkable similarities and these should be well addressed.
Accept (Poster). rating score: 7. rating score: 7. rating score: 6. <BRK>Originality, Significance: The idea of approximating the marginal gain with a neural network seems new, and also broadly applicable across applications of submodular optimization.<BRK>This paper combines combines submodular surrogates for sequential decision making with imitation learning. The learning algorithm is a modified version of DAgger which is consistent with the expert and provably near optimal utility.<BRK>The learned scoring function can be used as a greedy heuristic in combinatorial sequential decision making tasks, in which expert policy is expensive to evaluate. 2.Strong and weak points of the paper    (1) Strong points: The considered problem is interesting and meaningful. This paper provides a new  submodular norm  when learning the scoring function, which seems to be effective in the shown examples.
Accept (Poster). rating score: 8. rating score: 6. rating score: 6. rating score: 3. <BRK>This paper proposes a new key point based object detector, PolarNet, which predicts the distances between key points and corner pairs (such as top left and bottom right pair or top right and bottom left pair) on polar coordinates. The use of polar coordinates improves the performance of FCOS by more than 4% which shows the effectiveness of the polar coordinates. Cons:I am confused about the corner supervision in section 3.4.2. It seems to me that the corner supervision is to train PolarNet to predict the offsets which are in polar coordinates. Why do the authors still need the IoU loss?<BRK>For tan, its derivative is $\sec^{2}$. My major concern is about the clarity of the paper and some additional analysis (see cons below). Hopefully the authors can address my concern in the rebuttal period.<BRK>The overall writing looks good to me. The figures and tables are also quite informative. It could be better if the authors could describe more details in the caption. By the way, the black color of rho and theta should be changed into a lighter one. I wonder if the authors could explain more detail about why PolarNet is scale invariant? Post rebuttal comments The rebuttal and the paper revision address my concerns.<BRK>I checked the FCOS paper and found the R101 results in the paper is 43.2 but the number in this paper is 41.5. This paper proposed an interesting idea, but I think way it is presented is not good enough to be accepted. The authors propose a solution to regress to the pair of corners (either TL+BR or TR+BL) in the polar coordinate, and select the corner pair that gives the smallest variance during training. 4.Experiments are not solid:  There are ways to reduce variance of offsets under cartesian coordinate, e.g.only use points within the center region of the bounding box to learn offset.
Accept (Poster). rating score: 7. rating score: 7. rating score: 6. rating score: 6. <BRK>This paper investigates how to improve the test time performance of learned image compression models through finetuning of the full model. The authors finetune the model (both the model parameters and the prior on the latent space) for every test time instance, appending the model updates to the bitstream. Although on the surface the idea of doing instance specific fine tuning might seem to be impractical, it benefits from the fact that the extra encoding time of fine tuning the model is paid by the sender. The receiver only has to pay the extra cost of decoding the model updates, which is fast if the coding distribution is factored (as it is in this paper). I think these give a nice feel for the way the method works and the finetuning progresses on this particular instance. The prior will have been learned jointly with the encoder on the global model, such that the encoder maps to parts of the latent space that the prior assigns mass to. It might be interesting to see the results if the encoder and prior are finetuned but not the decoder. Although if you are finetuning (and communicating side information for the prior updates) then it is probably very little extra cost to also update the decoder. Given that they are already doing so for the latent space itself.<BRK>Pros:1.The paper is well written and concepts are clearly explained. It s unclear from the description if the evaluation on UVG actually "adapts the entire model to a single data instance" (i.e., *for each image*) as claimed, or amortizes the model update cost over a batch of all the images in a video. The paper claims that "In this paper we consider the extreme case where the domain of adaptation is a single instance, resulting in costs for sending model updates which become very relevant", but this would highly misleading if all the experiments were conducted in a batch compression setting. 2.If the experiment did perform per instance model adaptation, then it would be much more convincing to evaluate on standard datasets like Kodak and Tecnick from the image compression literature, instead of frames of UVG videos. 4.It would also be interesting to compare with approaches that optimize the encoded latents (e.g., Yang et al., 2020), which also achieve close to 1 PSNR improvement at equal bitrate without the overhead of decoder updates. Did I understand this correctly? Can the authors comment on this choice of their method?<BRK>Quality (5/10) The proposed approach is sound and it would have been interesting to see the gains which can be achieved by fine tuning the decoder of common neural compression approaches. However, while the approach is model agnostic, the results and conclusions are not. (A reader familiar with compression will be very well aware that a neural decoder _could_ be included in the bit stream, making the conceptual contributions less interesting.) That is, the proposed approach is likely less effective in a more realistic setting. If model complexity was a concern, the authors could have evaluated their approach on images instead of videos. Alternatively, they could have chosen a different video compression architecture of low complexity but one which is still practically relevant. The significance of this contribution is only limited by the lack of a meaningful results. Even JPEG (1992) allows us to fine tune the Huffman table for an individual image ("optimized JPEG"). Compressed model updates are also used in parallelized implementations of SGD (e.g., Alistarh et al., 2017). Clarity (8/10) The paper is well written and clear.<BRK>**Summary**The paper describes an instance specific finetuning method for image and video compression including finetuning the decoder. **Weakness**  Method has only been evaluated with respect to its own baseline method (image compression model without finetuning). Method has only been evaluated on one video dataset, but by compressing frame by frame, therefore not taking advantage of temporal redundancy. Given that it is an image compression method, the proposed instance adaptive method could also be evaluated on the e.g.clic validation set. Do the authors have some intuition, why some videos are easier to finetune than others? *Minor*References of arxiv papers, which have been published before submission deadline, can be updated with the respective conference.
Reject. rating score: 2. rating score: 5. rating score: 5. rating score: 6. rating score: 7. <BRK>In this paper, it is proposed to have a new problem setting of "unsupervised feature learning", which as claimed, is supposed to be different from standard continual learning settings. In this proposed setting, a data point can be seen only once by a model for training. I think, this is too strict a condition, as it is reasonable for a model to keep a data point in memory for a short while so as to use it for training across multiple epochs. In the experimental setup, neural baselines are trained only for a "single" epoch, in consideration of the proposed problem setting, which doesn t make sense for all practical purposes as neural models need a decent number of epochs for training. The proposed model for the introduced problem setting is more of an engineering approach, relying upon some basic techniques such as clustering, novelty detection. There is no clear motivation for the learning algorithm, it is not clear how the model is optimized wholistically. The model is claimed to be brain inspired; for instance there is a component in the proposed model which has a "hierarchy of increasing receptive fields", which is nothing but CNN like neural net. Accuracy can be misleading for multi class scenario. (6) Only 10 new classes introduced from the 5 phases, what about the datasets with a large number of classes?<BRK>##########################################################################Summary:The paper introduces a non parametric approach, STAM, for unsupervised progressive learning (UPL), a variant of continual unsupervised learning with a single stream requirement. The paper is well written, easy to follow. ##########################################################################Cons:I’m not sure if UPL is a real and practical problem. On the other hand, as STAM relies on prototypical features from online clustering, essentially no parameters need to be learned. Thus, the comparisons between STAM and MAS/GEM presented in the paper are unfair as I believe the MAS/GEM models were highly undertrained in this case. What if STAM/MAS/GEM are all trained for two epochs, or trained on a larger dataset for one epoch?<BRK>This paper presents an "Unsupervised progressive learning" (UPL) problem, where a model is exposed to data in an non iid manner, and each training example is presented once. Simple to continual learning, but a little more explicit in the connections to the way biological agents learn. The paper is interesting. I appreciate the straightforward outline of the problem and connection to biological learning and some of the motivations for the model (e.g., long term buffered memory of centroids). The model itself though isn t very clear in a few regards that I think are rather important. If so, I m not sure what is hierarchal here, it s just ordered in terms of "patch dimension". 2) How large does the LTM get during training? Is there a maximum size? My main concerns though are on the size of the LTM and if this is really doing anything other than KNN with K 1 over a buffered memory of examples. I need to have some evidence that the LTM is doing something *general* w.r.t.those cluster centroids.<BRK>The authors propose an approach (architecture + algorithms) to unsupervised progressive learning in a non stationary environment (the number of classes grows gradually) by keeping centroids at several hierarchies, using a combination of techniques from online clustering, via computing and updating centroids, with novelty detection, and dropping (forgetting those deemed outliers). They evaluate performance in a supervised setting where they describe how they learn centroid to label(s) mappings. What do you do if too many centroids? Also, different classes require different complexities(in terms of distance to use, manifold learning), and one parameterfor novelty detection and another for when to add a centroid to LTM,may not be sufficient... Perhaps because there is no explicit feature representation. The paper was overall clearly written, and the supplements provide much useful detail on the experiments. It would be good toquickly give examples of the patch dimensions. section 2.II:what is t in C_l(t) (in footnote, they say they drop time index, so tis probably time)Does the online update take place only if (patch is) not deemedoutlier? The text seems to imply that, but it s not clear.<BRK>The authors propose and formulate a new problem setting UPL which addresses a number of limitations of current methods. I think this problem formulation is very important and relevant and the authors provide a fairly solid foundation about how to address this problem. I found the non deep learning approach they took refreshing and their experimentation demonstrates the effectiveness of it in this setting. I found the evaluation relatively convincing. More specifically, I find the classification and clustering results good. Again, the classification results are difficult to interpret as you do use the labels (albeit not for learning the representation), but it becomes more like semi supervised learning at this point. Evaluation involving these would be much welcomed. This is a criticism of the structure of the paper (and perhaps the main message), more than the work itself. Can the authors speculate on potential future limitations of this for the setting? If not a full set of experiments, "We have experimented with CURL but we found that its performance collapses in the UPL setting" leaves me wondering what exactly the collapse is.
Reject. rating score: 3. rating score: 3. rating score: 3. rating score: 3. <BRK>Strong points:The correlation loss is a great idea to make model predictions as different as possible. Weak points:  The paper is difficult to read, and the line of reasoning is difficult to follow.<BRK>As I understand, the reproducibility defined in this paper refers to the prediction variance w.r.t.the random factors during training, e.g., SGD. If the reproducibility is the metric, then a single model or an ensemble model with fixed random seeds would trivially be the best model. Besides, the experiments are done on MNIST and a private dataset.<BRK>The authors also propose a loss penalty to combat this effect called "anti distillation". For example, why don t you use an entropy penalty on the average prediction of the ensemble? The paper is written with many tangents, where alternatives are suggested but never analysed and limited motivation is given for the current approach. "Dropout as a bayesian approximation: Representing model uncertainty in deep learning."<BRK>As is the work should be arranged to more clearly present the contributions of the authors and the effects of the proposed method should be more clearly illustrated. Outside of applying this loss to the logits or predictions from various networks it is unclear what is the main contribution of this work relative to related works. The tasks which this approach is tested on, MNIST and private CTR data, show negligible performance change and the results aren t clearly explained. Please provide clear captions on the charts in future and perhaps test on a tasks where ensemble methods show significant performance improvements.
Accept (Spotlight). rating score: 8. rating score: 7. rating score: 7. <BRK>The authors present a universal medical attack method that can consistently produce adversarial examples across several medical imaging domains. The authors achieve this by developing a novel objective function that includes two terms, which they refer to as stabilized medical attack (SMA). The authors then provide an insightful interpretation of their SMA loss via KL divergence. Succinct and clear, addressing a known problem with any CAD system. They provide good justification of the technique. Both empirical studies are sound and demonstrate the expected results. The paper is generally clear. Cons:It would be helpful for the reader/audience to have an associated figure that graphically demonstrated the point made in 3.2. There is a compelling geometric intuition behind the idea expressed in the text above Sec 3.3 that would help present the contribution of adding the STA term.<BRK>The authors proposed to introduce a combination of a loss deviation term and a loss stabilization term to generate more consistent adversarial perturbations on medical images. At the same time, the loss stabilization term ensures similar CNN predictions of this example and its smoothed input. The authors tested against 3 different medical image datasets obtained by different modalities. Overall, I vote for good paper, accept, after minor revise. Minor points:Since the authors demonstrated only with medical image dataset, the title Stabilized “Medical Attacks” seems misleading and inaccurate.<BRK>The paper proposes to use a regularization term for stabilizing the perturbation trajectories in generating adversarial examples for medical image tasks. Finally, they demonstrate the effectiveness of their method on three medical image datasets for separate computer vision tasks by comparing with the state of the art methods for adversarial attacks. 3.The paper provides good initial results, showing that to some degree their method is generalizable. Cons:  1.The datasets used are rather small and not well known within medical imaging community.
Reject. rating score: 4. rating score: 4. rating score: 5. rating score: 5. <BRK>**Summary:**This paper is about fine grained visual classification, which is challenging due to high inter class similarity, high intra class variation and potentially also class imbalance. The authors propose a regularization term that is added to a standard cross entropy loss of a neural network trained in a supervised fashion. **Pros:**  Overall, I think the contributions of the paper is interesting and useful, particularly the extension of the confusion based regularization for class imbalanced data sets. Unfortunately, the presentation of the idea needs significant improvement, see below. They should rather be specific and concrete. For instance, the statement "When inter class similarity prevails in a batch, the BCN term can alleviate possible overfitting due to exploring image features of fine details" is hard to understand since it was totally unclear what the BCN term actually is at that point of reading. The results barely improve over SOTA, particularly for the three FGVC data sets. Although the performance is also not significantly better than prior works, the direct competitor (PC) seems to under perform clearly. The proposed solution alleviates the problem, which is good. In light of this, I think it would make sense to build such data sets synthetically from the existing ones (like Cub, Car, Air) by removing samples to increase class imbalance. **Minor notes:**  The author names can be dropped with the ICLR citation format: "... Dubey et al.Dubey et al.(2018) construct a Siamese" on page 3. Statements like "The confusion related formulation for dealing with intra class variations and inter class similarity in FGVC have two main implications" on page 3 require the reader to be very familiar with these concepts.<BRK>**Overview:** The paper presents an extension to the Batch Confusion Norm (BCN) regularization technique so that it can account for imbalanced datasets. The extension to BCN implies adding a matrix that determines its values as a function of class imbalanced statistics contained in a batch. As it is common in various long tail recognition papers, the paper is not showing performance on head or tail classes. This is misleading as the method can helping more head classes and thus improving the overall classification performance. * The clarity of the paper is very good. The motivation behind the FGVC and LT learning problems as well as the proposed method is clear. Thanks to the clarity of the paper I believe the reproducibility should be good. Also, I believe that the paper addresses an important problem with practical value. * 1) I am not sure if BCN is a proper regularizer. While I understand the geometry behind minimizing the rank of the matrix P, I don t think this is a proper way of processing the columns of matrix P which are *posterior distributions*. 2) BCN is conditioned to operate if the classes in the batch are unique. 3) The results for the long tail methods are lacking more details.<BRK>This paper presents a novel technique for fine grained visual classification. This technique addresses the classic issues in this task of inter class similarity (coupled with intra class variation) and the “long tailed” dataset problem, prevalent in datasets such as iNaturalist2018. The technique is an extension of Dubey et. al which splits the mini batch into two halves and incentivizes the aggregate predictions of the two halves to be similar. Unless I have misunderstood, this sounds like it would be slower to train and I did not see any runtime analysis comparisons in the experiments section. Experiments showing how the new loss function effects training time would help alleviate this concern. The method does however improve performance on the tested benchmarks. Finally, I found that there were numerous grammatical and stylistic mistakes. The writing improves during the discussion of the mathematics of the technique, but the introduction, experiments, and discussion need work.<BRK>This paper proposes the batch confusion norm (BCN) for dealing with both fine grained recognition and long tailed recognition simultaneously. Paper weaknesses:  Although the proposed method is reasonable, some specific model designs are not quite clear. 1) Regarding Eq.(2), the reason why it requires to optimize the ranking should be further explained and its motivation needs to state. 1) The classification accuracy on these fine grained benchmark datasets and iNat18 are not significantly better than the accuracy of previous work. 2) Some state of the art methods are not involved in the experimental comparisons, such as [ref1 ref5]. Minor issues:  There are several typos and writing problems in this paper. For example, on Page 3, "Dubey et al.Dubey et al.(2018)", and "Chen et al.Chen et al.(2019)".On Page 4, "PC Dubey et al.(2018)".On Page 8, "And also solves the long tailed problem by an adaptive matrix term."
Accept (Poster). rating score: 7. rating score: 7. rating score: 6. rating score: 6. <BRK>I generally found this paper to be well written, thoroughly executed, and the proposed method performs well compared to prior work. 3.The data includes genre information, but this doesn t seem to be explicitly reported on except by way of the "style match" evaluation. The main text refers to (Ellis, 2007) for beat tracking, but the appendix refers to (Boeck and Widmer, 2013), which uses a similarly defined (but practically quite different) onset strength function. It may be worth including downbeat estimations (eg from madmom [2]) as an input feature at some point. I think there s a typo in the appendix on audio preprocessing: are features really extracted at 15400 frames per second?<BRK>The included demo video is also a great example. I thought showing this breakdown by time in addition to the overall FID score in Table 1 was very convincing. **Recommendation**My recommendation is to accept this paper. I would like to see more information about the balance of the dataset by genre or other important attributes and then also see some of the evaluation statistics broken out by those attributes. I definitely think that at least an overview of what features are used as input should be included in the main body of the paper. In the appendix, I think you should include much more detail about the audio features used as input. For example, what were the parameters for computing the spectral features?<BRK>The decoder doesn t model a distribution of pose information given audio, and I can t find an explanation anywhere in the paper. I think the human evaluation is a little bit unfair in this regard, as it does not take the variety into account. The stated justification for using an RNN as the decoder is that the decoder needs to be autoregressive. #### Technical novelty is unsubstantiatedThe biggest problem with this work is a lack of ablation study for the novel aspects of the proposed system.<BRK>3.Section 4: The experiment setup lacks important details. Regardless, these details are necessary within the main text of the paper and should not be relegated to the appendix. there also exists another large dance database [2] which may be worth mentioning in the paper. However, there is a lack of depth in terms of how these techniques are adapted for the particular task. Hence, I rate the paper as a 4/10.
Reject. rating score: 4. rating score: 4. rating score: 5. rating score: 5. <BRK>Then, they propose to create a swarm of networks by swapping a random non zero weight with a zero weight. Some of these networks happen to perform slightly better than the original one. Generally, I m not convinced that the binary networks would perform much better beyond MNIST and car examples. The paper raises a series of questions:  How accurately was the network trained? Did the authors rained the best possible binary sparse MNIST network before preceding to the swapping. This is not clear from the text. It would be great to at least repeat the procedure they describe for training multiple times. I assume that if the input is binary, simple Euclidean distance won t be a great measure. The results on the plots are rather the artifacts of the wrong affinity matrix than the properly of the weight manifold. How many networks are selected in the first step? How do first neighbor are chosen? It is an exhaustive search? What is the complexity? What "stochastic" about this algorithm?<BRK>In this paper, the authors study a procedure for pruning (sparsifying) and binarizing neural networks through a pruning procedure. They do this by taking a trained dense network, pruning the synapses to get to a sparser network, and then doing a stochastic search over "connection swaps" to further optimize the pruned network. Reasonable performance is shown on MNIST. I like the concept a lot: of searching over sparse network configurations to find high performance small networks. Is it the weakest ones? Or did you find those for which the gradients of the loss with respect to the weights were smallest in magnitude? b) what is the training procedure during step 4 (training after binarizing)? Or was this just done by adjusting the thresholds for individual units? Or some other thing? That should be discussed I think, even if there is not an immediately effective solution available. 3) This work doesn t seem architecture agnostic: you are still specifying the number of layers, conv vs dense, etc. It seems more like you have an approach for sparsifying (which could still be useful!). that might interest the authors if they are curious about genuine progress in architecture agnostic NNs.<BRK>Summary:In this paper, the authors have explored a "brain’s stochastic synaptic pruning" inspired method for architecture agnostic models. Authors have tested their methods on a static and dynamic tasks. Strengths:Both sparse, binary paradigms for neural architecture and sampling using random bit swaps are priorly less explored tasks, with very less literature. The accuracies listed in paper motivates us that the authors are taking right steps towards brain like architectures. The authors did not explain the reasons for architecture choice and epoch choice for sparse binary networks. How good is the network during transfer learning? The authors coul have tried other small datasets. Why the parameters in the architecture are choosen the way ther are presented. In GENERATING SPARSE BINARY NEURAL NETWORKS section   Abalation study would have helped understand the choice of stages. Diagrams are bit unclear.<BRK>The authors pay attention to Architecture Agnostic Neural Networks. I think their stochastic search algorithm is a kind of EA method. The pruning method in learning rule is the standard magnitude based pruning. Thus, from the view of technique, the contribution is somewhat weak, even though the conclusion of this paper is interesting. I am confused about the title of section 4.2 and section 4.3. Is it incomplete? Moreover, I think stochastic search is not suitable  for your algorithm since the whole procedure is much similar with evolutionary algorithm. The only difference is how to define mutation for Architecture Agnostic Neural Networks.
Reject. rating score: 6. rating score: 6. rating score: 7. <BRK>(I have updated my review to raise my reviewer score by two points after discussion with the authors; this is still a preliminary evaluation as I have not discussed the paper with the other reviewers) SUMMARY OF METHODThe paper discusses an approach to create "decoys", which are slightly altered versions of the input that preserve the activations of an intermediate layer. I recognize that Simonyan et al., 2013 discarded the sign information, but they were not trying to optimize the fidelity metric. The authors benchmark their method on several datasets and argue that their saliency maps are superior according to the fidelity metric (Dabkowski & Gal, 2017). STRENGTHS  This approach of enhancing saliency maps is very unique compared to other methods in the literature. The generation of decoys could be useful in other contexts (e.g.to understand which sets of inputs can compensate for or "buffer" each other). However, more to the point: it seems clear that the decoy construction process will only perturb a feature if they effect of this perturbation can be cancelled out by perturbing other features. But *a feature can be important even if the effect of perturbing the feature is not easily cancelled out by perturbing other features*   particularly given that the decoys are constructed in order to preserve the representation at some intermediate hidden layer. (2) I have a set of concerns pertaining to the evaluation using the fidelity metric.<BRK>Summary of work: a new saliency map method is presented, along with empirical work showing that it beats SoTA by some metrics. Recommendation: I lean toward accepting this paper. Reasoning: Strengths: the paper represents an advance in state of the art, based on a variety of quantitative metrics. It would be useful to have this paper in the literature. To my eye, the results for images and sentences aren t noticeably better than the alternatives. Furthermore, I found the writing fairly confusing I needed to read the "decoy" definition several times. Given this, it s not obvious that the Hessian at a given point captures significant information about robust inter feature interactions. * I find Figure 3 confusing (e.g., there are undefined abbreviations) and visually it s not clear that the decoy method is better. * I did not check the math in the appendix (if I understand correctly, this is not necessary in a review, but I wanted to make that fact explicit.)<BRK>They provide theoretical as well as empirical support for their method. This paper show a great improvement over saliency maps and integrated gradients with a smaller gain for smoothgrad. Comments:* One questions that arose while reading: why is there a significant boost with range aggregation compared to mean aggregation? Moreover, the intgrad_decoys for the volcano has an SF score of 0.36, but it seems to be capturing the base of the mountain in addition to the sky. Are there certain aspects of the saliency maps that boosts/biases this fidelity score? * The comparison between grad and intgrad with deocys is visually very clear. Perhaps a difference plot would be better at revealing what the decoy methods are able to better capture. * The robustness to adversarial attacks on images is not as convincing as the other aspects of the story.
Reject. rating score: 5. rating score: 5. rating score: 6. rating score: 6. <BRK>QualityThe paper proposes good ideas that are compelling and make sense. Using multiple agents and data augmentation seems directions worth pursuing to improve mixture representation learning. The experiments are mostly clear and well motivated. This somehow contradicts the motivation of the method and it is not evident why the proposed method is more efficient. Thanks for explaining this a bit more in detail. Bouchacourt et al.’s “Multi Level Variational Autoencoder: Learning Disentangled Representations from Grouped Observations” presents a similar idea of using content/class and style/state spaces (only with continuous variables).<BRK>This paper proposed a multi agent VAE model that combines multiple copies of VAEs with coupling constraints to improve its latent representation learning (by encouraging discrete variable consistency). The experiments show that the proposed model outperforms other discrete&continuous VAE models in terms of clustering ACC. However, it is hard to justify its motivation as the model is over complex, and the same task could be achieved by other approaches such as deep clustering. 2.The method description given in Section 3.1 is quite clear as the equations are self explainable. Also, to train such a model, the user also needs to produce a type preserving augmentation, which is very costly for clustering. 3.The introduction of this paper gives me a hard time to follow as the terminology used is uncommon in generative model literature (single agent, multi agent).<BRK> Summary : The paper proposed the new disentanglement approach based on the "wisdom of the crowd". Each agent receives a similar image that is generated from the data augmentation method (based on the VAE GAN technique). The encoder of each agent first estimates the categorical distribution and estimates continuous variables from the categorical data and original image. The evaluation also focuses on categorical data and the proposed method outperforms the other single agent baselines. I will finalize my decision after reading the reviews from the other reviewers and authors  responses to my concerns. Strong points: (1) I was impressed by the idea to generate a similar image with the same class label through techniques developed from VAE GAN since this idea reverts the unsupervised setting into a weakly supervised setting. Cons :The idea which leverages a similar image with the same class label is quite similar in the disentanglement under weakly supervised setting as in [1]. I vote for marginal acceptance.<BRK>The method presented seems to be quite novel, with various technical contributions required to jointly train the separate VAE models in a way that they didn t collapse to a single model. To my knowledge, the methods selected are good candidates for being SOTA methods for VAEs with discrete latent variables. However, I have some concerns with the presentation of the paper as is. Blum and Mitchell (1998). We would not expect this to be the case for a single agent or multiple agents. b) Consensus clustering aggregates the results of a clustering algorithm over multiple initialisations [3]. c) Co training is a method for training classifiers on multi view data such that they predict same labels for co occurring patterns in each view.
Accept (Spotlight). rating score: 8. rating score: 8. rating score: 7. rating score: 6. <BRK>[Pros] 	The idea of this paper is novel and interesting. Captions of tables should be put above the table contents. The experiments are sufficient and supportive to validate the effectiveness and statistical grounding of the proposed methods.<BRK>This was not very evident from (8). ###########################################################+ves:+ The paper motivates the proposed method very well, by exposing the cases of failures of certain existing approaches and addressing those shortcomings in the proposed method. + The experiments and the analysis are both comprehensive, and the paper has a nice technical depth to it. + The paper is very well written.<BRK>The synthetic data experiments are well designed, and it gives the reader a better understanding of the behavior of the proposed method. *******************After rebuttal period: thank you for answering my questions and for updating the paper. I feel readers will be more comfortable to have this in the main paper. Experiments show that the proposed methods work better than previous methods.<BRK>In the experiments, e.g., Table 2, in my humble opinion, some of resfults for comparison methods are incorrect. This is a well written paper, and the results are impressive. Hence, I guess that the comparison is not fair at all.
Reject. rating score: 4. rating score: 7. rating score: 7. <BRK>This paper introduced a regularization scheme based on the second order Taylor expansion of the loss objective to improve the robustness of the trained models. It is reasonable as long as this lower bound is tight, corresponding to a stronger attacker. In addition, the regularization term in (1) depends on $\Delta x$.<BRK>Summary:The paper proposed a regularizer loss as an alternative to adversarial training to improve the robustness of neural networks against adversarial attacks. The new regularizer is derived from a second order Tyler series expansion of the loss function in the model robustness optimization problem. Cons:  As also discussed in the challenges in Appendix E6, an expected advance of this approach would be the training efficiency. Some empirical analysis on the intermediate values would be nice. Overall, I think the paper explored an interesting and promising direction to improve network robustness using second order regularization and solid progress was made.<BRK>Authors propose a second order approximation based training procedure to build robust networks against \ell_inf and \ell_2 attacks. Another key observation from the experiments is that SOAR does not lead to as significant drops in standard accuracy (i.e.not under attack), which I do think is also another important contribution of this paper. It will be interesting to see the gains of SOAR on WideResNet as well.
Accept (Poster). rating score: 8. rating score: 8. rating score: 7. rating score: 6. rating score: 4. <BRK>##########################################################################Post rebuttal:I have read authors  rebuttal as well as the newly added contents. I recommend for acceptance and give my final score at 8. ##########################################################################Reasons for score: Extending the learning based CBF approach to multi agent problems is an important work in the area of multi agent learning. However,  I have some uncertainties over the the current version, see the below. 3.The author provides a classical high probability generalisation bound based on the Rademacher complexity. 3.This paper provides comprehensive experiments, including 2D grid world, ground robots, and 3D drones.<BRK>The paper is globally well written, and the underlying theoretical foundations seem sound, and experiments show strong results. However, the experiments leave out a lot of critical implementation details, raising concerns for the reproducibility of the work. The only question that I have is with respect to the choice of the neural network architecture. A PointNet is being used, what motivates this choice? In the context of the experiments, this neighborhood is unspecified. As for the comparison, I find the choice of some of the baselines inadequate. How many step does that usually take? What is the impact on the run time?<BRK>The paper provides some generalization guarantees for the approach as well. It s good to have proven generalization guarantees as well. Although the paper shows impressive performance on a variety of tasks, the learning setup is unclear.<BRK>However it may suffer from the exponentially large state and action space. Concern 1: The contribution of this paper is incremental. In addition, I am not sure whether the definition of equation (2) is reasonable or not. It suggests that safety is defined on the Euclidean distance between agents (the agent does not collide with each other). Is it possible to extend this definition to a more general setting?<BRK>The paper reviews some key ideas from the CBF literature and extends them to the multi agent, partially observed case, then presents a learning framework for estimating CBFs and policies jointly, and concludes with experimental results. I was able to follow this fine but (a small suggestion) it might be a good idea to spend a little more time on this for readers who have not heard of CBFs before. Is this _all state, observation pairs for agent i_? To me, this looks a little bit "decorative" (to use a term I ve seen several conferences use in instructions to reviewers). If so, it should be clearly cited as different from FACTEST. For now, I cannot recommend publication.
Reject. rating score: 2. rating score: 2. rating score: 3. rating score: 3. rating score: 4. <BRK>The authors propose a factorisation method based on soft value functions. I found that the paper is extremely poorly written which makes it very difficult to understand the overall method. The presentation is also quite arbitrary with discussion around results that seem unnecessary. Here are some of the  major issues:1.<BRK>The paper proposes a Q factorization method by assuming an energy based policies model. Q functions are formulated as soft value functions with the energy parameters, and this adoption renders the function factorization more flexible compared to existing ones. Strengths:+ The formulation of Q functions as soft functions, despite appearing simple, shows some effectiveness in a number of MARL tasks. The effectiveness of the proposed method is not yet well accounted for. The paper could be better positioned.<BRK>Additionally, about the integration of QPLEX and soft Q learning, I also have some concerns. The definition of soft value functions depends on the specific selection of the temperature parameter. ** Minor pointsSome of the claims in the paper need refinements. That is to say, the framework of CTDE itself cannot solve the problem of partial observability. In fact, QMIX, which is a VFD method, generally can not work very well in tasks with more than 20 agents. Typos: Last paragraph in the introduction: "it significantly outperforms other baselines, SMAC (Samvelyan et al.(2019))."SMAC is not an algorithm.<BRK>The proposed approach in this paper has some promising experimental results, but there are questions about the novelty and significance of the method. Starting with Definition 1, it seems like IGO is using an optimal *centralized* policy. There are also similar max entropy approaches, such as the paper below. The paper should discuss how the proposed method is an improvement over this other work and have a more comprehensive related work section. Figure 1 is helpful, but it should be described in the text to make the issue clear.<BRK>This paper proposes a novel MARL framework named FSV, which incorporates the idea of energy based policies and an efficient linear decomposition architecture in the joint action value function with multi agent maximum entropy reinforcement learning. Overall, this paper is well organized and easy to read. There are some questions. It seems that FSV cannot represent the non linear formation of Qtot and Qi. Especially, the ablation of FSV should be considered.
Reject. rating score: 4. rating score: 4. rating score: 5. rating score: 6. <BRK>This paper proposes an automated method to search for data augmentation, hyperparameters, and architectures using gradient descent, which is simple and easy to integrate. 2).It achieves co optimization on different components in one stage. Correctness: Are the claims and methods correct? Is the empirical methodology correct? Clarity: Is the paper well written? Yes, although a few aspects could be improved (see feedback). Relation to prior work: Is it clearly discussed how this work differs from previous contributions? Additional feedback, comments, suggestions for improvement, and questions for the authors: Clearly describe mathematically how the proposed method contribute collaboratively to the final testing performance including its generalizability. The mutual dependency between NAS and DAS arises from that NAS is based on the training data modified by the augmentation policy generated by DAS and the DAS is based on the network generated by NAS. Additionally, provide ablation studies on changing one single component while fixing others.<BRK>More general hyperparameter optimization techniques such as Bayesian Optimization struggle with the dimensionality of the architecture parameters. The authors describe a NAS+differentialble hyperparameter tuning technique that also include data augmentation, thus extending the setting somewhat compared to previous results 2. They provide experimental results on ImageNetThe paper is reasonably clear, and while some of the ideas are interesting (such as how to incorporate data augmentation as a differentiable NAS hyperparameter) I don t think this work should be accepted mainly because:* They describe only incremental improvements. There is no comparison to AutoHAS, which appears to be the closest method, according to the paper itself. And most results are on ImageNet only. ##### Questions/comments: In the results, it seems that the competing method use no data augmentation at all, is that correct? It would be nice to know the number of repetitions and standard dev/error, or if a single one at least have the source code available for better reproducibility. The validation set is used for selecting hyperparameters, thus should not be used for comparing methods, was a separate test set used for accuracy? It would be good to describe this more clearly in the paper.<BRK>On top of this differentiable procedure, an alternating optimization is introduced to train network parameters and hyperparameters. The paper shows that end to end automl is possible in a fully differential way which allows better models with fewer resources. 2.Even though, intuitively, this joint optimization should perform better than non joint ones, making such training stable does not seem trivial. The authors appear to successfully have taken care of it. Can the authors share some thought, experience, and intuition on this? 3.Similar to above, the numbers for DSNAS in Table 1 seem to be taken from DSNAS paper (Table 3) which are numbers from the validation set. **Recommendation**The paper tackles interesting and practical problems and shows the proposed method outperforms baselines. My main concern is whether DiffAutoML uses validation data that is not used by other baselines, which could be a reason for better performance. **Additional feedback** (Irrelevant to the decision assessment)  Since the most of NAS component in the full pipeline is from DSNAS, it would be better for readers if this is detailed in Related Works or Appendix.<BRK>Then use the DAG for neural architecture search. Given the data and architecture, it then alternatively update the model parameter and hyper parameter. The overall proposed framework is end to end. The authors also conduct ablation study to show the effectiveness of jointly modeling the three components (data augmentation, neural architecture search, hyper parameter optimization). Strengths:1) This work considers three important components in modeling process including data argumentation, HPO, and NAS. The different components may interact with each other to impact the performance. 2) This paper is clearly written and easy to follow. Otherwise, the result might still be dataset specific. However, the proposed approach should not be QuestionsWill the proposed approach generalize beyond computer vision task? Why are the data augmentation and neural architecture search grouped together? I was wondering what will happen if you group neural architecture search and hyper parameter optimization first? How could you demonstrate that the selected architecture by the proposed approach is better or make sense?
Reject. rating score: 4. rating score: 4. rating score: 5. rating score: 6. <BRK>**SUMMARY**The present paper considers the problem of context integration in probabilistic agent trajectory predictors, particularly Trajectron++. It would be interesting to see the tradeoff between using this type of training and injecting a top down image. The paper proposes adjusting the loss function by adding an unlikelihood loss term. It starts with the observation that these predictors often do a bad job at considering non drivable areas in their predictions even if context information is injected as part of the input. The unlikelihood loss is integrated into Trajectron++ and it is demonstrated that the resulting variant of Trajectron++ not only improves upon the prediction error but also reduces the likelihood mass in regions violating the context cues. MLE does not require the use of KL Divergences. Also, the work assumes the availability of an output distribution to draw K negative candidate trajectories from. This is not necessarily a major problem as the idea has value for itself. They may not contain K negative examples. One of my main concerns is that a method for context integration should be evaluated in comparison to multiple other methods for context integration (ideally on multiple predictors) in order to see which approach is particularly meaningful and why. This testing protocol seems interesting to me but I wonder if it is meaningful. Could the authors elaborate on that? Same question for the split you used in Argoverse? This would simplify understanding the extent of learning rate decay.<BRK>*Summary:* The paper addresses the problem of trajectory prediction, which is to predict the future trajectory of a vehicle given its current trajectory, taking into account the nearby vehicle data. The main contribution of the paper is to investigate the possibility of improving performance of trajectory prediction using negative trajectories, that are generated heuristically. This is likewise not useful. Therefore, I have serious doubts whether the results are significant with respect to the proposed contribution or are just a consequence of model tuning. In this paper, the proposed contribution has only been evaluated against one method. In fact, this is even explicitly agreed upon by the authors themselves in Section 3.3 ("Design of our Checker"). The extension involves simply re labeling a part of the predictions of trajectron++ and re training them. *Conclusion:* The incremental extension over Trajectron++ and heurisitc and unsound nature of the novelty.<BRK>The paper proposes an approach to improving the accuracy of trajectory prediction by adding a loss that minimizes the likelihood of trajectories that violate the contextual constraints. The negative trajectories themselves are sampled from the current version of the prediction model and are limited to those that violate the contextual constraints, such as the lane direction and driving area. Authors show that their method improves state of the art Trajectron++ on the widely used NuScenes dataset. Additionally, they compare to their baseline method on the Agroverse dataset, but due to absence of other results on the Agroverse dataset, this serves more as an ablation study. Many of the methods that authors compare with, including Trajectron++ and Social LSTM, provide results on those benchmarks and evaluation on those could strengthen the paper.<BRK>In this paper, the authors focus on vehicular motion forecasting on roadways. To this end, they propose an interesting tweak to existing approaches. In addition to maximizing the likelihood of ground truth trajectories, the authors consider an "unlikelihood" weighted subloss which penalizes sections of the event space that shouldn t happen with context (such a driving on the wrong side of the road). 3.The proposed approach (unlikelihood training) makes intuitive sense and is useful for any situation with negative examples or known prior boundaries on the event space. I am certain that estimating a probability distribution subject to boundaries on the event space has been encountered many times before in machine learning and statistics. Overall, the paper focuses on a timely and useful problem of vehicular motion prediction, but stronger quantitative results and more theoretical grounding would be ideal.
Reject. rating score: 5. rating score: 5. rating score: 5. <BRK>However, my major concern is still that the technical contribution of this paper is limited. To address this problem, the authors design an objective function to adjust the separability of the hidden data representations as a way to control the trade off between data utility and vulnerability to inversion attacks. [ ] For the threat model, the authors assume that the attacker not only has access to extracted features but also all network parameters of the trained model. It would be better if the authors provide the proof for this theorem. [ ] The authors fail to cite existing state of the art works on defending model inversion attacks, e.g., [1,2,3,4]. Additionally, in experiments, state of the art baselines are not adopted. Currently, there are some existing defenses against model inversion attacks, e.g., [1,2,3,4].<BRK>Post response updateI would like to thank the authors for their detailed response. The paper looks at the tradeoff between data separability in an embedding space and vulnerability to model inversion. At a high level it is not clear why the model inversion is a well motivated attack vector given that model inversion extracts an average representation of the points from a class, not specific training points. How does the approach from Section 3.2 relate to other losses like the triplet loss which compare distances between different points in the embedding space? Is a successful model inversion evaluated based on human perception and a similarity metric? Does the lack of similarity mean that there is no privacy leakage? The introduction discussed a distributed setting scenario but this does not seem to be considered in the experimental setup itself.<BRK>The paper aims at strengthening DNN against inversion attack. Following is my concern: 	Reducing hidden representation separability of data points in different classes equates to more confusion for classification. Thus, this proposed method could be helpful if the user is willing to give up on some accuracy in the hope of getting a more robust model. But it is not shown that the sweet spot exists as there is always a trade off. o	Also, as a suggestion, I think designing beta as a function of classe (c) would be more appropriate. In this paper only one layer is considered. The paper is understandable but there are multiple typos and the English could be improved.
Reject. rating score: 4. rating score: 5. rating score: 6. rating score: 7. <BRK>This paper proposes a neurosymbolic module network that predicts a program structure following a dependency parse, populates that program s arguments, and executes it to answer numerical reasoning questions over text. Results show that on numerical questions from the DROP dataset, the model outperforms that of Gupta et al.and is competitive with other approaches when appropriate assumptions are made. I don t think this paper has a particularly keen insight to solve the cold start problem for RL here, and the NeRd style data augmentation is not that big of a weakness. OVERALLNormally I am not a stickler for novelty over prior work.<BRK>The paper proposes a new model for numerical reasoning in machine comprehension. To this end, the prior work should be better discussed (notably there is no Related Work section at the moment). A key claimed advantage of the model compared to the prior art is that it trains end to end from the rewards as the only form of supervision. Two key quantitative results include: better performance on the DROP num datasets, compared to NMNs with less supervision and GenBERT without data augmentationcomparable to strongly supevised NMN performance on DROP Pruned num. It is encouraging that the paper reports successful training with RL. Data augmentation and strong supervision are not the same thing.<BRK>This paper proposed a weakly supervised module networks to solve numeric reasoning problems. The model is trained with reinforcement learning over the modules. They evaluated their model on a subset of the DROP dataset. I am not sure how it compare to other models in more general settings and/or on other datasets. Will your model work on questions that involves multi step reasoning? Can you show that?<BRK>Is that possible that NMNs or GenBERT could learn useful patterns from other types of questions to answer numerical questions? Neural Module Networks are a very promising trend for machine reading comprehension for that it explicitly involves the reasoning process, but training such a system requires expensive annotations. The idea behind the interactions, however, is not that complex. Compared to the two baseline works, i.e.Neural Module Networks (NMNs) and GenBERT, WNSMN only works for numerical problems. One of the crucial pathologies of weakly supervised learning is the spurious prediction, where the model outputs the correct with wrong steps. However, this paper doesn t analyze this problem with WNSMN. This could be studied with a small subset of human annotated data and is helpful to rule out the possibility that the model learns some un interpretable patterns. Is there a specific reason that you focus on numerical problems?
Accept (Poster). rating score: 8. rating score: 7. rating score: 6. rating score: 5. rating score: 5. <BRK>These different manifolds are the sub manifolds of some manifold M for a given disentangled generative model. The paper considers the fact that in an entangled model the sub manifolds are not homeomorphic and thus similarity across submanifolds can be measured to evaluate a model’s disentanglement. For measuring topological similarity, the paper then introduces Wasserstein Relative Living Times. Pros:  The paper is well written and the proposed metric is well articulated. For the unsupervised portion, since MIG was considered a comparision, can authors also compare their method with MIG sup?<BRK>This paper proposes a new metric to quantify disentanglement by leveraging ideas from manifold topology, more precisely the homology of conditional submanifolds. I’d still recommend publication, given the novelty and importance of a theoretical sound metric for disentanglement. 1.How exactly do you estimate the Wassertein RLT and compute the matrix M? The actual disentanglement metrics $\mu$ and $\mu_{sup}$ might deserve a \begin{equation} for clarity. 4.You never explicitly say if “high $\mu$  > high disentanglement”, which I think is how one should interpret Table 1 given the main text? I imagine that if the histograms vary as a function of their conditioning, this indicates entanglement? 4.Do the topological similarity matrices in Figure 5 provide interesting information about which factors are grouped together?<BRK>SummaryThe paper proposes a novel metric for evaluating disentanglement by taking a manifold topological perspective on the representations learnt. The key insight is that for a disentangled representation, when we fix a certain factor of variation at different values the topology of the conditional sub manifolds should be similar. Strengths+ Having an approach that is general and easy to compute across datasets and models makes a lot of senseWeaknessesIt would be nice to further clarify the intuitions for how disentangling relates to the manifold structures using more examples in the paper for people who are not familiar with the manifold topology literature.<BRK>Summary: Introduces unsupervised disentangling metric that measures homeomorphic similarity between submanifolds conditioned on a given factor, and homeomorphic dissimilarity on submanifolds conditioned on different factors. The paper also introduces a novel variation of RLTs that  employs wasserstein distance instead of euclidean distance. Strengths:* Novel application of topological similarity to unsupervised evaluation of disentangling. * Unsupervised and supervised metrics are both in the same units, and can be directly compared. It is possible that discrepancy is a good thing, and that the proposed model is capturing disentangling even better than the baseline metrics. However, there is not enough experimental validation to know whether this is the case. The paper would be stronger if it had more experimental validation, including a replication of the reported scores from the MIG (Chen et al., 2018) and (Kim and Minh 2018) paper, and deeper analysis explaining the relative differences in scores for models.<BRK>In contrast to existing methods, thispaper proposes an approach founded on topological considerations: byassessing the topological dissimilarity of submanifolds of a given model,which are conditioned on an individual factor. # Summary of the reviewThis paper is tackling a highly relevant topic (disentanglementanalysis) and provides a novel, fresh perspective that is mathematicallywell justified. Is it the  proposed measure of disentanglement? There are also some issues with the terminology that make   it harder to fully understand what is going on. by composition   >  by the composition   The term  group  is unfortunately somewhat overloaded as well; is   factor  more appropriate in some places? Or is the paper actually  talking about manifold groups? Some of the terms are overloaded and used multiple times:  latent  dimension  is used equivalently to  latent factor , it seems. I also have to point out  that the method used in this paper is only *invariant* under  homeomorphism! First, from the point of topological data analysis, the central algorithm is a comparatively small extension of the Geometry Score paper. I agree that this is a superb idea, yet the main contribution for me lies in the application of that technique to disentanglement—and for this to be fully understandable, some more work is needed. Alternatively, one could write  that the proposed approach assesses the *topological similarity* of  models.
Reject. rating score: 5. rating score: 5. rating score: 5. rating score: 6. <BRK>I think this could easily be misunderstood by the reader, as the established practice is that "models" refer to models for the task at hand (i.e., classification). It proposes a new task for faithfulness assessment, which classifies each token as either hallucinated or not. The paper shows an improved correlation (Spearman) relative to an entailment metric and a word alignment metric. I understand that the authors are trying to establish a new task and that there is little work to compare against, but the paper could have provided more in the way of ablation experiments. 2.Results lack interpretability: Two of the main tables (Table 2 and 4) do not seem to contribute much, as the different rows correspond to different datasets (underlying data to classify is different) and are therefore strictly not comparable. So it seems Tab. This is a bit of an odd argument to make for an *evaluation* metric, as this implies both the metric and the model are only given a source string to achieve their purposes. Overall, I think this paper s general direction with fine tuning on hallucination classification data is probably worth pursuing.<BRK>Summary: This paper proposes hallucination detection at the token level, which predicts if each token in the generation output is hallucinated or faithful to the source input. In contrast, previous studies usually work on the sentence level. It shows that the proposed method works better than existing entailment based and alignment based methods on the new task that it proposes. Similarly, for the creation of synthetic training data, I wonder whether it makes sense to replace tokens with different POS tags uniformly. It will be good to present more analysis in terms of both quantitative results and case studies on this aspect. Is the evaluation on all the tokens or only the hallucinated tokens? That is, a hallucinated NN, for example, might be worse than a hallucinated II, rather than asking how the labels are distributed by POS categories. So I still wonder if it makes sense (as a reliable metric) to measure hallucination at the token level (e.g., Table 5 Hal words %) but it remains unanswered. I would suggest doing more studies on downstream tasks as mentioned in your response to enhance the paper if you are "not proposing a reference free evaluation metric for quality estimation" (which seems a bit contradictory to "we hope to create a large scale pretrained evaluationmodel for any datasets or models to be evaluated" in the conclusion section btw).<BRK>This paper proposes a new task: Word level hallucination detection in text generation, where the authors identified two scenarios of word level hallucination (content insertion and incorrect substitution). Experiments show that the fine tuned LMs can achieve certain performance compared with human annotations, and that the token level hallucination is correlated to sentence level hallucination. Concerns:1) Hallucination is recognized as an important issue for text generation, and intuitively, detecting hallucination is also an important task. But the real benefit of detecting hallucination is unclear from this paper. I would be more convinced if the authors could show how their word level hallucination can indeed help text generation. Currently, the paper appears to be some fine tuned LM for a new task (whose real benefit is unclear) with little technical depth. 2) The categorization of "content insertion" and "incorrect substitution" is unclear and confusing. Is there any breakdown analysis on "content insertion" and "incorrect substitution"? 3) There s very little technical development and empirical analysis on the role of synonyms and stop words in hallucination. A synonym is supposed to have the same meaning as the original word, and is not hallucination. Synonyms are actually desired in many applications, like paraphrase generation and summarization.<BRK>Given a source input S and its output G generated by a sequence generation model, this paper formalizes the task of detecting hallucinated tokens as a labeling problem on the output G. In order to train the labeler, the method synthetically generates supervision data by using a BART model. In Section 6.2, the authors reported that they achieved decent performance on this task of hallucination labeling although it is far from being solved. The comparison between the proposed method and two baselines (entail and align) show that the proposed scores correlate well with human judges than the baselines. The methods used in this study, e.g., BART for generating pseudo hallucinated text and (XLM )RoBERTa for labeling hallucinated tokens, are appropriate. In addition, this paper does not explicitly explain how the research outcomes contribute to an advance in MT or summarization. Section 6 also demonstrates the usefulness of this work for machine translation trained with self training. The goal of the correlation analysis in Table 3 is unclear. For this reason, I increased my rating. This may be important because the Entail baseline works not at token level but at sentence level (it should not be used on a dataset only with non entailment instances). I would like to see results of a simple baseline in Table 2. For example, we can consider a baseline that indicates hallucinations for tokens that do not appear in the source text. Here is a summary of the methods.
Reject. rating score: 5. rating score: 5. rating score: 5. rating score: 6. <BRK>For the assumed node embedding system, the authors may evaluate how different node embedding systems impact the proposed method. MICRO Graph is proposed to enable simultaneous motif discovery and graph representation learning. Questions during rebuttal period Please address and clarify the weak points above. Empirical results on public benchmark datasets suggest the effectiveness of the proposed method. This paper s presentation also makes its technical details confusing. Meanwhile, I still have concerns on how the idea of contrastive learning is handled in this paper, which could have been better shaped. The authors investigate the problem of graph learning from a unique angle of motif discovery. 3.From multiple public benchmark datasets, the empirical results suggest the proposed technique could be promising in graph classification tasks. My major concern is on the technical quality of this paper. The generated node representations usually only work in a transductive setting such that the learned model may not be able to be generalized for unseen graphs. If the parameters in the embedding system are fixed, the subgraph segmenter basically generates fixed subgraphs by spectral clustering, and it is difficult to see the point of Equation (6). In sum, without clarification, the technical discussion is confusing. The authors may need to clarify what the unique aspect is in the proposed method. For the discussion in 3.4, it is hard to see why it is reasonable to use subgraph relations to define positive/negative sets, as the difference between a graph and its subgraph could be significant in many cases. 2.It is still unclear how motif discovery impacts graph representations.<BRK>This paper proposes to learn the sub graph patterns from a collection of training graphs. The key idea is to partition each graph into segments and enforce a global clustering of the subgraphs. The partitioning is also guided through contrastive learning, i.e., subgraphs should have a larger similarity with the graph it is drawn from, compared with other graphs. The learned GNN (that generates node embedding) will then be used to some downstream learning tasks with or without further fine tuning. The notation of the paper is very hard to follow. Furthermore, what is the relation between s_i and S_i (both board), and what is the difference between S (bold) and S (not bold)? It’s quite confusing to me. However, the grouping part and the contrastive part may conflict with each other in that some sub graphs are shared among different graphs, which can be quite common in chemical compounds. Under (5), it is mentioned that spectral clustering is used to partition the graphs; is it done end to end and if so where is the loss function corresponding to this operation? a threshold eta is used in the indicator function and how to choose eta (considering that it is used directly on a set of variables)? Experimental results are quite strange in that the transfer learning setting (which further finetunes the learned GNN based on a small set of labels, as in Table 1) leads to even worse performance than the feature extraction setting (in which no fine tuning is performance, as shown in Table2) and the gap can be as huge as 15% in accuracy!<BRK>Overall:This paper proposes an interesting framework + It extracts subgraph(s) for each graph from node affinity matrix and spectral clustering, together with the help of motifs. + It learns the motifs by clustering the subgraphs. + It applies contrastive self supervised learning on the graph subgraph pair. Strengths:+ The idea is novel and seems promising. Weaknesses:+ This paper is not well written, especially the notations. 2.S3.2, should be  ... we can extract M subgraphs … 3. S3.2, according to  sampling a subgraph from a graph , does this mean each graph has one and only one subgraph? 4.S3.2, ‘... apply the subgraph index as a mask to its subgraph embedding …’ I can understand the following equation but not this sentence. e.g., what are S and Q? 1.GROVER [1] is the SOTA, where it randomly masks a subgraph, which is highly relevant to this paper. 2.GNNExplainer [2] is learning the motif in an end to end way, the authors could also consider comparing with it. For now, I would reject the current version.<BRK>Paper SummaryThe paper describes a self supervised framework to extract graph motifs and use them as input for downstream contrastive learning. The framework contains three components: (a) motif guided segmenter to derive node subgraphs, (b) a motif learning   a clustering task among the subgraphs to identify concrete graph motifs and (c) contrastive learner for downstream graph tasks. The framework is evaluated using from a large scale chemical compound graph dataset. Positives:* The framework presented to identity graph motifs and then use of learned motifs in contrastive learning for graph representations is very interesting and does lead to substantial gains in performance at least in the datasets tested * Experimental design to evaluate the framework is well thought too to specifically test the different pieces and components. * The paper is generally well written. An algorithm/pseudo code in the supplement would have made it even easier to follow given the many moving pieces. Concerns* A concern is that the motif learner enforces all clusters to be have similar size. * The authors should provide some commentary on how the number of clusters used for spectral clustering and what their impact is on downstream results are * The whole graph embedding on Page 5 uses an average of all node embeddings for the graph. Does this put a limit on the size of the graph itself for the framework to work? If the graph is large, the average node embedding is probably not an accurate representation for the graph ?
Accept (Spotlight). rating score: 8. rating score: 8. rating score: 6. rating score: 6. <BRK>It can be a little confusing for the readers. The authors use reverse KL divergence between the group distributions as the penalty and use prior work to partition the datasets into groups. They use The results look promising across datasets, though it is slightly lower in the  in distribution  setting.<BRK>To make the study tenable, the authors make use of meaningful synthetic datasets, and propose an intuitive regularization to overcome the systemic biases. The analysis done in the paper is very methodical, and the presentation is very clear.<BRK>5.Equation 9 and surrounding text: I would change reverse KL to just KL since, in this context, this not an obvious forward/reverse direction. I think the case made for the importance/significance of the method could be improved, but think the paper should be accepted regardless. 4.Equation 6: $\ell$ is not defined.<BRK>I found the paper to be mostly very well written (with few exceptions), and especially sections 1 and 2 very easy to read and understand. With the same partition networks used for PGI? The part about partitioning is not detailed enough. From page 3 I understood that there might be 2 categories (easy and hard), is this the case? I haven’t seen any mention of early stopping in the experiments.
Reject. rating score: 3. rating score: 4. rating score: 4. rating score: 4. <BRK>For all these reasons, I believe that the empirical study presented in this paper is not strong enough to justify its publication in this conference.<BRK>The paper deals with supervised graph classification comparing GNN and graph kernel approaches. Just stating number of permutations as a reason is not enough. To that, the authors generate small scale, random graph datasets consisting of various types of graphs.<BRK>Comments: This is a very interesting empirical study including not only GNNs but also graph kernels in the scope, but at the same time, it would be still very hard to interpret those presented results. Also, the paper discussed several concrete examples that have a large disagreement on the similarity against the exact similarity eq(1). A simple WL test seems enough to discriminate these pairs. The pre training (?)<BRK>Therefore, in my opinion, the paper is not ready for publication. The choice of the similarity measure should be justified.
Reject. rating score: 4. rating score: 6. rating score: 7. rating score: 7. <BRK>#### SummaryThis work proposes a technique to compute sparse patterns in the input space that are highly predictive of a class, even when added to samples from a different class (non target samples). Meanwhile, the impact of proposing a "global targeted adversarial attack" based on one sample seems promising, but it is out of the scope of this work s analysis. A few remarks on the patterns are made and also some claims regarding the link to the model s learned features (like position, structure, and number of shapes). #### Strong and weak points of the paper:##### Strengths:   The paper is easy to read and the main ideas can be followed. A wide array of experiments are conducted with a small and (at least a subset of) a large dataset. In particular, the canvas image is often not shown (especially relevant for Figures 1, 2, and 7).<BRK>This paper proposes a visualization method to reveal the class specific discriminative patterns of DNNs in the input space. From the experimental results, the authors conjecture that images trained on natural data can have backdoors. I do not fully agree that a desirable canvas has to be a neutral or unbiased image. However, some of its main conclusions do not seem to be well supported by the experimental results and analysis. Such patterns are shown to reveal the abstract shapes and texture of the target class. The conjecture that "DNNs trained on natural data can also have backdoors" does not seem to be well supported by the "predictive power" in Figure 3, due to the definition of predictive power. However, by definition of predictive power, it is evaluated on the examples that are used to generate such patterns, since $ACC(f(x_n+p_y), y)$ seems to be defined on the nontarget class images. I think it would be much more convincing if $ACC(f(x_n+p_y), y)$ is defined on a different set of nontarget class images that are not used to craft the patterns. 3.Patterns shown in Figure 6 (left) do not seem to reveal the trigger at all. I feel contributing a new method with some improvements is worth an accept.<BRK>Informally, suppose you have an image x that is "representative" of the class y and let X be a set of images that belong to other classes. The authors propose an optimization problem that looks for a mask (i.e.set of pixels) along with values of those pixels such that when this pattern is added to any image in X, the model will predict the new image to have the label y. Despite its simplicity, it can reveal clear patterns, particularly on high resolution images, such as ImageNet. When multiple samples are used, multiple patterns are detected at different locations as shown in some of the figures. The algorithm seems great for providing interpretability per sample. I would suggest that the authors mention, at least, in the paper that this can be used sample wise as well for interpretability.<BRK>It proposed a class wise pattern searching method to demonstrate the existence of class wise predictive patterns in the input space. I think that the motivation is very clear,  and the writing is readable, though some minor typos or inconsistencies should be corrected. More details about the minimization of $\mathcal{L}$ in Eq.(2) should be added. And, there are many hyper parameters in the algorithm. And the presented results indeed provide some insights of these training settings. The proposed method reveals many interesting insights of what DNN has learned from the data under different training settings. However, if the insights could be used to improve the current training method or DNNs, the value of this work could be larger. I would like to see some discussions about this point in this manuscript.
Reject. rating score: 5. rating score: 6. rating score: 7. <BRK>My main concerns are (a) the polynomial time complexity of the method makes the proposed protocol hard to be useful in real applications, and (b) the experimental evaluation is done on very small matrices (i.e., 2000 x 2000 shape) as well as small number of servers. On the positive side, the paper is a solid theoretical work on the distributed CSSP problem. #### Strong points:* Well written paper with a clear contribution statement* Related work is up to date (to the best of my knowledge)* Concise algorithm description and corresponding theoretical guarantees. #### Concerns: * The polynomial running time guarantee makes the practicality of the proposed algorithm marginal. How does the distributed protocol compares with a centralized one?<BRK>The proposed distributed algorithm guarantee an $\tilde O(k^{1/p 1/2} )$ approximation to the best subset of columns with $\tilde O(sdk)$ communication cost per round and polynomial time. The lower bound analysis show that such communication complexity is near optimal. I think the theoretical contribution of this paper is valuable, however, the contents is somewhat limited to the audience of ICLR. There are some questions/comments:1. I strongly encourage the authors apply the proposed algorithm to address the real world problems such as computer vision, image processing which is mentioned in introduction. I think one of the reason may be the $l_p$ norm for $1\leq p<2$ is not a unitary invariant norm and SVD do not give an optimal low rank approximation. 3.The presentation could be improved: a) The operations on Server and coordinates should be described in multiple lines.<BRK>How can we know that it is? Moreover, even if the sampling matrices $T_i$ are chosen to satisfy Lemma 5, how do we know that the matrix $T$ obtained from the $T_i$ s in Alg. The paper primarily provides theoretical results, but also does some experiments. There are a few details in the proof of Theorem 6 that are unclear. Moreover, the readability of the paper could be improved. The paper ends abruptly with no conclusion. The paper provides both theoretical and experimental results. This makes the proof of Theorem 6 confusing. 4: It is not clear how Alg. 4 from the supplement is applied here.
Accept (Poster). rating score: 7. rating score: 6. rating score: 6. rating score: 5. <BRK>Compared with previous results which rely on Lipschitz condition, this paper assumes smoothness condition and Polyak Lojasiewicz Condition, and then prove the excess generalization bound that is a summation of $\frac{1}{n\beta}$ and empirical optimization error. What s more, authors analyze some common stochastic algorithms as concrete examples to show the corresponding theoretical guarantee.<BRK>Strength:+ The theoretical result is novel, according to the authors this is the first paper to model the generalization bound in terms of a $O(1/n\beta)$ term plus the convergence rate of the optimization algorithm+ The authors applies the main theorem on practical algorithms and derive convergence rate on generalization error while previous literature on those algorithms are mainly on  training error. The two paragraphs, "Algorithmic Stability" and "Generalization Analysis" all ends up with areas that stability/generalization is applicable, while I care more about how previous literature shed lights on this work.<BRK>This paper studies the generalization performance of stochastic algorithms in nonconvex optimization with gradient dominance condition. Can the authors explain more about that? The main idea for the authors to obtain such an improved bound is an advanced analysis based on a weaker on average stability measure.<BRK>The whole paper is focused on the theoretical part. In terms of this constraint, I saw that the authors tried to investigate and interpret it.
Accept (Spotlight). rating score: 7. rating score: 7. rating score: 7. <BRK>Summary:This paper proposes R WAE to learn disentangled representations. Benefit from the characteristics of WAE, this paper shows that R WAE can better disentangle the sequential data into content space and motion space. R WAE achieves state of the art performance in both disentanglement and unconditional generation. The proposed method has theoretical foundations and shows excellent results. Pros:    The paper provided strict theoretical formulations, like the comparison between WAE and VAE, how WAE can generalize to the sequential format, and the connection between mutual information(MI) and the objective function of R WAE. The authors provide sufficient experiments on multiple datasets. However, during the inference phase (Fig.1 (b)), the dependency is ignored. More analysis and discussion are probably needed for this result. The comparison between R WAE(GAN) and R WAE(MMD) can be further discussed.<BRK>This paper focuses on learning disentangled representations for sequential data. The theoretical analysis is provided by the method. In general, the paper is easy to follow. The method looks relatively straight forward, because it extends a recurrent VAE framework by replacing the reconstruction loss term with a Wasserstein distance. The superiority of the method is supported by both theoretical analysis and experimental performance. On page 6, the authors claim that the regularization term in R WAE is superior to a VAE model. In general, I suggest accepting this paper, because the proposed method is supported by theoretical analysis and gives a good experimental performance. On page 6, Eq.(21) should be changed to Eq.(13).<BRK>Summary:This paper extends the Wasserstein autoencoder for learning disentangled representations from sequential data. Ablation studies are lacking. Experimental results show that the proposed R WAE model outperforms baseline DS VAE/FHVAE/MocoGANPros:  Extending Wasserstein autoencoder to model sequential data and learn disentangled representations is novel and well motivated  Experimental results demonstrate the advantage compared with the baseline models in terms of disentanglement performance and generated data qualityCons:  The appendix requires proofreading   the authors left derivations in the appendix, but it contains incomplete sentences and inconsistent notations. Can authors also present the results for TIMIT/Sprites/SM MNIST?
Reject. rating score: 3. rating score: 4. rating score: 6. rating score: 7. <BRK>This work tries to identify  local patterns that can be used to provide explainability to GNN models. However, the manuscript was not well written with some confusing notations and logical problems. It should be beneficial to readers if the manuscript can first clearly demonstrate problems in existing graph classification problems. Some analyses are not solid. This is not quite true. For instance, GlobalAttention (https://arxiv.org/abs/1511.05493) is also an explainable readout function. ASAPooling (https://arxiv.org/abs/1911.07979) is a recent work that also learns a sparse soft cluster assignment for nodes in the pooling phase. In addition, the proposed method performed worse than many baseline models in many cases. For experiments on OGB datasets, it only compares with original GCN model, but there has been a lot of state of art models proposed several month ago, achieving much better results than this paper. 1.Equation (2): The trainable weight matrix is missing in GCN(A, X). X is first used as the feature matrix but later used to indicate node vector. ” This may be true, but the overall graph topology information is omitted.<BRK> Summary In this paper, the authors investigate how to improve pooling functions in graph neural networks for the purpose of better addressing graph classification problems. The authors may give a more accurate mathematical description. Although pooling for graph classification is a meaningful problem, it is difficult to see the unique perspective or value in the proposed technique compared with existing approaches. For now, the technical depth in this paper seems to be limited, and more convincing empirical evidences are expected. I keep the rating as it is. For the core idea in this paper, the authors may need to discuss its connection with existing literature, and highlight the unique perspective. From the current draft, the proposed StructAgg looks like incremental changes to the existing methods. The theoretical results in Theorem 1 may need more work. 3.The empirical evidences could be stronger. The authors may consider more datasets that can highlight the value of StructAgg. The authors may connect StructAgg with the existing GNNs, and demonstrate the incremental gain from StructAgg. For the evaluation in Table 3, the claim on "node embedding" may not be sound.<BRK>In this algorithm, a structural representation of node is constructed through concatenation of latest p layers of node presentation in graph neural network, which consists of information from the p hop neighborhood. The paper is good quality and its demonstration is clear. The idea behind is original. However, learning embeddings through concatenation of multiple layers of neural representation is not completely new. The contribution of this paper to the community is not ground breaking as well. From the experiment results, it is hard to stress its significance as in most of times this method does not beat the state of the art algorithms. 2.The proposed representation learned by StructAgg, is able to provide the clustering of nodes as well as the node embeddings that are invariant to the permutation. 3.The StructAgg only need to concatenate the existing learned representation in lower layers, thus it is easy to implement. In some graph like trees, it is fine. But in complex graphs, with larger local structures, it is not efficient in both storage and computation. It can be imagined that with the number of structural classes increases, the label assignment may not be consistent and will result in decrease of performance. It seems that this method is more suitable for two or three structural classes as shown in the experiment. In StructAgg, since the aggregation happens only within the same cluster, the noise level in different clusters may varies a lot, depending on the size of cluster.<BRK>This paper focuses on deriving explainable features for use in graph classification. Experiments demonstrate the effectiveness of the proposed approach as it provides comparable performance while providing some explainability. This is an important unsolved problem, and this work provides one such approach to obtain more explainable and intuitive features/embeddings for graph classification. Please provide a better example such as the one used in the recent survey paper “On Proximity and Structural Role based Embeddings in Networks” (see Figure 1) where this is not the case, and one must look at the actual structural properties to discover roles. There are also incorrect references to a paper from 2018 for structural roles that need to be fixed.
Reject. rating score: 5. rating score: 5. rating score: 6. rating score: 6. <BRK>Summary:This paper mainly proposes to learn attention based edge coefficients by incorporating information from farther away nodes by means of their shortest path (powers of the adjacency matrix). 2013.Footnote 2 on page 5 is missing a proposition reference. Strong points:The numerical experiments show slight improvement. In light of the authors  response, and after a thorough and careful discussion with the other reviewers, I have decided to update my score to 5 (five). In summary, I appreciate the authors  effort to signal the differences between their work and Elvin et al.While I agree with these, I still think this is only an incremental contribution. Weak points:The multi hop attention network has been done before (see below). The novelty thus resides only in the spectral analysis and the page rank equivalence. Major comment:1) Assuming there is only a scalar assigned to each edge (i.e.\mathcal{R}   \mathbb{R}), then (1),(3),(4) are a particular case of (39) (41) in Isufi et al, 2020, where A_{k}   alpha I for k<K and A_{K}   H (the H matrix in eq.4).This renders (1), (3), (4) s only novelty to be the potential for vector valued edge weights. I think that the most novel contribution is on the spectral analysis. But this is only stated, with no real insights developed, and not emphasized enough. For further reference, after carefully discussing the paper with other reviewers, these two published papers were also pointed out where multi hop attention is addressed. It s just that X is very straightforward: number of nodes x number of channels. However, this new, learned, matrix description will most likely not even share the same eigenvectors as the original matrix description of the graph.<BRK>Comparison with GAT and Diffusion GCN with LayerNorm and FeedFwd components on multiple train/test/val splits for smaller datasets or for other datasets from OGB will strengthen the results for node classification. The message passing paradigm is commonly adopted in GNNs because directly computing the higher powers of an adjacency matrix is not scalable. The same scalability concern is present for this work, which tries to obtain attention scores for indirect neighbors directly. Thus in order to solve this issue, the authors propose to diffuse the learned attention scores from their 1 hop neighbors to neighbors that are multiple hops away, thereby providing a means to directly obtain attention scores over indirect neighbors that are reachable from the nodes. ——Pros:	The paper is well written. In that case, it is not immediately clear how a similar diffusion, when used for propagating attention scores from immediate neighbors to neighbors multiple hops away, will be more powerful. Currently, it is not clear how much of an improvement is achieved because of these standard two components. This would help us clarify how much of the gain in performance depends on the page rank based propagation compared to the attention propagation. The teleport probability of Diffusion GCN should also be similarly experimented with and the analysis should be compared with the plots in Figure 3. Ignoring the benefits of LayerNorm that the MAGNA can leverage, comparing its No Feed Fwd version with Diffusion GCN, which is also based on a page rank formulation, MAGNA gain ~1% improvement on Cora and Pubmed dataset whereas it falls behind by ~1% in Citeseer. (e) KG Completion: Missing baselines and model variations   		  Missing comparison with Self attention (GAT) based knowledge graph embedding model, KBGAT. Nathani et al., Learning Attention based Embeddings for Relation Prediction in Knowledge Graphs, ACL 2019		  Additionally, it would be helpful to have similar model ablation studies of MAGNA model as in Table 1 for KG Completion. (f) Depth Analysis:			  Diffusion GCN comparison missing. MAGNA only has weights associated with 3 layers, unlike with GAT, which has weights for every propagation step. ——Questions during rebuttal:	  Kindly clarify concern (i)	  Check experimental concerns above for additional ablation and baseline variants that is required to disentangle and appreciate the usefulness of the primary contribution, the attention diffusion component.<BRK>Post Rebuttal Update  After the author s rebuttal and the discussion with other reviewers, I have decided to lower my score to 6.   Summary  This paper proposes MAGNA, a multi hop self attention mechanism for attention based graph neural networks. Additionally, even though the authors provide some theoretical grounding for their work, some of it is a bit disconnected from the rest of the paper, like the relation with PageRank introduced in section 3.2, which is not referenced or discussed in any other section. The proposed MAGNA method is an extension of GAT networks that introduces a diffusion step on the computed attention coefficients, following a similar approach (Diffusion GCNs) that has been used for GCNs. * The paper is well written. #### Cons:* The proposed method MAGNA seems to be similar to the APPNP method proposed in [Klicpera 2019a] that also uses diffusion to increase the neighbourhood around each node in a GCN layer (without attention). * The motivation behind including layer normalization and deep aggregation or why they are useful isn’t entirely clear. #### Questions:1) MAGNA has 3 main differences compared to GAT: layer normalization, diffusion and deep aggregation. If so, is there an ablation test where MAGNA just uses the diffusion step but without the other two components? 2) Since the paper puts a lot of emphasis in the multi hop capability of MAGNA, and related to my previous question, it would be quite interesting to compare MAGNA against a GAT network that has a larger receptive field, for example by using a multi hop adjacency matrix computed with the diffusion process from [Klicpera 2019b] (called sparsified matrix in that paper) instead of the 1 hop adjacency matrix used in the original GAT paper. With this comparison we could see the benefit of using the diffusion of the attention values instead of just increasing the receptive field of GAT by allowing each node to pay attention to nodes up to K hopes away. Doesn’t MAGNA compute the same number of attention values as GAT and it then diffuses them? Is that the same? Because of these reasons, I vote for accept.<BRK>The main contribution consists in considerably increasing the receptive field by considering a multi hop neighborhood instead of the standard one hop. The technical challenge consists in obtaining attention scores for all relevant nodes in an efficient way. MAGNA solves this by using a diffusion based technique combined with a geometric distribution. The authors show that the latter further allows for approximations, and also give interesting theoretical insights (e.g., show a relation to page rank). The paper is overall well written, related work is considered adequately, the idea is interesting, and the results are convincing. The ablation studies give interesting insights in the effects of different parameter choices. Smaller comments:  p.3: "degenerate categorical distribution with 1 category": I think this should be explained. ": I assume you mean "directly connected" since the whole approach seems to consider only connected nodes? p.4: "as well as good model generalization. ": How does the diffusion process ensure good model generalization? p.5: Footnote 2: ?? Update after Rebuttal: I have read the other reviews and authors  responses. While I do think the novelty of the contribution is sufficient, given that  the paper referenced in another review has not been peer reviewed yet, the new ablation results in Table 1 show that the paper s contribution is not outstanding.
Accept (Poster). rating score: 8. rating score: 7. rating score: 6. rating score: 5. <BRK>I’m assigning a score of 8 (a very good conference paper), and I think that the paper is more or less ready for publication as is. If the underlying domain has a uniform discretization, the fast Fourier transformation (FFT) can be used, allowing for an O(nlogn) evaluation of the aforementioned convolution operator, where n is the number of points in the discretization. Strengths and weaknesses:Much of the theoretical legwork for this paper, namely, neural operators, was already carried out in previous papers (Li et al.).<BRK>To this extent, it proposes a neural network architecture where (in essence), the linearity is not applied in the Fourier space but in the state space. Strengths:Overall, the paper is well written. The theoretical and experimental sections are, for the most part, clear and concise, although some important details remain unclear/lacking. The method seems to be quite generic and can be applied to a large range of PDEs. The distinction between training data and test data is not clearly specified. It would have been interesting to analyze the expressiveness of the architecture. As one of your motivations behind this work is to learn the operator, it could have been interesting to test your approach using different sample points as in the training data.<BRK>Paper Summary: The authors proposed a novel neural Fourier operator that generalizes between different function discretization schemes, and achieves superior performance in terms of speed and accuracy compared to learned baselines. I have to admit that my understanding of this paper is rather limited and I have lots of conceptual questions about the implementation. These two seem to be in conflict.<BRK>In this work, the general kernel is replaced by a convolution kernel which is represented in the Fourier space. Pros.1.The authors address an important and practical problem 2. The numerical results are impressiveCons. 2.Clarity of the paper. In practice, the reader should follow the intuition and the motivation given in Li et al in order to grasp the idea of neural operator. To summarize, I believe that this work should be published. I hope that the authors are able to address my concerns.
Reject. rating score: 2. rating score: 4. rating score: 5. rating score: 5. <BRK>Until one consults (Schonfeld et al., 2020) it is not clear what the per pixel feedback means, it s also not clear what is inpainting regularizations. These details are fleshed out in the appendix and in the prior work. I believe the flaw in presentation is due to weak originality of the paper. **3.Results**By looking at the results I cannot see significant differences compared with (Siarohin et al., 2019b). The improvement over FOMM is 0.0012 in terms of L1 on VoxCeleb and 0.002 on Tai Chi HD and 0.0016 on BAIR.<BRK>The overall framework is based on FOMM (Siarohin et al.2019).It merely proposes a simple way to identify difficult pixels and have the generation module focus on the more difficult parts. The mixing operation is similar to CutMix (Yun et al.2019).Different from CutMix that is demonstrated useful for many important computer vision tasks, the proposed data augmentation technique is designed and demonstrated only for a specific application, image animation. The improvements are very subtle. The paper is not well written. Also, I still think that the visual improvement is very subtle. For the six added videos, only fig_a2_label.mp4 shows clear improvement visually.<BRK>The occlusion mask indicates which pixels have heavy motions and most of these pixels are on the edges. After reading the paper, my feeling is that the author slightly changes the formula in CutMix and use this trick to get better reconstruction results.<BRK>+This paper is well written and easy to read. Most of sections are clearly presented with sufficient details. The main concern is its novelty. The proposed pixel mixing method for generative network training is interesting, but it doesn t show significant improvement over the baseline. It d be better to include video results for evaluation.
Accept (Poster). rating score: 7. rating score: 7. rating score: 7. rating score: 6. <BRK>This paper addresses the potential correlation between saliency map values and model generalization ability in image analysis with a focus on medical image. The paper falls well within the scope of the conference. It is overall well written, but it is so crammed with information that, in order to comply with paper length limits, its  digestion  and interpretation becomes difficult at times (this is clear in the case of images, whose meaning often has to be half guessed due to lack of details)The problem this study deals with is definitely important in the context of medical decision making on the basis of image, and the problems it pinpoints are more than real (small sample sizes, inter site heterogeneity, bias due to human intervention in the masking process, etc)The proposal seems sound to me and the experiments convincing.<BRK>This seems like a strong hypothesis to me, especially when considering e.g.different imaging sites, or image resolutions. Figure 5 can be difficult to investigate without colormaps. Making this a sufficiency condition is a step that is not well motivated to me. 4) The authors mention that masks represent a “good” attribution map. However, the saliency map reflects that the attributions are high for the confounder. Given my own experience of attribution techniques, it is likely that using different attribution methods will lead to different conclusions with respect to the correlation between IoU and AUC. Clarity:  Mainly, I found the paper well written and the experiments clear. Hyper parameters were described and confidence intervals were provided on all results. Overall, I think the experiments were well executed.<BRK>The authors employed a number of existing methods as well as proposed and implemented their own technique (ActDiff) to align saliency maps with causally plausible regions. I believe this paper is valuable for the field and community and therefore I recommend this paper to be accepted. Importantly, the paper raises a very interesting point, challenging the status quo in the field. For all these reasons, I vote for accepting.<BRK>Maybe it s because the VPC has smaller bounding box and thus only access to such region is too difficult? Overall evaluations  Overall I like this paper. The hyperparameter tuning is very thorough. They show that (1) these methods (sometimes) hurt generalization when spuriousness does not exist, and (2) the model s saliency map is only weakly correlated with generalization performance, and thus doubting the validtiy of using saliency maps for diagnosing whether a model is overfit to spurious features. 3.In all the experiments, the validation set is always assumed correct without any background shift. It seems Synthetic and Xray SPC have much higher IOU overall.
Reject. rating score: 4. rating score: 4. rating score: 5. rating score: 6. <BRK>This actually leads to a bigger question regarding the problem setup in this paper: throughout the paper, the tone is the system takes advantage of users to maximize its “commercial benefits”, which is very shortsighted. However, my concerns still remain, and I am not convinced to improve my overall evaluation. The idea sounds interesting and feasible to me, but the presented technical solution is unfortunately opaque and hard to follow. The reported experiment results are also not easy to follow.<BRK>The objective also accounts for the uncertainty of item representations which is particularly interesting. Experiments are conducted and results presented. The paper is well organised and it would be even better if lemma 1 and the proof of corollary 2 were moved to Appendix. Concerns:I was unable to find any empirical evaluation on test data for the proposed approach, in addition, lacking experiments that compare the proposed method with competing methods is another major concern. Updates: I would like to thank the authors for their response and the updated draft.<BRK>It is motivated by the fact that most recommender systems don t take into account that user may be highly uncertain about value/utility in certain dimensions (e.g., color of a product) while more certain about others. The platform can use this information explicitly while optimizing for what to show. The papers say its the representation of the item. Is it just user imagining that the item i will be displayed? This is very unclear. seems straightforward as well. Overall, the paper is well written, although the ideas are relatively less novel.<BRK>Cons: 1.Although several insightful experiments are provided, I still have several suggestions on the experiments to enhance the quality of the paper: (1) Although the data is from a real world scenario, there are only results on expected utility. Hope the authors can address my concern in the rebuttal period. Thus, is it possible to select another metric as a supplement and have several comparison experiments?
Accept (Poster). rating score: 8. rating score: 7. rating score: 7. rating score: 4. <BRK>Summary: The paper examines the security of a recently proposed privacy scheme, InstaHide Huang et al.2020, that can be used to generate synthetic training data. Under a standard Gaussian distributional assumption on the data, the authors propose an algorithm that can extract private information from the synthetic vectors generated by InstaHide. The main result shows that if the public and private data are both drawn from an i.i.d.Gaussian distribution then it is possible to reconstruct (some of) the private data if we have the access to output of the InstaHide on multiple queries and public data. This is an interesting result, implying that a careful discussion of security for InstaHide should be sensitive to the properties of the distributions generating the private and public feature vectors. The paper is well presented, however, most of the interesting theoretical results are in the appendices. I have not verified all the technical details, but overall the reconstruction idea looks sound. The result is specific to InstaHide scheme and is not a general statement about private distributed learning.<BRK>The purpose of the paper seems clear: it proposes an attack to the recently proposed algorithm called Instahide (ICML 2020) which is a probabilistic algorithm for generating synthetic private data in the distributed setting. The attack proposed in this paper is considered for the case where the private data is i.i.d.Gaussian distributed, and Thm 1.1 says that one can recover k original feature vectors with O(k^2) + O(M^2) computational complexity, where M is the total number of original data elements. As there is an attack for the Gaussian case (as given here), the assumptions on the distributions of the original data have to be something different if theoretical guarantees for Instahide are desired.<BRK>Huang et al.(2020) proposed InstaHide for this problem. The InstaHide can be summarised as follows:From a random convex combination of k_pub public and k_private vectors,Multiply every coordinate of the resulting vector by an independent random sign in {+ 1},and define this to be the synthetic feature vectors. The main result of this paper is to show when the private and public data is Gaussian, then they can recover the private feature vector form the given synthetic dataset and the public feature vectorsI think the topic is good. Under many situations, we should focus on security of private information. But in practice, the public and private dataset can be any. I think it is the affine phase retrieval or phase retrieval with background information.<BRK>This paper investigates the security of InstaHide, a recently proposed algorithm for scrambling a secret image data set so that it is still useful for learning but doesn t allow inferences about individual images in the data set. The attack works by reduction to a new variant of the phase retrieval problem with hidden inputs. The paper s results appear to be correct. I don t see why that s true in the Gaussian setting—isn t the mutual information between the synthetic data and the secret data infinite (since the coefficient vectors are discrete)? (I ve simplified the scheme a bit—the $n$ s and $k$ s can be different for the public and private data—but the simplifications don t change this discussion.) The paper mentions $k 2$ as a reasonable value. For InstaHide to be a reasonable approach to image data privacy, it should be hold up to settings where an attacker knows quite a bit about the format of the input. Then the linear combination of a small number of them would also be sparse with high probability, and its absolute values would exactly reveal all of the component grayscale images. One would start getting attacks with $M 1$.
Reject. rating score: 2. rating score: 2. rating score: 3. rating score: 4. rating score: 5. <BRK>2.The equations (1) and (2) are for 1 dimensional random variables. The $N$ quantile values defined in this paper are for which random variable? What is the meaning of this output? 4.The "$+\infty$" and "$ \infty$" notation is confusing. Did you perform some sensitive analysis on the choices of $a$ and $b$? 5.The authors claim that the WGAN training is slow. It may benefit to compare  QRGAN with these methods. The generative images are not demonstrated. It contains only well known results.<BRK>– Summary – The paper proposes a new GAN method that applies the quantile regression of reinforcement learning into GAN and aims to show this helps to estimate the 1 Wasserstein distance better without gradient regularization. I tend to reject the paper. W6   Many mathematical notions are not explained, e.g., What is $\rho_{\hat{\tau}}$ in Eq.4?How  do the authors implement with $a   \infty$ and $b    \infty$?.<BRK>Cons:    Overall, there are so many typos and grammatical errors in this paper that they make it difficult to understand the content of the paper. The authors must look for these errors and correct them. The authors state that the relationship between quantile regression and 1 Wasserstein distance is shown in section 2.1, but this is not explicitly shown. In Eq.(4), you state that a and b are set to +∞ and  ∞ respectively, but how were these infinities implemented in practice? Also, as far as I read, there is no indication in the paper of how the number of quantile values was set up in the experiment. In section 3.2, the authors should show the image actually generated by GANs.<BRK>The idea is to evaluate the quantiles of discriminator outputs instead of just using a single scalar discriminator value: while all quantile discriminator outputs for real/fake data is pushed to +infty/ infty, the generator s cost function tries to push the quantiles of fake data to be as realistic as possible (+infty). Experimental results show very good mode coverage on Gaussian Mixtures. Reasons for score: While the paper presents a reasonable and (to me) novel idea in GANs, the experiments fall short in comparing to the state of the art, and also the ingredients in the method are not sufficiently dissected to attain a sufficient understanding. Hence, I suggest to reject the paper in its current form. lack comparison to state of the art (sota) methods, which is necesary to put the work in context. similar for the Mixture of Gaussian: comparison to other standard methods in the literature that tackled this problem are missing (e.g.Unrolled GANs).<BRK># General statementsthe core contribution of this paper is to train GAN by designing the discriminator so that it outputs a whole distribution instead of a point estimate for "realism". The nice feature is that using losses over quantiles means using a wasserstein distance, which has strong properties in a GAN setting. I would have written min |a D(x_fake)|. * The actual performance is rather disappointing, since the authors do not clearly manage to demonstrate any superiority of their proposed approach vs author (classical) methods, except on toy data. * "and gradually reach to the optimal policy. what should I understand here ?
Reject. rating score: 6. rating score: 7. rating score: 7. rating score: 7. <BRK>This paper studies the change point detection problem. The classical studies in change detection problems are based on the known prior and posterior parameters, i.e., knowing the distribution (parameters) before and after the change points. Recently, people are extending the results to the case where the prior parameter is known and the posterior parameter is unknown (anomaly detection) or with some sampling cost constraints (data efficient change detection). However, this work proposes an algorithm that generalizes the CUSUM approach to the case where the parameters are unknown. The idea is very interesting and I believe the impact of the algorithm could be of significance given its potential in real world applications. Besides, I have the following comments. 1) The paper title is for multi rask problems. However, it seems to me that the proposed algorithm is very general for change detection problem. Except the one subsection in the experiments, I didn t see much connection to multi task problems. 2) The theoretical results are not very strong. There is no Theorem one can claim for the performance of the proposed algorithm. As the algorithm is an approximation to some optimal approach, one may provide a result in the form of competitive ratio or convergence rate. However, Lemma 3 is only some asymptotic behavior of the loglikelihood. 3) How should one choose the hyperparameters like c and epsilon? Are the results in section 5 tuned by grid search and presented the best one?<BRK>*****  Paper s Summary  *****This paper considers the quickest change detection (QCD) problem where pre change and post change distributions are unknown. For such problems, the authors proposed approximate algorithms in MIN MAX and Bayesian settings. The performance of proposed algorithms is verified using synthetic data and a reinforcement learning environment. ***** Paper s Strengths *****The proposed algorithms are the approximate methods that have a near optimal performance for QCD problems with unknown pre change and post change distributions. Further, these algorithms work for a more general class of problems as they do not require restrictive conditions like IID samples, specific distributions, etc. The performance of proposed algorithms is better than existing algorithms. ***** Paper s Weaknesses ***** The proposed algorithms solve an optimization problem (depending on the setting) for minimizing the delay in change point detection. As no deep learning models (or even a variant) is used to solve the change point detection problem considered in the paper, this paper seems to be outside of the ICLR scope. I find it very difficult to understand the paper in even 2 3 read. The authors need to improve overall writing quality so that it becomes easier to read and understand. ***** Comments ***** Some notations are not defined upfront e.g., Line 2 on Page 3: $S_n^{\theta_0, \theta_1}$ and $B_\alpha$. *****  Questions for the Authors  *****Please comments on how your paper fits the ICLR scope. After reading the rebuttal and comments of other reviewers, I am increasing my score.<BRK>This paper studies the quickest change detection for Markovian data, when both the parameters of pre  and post change distributions are unknown. The main contribution is a scalable algorithm that sequentially estimates the unknown parameters and plug in to classical detection schemes to get the stopping rule. A notable feature is that this is a joint estimation and detection framework. And the authors incorporate several tools, like SGD, annealing, penalization, into the detection task, which turns out to have good performance compared with existing benchmarks. Overall, this paper is clearly written and well organized, and the numerical examples support the claims made in the paper. Minor comments: 1. Usually in classical change point detection literature, people assume the pre change distribution is known since it can be estimated from historical (nominal) data, and the framework proposed in this paper can obviously be applied in such a setting as well. In such a case, the GLR and adaptive methods do not need to learn theta_0 offline and we can have a fair comparison of the performance of learning post change parameters and also the detection delay. 2.In Appendix A.1, the introduction to SHIRYAEV Algorithm, it seems that there is a missing \rho in the denominator of the statistics S. The reason is that only under this \rho scaled version of likelihood ratio can the recursion in A.1 holds. After rebuttal  Thanks to the authors for the response and updated paper. I keep my original score and recommend acceptance for this paper.<BRK>The author(s) propose a quickest change detection technique under known parameter scenario. They use a Markovian dynamics to generate the pre and post change point distributions and use a Shirayev test statistic based on the asymptotic behavior of the optimal delay under know parameters. The proposed methodology is validated on synthetic data and a multitask reinforcement learning example. There are several issues which restricts the paper to reach an optimal level. These are highlighted below Heavy dependence on Tartakovsky and Veeravalli (2005) results with known parameters. The contribution in the current paper seems a bit incremental. The algorithm 1 looks promising however some of the hyper parameter choices such as c, $B_\alpha$, $\epsilon$ and $N_e$ are not clear. For a practitioner which one is better to choose  Shirayev test statistic or Cusum? It was not quite clear to me why does one need both annealing and penalisation? I thought adjusting underestimation/overestimation of ${L_t}^*$ will be sufficient as behavior of pre changepoint will complement the behavior post change point. Does other choice of entropy based loss functions matter instead of KL divergence or KL divergence is the most natural choice here?
Reject. rating score: 3. rating score: 4. rating score: 4. rating score: 6. <BRK>This paper applies a weakly supervised learning approach to identify factors of object postures in an image dataset. It does not necessarily require posture grouping of objects. Affinity Cycle Consistency loss is set up to automatically map objects of similar active postures between the two image sets (objects of similar postures are supposed to be the nearest neighbors in the learned embedding space). The experimental study verifies the validity of the proposed factor isolation algorithm. Generally this paper is well written and clearly explains the motivation/problem definition. However, we have the following concerns on the innovative contribution of this work. The innovation of this paper is very limited. The only difference is: this work is built based on affinity relation between pairs of images with similar postures, while the original idea was applied for temporal sequence alignment in video processing. Not significant algorithmic innovation is introduced, compared to the previous work.<BRK>More specifically, the authors aim to extract representations of the factors of variations that are shared across groups. ACC imposes a soft version of the cycle consistency on the nearest neighbourg relationship between the two learned sets of embeddings. ################################################Strong points: Representation learning for grouped data is a relevant topic, for which the submission proposes a simple and effective method. The authors present a practical use case in section 4.3, showing how the method can help tackle the domain gap problem. The paper is well organized, formalism is clear and related work is informative. Weaknesses: ACC, as noted by the authors, is very similar to the previously introduced Temporal Cycle Consistency. The main contribution of this work is to provide empirical results when applied to a more general context. The paper lacks crucial discussions about why ACC allows learning a good alignment between sets in the general context. While the experimental results are intriguing, they aren t very convincing in terms of the usefulness of the proposed method. This makes it difficult to assess how much of the results can be attributed to the use of ACC and how much is due to other inductive biases. I might change my evaluation if this point is clarified (either via discussion or by additional control experiments).<BRK>The paper presents an approach to isolate factors of variation using weak supervision in the form of group labels. The proposed method Affinity Cycle Consistency (ACC)  claims to work with these group labels, which are weaker than the more common, one factor per group type labeling. While I appreciate the simplicity of the approach, there are some important concerns which the paper fails to address adequately. Experiments of Fig.3 indicate that with one inactive factor, two of the remining five active factors are isolated, and with two inactive factors, one of the remaining four active ones is isolated. This behavior is not clear as to why it happens? This behavior should ideally be explained through further analysis experiments. Similarly for other datasets. It is not clear how the temperature value (as used in Definition 2) is set.<BRK>This paper uses affinity cycle consistency to isolate factors of variation with only weak supervision on set membership. The weakness is that the experiments and discussions do not cover enough cases for extracting and suppressing factors. I recommend that this paper is marginally above the border. Isolating factors is a difficult and important problem, so the progress is good. The following are some concerns. To isolate all the active factors, set A should contain more elements than used in the experiment. It would be more helpful to find how many elements are necessary from theoretical and / or empirical perspectives. The paper does not provide code to reproduce the results.
Accept (Poster). rating score: 7. rating score: 6. rating score: 6. rating score: 6. <BRK>Strengths:* The authors tackle a problem that is impactful but not very well studied in the community, and propose a novel approach to do so. Weaknesses:* Experimental evalution could be more thorough. (1) Is there any related work on this topic (or simple baselines) that are worth comparing too? (2) The authors use the FID metric to evaluate the perceptual quality of their generated images, at different sampling budgets. Overall, I lean towards acceptance, based on the novelty of problem & approach.<BRK>#### ReviewThe paper studies the task of generative modelling with what is referred to as  anytime sampling , that is sampling with graceful early stopping, where computation time can be traded off with sample quality. The most serious criticism I have is that I felt that little attention was given to motivating the problem. If that is so then I m surprised there hasn t been more work addressing it. Could the authors add some more detail on their motivations for working on this, perhaps including specific examples of use cases? Compression was mentioned very briefly near the end of the paper when discussing related work. This seems like a natural application to me, and indeed what s referred to as  progressive decoding  in the compression community is a common feature of modern codecs. I m by no means an expert on the niche topic of anytime sampling, but there are a few papers which I think are sufficiently closely related that they should be mentioned.<BRK>The authors propose a novel way to trade off sample quality with computational budget in autoregressive models by performing the autoregression on the ordered latent space of a generative model. They show analytically and empirically that the tradeoff is monotonic and that it approaches the full performance at increasing computational budgets. Major comments:  I appreciate the application to VQ VAEs, since in this model, the autoregressive component already plays a major role. Maybe something like Fig.3 on a standard VAE could be helpful. If I understand correctly, the VQ VAE in the experiments uses a latent PixelCNN, while the OVQ VAE uses a Transformer. I think it would be a fairer comparison to also include a standard VQ VAE with latent transformer, as an ablation to see how much of the effect is really due to the ordered latent space.<BRK>Why is the original motivation for using the geometric distribution not applicable in this work? Authors adapt the *nested dropout* idea by Rippel et al.(2014) to encourage the discrete autoencoder to order latent dimensions by their "importance" for reconstruction. EDIT:I thank the authors for their detailed response. I also appreciate the effort that s been put into refining the draft and undertaking the additional experiments. Most of the points I ve raised have been addressed. paper s relationship with Rippel et. al s work is discussed. The write up itself could use some work, as also suggested above. al (2014), where $p_B$ is taken to be a uniform distribution.
Reject. rating score: 4. rating score: 5. rating score: 6. rating score: 6. <BRK>Summary:The authors propose a method for latent domain learning, where input data come from different domains and the domain labels are unknown. 2."Section 3.4 Weighted Domain Transfer" is not well motivated and very confusing. Pros:1.Latent domain discovery is a very interesting topic. 2.Empirical results show that the method brings improvement to minority domains.<BRK>[b] The visualization of $\delta$ sounds interesting but I can not understand the meaning. Based on these, I increased my score to 5. Based on this, my final score is 5. I like the analyzed scenario and I think it can have a strong practical utility. Based on these, I recommend a rejection at this time but encourage a major revision for resubmission. These kinds of experiments are sufficient.<BRK>Why do you think that is? h. Minor formatting issues. 4.Provide supporting arguments for your recommendation. 5.Ask questions you would like answered by the authors to help you clarify your understanding of the paper and provide the additional evidence you need to be confident in your assessment. 6.Provide additional feedback with the aim to improve the paper. What is the motivation of doing this? f. Based on Table 3, the positive effect of WDT is not strong.<BRK>The paper describes dynamic residual adapters designed to adaptively account for latent domains, and weighted domain transfer. This framework injects adaptivity into networks, preventing them from overfitting to the largest domains in distributions, a failure mode of traditional models that are exposed in latent domain learning. This paper is well written. In this part, the authors introduce the algorithm.
Accept (Poster). rating score: 8. rating score: 7. rating score: 7. rating score: 6. <BRK>Up to the difference in factors hidden in $O(\cdot)$, these rates match the rate of SGD in the convex and strongly convex cases under interpolation assumption, which partially explains why neural networks with non smooth activations can usually be trained as fast as ones with smooth activations. Anyway, authors should add a formal definition. "Unified optimal analysis of the (stochastic) gradient method." ## Final RemarksTo conclude, the paper makes an important contribution to the theory of stochastic optimization under interpolation. **Clarity, motivation, and related work. ** Authors proposed lower bounds showing that $O(1/\varepsilon)$ is the best convergence rate one can hope for in this setting. Moreover, the authors show that SSGD complexity bounds match the proposed lower bounds.<BRK>This paper studies SSGD method to nonsmooth optimization problems with interpolation condition. What is more, it is proven that O(1/\epsilon) is optimal for the subgradient method in convex and interpolation setting. 2.This would be a helpful result to investigate interpolation models, which has attracted attention recently. It seems that the authors blame the nonsmoothness in neural networks to the nonsmooth activation function. Fundamentally, is this crucial and why?<BRK>Summary:The paper considers stochastic finite sum minimization with a special structure of composition of a smooth univariate loss with a non smooth Lipschitz function. Using the former bound, sublinear 1/k rate is proved for stochastic subgradient method under convexity and interpolation conditions. The obtained convergence rates are interesting. Pros:1.Faster convergence rates for SSGD under interpolation condition, which match the non accelerated bounds for the smooth case. 3.Proposition 4.1 shows that this setting is very close to the smooth optimization case. Some related literature is not mentioned.<BRK>Summary:This paper considers the behavior of the stochastic subgradient descent method under the interpolation condition. They provide convergence rates for both convex and strongly convex objectives. Hopefully, the authors can address my concern in the rebuttal period. 2) Interpolation condition is a core concept in this paper. It would be better to give an example of an overparameterized model, which also helps to explain the convexity assumption.
Reject. rating score: 4. rating score: 4. rating score: 5. rating score: 5. <BRK>  Summary      This paper proposes an interpretable RL agent architecture that uses attention masks to produce visual explanations of the action selected by the policy and output of the value function       The authors demonstrate their method on 3 Atari games and use A3C as the training algorithm  Strengths      To the best of the reviewers knowledge, this is the first work to apply this type of visual explanation to RL      The interpretable agent performs on par with the black box one. Human studies to verify that the explains actually help the humans understand (or predict) the agent s decision would be very helpful in this regard. Using the attention masks to to interpret the decision of the agent based on just the current frame is misleading. I am still not convinced by the quality of the provided visual explanations nor am I convinced that the attention is well correlated with the current frame (the additional experiments provided do help somewhat in this regard, but are not extensive and reasonably inconclusive).<BRK>This paper applies the mask attention mechanism on the DRL model (actor and critic), to make the learned policy explainable. The empirical results in Atari 2600 show that the performance of A3C is further improved by implementing mask attention on the actor and critic network separately. Strength:   The paper is well written and easy to follow. The comparison of the score by the inverting gaze area is interesting. It is not a new concept that implementing an attention mechanism in a deep neural network to make it explainable. This key idea of this paper is similar to them, inserting the attention mechanism into the network to explain the RL policy in a top down fashion. It is necessary to further discuss and compare these works in the paper. The method is only evaluated in three environments. It is necessary to validate it in various environments, especially showing the results in 3D environments (visual navigation, robot arm manipulation). It would be nice to report and discuss the failure cases in the experiments, instead of only the success cases. "Reinforcement learning with attention that works: A self supervised approach."<BRK>This article proposes to include an attention mechanism to Deep RL (focusing on actor critic architectures), to provide a visual "explanation" of the learned policy. The proposed approach is to learn attention maps to inhibit part of the visual feature, separately for the value and policy networks. The attention maps being differentiable can be learnt jointly with the rest of the network. More importantly, the novelty of the article is not clearly argued: The use of attention maps to analyse and explain deep neural networks is not new in itself, and learning attention maps to improve vision tasks is not new either. Another issue is that I think the use of the term "explanation" is a bit misleading in this article: the proposed approach provides activation maps, which offer some hints at the system s process, but still require a large amount of human interpretation for an actual explanation.<BRK>Summary:The paper introduces an attention mechanism into A3C based reinforcement learning agents to identify the attended visual regions for vision based reinforcement learning tasks. They evaluated the proposed attention mechanism on three selected ATARI games (namely, Breakout, SpaceInvader, and Ms.Pacman) and show that the attention mechanism generates intuitive visual attention in decision making. The identified attention regions are intuitive and action conditional. The analysis of the learned attention masks seems selective. Some automatic metrics or systematic studies of different game categories (shooting, maze like, and ball and paddle) may shed light on the learned attention s general property. How is the proposed attention mechanism different from previous ones? Does it address some limitations of previous methods, such as capturing more action conditional information or more robust to initialization conditions? Some clarification on this would be helpful.
Accept (Poster). rating score: 6. rating score: 5. rating score: 4. <BRK>The authors address neural architecture search (NAS) scenarios. In particular, a framework, MetaD2A, is proposed, which yields a neural architecture for a new dataset. In a nutshell, the framework learns a "dataset to neural network architecture" transformation using a database of datasets and architectures. Each dataset is encoded via a "set encode" and the architecutres are obtained via a "graph decoder". The experiments demonstrate the usefullness of the approach and its improvements over conventual NAS approaches. The approach could be described Positive:  The results look very solid and indiciate improvements (time/prediction performance) over existing approaches  The paper is well written and structured  Additional details (e.g., implementation details) are provided in the appendixNegative:  The authors claim that NAS with meta learning has only been done with small datasets in the past. I have to admit that this is not precisely an area of my expertise, so I might be missing something.<BRK>The authors propose a novel framework named MetaD2A. There are mainly three components in MetaD2A: a set encoder, a graph decoder and a meta performance predictor. I think the main contribution of this paper is their intuition that performing neural architecture search rapidly from the datasets, while the components are all proposed before in different NAS scenarios. In summary, the paper has following drawbacks that need to be further concerned:1. Thus, it will limit the generalization ability of the whole algorithm. 2.In Table 1, MetaD2A is pretrained on Meta ImageNet while other baselines are trained from the scratch. 2.In ablation study section, they only compare MetaD2A with random, which is a somewhat weak baseline. However, the improvement of their method is not significant. 2020.[2] Lian, Dongze, et al."Towards fast adaptation of neural architectures with meta learning."<BRK>Pros:1.This paper proposes new scene, where a meta model is pre trained on some datasets, and transfer the learned meta feature onto other datasets to do fast adoption. This scene can be applied in variety of domains. 2.The experiment shows this method can fast adapt NAS from one image dataset to others and achieve SOTA performance. 3.The paper is well organized, the paper structure is clear. It seems that the proposed framework is just putting existing models together. Your graph decoder may fail. Since your framework needs other methods (GDAS / NAS Bench 201) to provide training material. These materials are all good architectures. Your experiment should prove that your model can generate variety of architectures. Overall Review:This paper proposes a new scene of fast adaption of NAS, which may be a good direction of NAS & meta learning. The paper proposes a framework to generate good architectures according to the datasets. [1]Does unsupervised architecture representation learning help neural architecture search?
Reject. rating score: 3. rating score: 4. rating score: 5. rating score: 7. <BRK>Summary The paper studies a certain notion of spectral sparsification of directed graphs. It claims the existence of nearly linear sized sparsifiers under this notion, and suggests empirical methods to produce such sparsifiers in nearly linear time. Section 6: The experiments in the main text concentrate on measuring the relative condition number of the sparsifier w.r.t the original graph as per the sparsification notion studied in this paper, but as written above it is not clear what do we actually get out of the sparsifier. Post discussion update: I thank the authors for their participation in the discussion. I recommend putting more effort into clarifying the mathematical derivations and into positioning the paper correctly w.r.t.prior work on the topic.<BRK>Authors propose a novel method to approximate a given directed graph with a´nother one (the sparsifier) which has fewer edges. The proposed method seems promising. 1.The main algorithmic result, Algorithm 1 should be placed in the main sectino of the paper and not an appendix. 3.The comparision for the linear solver application should also inclucde the spcial case of undirected graphs.<BRK>The algorithm is implemented, and some experimental results for the effectiveness of the sparse approximation as preconditioners are shown. The experimental results don t involve end to end uses of directed Laplacians / directed random walks for learning tasks, but are more about approximations for solutions of linear systems / pagerank produced. Furthermore, I m unfamiliar with uses of methods such as GMRES in learning tasks. So given such concerns, I m unsure how relevant this paper is to ICLR.<BRK>The main contribution is the proposal of a unified approach for spectral sparsification of such directed graphs. One of the main advantages of the approach is that it does not require the underlying directed graphs to be strongly connected and aperiodic, unlike recent work from the literature. What about comparison with other approaches? The authors rightfully point out that naive symmetrization schemes from the literature may damage the structural properties of the graph.
Reject. rating score: 5. rating score: 6. rating score: 6. rating score: 7. <BRK>This manuscript presents a method for generating protein sequences conditioned on protein structures. The authors propose to learn this model jointly with a sequence encoder, combining the sequence and structure representations to decode the sequence during training. 3.Examine the sensitivity of this method to rotated structures. I appreciate the new experiments and some of the clarifications. However, some of my fundamental issues with this work are not addressed. I fail to see how this claim is supported by the presented method or experiments. I think this work has promise, but is not yet ready for publication in its current state. A comparison against Ingraham et al is critical to understand if the proposed voxel based structure representation is better than other methods.<BRK>There are two central contributions   the first is a novel fold representation, in which the 3D structure is represented by the voxels of secondary structure elements, and then a fold representation is learned via a transformer based structure encoder. The authors provide a good summary of related work. To proceed with the novel representation, the authors first scale each protein structure to fit into a fixed size cubic box, discretized into fixed size voxels, and extract the alpha carbon coordinates of each amino acid. Here fold classification is used for the intra domain task, while for the cross domain loss they maximize the cosine similarity between the outputs of the sequence and structure encoders in addition to a cyclic loss. The performance of this approach is impressive, however I am surprised by the decision to leave out methods that do not focus on the inverse folding problem with a flexible backbone constraint   I would like to see the performance comparison with these methods. Please could the authors adequately justify this decision, or provide these comparisons.<BRK>Transformer is used for sequence modelling and structural modelling. Comparisons with a principled based method   RosettaDesign and two machine learning methods   cVAE and gcWGAN at both sequence level and structural level show that the proposed method can achieve better performance on sets of proteins with limited sizes. However, I also have the following concerns. Majors:1.A possible major drawback of the proposed structural representation method is that it relies on rescaling of very large structures to the 3D frame of fixed size. In the experiments, proteins with more than 200 amino acids were filtered out. How much gain was obtained in comparison with just direct optimization of the objective function with five terms in terms of learning curves and performance at sequence and structure levels?<BRK>This is a great idea and seems to play an important role in Fold2Seq’s performance. I think it should be both based on fold and sequence similarity. 4.It is not clear to me why the authors did not compare to methods without the flexible backbone constraint (e.g., Ingraham 2019), the rationale is not fully convincing. The authors shift the structure such that its center of mass is at the origin and its first CA is on the negative side of the Z axis.
Reject. rating score: 4. rating score: 4. rating score: 5. rating score: 6. <BRK>The paper is about leveraging the GAN idea to get robust features that are insensitive to adversarial attacks. Specifically, with a shared encoder/embedding, the feature of the adversarial attack are considered as the fake sample, while the feature from the real image is the real sample; by forming an adversarial game in the feature space and by jointly considering the classification loss, the AFD method is proposed. But the current manuscript may not be ready for publication. For example, too many materials (like experimental results) are given in Appendix, which makes the main manuscript less self contained; I would suggest rearranging the contents and put the important ones in the main manuscript. In the last paragraph of Page 2, the authors stated that “the proposed method outperforms existing methods by a large margin. However, this might not be true, by considering the performance shown in Tables 1 and 2 and the high variance observed in Table 1 and Figure 4. In the last paragraph of Page 3, the output of Da_psi should be real and the ideal discriminator output is {0,1}. A GAN matches two distributions. Many typos exist. In the last paragraph of Page 7, the representation sensitivity Sc is not defined.<BRK>The paper proposes an adversarial defense method that builds a robust classifier by using a min max optimization in the feature extractor. Pros:1.The proposed idea is interesting and reasonable. The only difference is the paper further uses a discriminator which involves a different network in the training process. 2.The results might not be reliable. while [1] uses a similar idea, it later shows a significant performance drop by using some later proposed adversarial attack method like Autoattack[2]. Specifically, it reduces the performance to from claimed 60.6to 36.64. For example, Figures 3  doesn t have x axis label. I personally find it very confusing on the different x scale used in the figure. (ii) (iii) is quite unrelated to the obfuscated gradients problem. Attacks like B&B or FAB are more suitable. "Defense against adversarial attacks using feature scattering based adversarial training."<BRK>This paper proposes to leverage an additional adversarial discriminator to distinguish between the clean and perturbed inputs from the representation level. However, the baselines TRADES is evaluated on ImageNet which is a more challenging task and it would be good to evaluate the performance on large scale dataset for the proposed method as well. From the methodology perspective, it would be good to analyze the difference between the distributions of benign and adversary representation. It would be important to evaluate and confirm that the learned representation of the benign and adversarial instances can indeed be learned and separated by a trained discriminator, which may lead to further interesting analysis and findings. [1] Liao, Fangzhou, et al."Defense against adversarial attacks using high level representation guided denoiser."<BRK>Summary:This paper proposes Adversarial Feature Desensitization (AFD) as a defense against adversarial examples. The authors showed through experiments on MNIST, CIFAR10 and CIFAR100 datasets that AFD mostly outperform previous defenses across different adversarial attacks under white  and black box conditions. Pros:+Strong defense performance+Novel ideaCons: No discussion on the scalability of AFD defense or results on larger dataset such as imagenetRecommendation:The idea is interesting and is backed by strong empirical results. It would be good to mention how exactly the black box attacks are conducted. It would be more convincing that AFD is not relying on obfuscated gradients with results on this form of black box attacks.
Reject. rating score: 5. rating score: 5. rating score: 7. rating score: 8. <BRK>The idea is interesting and novel in the sense that for existing methods in planning, in each step we expand the search tree outward by a step size, but not split the search space in half and work on each subproblem. The analysis of both experiments are comprehensive. However, my concern on this part is that the evluation is not convincing since there is only one baseline which is MCTS, which itself is not the best planner for goal directed planning. To improve the paper (but might not neccessary), I suggest the authors to replace the continuous grid world with other control tasks such as robot arm control. In general, I think the paper presents an novel approach which is promising and potentially beneficial to other areas in machine learning. Obviously, MCTS is far from the best performing algorithm on grid world navigation tasks.<BRK>DC MCTS assumes a (suboptimal) goal directed low level policy and its oracle value function. pros:  It is interesting to perform planning for sequential decision making problems in a non sequential manner via a divide and conquer approach. In the experiments, the baseline algorithm seems to be weak. comments and questions:  It seems to be unfair that the training strategy for the heuristics is different for DC MCTS and the MCTS baseline. It would be great to see the performance of MCTS that uses the same heuristics training strategy as DC MCTS. It would be great to see a comparison with the PUCT baseline that works in the original action spaces, where the prior policy and value function is trained using HER.<BRK>The paper is reasonably well written and easy to understand although the text might benefit from an illustrated example. The DC MCTS algorithm seems quite interesting and novel, as far as I can tell, although some of the assumptions (e.g., "task embedding") may reduce applicability. I was expecting it to follow the typical anytime planning approach in MCTS where search and behavior are interleaved for each decision in the actual environment. The algorithm leverages this policy in conjunction with a high level planner to decompose the large navigation task (on which the low level policy is likely to fail) into smaller sub tasks. However, the experiments suggest that the algorithm is extremely sensitive to noise or error in the learned components. How sensitive is the overall performance to the choice of the low level policy? While this is intuitively clear in the domains considered, I think a more formal description would be better.<BRK>Summary This paper proposes Divide and Conquer Monte Carlo Tree Search (DC MCTS) for for goal directed planning problems (i.e.problems where reaching a specific goal state is the objective, like traversing a maze with specified start and goal positions). Questions for Authors Would it be possible to clarify in more detail how the standard MCTS baseline can be implemented in terms of the Algorithm 1 pseudocode? Planning problems are modelled as AND/OR search trees, where OR nodes are labelled by a start state s and a goal state s , and AND nodes are labelled by triples (s, s , s ). The MCTS can construct a plan by inserting subgoals such that they become easier to solve for the low level policy by searching this tree.
Accept (Poster). rating score: 6. rating score: 6. rating score: 6. rating score: 6. <BRK>Summary This paper tackles the problem of catastrophic forgetting in a continual learning scenario, in which the same classifier is trained incrementally on new classification tasks, each defined on a new set of output classes, and asked to retain performance on all the previous tasks. Additionally, this paper proposes to store gradient attribution maps in the replay buffer and use the stored gradient attribution maps as additional targets for the current model (RRR loss): the model should have "explanations" of previous examples that roughly don t change upon seeing new evidence from other classes. The paper is well written and clear. Pros   Approach is simple, quite general and improves on a variety of baselinesCons   Is it really the "explanation" that is important ? The general idea is still to "summarize" the previous model but only use feature maps instead of gradient attribution maps. I think one of the baseline missing in this paper is to use the feature maps themselves as targets for the distillation, would that be feasible ? I d suggest the authors to put in a Table the ablations results with respect to the different XAI methods and some of the most important differences in performance for Figure 3.<BRK>The proposed technique is simple. It helps existing replay methods to boost their accuracy for class incremental learning. In addition to save some training examples of previous tasks or classes, it also saves the saliency maps of these examples as explanations of classification. For continual learning, it includes additional term in the loss function to take care of the saved saliency maps. Experimental results show that this additional saved information can help improve existing replay methods for class incremental learning. Can you give some reasons why the proposed method works? If that is true, the saliency maps may help the continual learning performance in some cases but harm it in some other cases? Can you try CIFAR10 and MNIST with 2 classes in each task? When you have fewer classes in a task, the second role may be more important. Can your technique help those methods that don’t use replay? I thought M^{RRR} is the model explanation memory. Are they the same? Anything wrong there?<BRK>This paper proposes a method for continual learning of a sequence of supervised tasks which is based on memory replay for remembering evidence from previously made decisions. This evidence relies on explanations of those decisions rather than data. These explanations are incorporated directly into the loss function. I would like to remark as strengths of the paper: 	The method adopts a novel approach of incorporating explanations along with data for  	The experiments provide clear insights onto which kinds of explanations would work better, by exploring several strategies like vanilla backpropagation, Grad CAM, etc. Other comments:  	I suggest to improve clarity of the paper by improving some of the language used, e.g.in page 4 it’s said that “We show below that combining RRR into the objective function of state of the art memory and regularization based methods results in significant performance improvements.”. In this case, “significant” is clearly not the word to be used as the paper does not include statistical significance tests supporting this claim! Questions for authors:   Please address questions in the comments above.<BRK>**Summary**This paper delivers an additional training objective for continual learning which regularizes the update of the model parameters by making them not deviate a lot from its temporal snapshots of the visual saliency map. The additional information, *reason*, that the regularization exploits helps to solve the problem of catastrophic forgetting in addition to providing a qualitative explanation. **Recommendation**I vote for acceptance as the paper brings a notable performance boost in continual learning with a simple yet meaningful objective. **Supporting arguments for the recommendation**In addition to the performance improvements described above, the method presented in this paper is expected to be of great help to the XAI community as well. Considering the impact of this paper on the XAI community that saliency maps and other explanation techniques (although they have to be differentiable w.r.t.parameter) can lead to such direct quantitative performance improvement beyond just XAI use, I think it is sufficient to accept this paper.
Accept (Spotlight). rating score: 8. rating score: 8. rating score: 7. rating score: 7. <BRK>**Summary of paper**The authors introduce a data relabeling method that they claim is the first that both allows for data dependent noise and is theoretically guaranteed to converge to an optimal model. The improvement on both synthetic and real data seems quite large. Figure 2 caption, "closed to 0 or 1" should be "close to 0 or 1". **Conclusions**Quality: Overall I like this paper.<BRK>The paper presents a learning method for the scenario of feature dependent label noise. A framework where label noise diminishes away from the decision boundary is established and a relabeling strategy based on this by relabeling highly confident points is proposed. The method is a straight forward adaptive method which the authors both theoretically and empirically explore in detail.<BRK>However, the noise can be with different distributions. Hence, the paper may provide a novel way to deal with noise labels. Feedbacks:[1] I found that the step size \beta has an influence on the threshold \theta, and how to set it. Whether the proposed method is suitable to the high level noise. [4] The details of all the compared method may need to be provided.<BRK>The paper is well written and reasonably easy to follow. Even though the empirical validation is reasonably thorough for a paper that also includes a theoretical analysis of the novel approach, it is here that the authors could improve the paper the most. Ideally, the paper should include another 2 3 real world domains in this evaluation. At the very least, they could add to the evaluation the CUB 200 dataset from [Yi & Wu, 2019], on which PENCIL was evaluated for robustness on less noisy data.
Accept (Poster). rating score: 7. rating score: 7. rating score: 6. rating score: 6. <BRK>This paper presents Neural Event ODEs, a method to extend Neural ODEs for modeling discontinuous dynamics in a continuous time system. The simplicity of the approach is another advantage, and I can see many possible applications/use cases that can benefit from such an ODE solver. No supplementary materials have been submitted in which this information could have been outlined.<BRK>This work provides an extension to the neural ODEs framework to include discrete changes (i.e.switching) in continuous time dynamics. Could you please add an explanation in the rebuttal and in your revised version of the paper? I recommend acceptance.<BRK>The main reason for my low positive score is that the originality of the idea is rather low, and I view the paper mostly as an incremental addition to the toolbox of neural ODE methods. 2.In the experiments, it would have been interesting to see comparisons to not purely learning based models. This allows neural ODEs to model abrupt changes in the dynamics (such as collisions or switching dynamics). I liked this paper and found it interesting and well presented.<BRK>What is the odesolve used? _______________________________________________________________________________________________________________________________________________________UPDATE: After reading the rebuttal I think most of my concerns have been addressed and I am updating my score accordingly. 2) I found the experimental part of the paper to be weak. c) in the collision example, table 2 indicates neural ODE has similar (if not better) performance than the proposed approach.
Reject. rating score: 5. rating score: 5. rating score: 5. rating score: 5. <BRK>##########################################################################Summary: This paper demonstrates the relationship between adversarial transferability and knowledge transferability. Then under mild assumptions, they prove adversarial transferability indicates knowledge transferability. The authors further justify their theory with experiments. ##########################################################################Reasons for score: Overall, I believe this paper studies an interesting problem. The relationship between adversarial transferability and knowledge transferability is of interest to both transfer learning and adversarial attack community. However, the paper does not provide how to further interpret their findings our apply them in practice. ##########################################################################ProsThe theoretical analysis seems sound and clearly organized.<BRK>Summary and contributionsThis paper studies the relationship between adversarial transferability and knowledge transferability. By defining two quantities to measure the adversarial transferability, it shows that adversarial transferability measured in this way indicates knowledge transferability both theoretically and empirically. Its backgrounds and theoretical results and proofs are clearly presented. The authors also do not clarify the reasonability of comparing vectors of different dimensions which is necessary for most settings of this paper. The setting of their experiments is clear and complete. Authors show they have a good understanding of prior work s contributions, especially the three types of knowledge transferability.<BRK>This paper studies the relationship between adversarial and knowledge transferability. It develops two metrics to measure adversarial transferability and empirically shows that adversarial transferability correlates with knowledge transferability. Pros: The paper studies an interesting problem. This paper is interesting in that it tries to quantify their correlation. Cons:The paper provides an interesting observation but fails to further investigate how this observation can be used to gain better understanding of either fields of adversarial examples or knowledge transfer or how the insights can lead to better techniques. What are the takeaways?<BRK>Theoretical analysis is conducted, revealing that adversarial transferability can indicate knowledge transferability. In this procedure, two quantities are formally defined to measure adversarial transferability from different aspects. ##########################################################################Reasons for score:Strengths:  This work study the relationship between adversarial transferability and knowledge transferability, which is still under explored. Both adversarial transferability and knowledge transferability are defined quantitatively, which enables in depth understanding of them. Although the problem is fundamental, this paper does not seem to be an adequate exploration of the relationship between adversarial transferability and knowledge transferability. The three experiments repeatedly prove the positive correlation between the adversarial transferability and knowledge transferability in three knowledge transfer scenarios, which seems to be repetitive.
Reject. rating score: 5. rating score: 5. rating score: 6. rating score: 7. <BRK>This paper revisited two important stochastic algorithms, AdaGrad and AMSGrad. The improvement results and the dependence on the extent of interpolation violation seem interesting. The stepsize lies in a bounded interval lower bounded away from zero, so that the randomness of stepsize is well controlled. ***For future improvement, I think authors should emphasize the difficulty of the analysis more clear. In principle, stochastic line search under overparameterized regime does not make things harder because the stepsize is lower bounded. It seems the analysis of momentum plus this observation is enough for the analysis. It would be useful to provide some insights and challenges of the analysis.<BRK>This paper analyzes adaptive algorithms such as adagrad and AMSGrad in a finite sum optimization problem. I do not think Theorem 1 is particularly novel. Is it just for empirical performance with no theoretical benefit yet? It is not obviously presented this way, so if so I think some remarks to this effect are in order. The assumption on bounded eigenvalues for the results on AMSGrad seems a little troublesome to me: I am worried that all of the adaptive nature of the preconditioner is irrelevant and these assumptions are doing all the heavy lifting. I would be happy to hear otherwise, though! I am not confident that the other results are significant on their own. I would have preferred to see more standard deep learning benchmarks on non image tasks as well,, but I am not an expert here and so would defer to other opinions. I do believe it has been assumed in some past literature, but this does not make it actually true.<BRK>The authors show that the convergence rate is O(1/T). In addition, when the interpolation is approximately satisfied, the authors show the convergence to a neighborhood of the solution. Overall, I find the paper easy to read. Can the authors  analysis adapt to such situations? 2.Taking the logistic regression as an example again, the "minimizer" is not within a bounded region. 4.I think it would be informative to add the result when the loss is strongly convex, where we can have the bound for the solutions.<BRK>I do not feel the technical results very solid, as some difficult to check properties are just put as assumptions (see cons). The results provides insights regarding why adaptive gradient methods may converge faster when the interpolation assumption is satisfied. The other clarifications are OK to me. I suggest the authors add some clarification regarding 4 in the main text. 3.The Polyak step size is well motivated in the interpolation setting. In particular, this paper assumes the sequence of iterates is bounded in a set of radius D and the eigenvalue of the preconditioning matrices are bounded. 4.It is claimed that the step size chosen by the proposed conservative Lipschitz line search method is bounded in [2 (1   c) / L_{max}, \eta_{k   1}]. 3.The abbreviation SPS is not defined when it appears for the first time in the main text. ##########################################################################After reading author rebuttal: I think the value of this work is to demonstrate the possible benefits of an interpolation assumption. It is more reasonable to move Appendix C.2 to the main text though I guess it would be difficult in practice.
Accept (Poster). rating score: 9. rating score: 6. rating score: 6. <BRK>In this paper, the authors propose a generic system for performing one shot audiovisual synthesis from only one small sample. The results are impressive for in the wild speech synthesis and their approach could have a broader impact in the community. + Comprehensive analysisWeaknesses:    No theoretical novelty. It seems much of the benefits of the approach comes from the extra data and training procedure.<BRK>This paper covers a very interesting topic and method to convert any input speech to many audiovisual syntheses via exemplar autoencoders. The manuscript is well written and presented. However, there are a few major concerns. This approach required only a small amount of data to train the exemplar autoencoders when learning specialized models tailored to particular target data. I think this approach is still in an immature status. Hence, more work is required to improve on the audio decoder part.<BRK>Interesting paper covering a lot of ground; exposition could be tightened/clarified? Maybe they could clarify their terminology or re read the draft from the perspective of someone who is not as "close" to the work as they are. The contrast with zero shot conversion is very interesting   but here too I feel I am somewhat missing the explanation of the essence of these zero shot methods   though it is possible most readers do not need any additional explanation. In particular, when I see Fig 2a, I m wondering how the content and speaker embedding vectors are trained (and on what data), and the text doesn t quite clarify that for me. as a seminal study in this area.
Accept (Poster). rating score: 8. rating score: 7. rating score: 4. rating score: 4. <BRK>Texture vs shape sensitivity is a basic and important question in understanding how deep convolutional nets work. This paper investigates this question by asking how CNNs represent shape versus texture information internally. Several different network architectures and layers are compared; and results are compared to baselines from previous papers as well as natural lower  and upper bounds. The paper makes several contributions, extending beyond Geirhos 2018 by investigating the internal representations of shape  vs texture rather than just the external performance characteristics. The use of segmentation readout, tested several ways, provides helpful and new qualitative visualizations. The paper could be strengthened if it proposed further testable hypotheses. The paper does confirm that the measured shape dimensions is lower for BagNet architecture with reduced receptive field, but beyond observing correlations, the paper does not try to directly verify that the identified shape dimensions are responsible for the network’s shape recognition abilities. For example, would the shape segmentation readouts be more damaged by removal of the most shape specific dimensions (vs other neurons)? Or would the removal of these dimensions affect external shape or texture performance? If an effect like this were verified, it would strengthen confidence in the proposed correlation as dimensionality measure of shape and texture content. **edit, after revisions**  As mentioned in discussion below, I think the addition of intervention experiments makes the effects very clear and strengthen the paper. I revised my score from 7 to 8.<BRK>Pro’s: * The question that the authors are trying to understand is interesting and relevant in the field of object recognition, texture/shape bias, and learned representations in deep neural networks. This is not “yet another paper on trying to overcome the texture bias without any intuition what so ever, to try to get better numbers (as has unfortunately been done in computer vision these days)”, on the contrary, this paper is about understanding the underlying mechanisms of the texture/shape bias that extend the final stages of computation in the visual hierarchy, and is why I am leaning towards accept. Nearly all figures in the paper are clear and help convey what the authors are trying to express (although see somes clarification points)* The paper is clear and easy to read/understand. It would have been great to compare the result with that of Stylized ImageNet as well (that presumably will have a greater increase in shape tuned neurons vs texture tuned neurons). I have an intuition that they are related to the depth in the hierarchy of the network, but not on the layer type or the computation (example: is it a conv layer, is it rectified?) (presumably it is the input/image so it is redundant?) Or more than one? Altogether the paper is well written and there is a good body of results/evaluations that support some of the claims the authors are doing. The problem they are working on is interesting and relevant to the ICLR and greater CV/ML/Vision Science community. I would like to thank the authors for clearing up my concerns and would like to raise my score from 6 to 7. I think this paper would be a good poster that can provide complimentary insights through different experiments to the recent paper of Hermann et al.NeurIPS 2020.<BRK>The second, evaluates the per pixel shape representation, by attempting to generate the input s segmentation mask from the given representation (activation of the image at that layer). *****Pros:The method provides a comprehensive review of the encoded shape information, both on the representation level and pixel level. The degree of shape representation as a function of epochs or time trained. The paper is also very clear and results are presented well and well explained. Eq.1 upper bounds the mutual information using variable p_i, which can be calculated. Why is this bound tight? Regarding the second of the algorithm (B in Fig 2), existing algorithms in literature, presented for example in [1], learn a classifier, that given the representation of an image, attempts to classify it shape. In addition, the use of a classifier as in [1], could also be used to quantify the use of shape in different representation layers. Does the existing measure reveal something more interesting in Tables 1 4 and Figure 3? ********Overall, while the paper provides a comprehensive analysis of the use of shape in CNNs, both on pixel level and the representation level, I am not sure of the correctness of Alg.1 and the originality and relevance of Alg.2 given existing work. Hermann et al.,<BRK>This paper performs a detailed analysis of shape vs texture tradeoff in deep neural networks. 1.The contributions of this work is limited. First of all, the methods of this work is adopted or based on previous methods. The idea of using “read out module” to segment images is similar to the idea in (Hermann et al, 2019) where a linear classifier is trained to predict the shape label. Moreover, the results are not interesting. The analysis conclusions made in this paper are all expected and not surprising, though i have to acknowledge that this is a more fine grained study. I would suggest the authors to stress a few key messages that you are trying to make in this paper, rather than the lots of analysis results presented in the current one without a central conclusion. I would like the authors to spend some time to discuss the motivation for using the two analysis methods and the dependencies between them. This will make this work coherent and consistent. 3.The design of readout module is not clear to me. The current one seems to be too simple, can their be other designs for the read out module? What are these two images? What does GT means? After rebuttal:  I appreciate the authors for the response. However, I feel that the conclusions drawn from this paper are not focused, there is no central message that i can get as a clear take away for understanding CNNs after reading the paper. I do agree with each of the findings the authors make, it is just that the presentations and writings of the paper make me confused about what the authors are trying to convey through this paper.
Accept (Poster). rating score: 7. rating score: 6. rating score: 6. rating score: 5. <BRK>SUMMARY OF CONTRIBUTION:This work presents the LiSP architecture for model based lifelong learning and applies it to the problem of learning in continuing tasks with non stationary rewards and no state resets. At the very least, however, the presentation of the algorithm and the experimental results needs significant improvement, and my score reflects the current state of the paper, but could increase if the presentation issues above were addressed. The fact that the agent does not need to learn its policy from scratch potentially makes this approach far more robust to the permanent failure states. It might be helpful to provide a more detailed discussion of the learned practice distribution, and why it leads to such a substantial improvement over the uniform distribution used in the original DADS algorithm. 4) In what experiments is pretraining being used?<BRK>### Strength  The problem is significant and pretty relevant to the field. LiSP seems to work well on the reset free environments considered in the paper. ### Weakness  The scope of non stationarity investigated here is pretty narrow (only focus on non stationarity of tasks, as mentioned in the appendix). We only have numerical results on the environment returns here. There is still room for improvement in terms of the writing: as raised by the other reviewers, the text is a little bit dense to read. It would be great if the authors can further refine and brush up the flow of the paper to make it more accessible.<BRK>In Figure 4, SAC should be seen as a lower bound for the baselines, and having some other, more reasonable non stationary RL baselines would be useful. ## Areas to improve* The paper is somewhat dense to read. * One significant weakness of the paper is the lack of comparisons with the baselines, which makes it hard to understand the usefulness of this work. The experiments in the appendix are interesting too.<BRK>This paper presents a lifelong reinforcement learning framework in a non stationary environment with non episodic interactions. This approach is evaluated with Hopper and Ant tasks. Cons:  The evaluation is too simple in comparison with the goal of lifelong learning. I have doubts that the skill formulation with some simple prior $p(z)$ is sufficient for complex tasks. The formulation is less ambitious than what I would expect an accepted paper to be: this paper seems to learn a some what fixed environment and just solve the tough model based reinforcement planning problem with mpc.
Reject. rating score: 4. rating score: 4. rating score: 4. rating score: 5. <BRK>However, it seems like there s still some open questions about the types of improvements being made and what this implies about the LM s attention mechanism. It seems like more quantitative analysis would be needed to determine how much the LM s attention is correlating empirically to factual knowledge or if there are other factors that are affecting the downstream improvements. Additionally, there s some limitations to the way the language model is being leveraged and the types of knowledge it can extract. I also wanted to mention that I appreciate the addition of the suggested related work, but I would still suggest that the authors consider looking into more detailed means of comparison in the future (especially to the Petroni work), since this seemed to be a concern in multiple reviews. Original:This paper is aimed at using pre trained language models to create open ended knowledge graphs. Improvement over previous state of the art models. The experiments seem easily reproduceable. The algorithm presented here is able to be used in an unsupervised way and can work with both open ended and more structured knowledge graph schema. To name a few:   “Language Models as Knowledge Bases?” EMNLP 2019   “Commonsense Knowledge Mining from Pretrained Models.” EMNLP 2019   “Comet: Commonsense Transformers for Automatic Knowledge Graph Construction.” ACL 2019  Although the proposed model achieves quantitative improvements over Angeli et al.(2015), it seems unclear whether these improvements are due to the factual knowledge encoded in the pre trained LM, as claimed. It seems possible that these improvements could instead be due to the high quality semantic/syntactic relations encoded in the attention mechanism. The title of the paper is a bit strongly worded and may be over claiming what is shown quantitatively in this paper.<BRK>Weaknesses:  The main evaluation in the paper is done on the grounded facts after the mapping stage. In this case another strong baseline would be to learn distantly supervised entity and relation extraction methods. Of course, this approach would not be able to produce the ungrounded facts, but when evaluating only the grounded facts it would be nice to see how the proposed approach compares to this more traditional setting. Specifically, what is the size of the KG available for entity / relation linkers, and were there any overlapping facts between this and the facts used for evaluation? Several recent papers have looked at probing LMs for knowledge facts (e.g."Language Models as Knowledge Bases" Petroni et al, EMNLP 2019, and follow up papers). These are very relevant, as another approach for constructing KGs from pretrained LMs is to use natural language templates. But there is no discussion of these works in the paper. Selecting the threshold for the matching degree seems to be an important step influencing the quality of the final KG, but it seems to have been done in an ad hoc manner here. This is clearly not true as a large amount of knowledge is also stored in the word embedding table.<BRK>This paper targets an ambitious question   constructing a knowledge base from scratch using pre trained language models. First, the insight that knowledge base is embedded in the world knowledge, which is captured by the pretrained embeddings, is not new. [1] studies the joint embedding of knowledge base and text and thus performs knowledge base construction in a semi supervised fashion, which seems to be more promising than reconstructing the knowledge base from scratch in an unsupervised fashion. I m curious to see the performance of the power of existing knowledge bases, the signals there can be used to initiate the training. It s hard to understand the current performance without a reasonable baseline. Apart from that, it would be nice to see the error analysis   what s the common pattern where MaMa fails? In addition to the above major comments, I have the following questions:(a) In the map phase where MaMa converts a sentence into facts, do we have an underlying assumption that the input sentence is a fact passage. (b) What if the input passage contains more than one fact? [1] Wang, Zhen, et al."Knowledge graph and text jointly embedding." Proceedings of the 2014 conference on empirical methods in natural language processing (EMNLP).<BRK>It first creates ungrounded triples (a.k.a.OpenIE) from raw text by looking into the attention weights between words and finding a sequence of words through a beam search that has high attention weights between every consecutive words. The proposed method only requires a single application of BERT per paragraph (instead of, for instance, applying BET for every possible triplet), which is relatively efficient. Weaknesses:  The head and the tail entities are always single words, whereas many entities such as names have two or more words. The comparison is made only against Stanford OpenIE, which was proposed 5 years ago. The paper highly depends on the grounding techniques from many years go, namely Spitkovsky & Chang (2012) and Stanford OpenIE (2015). Given that the proposed model and Stanford OpenIE have similar performance, I think it is necessary to perform ablation study and show how much is the dependency. In the worst case, I am worried that most of the work is being done in this grounding stage (i.e.ungrounded triplets are not so good)  If all possible head tail pairs are enumerated in a sentence, my expectation is that there will be a lot of garbage triplets. However, I am not 100% convinced how this could work so more analysis would be helpful, and given that Stanford OpenIE is pre BERT from 5 years ago, the gain seems to be not so exciting.
Accept (Oral). rating score: 8. rating score: 8. rating score: 8. rating score: 7. rating score: 7. <BRK>This paper analyzed the averaged SGD for overparameterized two layer NNs for regression problems. The paper is well written, and the result looks very interesting. Here are some comments about writing. Is that possible to avoid it? The following paper improved the cost per iteration, in the NTK overparameterized regime. On the convergence rate of training recurrent neural networks.<BRK>Indeed, despite Proposition A and the sketch of the proof, the article consists in a *plug and play* result on averaged SGD. Indeed, despite the fact that optimality in RKHS can be technical to introduce, I found the paper very clearly presented and well motivated. The references are also both precise and sufficient to understand well the problem.<BRK>Based on this, the authors derive a convergence rate for the predictor constructed from the T th iterate of averaged SGD and the Bayes predictor in terms of the L2 distance wrt the distribution of the features. It seems that the paper solves an important problem related to the training of neural nets. The paper is rather well written even if some paragraph are hard to understand for a non expert.<BRK>In Particular, the authors derive minimax optimal learning rates of the averaged stochastic gradient descent method for over parametrized two layer neural networks with smooth activation functions. One difference of this paper from other studies is that the positivity of NTK is not required in the error analysis. Numerical experiments are also illustrated to confirm the theoretical results. The paper is well written and interesting to read.<BRK>Summary: This paper considers the convergence property of averaged stochastic gradient descent on a overparameterized two layer neural networks for a regression problem. ##########################################################################Pros:  + The paper is technically sound.
Reject. rating score: 4. rating score: 5. rating score: 7. <BRK>It proposes to solve the CMDP by decomposing it into a pair of MDPs; i) a reconnaissance MDP (R MDP) that is trained with the help of a generative model, and ii) a planning MDP (P MDP) that is trained given a threat function (i.e., previously trained for the R MDP). Overall, I would say this is the weakest part of the paper. ii) It is not clear if the selected experimental benchmarks are challenging. Looking at Table 1 and 2, it seems either i) the selected domains do not benefit from long term planning, and/or ii) the proposed method learns short sighted policies. iii) The experimental results require better presentation.<BRK>This paper attempts to propose a practical method for solving constrained MDP via decomposing the original problem into two MDPs. The policy is then used to define secure actions to help solving the second MDP that maximizes the reward. While the method does not guarantee to find the optimal safe policy, the authors claim (and show in few experiments) that it can perform better than classical methods. While the approach seems simply and intuitive, it is not clear whether the contributions are significant for the community. More thorough experiments would be appreciated. In addition, beyond MPC, is it possible to add other baselines that use a learning based approach with access to generative model? If possible, I think this will help to further clarify the benefit of the overall idea of decomposition.<BRK>This paper proposes an approach to learning safe reinforcement learning policies using a simulator. At the same time, the existence of the simulator does not make the problem trivial, as the optimization remains challenging, and the relative efficiency of the proposed approach is an argument in its favor. Actually, the paper also proposes to decompose the safety reward into scores w.r.t.various events such as colliding with the various objects in the environment, and uses a linear combination of these component scores to obtain an aggregate safety score.
Reject. rating score: 5. rating score: 5. rating score: 6. <BRK>Summary:This paper proposes cWGAN GEP for long range forecasting of time series data. cWGAN GEP is a combination of data generation of data prediction, where given some observations, GAN iteratively generates a short synthetic time series data, and an LSTM subsequently makes a long range prediction based on the generated synthetic data. In order to train both components, the authors use information theoretic clustering. The claim that the proposed hybrid approach is helpful for long range forecasting is not strongly justified.<BRK>This paper presents a framework for long range forecasting of time series data using synthetic data. Therefore, the contributions of this work are over claimed by the authors in the introduction part and its technical contribution is minor. 2.The idea of utilizing synthetic data for improving the long range forecasting of time series data is interesting. Cons:My main concern towards this paper is that it lacks novelty on the technical side.<BRK>Summary:  This paper proposes a technique for long range time series forecasting that leverages a generative model to extend the existing time series to train a time series model to make the final prediction at horizon N.  The authors design a GAN, exploring an enhanced loss function, and provide a methodology by which to train it. In our own work we have found that, for AR models, it depends on the time series itself. This metric has some drawbacks; it is not at the scale of the data and it amplifies changes in error. cons    The paper chooses to use MSE to compare models. Abstract "for THE next few time steps"S3.2P1 "improve classification results"S3.2P2 "to generate synthetic" This raises the possibility that the concluding statement of "significantly outperform(ing)" other techniques may be a bit exaggerated. suggestions / questions   I enjoyed this paper and its topic.
Accept (Poster). rating score: 7. rating score: 7. rating score: 6. rating score: 4. <BRK>Verification of the hypothesis: The paper qualitatively and quantitatively shows the influence of a shape biased and a texture biased model.<BRK>In general, the proposed method itself is quite simple and has a lot of familiarities to the previous approaches, which the authors note. 3.The paper is extremely well written and concise.<BRK>end of p.7 "which can BE cropped"  personally, I think it s a shame one has to go to p.5 to understand what the method is actually doing when it is so simple. This alone seem to provide a clear boost on IN and different variants.<BRK>###################Weaker aspects:  method is a  bit too simple (however, it is effective). the method would benefit from a more principled way of choosing textures.
Reject. rating score: 4. rating score: 5. rating score: 5. rating score: 6. <BRK>This paper proposes a new optimizer called Adam+, with two main distinctions from standard Adam template: 1) the first order moment estimate is computed using the gradient evaluated at an extrapolated iterate. To sum up, I vote for rejection since 1) the analysis and parameters require strict condition, 2) it is not clear if the analysis illustrates the practical performance (very small $\beta$ is needed in theory), 3) practical merit is unclear since the algorithm needs to be tunes similar to SGD and the results are also similar to SGD. Weaknesses and suggestions for improvement: I have several concerns about the potential impact of both theoretical and practical results. Here, I have several questions. So, I do not understand what is adaptive in the step size or in the variance reduction mechanism of the method. For experiments, the authors say that Adam+ is comparable with "tuned" SGD. Then, what is the advantage compared to SGD?<BRK>Objective of the paper: The paper proposes a new optimizer "Adam+" that computes the first moment estimate at extrapolated points and the step size is normalized by the root of the norm of the first moment estimate. The paper establishes a convergence theory for Adam+ and conducts experiments on different deep learning tasks to demonstrate the advantage of Adam+. The paper proposes a novel algorithm to train the deep neural network. It has both theoretical guarantee and empirical evidence to justify the advantage of Adam+. Moreover, it is known that the performance of Adam is sensitive to the learning rate [1]. I do not recommend the publication for now.<BRK>Summary: This paper proposes the Adam+ algorithm that maintains an exponential moving average of the first moment and normalizes it by its $p$ th moment for some $p \in (1/2, 1)$. (4) Page 13, the last equation: the first term should has a constant $8$. (3) The algorithm performs well on the examples considered in the paper. Is the analysis for Adam+ unnecessarily loose, given that Adam+ and NIGT are almost identical? (5) For experiments on CIFAR10 and CIFAR100, the performance of Adam seems to be very poor.<BRK>This paper propose a new variant of ADAM, which replace 2nd order momentum with 1st order momentum. In addition to the training curves, it is better to present a quantitative empirical results (for specific metrics) when comparing different optimizers. 3.The theoretical analysis and implication is very clear. The variance reduction property of the 1st order momentum provides a good characterization of the convergence of the algorithm.
Reject. rating score: 5. rating score: 6. rating score: 6. rating score: 7. <BRK>2  On the image classification tasks for Place LT and iNaturalist2018, the proposed method achieves state of the art performance. 3  On the image segmentation tasks, the proposed method achieves the best mean accuracies and competitive mean IoU in comparison with several typical loss functions. 4  The overall paper is clearly presented and easy to follow. For example, the method sometimes achieves worse mIoU in comparison with Focal loss (as shown in Table 1 b and Table 2 a). As a paper based on image analysis, the visualization comparison experiment is also important and necessary. This could be easily addressed but should be included nonetheless.<BRK>A novel recall loss (RecallCE) that considers dynamically changing class recalls is proposed in this paper to mitigate class imbalance in long tailed recognition problems. Relationships between RecallCE and existing widely used loss functions are mathematically shown. Paper strengths  The paper does a good job introducing the proposed loss by gradually progressing from conventional CE to InvCE, and from InvCE to RecallCE. And it also relates RecallCE to Focal loss. For a training batch that includes all the classes (e.g., Cityscapes), RecallCE has no hyperparameter. Semantic segmentation is a pixel wise classification task which is still essentially a classification task that should benefit from SDN. One closely related prior paper is not cited/discussed or compared with. This prior paper introduces Seesaw loss which dynamically adjusts class weights based on accumulated/seen training samples. Overall, this paper is good but the authors need to address some of the weaknesses to make it more convincing.<BRK>This paper proposes a new loss based on the Recall metric to deal with imbalance problems in several visual recognition tasks (i.e., classification and segmentation). The paper is easy to follow. Results show that the proposed loss outperforms the other losses compared in the paper. For example, the F1 score of a class is given by the harmonic mean of precision and recall, combining both. I also believe the experiments are poorly conducted. Authors resort to mean IoU and mean Acc for segmentation, and accuracy for classification. Is the improvement due to the model improves the performance over all the classes? Related to my previous comment, authors only show the per class mIOU and Acc on the segmentation task, and comparing to CE and CB CE (Fig.2a and 2b). These results, together with the values reported in Tables 1 and 2 (where CE outperforms the proposed loss in 1 case, and in the other 3 cases the improvement over CE is marginal), make me wonder about the usability of the proposed loss.<BRK>The paper presents a learning approach to tackle class imbalance, resulting in a good performance on frequent as well as infrequent classes. The main idea is to use the recall performance of class to adaptively weight it, such that the performance is balanced across the categories in a dataset. The paper is overall presents a strong set of ideas, is clearly presented with several performance improvements. Pros: + The loss can dynamically adapt during the training process based on the recall rates. + Exponential moving average is used for a feasible computation of recall with a large number of classes. + Good performance is demonstrated on classification and segmentation tasks. + Some of the drawbacks of statistics balanced losses can be avoided with the use of proposed loss, e.g., reduction in the false positives compared to CB loss. This is not necessarily a negative thing in itself, however, it is not clear to me if the benefit for representation learning is due to the decoupled design or the proposed recall loss. Can the authors comment on this? The idea to use performance rates (e.g., recall) in class imbalanced learning is not entirely new.
Reject. rating score: 4. rating score: 5. rating score: 6. rating score: 8. <BRK>My main concerns are: (1) the weakness of the theoretical property of “weak stability”, (2) lack of a more complete evaluation of convergence, (3) general confusion throughout reading the paper. The paper shows that this algorithm results in each step generating a “weak stable fixed point”. I understand that these things are expensive and I have this comment for the vast majority of work in RL I have ever read, but showing plots like figure 5 is misleading, esp.<BRK>There are a lot of interesting ideas in the paper and the experiments are promising, but many of the details in the paper are not clear. A more detailed description of the algorithm should be given along with the assumptions needed. These issues should be clarified in the paper. Also, the method is based on theory for converging to a (weak stable) fixed point.<BRK>The authors propose, at iteration k, to find a weak stable fixed point for a game M_k, defined by a matrix of payoffs given by the (expectation value of) advantages. Strengths  The paper is written clearly, and the technique is relatively straightforward. Weaknesses  I would have liked to also see some comparison with techniques like LOLA, symplectic gradient adjustments, etc, as it s unclear what equilibrium MATRL converges to. Are MATRL solutions still a  true  Nash equilibrium of the underlying game? I would have liked to see more reporting on the runtime of solving the meta game.<BRK>The paper addresses the important problem of the non convergence of independent learning in MARL. The algorithm proposed is novel and well justified, and the experiments show that it yields an improvement over independent learners. 3.The experiments demonstrate the advantages of the algorithm, in a variety of domains and with a fair comparison to the IL methods MATRL is being based on and other relevant baselines.
Reject. rating score: 4. rating score: 4. rating score: 5. rating score: 8. <BRK>This paper proposes a general Bi tuning approach to fine tuning both supervised and unsupervised representations to downstream tasks. The main contribution of this paper is integrating two heads upon the backbone of pre trained representations: a classifier head with an improved contrastive cross entropy loss and a projector head with a newly designed categorical contrastive learning loss. In particular, the Introduction section has a nice flow and puts the proposed method into context. + The results section is well structured. Cons:   The key concern about the paper is the lack of enough novelty. It seems that the core idea of Bi tuning is the last overall loss function: $L_{CE} + L_{CCE} + L_{CCL}$, but the author stated that "the naive combination of the supervised cross entropy loss and the unsupervised contrastive loss is not an optimal solution for fine tuning" in section 4.4.<BRK>Summary:The authors propose to augment the regular fine tuning stage by two additional loss: a contrastive cross entropy loss (L_CCE) and a categorical contrastive learning loss (L_CCL). The numerator can be bigger than the denominator in this case (and then the log term will be negative). Exploring the intrinsic structure of the downstream task is an interesting direction to improve transfer learning performance. Weakness:The additional loss terms are not well motivated and difficulty to justify.<BRK>### SummaryThe paper suggests to extend fine tuning of pre trained representations by adding additional contrastive losses to leverage the intrinsic structure of the downstream training data. The authors call the presented method Bi tuning and evaluate it by comparing it to other variants of fine tuning. Which model and data were used? One point that I found not entirely clear in the motivation is the focus on the conjunction of *fine tuning* and *supervised contrastive learning*. If the loss is universally applicable, shouldn t it also work for training in general? I think at least *some* comparison to results from the literature should be possible or it should be carefully explained why such an overfitting to the proposed algorithm clearly did not happen. * Again in a similar fashion, Table 4 evaluates only one target data set, why only this one? how exactly is the cross validation performed here? It seems recently there are several works using fine tuning to ImageNet, which may count as large scale, too?<BRK>Despite introducing an additional head and two additional losses during fine tuning, the proposed algorithm does not introduce more hyper parameters than other alternatives, which makes it easier to be applied by other researchers. This reviewer believes that some claims are a bit overstated (e.g.claiming that the proposed approach is a "framework"), but there s no doubt that the proposed algorithm achieves excellent results and the experimental work is solid. Pros    The authors use two contrastive losses during fine tuning, which has not been deeply explored, since most works typically use only class cross entropy loss. The paper builds on the intuition that both class discriminative information and intrinsic structure of the downstream task are useful for fine tuning, and existing fine tuning approaches only use the former. to show whether the benefits transfer to other architectures. Some typos that I ve found. "Previously, we propose"  > "Previously, we proposed". "As shown in 1"  > "As shown in Table 1".
Reject. rating score: 5. rating score: 5. rating score: 6. rating score: 8. <BRK>This paper introduces a margin term for positive pair in contrastive loss, and discovered an equivalent rule between margin m, temperature \tau, and number of negatives K. The authors demonstrate that once they set the three values following the equivalent rule, then relatively smaller number of negatives can also work well for contrastive learning. Theoretically, the authors show that once you set the term contains K in the lower bound of MI to be constant, then the lower bound will be irrelevant to K. The equivalent rule is very interesting, but the theorem and Figure 1 is a bit trivial to me(see cons). The experimental results with MoCo, MoCo v2, and SimCLR shows that the equivalent rule improves the stability over different number of negatives for contrastive learning. I believe there is a mathematical issue of directly setting up $m \tau \log \frac{\alpha}{K}$ as in Eq(5). (2) The first bullet of contribution section that says "large size of negative samples is critical" is a wrong interpretation. Yes, it s in SimCLR and SimCLR v2.<BRK>This paper studies the performance of contrastive learning under a varying number of negative examples. I found this topic very interesting, and the proposed solution to be quite simple. This suggests alpha is not some number one could arbitrary set to improve the bound, and it may actually be related to K, which makes the current theoretical analysis potentially flawed or incomplete. The improvement on the best of MoCov2 result looks somewhat marginal and may not be related to the number of negatives (which contradicts the claim on why SiMo helps), as the performance of MoCov2 has saturated with increased K (Table 6). If the author’s claim were true, it should be possible to improve SimCLR s performance on small batch size (to reach the same performance of large batch size training), but this was not shown in the paper (Figure 2b). Other minor issues: 1) for Table 7, the number of training epochs is not specified. 3) the methods in Table 2 looks a bit arbitrary, several existing similarly/better performing methods (e.g.BYOL, SWAV) are not included. In MoCo, one could always easily buffer negatives with EMA the network, so there s no need to use a smaller set of negatives. I d encourage the authors to update/complete these results regardless the paper is immediately accepted by ICLR.<BRK>The authors propose Eqco which can get rid of the effect of the large negative sample size in contrastive learning. The authors propose Eqco which introduces an margin term to the InfoNCE loss. 3.The authors empirically shows that the result algorithm Simo is less sensitive to negative sample size. 4.The authors prove that large negative sample size does not affect the lower bound and the bound of the gradient. Cons:1.In figure 2, even though the accuracy is less sensitive to K. But increasing K still helps the performance. This result is against the theorem 1 and 2. Is there any other explanation? This needs a discussion3. Significance:The problem is significant and the authors made some progress on it. Originality:The paper is novel.<BRK>Previous contrastive learning methods heavily rely on a large number of negative samples. More theoretical and experimental analyses are expected to address this issue. The number of negative samples can be significantly reduced by tuning the margin term, while the performance remains more stable compared to previous contrastive learning methods. It is not clear why letting $\theta_k \theta_q$ cannot ensure the loss becoming smaller. I encourage the authors to include the numbers in the paper. This paper proposed equivalent rules which successfully reduce the number of negative samples for contrastive representation learning while maintaining the performance. Second, the computational cost is provided in the feedback. This motivation is very novel and interesting, and I believe the findings in this paper significantly contributes to the community of contrastive representation learning. \* In the proposed SiMo, all the negative examples are obtained from the current batch via the momentum encoder rather than the dictionary. Table 3 in the appendix shows that momentum update is important. Note that the momentum encoder in MoCo is used for the consistent dictionary of negative samples.
Accept (Poster). rating score: 7. rating score: 6. rating score: 6. rating score: 5. <BRK>The paper would be greatly improved by a simple 1 to 3 dimensional example of their proposed probe and how it could discover an embedded submanifold with hyperbolic structure. The work convincingly shows that their probe offers better performance, especially in UUAS, compared to Euclidean probes, which is quite interesting. #### Reasons for score:The paper presents a simple method to examine syntactic structure in contextual embedding models, as a nice extension of the seminal work on Euclidean probing, and should be directly of interest to the ICLR audience. An example in low dimensions with visuals of a hyperbolic submanifold, and a Poincaré probe interacting with it, should feature prominently.<BRK>This paper proposes probing BERT representations by projecting them into a Poincare subspace. The proposed approach is used to probe ELMO and BERT for both syntax and sentiment in comparison with the conventional Euclidean probes. The paper is also well written and the authors are rigorous when discussing their results rather than trying to oversell.<BRK>This paper proposes probes based on hyperbolic embedding spaces, and compares them to the behaviour of Euclidean probes from recent work. The main result is that these probes allow for better recovery of syntactic properties of sentences from contextualized word embeddings compared to context independent ones, when comparing them to euclidean probes. On the whole the paper is well written with great visualizations. E.g.looking at the results in table 1, how do we know that the differences in the scores when using contextualised embeddings are for a good reason? Some of the differences are small, especially in the case of the depth probe.<BRK>The probes take the form of a low dimensional projection of the word embeddings, and obtain the dependency parse and sentiment by considering the distance between the pairs of embeddings or between the embeddings and a class embedding respectively. The hyperbolic version of the probes consistently out perform the Euclidean one, which is encouraging, and the authors provide some useful visualizations of the learned projections. However, the experiments fail to account for one possibly relevant difference between their Hyperbolic and Euclidean setting. The paper is also at times difficult to follow on its own, as it relies a bit too much on cited work. Is that right?
Accept (Oral). rating score: 8. rating score: 8. rating score: 7. <BRK>This paper challenges the common belief of the inherent tradeoff between robustness and accuracy. Instead of recent methods improving accuracy while maintaining robustness, this paper proposes a geometry aware instance reweighed adversarial training (GAIRAT) method to improve robustness while maintaining accuracy. Specifically, several papers are challenging the inherent tradeoff, e.g., using more data [1], utilizing early stopped PGD [2], and incorporating dropout [3]. This paper still challenges the inherent tradeoff. a) This paper explicitly argues that the overparameterized networks that have enough model capacity in standard training suffer from the insufficiency in adversarial training (though many studies have already shown AT needs the large model). b) This paper argues that under limited model capacity, adversarial data should have unequal importance. Unequal data s treatment was explored in the traditional ML methods several years ago, but it is rare in deep learning at this moment. The experiments are comprehensive over different network structures, datasets and attack methods. Cons:1.The design of weight assignment function in Section 3.3 seems heuristic. For example, why the robust overfitting exists in standard adversarial training?<BRK>It improves the robustness while keeping the accuracy. To achieve this point, the authors find that adversarial data should have unequal importance, which naturally brings geometry award instance reweighted adversarial training (GAIRAT). The common belief is that robustness and accuracy hurt each other. However, this paper shows that the robustness can be improved while keeping accuracy. As far as I know, this point has never been explored before. First, the authors use Figure 1 to illustrate the GAIRAT, which explicitly gives larger weights on the losses of adversarial data. For example, PGD 200 has been used to verify the robustness of GAIRAT. Cons:1.In the top right panel of Figure 10, the SVHN experiments have a period of increasing robustness training error for GAIRAT. 2.Although authors show that model capacity is not enough in adversarial training, how large the DNN should be enough?<BRK>Summary:The paper focused on the sample importance in the adversarial training. The authors firstly revealed that over parameterized deep models on natural data may have insufficient model capacity for adversarial data, because the training loss is hard to zero for adversarial training. Then, the authors argued that limited capacity should be used for these important samples, that is, we should not treat samples equally important. Pros:  The finding on insufficient model capacity is very interesting. The experiments demonstrate the effectiveness of the proposed method. Cons:  Treating data differently has been investigated in related work like MART and MMA. The authors should discuss the difference from these methods. The weight function of Eq.(6) lack some intuitive explanations. Why such a formula? PGD steps are also investigated in CAT and DAT papers. The experiments should compare with some baselines considering the example difference, such as MART, MMA.
Accept (Spotlight). rating score: 8. rating score: 7. rating score: 7. rating score: 7. <BRK>This work introduces MUSIC, a framework for intrinsically motivated RL, where the intrinsic reward comes from maximizing the mutual information between the agent s state and the surrounding environment s state. I expect that the field will find both the technique to be useful as well as the general insights brought about through this work. ### ClarityThis paper is, for the most part, a model of clarity. The experiments are thorough and well organized, addressing an impressive number of (literally enumerated) questions.<BRK>Summary: This paper introduces MUSIC, a reinforcement learning approach that separates the agent state from its surrounding state and trains the agent to maximize the mutual information between the two. This implies that the agent has control over the surrounding state. This paper is well motivated and the approach is interesting. The equations are referred to before they are introduced which was unexpected.<BRK>## SummaryThis paper proposes the use of state control as intrinsic motivation. The idea then is to maximize the mutual information between the agent s internal state and the environment state. ## Drawbacks:  While I find the idea and the experiments quite interesting, I do feel there is some improvement necessary in presenting the idea and comparing it with the idea that is most related to MUSIC, which is empowerment. A practical use case for embodiment is extremely interesting and this paper champions such a use case fairly well.<BRK>The paper propose MUSIC, an RL algorithm for learning controllers in a unsupervised way. They key idea of the proposed algorithm is to separate the state of the robot like joint angles from the state of the environments such as location of an external object and optimize the mutual information between the two set of states. I think the paper introduces an interesting idea for training unsupervised skills for manipulation tasks.
Reject. rating score: 4. rating score: 5. rating score: 5. rating score: 6. rating score: 7. <BRK>The paper proposes to use a CNN to compute an initial guess for the iterative Newton Rhapson solution of a coupled PDE system used for semiconductor device simulation. The text states that the network was trained with backprop. The idea to take a data driven approach to compute the initial approximate solution in the context of the semiconductor simulation domain is interesting. Most prior work utilizing ML to accelerate PDE solution tried to generate such solutions directly.<BRK>The paper proposes a method for generating an initial guess for numerical simulations of semiconductor devices. Using CNNs to generate a better initial guess speeds up simulations by a significant constant factor. Even a non expert like myself is able to follow the paper.<BRK>The experiment on the MOSFET problem shows that, compared with the conventional numerical solver, this hybrid strategy can accelerate the simulation by more than 12 times. This paper has no contribution in machine leaning, and instead it is an application for solving PDEs. It may be more suitable to publish this paper in an engineering journal instead of a machine learning conference. However, in practice this may not be true.<BRK>The submission proposes speeding up MOSFET simulations by learning the electrostatic field by example. However, the application doesn t seem to require a convolutional network.. the input space is only 6 dimensional, so simple techniques ought to work well. You d have to show that the neural network was substantially better or cheaper than this technique.<BRK>Contribution: Authors propose using a neural network to learn an approximate solution for desired boundary conditions in order to accelerate the semiconductor device simulation.
Reject. rating score: 5. rating score: 5. rating score: 5. rating score: 8. <BRK>For instance, RetroXpert (Yan et al., NeurIPS 2020) achieves 65.6% top 1 accuracy (reaction type unknown) and 70.4% top 1 accuracy (reaction type known). This is much better than the dual model (55.2%, 67.7%). The forward predictor is trained on a mixture of original data and samples drawn from the backward predictor. The dual model shows improvement over template based and template free baselines. I believe the dual formulation can be applied to these above state of the art models (if code is available). I suspect the weak result is primarily due to the base model (e.g., transformer). I am happy to adjust my score if there are stronger results and the clarity can be improved. It will be interesting to see how this improves the forward prediction performance. One suggestion for clarity is to put perturbed / bidirectional models into appendix since they are not helpful anyway... ### Post RebuttalI would like to thank authors for their response. I think the paper needs to be improved further to get accepted. I do believe that the proposed dual learning method is promising, but empirical evaluation is still lacking. So my review score stays the same. Why SMILES randomization matters for templated based methods? 4.$X$ is a set of compounds, how do you generate a set of compounds in an order invariant manner? Why not the other way around? 2.At inference time, it is hard to sample from EBM. Therefore, authors propose to rank the candidates generated from other models (reaction templates or transformers). This really limits the performance (and applicability) of the approach.<BRK>This paper shows the reformulations of typical seq2seq retrosynthesis models and graph2graph models. Experimental results show that the dual training models perform better than conventional retrosynthesis models in template based and template free retrosynthesis frameworks. # Comments #I have two major concerns. First, I cannot clearly understand a new insight or knowledge that is brought by the EBM based re formulation of retrosynthesis models. Any probabilistic models can be described by the full joint distribution of all variables. In my understanding, the EBM is a way of modeling the joints with the log linear model plus the potential function. Please clarify what readers can learn about "connections and ... differences between models, ...understanding of model design [abstract]" from the EMB modelings. To the best of my knowledge, the current best performing retrosynthesis models are [1] and [2], and the scores of these models are higher than the reported results of the proposed dual models. Personally, I am interested in how the EMB can interpret these latest models. [2] Yan+, "RetroXpert: Decompose retrosynthesis prediction like a chemist", chemrxiv:11869692.v3, June 2020. # Evaluation points #(+) First to apply the EBM for retrosynthesis prediction models(+) Proposed a forward/backward simultaneous (dual) training( ) It is unclear what we can learn by rewriting the retrosynthesis models with EBM. ( ) Reported results do not update the State of the Art prediction accuracy of retrosynthesis models in the literature<BRK>In this paper, the authors use the framework of energy based models to describe several known approaches for ML based retrosynthesis in a unified way. Based on this analysis, a dual model is proposed which used a duality constraint as a regulariser, leading to improved performance over several baselines models, but not over SOTA (see below!). Overall, I think this paper should be accepted at ICLR after the following points have been addressed (or the authors commit to provide the additional data in the camera ready version if the time of the rebuttal phase is too short to run additional experiments). However, in the current form, the paper is not ready. Ablation Experiments:Is the duality constraint actually needed? It would be important to perform the ablation experiment where $\beta$ is set to 0. Prior work:The state of the art claim is not correct. Yan et al achieve higher performance on the same dataset https://chemrxiv.org/articles/preprint/Interpretable_Retrosynthesis_Prediction_in_Two_Steps/11869692/2 which has already been published last February! Therefore, please remove the SOTA claim from the paper, and acknowledge the Yan et al work. The usefulness of SMILES augmentation has been previously shown, please cite the previous work by Arus Pous et al https://jcheminf.biomedcentral.com/articles/10.1186/s13321 019 0393 0 The Coley et al 2017a paper that the authors cite in the introduction is a wonderful paper, however, it is not concerned with retrosynthesis. With Type, do you mean the reaction type is given?<BRK>SUMMARYThis paper uses the statistical physics inspired energy based model formalism to study the by now "canonical" problem of retrosynthesis using deep learning. The authors use an interesting variant that combines forward and backward prediction. PROS  This reviewer believes that energy based models have an elegance and connection to statistical mechanics that should be explored more in the area of machine learning. This work goes in this interesting direction. Based on the above, and as far as the reviewer is appraised, this is a unique, non derivative direction in the field and therefore deserving of consideration for acceptance. The dual model seems to be very useful given the increase in template based and non template based model performance. This could be applied to other transformer based tasks in chemistry and graph based ML  The authors compared their models to a variety of SOTA models and approaches, they also were thorough and explored both DeepSMILES and SELFIES. CONS  Some of the mathematical formalism could be moved to supplementary to allow for better discussion. MINOR FORMATTING  The authors may want to give the manuscript a pass for grammar. There are missing articles in a few sentences.
Reject. rating score: 3. rating score: 4. rating score: 5. rating score: 7. <BRK>After this remapping procedure, the authors introduce a new plot to visualize calibration. Empirical benchmarks are run on a suite of UCI datasets. My high level problems:  The remapping that the authors propose is just using a normalizing flow with a simple quantile calibration. Any model can be recalibrated using any other model here. Is there some special reason for normalizing flows here? Why not use those?<BRK>The paper proposes a normalizing flow approach for calibrated predictions in regression tasks. It is not clear what the technical contributions are. Also, most of the important details are missing. Below are some general questions or suggestions:1) While the paper s principal focus is on calibration and recalibration, it is unclear why there are claims to address aleatoric uncertainty2) The concept of recalibration is introduced in Section 2 as a classification problem. Why are calibrated CDFs uniformly distributed? 1) What is $\sigma$? How is normalizing flow extended to multivariate calibration?<BRK>This paper proposes a method for calibrating uncertainty estimates for regression models. The authors use conditional normalizing flows for this task. If the model in step 1 is perfectly calibrated, this density should be uniform, but in practice it seldom is. There has been a lot of work on calibration estimates for classification tasks, but there are less methods for regression. Using flows here is a cool idea. A possible answer mentioned in the paper is that flow based recalibration can be used to compute distribution statistics, such as the mean, while isotonic recalibration cannot compute these statistics.<BRK>The paper proposes to use normalizing flows to improve estimates of aleatoric uncertainty in regression tasks. The authors also introduce a plot that is useful for diagnosing calibration issues. I am not sure if it is worth stating in the paper. Questions for the authors:  I wonder whether using flows to recalibrate is susceptible to overfitting. The paper focuses on aleatoric uncertainty.
Accept (Poster). rating score: 9. rating score: 7. rating score: 6. rating score: 4. <BRK>A recent trend of  pruning at initialization  in neural network pruning has left me baffled. Now on to the nitty gritty of the paper. Are we looking at this as a research exercise? The only qualm I have with the results are that there are not more on different architectures... but I hardly think that s necessary to make the point.<BRK>##########################################################################Pros:+ The organization of the paper makes it easy to follow the logical progression and points being raised. With this in mind, I ll update my rating to a 7. and when compared to magnitude pruning after training (these three are invariant to shuffling and re initialization).<BRK>This comparison sheds some light on why pruning at initialization is inherently hard. ## WeaknessesThe main weaknesses of the manuscript in my opinion are as follows:1. I found the experiments in this paper to be thorough but I think the deduced conclusions are slightly off with respect to the observations in the experiments. 2.Extensive analysis shows that there is a performance gap of PaI methods compared to Pruning after Training (PaT) methods (understandably) and attempts to explore the potential reasons.<BRK>The paper is written very well, and some of the findings are interesting leaving important questions to address. In short, this work does not explain "why all methods fall short of magnitude pruning after training" in a convincing manner, and the contributions are considered not quite significant overall.
Reject. rating score: 4. rating score: 4. rating score: 4. rating score: 6. <BRK>This paper presents GLUECode, a benchmark for evaluating machine learning models of source code. There are a few typos. The authors also presented results of several baselines on the benchmark. Machine learning for source code has attracted a lot of interests in recent years. The authors also performed some GLUECode tasks and presented results for several baselines, which show that there is ample room for progress on GLUECode. Section 2.1 also dedicates to this. However, it is not clear what global context is considered and how it is incorporated by the benchmark. In a ML for SE work, researchers may use various global contexts such as UML diagrams, library/API dependency, inter procedural data/control flow, commit data, etc. It is not clear how these global context information can be satisfied by the benchmark. The authors can also describe more about the unique advantages of using the proposed benchmark. They can also process the source code using existing static analysis tools to obtain the data they need and share the data. This is particularly important for a public benchmark.<BRK>This paper seeks to present a benchmark of tasks which requires going beyond this, by requiring global (inter procedural) analyses and structured representations. This is a good objective and the community would certainly benefit from such a benchmark. For the null dereference prediction task, the paper uses a static analysis tool, Infer, to obtain labels. Infer is stated to return an execution path exhibiting null pointer deference. The results for the completion task are not made available for review. * The relative performance of the baselines on the NullToken task is surprisingly. First, the baseline methods consider only sequence based models even though the paper explicitly wants to promote structured representations. Second, the paper does not use any global reasoning in the baselines. This would help to concretely claim that at least some of the benchmark tasks require global reasoning, including method naming and method call completion as conjectured by the authors. I also have other concerns above the experiments, which I list below. I don t agree that just because a paper targets a single task, it fails to capture the multi faceted nature of code. * I like that the benchmarks come with pre processed inputs in different formats.<BRK>However, the analysis to support the dataset and the proposed tasks as a benchmark does not address some critical concerns (please see weakness section below). Summary:The paper presents a dataset of source code that allows experimenting with different representations and proposes five tasks to evaluate local and global reasoning capabilities of a source code machine learning model. This could be useful for future research in ML modeling for source code. “NullToken” task is presented as the task that benefits most from global reasoning, which is the primary contribution of this paper. However, the dataset size for NullToken task is small, which significantly reduces the usefulness of the task in evaluating the models. 3.The tasks that require global reasoning are mostly generation tasks in the benchmark.<BRK>### Summary ###The paper presents GlueCode, a new benchmark suite for evaluating source code learning models. Particularly interesting are the tasks that require fine grained reasoning about the control and data flow of programs. * The only contribution of this paper is the benchmark suite. This is not really a weakness, just a comment. * For the code completion task, it should be clear whether comments are part of the permitted/desired prediction context. In recent work, we are seeing increasing importance of natural language hints, and an explicit decision is required about this in the benchmark suite. However, my current score is calibrated to take novelty into account when comparing to other papers.
Reject. rating score: 3. rating score: 3. rating score: 5. rating score: 6. <BRK>The motivation of this paper is good but lacks extensive experiments to validate the hypothesis. 4.On page 7,  We can only compare the model trained from scratch due to time limitations involving the download of the Kinetics dataset for a full comparison and we reserve this for future work. I think larger scene related datasets e.g., Kinetics 400 or even mini kinetics should be used in the experiment.<BRK>R(2+1)D models are simpler, more effective (in many cases), and much easier to interpret than 3D CNNs. Weaknesses:  The writing of the paper could be significantly improved. The complexity analysis should include not only the number of parameters but also the FLOP counts.<BRK>However, the paper is not well written, especially in the method and experiment sections. 1 The motivation to introduce 3TConv is not well claimed. Why it should be designed in the formulation of Eq(3)? 2 Section 3.2 is not quite clear for me.<BRK>The authors can do some analysis and comparison with such networks. [1] J. Lin, C. Gan, and S. Han, "Temporal Shift Module for Efficient Video Understanding," in ICCV, 2019, pp. Maybe some background knowledge can help readers to understand the core idea of the paper.
Reject. rating score: 5. rating score: 5. rating score: 6. rating score: 7. rating score: 7. <BRK>**Summary**:In this article, the authors characterized the implicit bias of gradient descent based learning in the setting of wide single hidden layer neural networks with ReLU activation. **Strong points**:This paper provides a precise characterization of the implicit bias of (full batch) gradient descent method in training single hidden layer NN, by studying the resulting solution in the setting of large network width. The major contribution of this work, I believe, is to provide *explicit* characterization of the impact of the distribution of the random initialization on the resulting solution, which is then connected to cubic spline interpolation. **Weak points**:My first concern is that the proposed analysis seems somewhat incremental (compared to previous efforts discussed in P4) and fails to provide sufficiently novel insights on the implicit bias of gradient descent: it is good to have the explicit form as in (1) that depends on the random initialization and the second derivative, and I believe it worth more than a single paragraph of discussion and a single figure to illustrate its practical implications, e.g., how does the number of training sample $M$ come into play? * Below (9): it would be helpful to clarify the conditions under which Lee et al.(2019, Theorem H.1) hold and if they are compatible with the assumptions for instance in Theorem 1 of the present article. * Above (10): "converge to the unique global minimum": how is the uniqueness ensured here? If yes, it would be helpful to point out in which Section of the appendix is this proved. * Theorem 4: it would be helpful to clarify whether $\lambda_\max(\hat \Theta_n)$ is of order $O(1)$ with respect to $n$, that is, does $\eta < \frac{2}{n \lambda_\max(\hat \Theta_n)}$ mean the the step size should scale like $O(n^{ 1})$ in the $n \to \infty$ limit?<BRK>This paper analyzes the implicit bias of gradient descent on a wide two layer network with standard parameterization and initialization and the squared loss. It is first proved that in this setting, gradient descent on both layers is close to gradient descent on the second layer. Then the implicit bias of gradient descent on the second layer is characterized for a 1 dimensional regression problem, which can also be generalized to the high dimensional case. I think it is nice to have an explicit characterization of the minimum kernel norm solution. However, the current presentation also has many limitations:1. Theorem 1, 2 and 6 consider gradient descent on an adjusted training set. Moreover, above Theorem 6, it is said that "If u and v in the solution of (17) are small, then the solution is close to the solution of (16)." 2.The function g given by Theorem 1 and 2 are similar to the results presented in (Savarese et al., 2019) and (Ongie et al., 2020). 4.The appendices should be included.<BRK>The same for a conclusion, for the authors it may be reiterating the same ideas, but personally I found conclusions the best place where one can quickly get a flavour what has been done in the paper to assess whether it is actually worth spending time reading it in details. Regarding discussions, I appreciate that there is discussion for Theorem 1, but there are theorems and propositions stated in the formal language only which would benefit by being repeated in plain English. If they are just technical results required for the main proof, they may be moved to appendix then. There is a minor mistakes:  In the first line of Conclusion: "obtained aN explicit" The paper considers the implicit bias (i.e.why neural networks generalise well) in gradient descent learning of wide neural networks for the regression problem. The theoretical results are first stated for wide ReLU network for a 1D regression problem. Therefore, this review would be mostly a feedback on overall structure and clarity of the paper. It is packed with content without too much space to discussion of presented results to the point that the conclusion section is missing in the paper* the structure of the paper is very odd which leads to future references that appear only 3 pages afterwards (see details below)As mentioned I can t assess the main content of the paper, but with my educational guess I would recommend to reject the paper in the current version. Careful revision is required to make it a complete piece of work (add conclusion and discussion) and make it more accessible for wider machine learning audience. This also leads to these inconvenient future references. Some other suggestions/concerns:1. ReLU is introduced in the last paragraph in Introduction, but used in the previous paragraph2. Notation clash: in section 2 sigma denotes an activation function (different from ReLU) and in section 3 sigma denotes a parameter of initialisation distribution: Gaussian and uniform5. ASI is not defined<BRK>This paper presents a function space view of 2 layer ReLU neural networks and the implicit regularization associated with full batch gradient descent for various initializations of the weights. The resulting regularizer is akin to a weighted 2 norm of the second derivative, where the weighting function depends on the distribution of the initial weights. This result addresses an important open problem, provides interesting insights into the role of initialization and implicit bias associated with training, and is supported by nice illustrations. Much of the analysis seems to depend on the optimization occurring in the "kernel regime". It is unclear when this is or is not a reasonable model. Although the paper under review considers unregularized losses, there are multiple studies showing that gradient descent initialized near zero induces 2 norm regularization. So a natural thought is that the Savarese result would also extend (with some non trivial technical work) to unregularized settings with gradient descent. I would like to see a more detailed discussion of this potential discrepancy. Section C.4 does not make this clear.<BRK>Paper Summary This work analyzes the implicit bias of gradient descent for wide, 1 hidden layer networks used for regression and provides a characterization of this bias in function space. At a high level, as network width increases, gradient descent leads to a solution given by a variational problem penalizing the product of the curvature and the square of the second derivative. The proof of this proceeds as follows: (1) The solution given by gradient descent on the network can be approximated by the solution by gradient descent on a linear model. (3) The implicit bias for this problem is linked to an alternate optimization problem. I also particularly liked that the authors provided an interpretation paragraph on page 2, which among other points addressed the reason for involving a linear adjustment to the training data. Appendix C clarified a lot of the points regarding related work in the main text especially regarding comparisons between the results of this work and those of Savarese et al.2019.######################################################################3. While I found this to be an interesting and rigorous work, I feel it would be helpful if the authors could provide a bit more intuition around the technical results for the generalizations. One example of this is the need to jump back and forth between equations 15, 16, 17 of the main text in Appendix D. 3.3. I feel the authors could present some of the assumptions more clearly in the main text. 3.4.(Very Minor) I think this work could benefit from a discussion of how the implicit bias identified in this work could connect with generalization. ######################################################################4. Score and RationaleI would vote for accepting this paper. I found the result to be insightful in characterizing the inductive bias of 1 hidden layer fully connected networks used for regression. The author s present a rigorous analysis, which they complement with a number of empirical and theoretical examples.
Reject. rating score: 5. rating score: 6. rating score: 7. <BRK>Non autoregressive decoder (NAT) greatly improves translation efficiency, but often relies on iterative refinement (IR) to retain the translation performance. This paper targets at alleviating this IR bottleneck by proposing hybrid regressive translation (HRT), which combines AT and NAT in two stages. The authors always use the format of “author (year)”. Overall, this IR issue is under studied in the literature, which deserves more attention. My major concerns:  Firstly, only comparing with MP10 is not enough. However, we observe a reversed trend with MP1 in Figure 1.<BRK>This paper s main topic is the actual usability of the current status of non auto regressive translation (NAT) models. The ideas of inducing skip AT and skip MT are really unique and somewhat innovative (since, I guess, no other researchers hardly think to employ such skip decoding architecture). However, this paper does not tell readers why this observation happens. 3,The proposed method consists of many new components. This is a really nice analysis. However, the performance differences in −FT, −RPR, and −MixDistill are somewhat marginal.<BRK>This paper proposes a hybrid regressive machine translation (HRT) approach—combining autoregressive (AT) and non autoregressive (NAT) translation paradigms: it first uses an AT model to generate a “gappy” sketch (every other token in a sentence), and then applies a NAT model to fill in the gaps with a single pass. The idea of combining AT and NAT is interesting, and the paper is very clearly written.
Reject. rating score: 3. rating score: 4. rating score: 4. rating score: 5. rating score: 6. <BRK>SummaryRobustness in the multi agent setting is a nuanced concept, as (a subset) of agents can act adversarially, while the non adversarial agents can be trained to be more robust. The authors implement learning using QMIX and evaluate on SMAC. WeaknessesThroughout, the writing is not very clear. I assume the authors are getting at the normal definition of correlated eq: there is some (global) random variable that the policies are conditioned on, but the text is very unclear about this. This is where the notation again causes confusion.<BRK>This paper considers a case in the cooperative multiagent reinforcement learning where one single agent can behave adversarially. Question 2: my main concern is about the novelty of this paper. The idea seems to be almost the same especially when it is applied on QMIX. Can you discuss the difference between your method and MAVEN[1], where the author also introduces the latent variable z and uses the mutual information on z and observed trajectory.<BRK>Summary: The paper addresses the issue of robustness in cooperative multi agent RL setups, where the inclusion at test time of an agent that makes error or is even adversarial can drastically decrease performance. The issue is that this strategy is not robust to adversaries. al) are used as baselines in this paper, while they clearly should be. The writing is poor and can often be very confusing.<BRK>This paper describes a new method for making cooperative MARL algorithms more robust to teammate mistakes. The approach learns correlated equilibria that depend on a global random variable. Sometimes, such information is available (or can be available cheaply) so why not take advantage of it? There are some questions about the exact robust MARL setting that is approached in this paper. The algorithm is somewhat straightforward, but there are novel aspects and the idea is promising.<BRK>The solution relies on attempting to find a correlated equilibrium by providing agents with a common source of randomness. The experiments seem sound and prove the point. However, some details seem to be missing, potentially raising concerns for reproducibility, and the exposition could potentially be improved for better clarity. The paper specifies that the method uses a 3 dimensional uniform distribution in $[0,1]^3$ for $z$, however the design decisions behind such a choice are not discussed or experimentally backed. Finally, some details are lacking in the architecture used:* The text says that the latent code $z$ is added as input to the Q network, but doesn t specify the architectural changes that are made to made this possible.
Reject. rating score: 2. rating score: 2. rating score: 3. rating score: 3. <BRK>In fact, this is more akin to a selection procedure. Add some figures explaining the whole procedure (from hyperparameter evaluation to multi objective selection)   Evaluate against baselines   Provide some justification for the steps of the method (i.e.in 3.2)   Have someone not familiar with the work revise the paper before submitting. The whole paper needs to be rewritten with an emphasis on properly defining terms, objectives and research questions. I strongly recommend that this paper be rejected.<BRK>The final performance of the models as a result of hyper parameter selection is not provided. 3.Comparison with baselines is missing. Clarifications neededThe paper can be improved by adding the following information. Cons1.The paper appears to use exhaustive enumeration before finding Pateto frontier.<BRK>Aggregation of objectives to get scalar values has been studied at length in multi objective optimization, so it is hardly novel. Concerning optimization of hyperparameters, there are many works applicable that would be much more efficient than a crude grid search. Predictive entropy search for multi objective bayesian optimization. Since no comparison is provided with methods from the literature, that the proposed method is not novel, I thus recommend rejection. Typos:Eq.4: sise → sizeP2: the nearest Pareto front to the origin?<BRK>This paper proposes MTMC, a method that solves the pareto optimization problem for multiple tasks and multiple criteria. Con:1, No explanation on why this proposed method should work well. 2, No comparison to state of the art method (or any baseline methods). 3, Experiments are not convincing. Please include descriptions of this problem in (Akhmetzyanov & Yuzhakov, 2019) and add more realistic real world problems.
Reject. rating score: 5. rating score: 6. rating score: 7. rating score: 7. <BRK>I am not sure what would a significant conclusion one can draw from such examples. At the core of the proposed method sits an augmented memory that the agent is allowed to write on and read from at the time of decision making. This allows the agent to store some of its past to be used for future decision making. The authors raise a few training, stability, and limitation issues in prior work. They are useful in understanding the paper. The idea of using augmented memory is exciting and fundamental and definitely worth expanding. However, I found a few issues with the presentation and contribution of this paper that I would be happy to share. 1) Second line of abstract:"Learn  ing memoryless policies is efficient and optimal in fully observable environments." I guess the authors mean the agent using q learning or 5 step actor critic were not able to learn how to use ... . I will discuss with other reviewers and AC, and update my evaluation further. In the analysis I found in the appendix, it seems the authors approach finite ones. I checked the referenced paper,  Jaakkola et al.1995, there it was also not clear what is q.<BRK>This work proposes a lightweight approach to control memory in POMDPs. It is an alternative to heavier overhead approaches such as recurrent networks or memory augmented networks (Oh et al.2016).The main contribution is to resurrect the old idea of simple lightweight memories and address what made them fail in complex domains by developing novel memory systems. The novel type of memory developed is the $Ok$ memory, wherein the agent is given a choice whether to push the most recent frame into the memory buffer or not. The commonly used $Kk$ memories in contrast push the most recent state into memory by default. + The paper develops the theory of memory augmented POMDPs from basic principles and formalizes the learning problem. But the system remains limited by having to store a fixed amount of full observations, as compared to more complex memory systems that can chose *what* to write along with when to write (eg.Neural map by Parisotto and Salakhutdinov 2017). Therefore, while the theory and evaluation are extensive, the memory system itself is limited and inefficient for environments where certain features of the state may need to be extracted and tracked for a long duration (not just a few frames).<BRK>This paper focuses on reinforcement learning in partially observable environments, and revisits the approach that consists of extending the agent with an external memory. The ideas proposed in the paper are simple and elegant. It seems surprising that this idea has not yet been proposed, but the paper shows great effort from the authors to look for related work, and discuss it. Clarity: my main complain with this paper is that it takes a very long time for the reader to see what the main contribution of the paper will be. I thank the authors for having followed my advice about this point.<BRK>The paper extends the agent actions with an ability to write to an external memory. The paper explains the difficulties with bootstrapping and policy improvement in POMDPs. The paper proposes simple memories for storing a buffer of k observations. The agent has the ability to push or not to push the current observation to the buffer. The paper is nicely written and clear. I welcome that the paper tries to use RL to control the external memory. This topic deserves more research. 2) It would be nice to mention the limitations of the different memories. You can also discuss the need to use long n step returns, e.g., n 2048. 3) The proposed Ok, OAk memories are not better at all tasks than LSTM. Consider using these memories *together* with LSTM.
Reject. rating score: 5. rating score: 5. rating score: 6. rating score: 7. <BRK>Specifically, the proposed method replaces a dense linear layer by a composition of three layers, which are smaller and can be computed faster by FJLT. In all the experiments, the final hidden layer is replaced by the proposed architecture. It is argued that by replacing a dense layer with a truncated butterfly network and a dense layer, the number of parameters is reduced from $kn$ to $k\ell+O(n\log\ell)$. I expect the paper would report the parameter numbers in the experiments. Theorem 1 shows that replacing a dense layer by two truncated butterfly networks and a dense layer will not be too different from the original network.<BRK>In general, the paper follows the idea of sketching to design new architectures that can reduce the number of trainable parameters. The paper’s aim is to establish that linear layers can be replaced by butterfly networks and uses three different experiments to show this.<BRK>This paper proposes to impose a particular sparsity structure (butterfly network) to replace dense connected layers in deep neural networks. It is motivated by the theoretical results involving the Fast Johnson Lindenstrauss Transform (FJLT). Is this last layer the one that is replaced? In this architecture, the sparsity pattern is fixed, and it seems to work very well for the dataset used in the paper. It would be good to show some variability of results (error bars).<BRK>The paper provides an interesting and novel use of butterfly factoziations in encoder decoder networks. I think that low matrix approximation experimentation (Sec 7) can be more thorough. The parameters are chosen so as to keep the number of weights in the (replaced) encoder near linear in the input dimension. ######I vote for accepting the paper.
Reject. rating score: 4. rating score: 4. rating score: 5. rating score: 6. <BRK>The authors propose new and, in particular, parameterized aggregation functions for GNNs in order to especially support the construction of deeper GNNs. However, in my opinion, the paper is missing the theoretical justification of its proposals and I have concerns about the results (details below). Hence, I do not vote for acceptance. ( ) Approach: The authors define several parameterized aggregation functions but the motivation is left unclear. This is especially strange given that the OGB code supports several folds. * Table 2 is supposed to show the comparison with SOTA, but I claim that it is lacking. The experimental results alone do not offer a theoretical justification. The paper does not have to contain the latter but then the evaluation would have to be exhaustive. Similarly, related approaches have to be considered adequately to have an appropriate comparison to SOTA.<BRK>This paper proposes a general aggregation function which summarizes sum, max, and softmax operations etc. I list the pros and cons as following. Also, this paper proposes an interesting perspective to study this problem. 2.This paper conducts extensive experiments and analysis of different aggregation functions and their combinations. It would be great if authors can state the novelty of this paper compared to DeepGCNs. Also, what makes difference between the proposed aggregation function and softmax. 3.The comparison to other methods (GraphSAGE and GCN) is not fair because they are shallow models while the model used in this paper is quite deep. I d like to see more fair comparisons with the same model size. 4.This paper is based on DeepGCNs.<BRK>——Pros:	A straightforward and elegant solution 	The Paper is well written and is easy to follow. Since the OGB benchmark is new and the reported GNN models on the OGB leaderboard were only run for 3 layers, it is crucial to analyze all the variants discussed here in detail to appreciate the performance gain achieved by the proposed generalized aggregation function. There is both a drop and gain in performance with the adoption of the generalized aggregation functions. Hence, it is not clear whether both the generalized functions aid in improving performance without seeing the results on other datasets for all the four or two versions of the learned generalized aggregation functions (as in Table 1.d) 	(iv) Need a statistical significance test report. ——Questions during rebuttal:	Kindly address the concerns raised above. I agree that doing a detailed ablation study on a large dataset is expensive. In which case experiments on either synthetic or other smaller real world datasets would be helpful.<BRK>#####Summary#####This work proposes a generalized aggregation function for graph neural networks. The experimental results on OGB is impressive and can demonstrate the effectiveness of the proposed approaches. (2) The included experiments are comprehensive and can demonstrate the effectiveness of the proposed method. (3) This paper is well organized and easy to follow. A comparison of efficiency between proposed aggregation functions and existing simple functions should be included. However, an ablation study to show the respective improvement brought by the increasing depth and proposed aggregation should be considered. (2) It would be better to provide some theoretical support if possible.
Reject. rating score: 4. rating score: 4. rating score: 5. rating score: 7. <BRK>This paper defines a new "distance to the decision boundary" and empirically evaluates the corresponding active learning algorithm. Unfortunately, the motivation for this new distance to the decision boundary is lacking. Clarity:   There are a lot of quantities, functions, and assumptions floating around and it is not clear how they are motivated or related. In Theorem 1: it appears that $a(x,w)$ is undefined in the proof, or at least is defined after it is used. In Theorem 1: the statement is not self containedCorrectness:   In Theorem 1: it appears that if $x$ is on the decision boundary (i.e.$x^T \hat{w}_t   0$) then the first case does not hold. Perhaps I didn t explain my concern about the motivation. In other words, it seems that you are extrapolating from the SVM case to the neural network case and it s not clear which of several equivalent things will work in the neural network case. So I really don t find the motivation or theory convincing.<BRK>For example, I don t understand why Eq.(2) is a kind distance measure between a sample x and a hypothesis. Assumption 1 is states that $\rho(h,h )$ is an increasing function of $\|w w \|^2$. Similar for the case of Assumption 2. Please either change the statement of the assumptions or change the experiments in Figure 1 & 2. But in any case, we need more extensive experiments to make any more interpretation. The authors hoped to connect their algorithm with some theory, but the assumptions are not stated in a precise way. Overall, the proposed algorithm seems to be a heuristic algorithm without precise theoretical justification, and the experiments are not extensive in some aspects.<BRK>The experimental results are quite promising, with the proposed method outperforming many existing methods for active learning on neural networks. There are several cases where quantities are used and discussed before being defined (Assumption 2 in the start of section 2) or before being rigorously defined ($f_m^{(x)}$ in Assumption 2 and section 3), or without being defined at all ($\mathcal{P}_t$ in algorithm 1). In Theorem 1 it should be more clear that the theorem only applies to the case of a linear classifier described above it, as right now a brief read through the paper suggests it applies for the neural networks which are used throughout the rest of the paper. Theorem 2 however is a very strong statement, and there is not sufficient justification for this strong a claim. Additionally it is unclear what the authors mean by "When the version space exists...". Overall I think this method is exciting, but would benefit from additional explanation.<BRK>The paper proposes a (theoretical) sample distance to the decision boundary that relies on the least probable disagreement region (LPDR) that still contains the sample. Consequently, an iterative active learning algorithm is proposed which adapts the variance of the noise term in order to select the predefined number of samples. I think the paper is relevant for the ICLR and (active) learning community in general. However, it should be noted that the experiments were only conducted on image datasets and corresponding CNN architectures. However, the authors do not provide source code etc. in order to easily reproduce the results, which I would highly encourage. (2) Please revise the second last sentence on page 3.
Accept (Poster). rating score: 9. rating score: 7. rating score: 5. rating score: 4. <BRK>The paper develops techniques, systems and models to leverage conditional computation, greatly scaling model size while only requiring sublinear computation with respect to model size. As a result, a sparsely gated MoE model with 600B parameters for NMT tasks has been trained efficiently (4 days on 2048TPUs) with new state of art accuracy. Merits of the paper:  Provide an effective strategy to address the expensive computational cost of training giant models  Offer a comprehensive solution covering techniques, systems and models   Well designed APIs and implementations to express a wide range of parallel computation patterns with  minimal changes to the existing mode code   New state of art models have been trained using the strategy and systems, as a great demonstration on the effectiveness and efficiency of the approach. Places to improve   It is helpful to further clarify the difference of this work comparing with the prior work Shazeer et al.(2017) beside transformer vs RNN. For example, are the features like load balancing, efficiency at scale, auxiliary loss, random routing new contributions of this work, or similar as the prior work, or with certain incremental improvements? It would also be helpful for readers to understand, which techniques are generic and which are model type specific. Both this work and the prior work Shazeer et al.(2017) apply conditional computation on neural machine translation tasks. It would be helpful to comment on the generality and effectiveness of the solution on other types of tasks.<BRK>## SummaryThis paper addresses the training efficiency issue of large scale models. The authors proposed to use GShard API to implement Transformer model with Sparsely Gated Mixture of Experts, allowing sublinear scaling of the computation cost. Experimental results on a multilingual machine translation task show that the proposed method can use computational resources efficiently. ## Strong points* The training efficiency of giant models is one of the significant issues in the field and will attract practitioners  attention. * Experiments using more than one trillion model weights are not ready and not shown in the paper. * Before explaining its efficient implementation on page 3, it would be nice to explain why a sequential implementation of the gating function is required. I guess the reason is that the assignment of an expert depends on the assignment of other experts. * It is better to have a reference to A3.3 in the main body.<BRK>The proposed method is tested on a large multilingual machine translation dataset and shows performance gains over models trained on a single language pair and a model trained without MoE. Reasons for score: Although the improvement of translation performance presented in the paper is significant, my overall feeling is that the improvement over [1] is incremental and not clearly justified. Pros: 1.The improvements in translation accuracy are significant and show the effectiveness of MoE for Transformers. Cons: 1.The novelty of the paper, especially compared with [1]. In [1], MoE is implemented as a position wise feed forward layer, which can be directly applied to Transformers with no modification. However, this improvement is not thoroughly evaluated in the paper. I would expect an ablation study to measure the impact on the speed and accuracy of this improvement. 3.The paper proposed a library and a set of APIs for the XLA compiler to make the implementation easy. How does this work differ from these works, especially [3]? However, all of the evaluations are not on any standard MT test sets (e.g.test sets of WMT). I understand it’s impossible to evaluate the performance of all 100 languages with WMT, but including the results of the standard benchmarks will make the performance gain claimed in the paper more intuitive and stronger. 5.For models of different sizes, it would be better to include a comparison of models training with a similar compute budget (e.g.training the models with the same TPU days) to better show the benefit of large models. Algorithm 2: Please define H and M.References:[1] Shazeer, Noam, et al."Outrageously large neural networks: The sparsely gated mixture of experts layer." arXiv preprint arXiv:1701.06538 (2017). "Optimizing data intensive computations in existing libraries with split annotations."<BRK>Claims: Architecture based on transformer + mixture of experts Number of parameters can scale 5 10x larger than transformer only architectures, while simultaneously achieving 10x shorter training time Increased number of parameters allows improvements in BLEU score  MoE architecture allows sub linear computational scaling with increasing parameters, even when mixed with transformer layers Argues that conditional computation, via the GShard module, is a good way to tackle computation cost and complexity of parallel programmingPros: 600 billion parameters is   as far as I am aware   a new record for the size of transformer related models. Decreasing computational cost and programming complexity is an important goal   however, I think this should be studied not just at the rarefied high end of extremely large models and datasets, but on more typical scales that would be accessible to more research teams. This will greatly help with reproducibility. However, I have concerns that the huge size of the sole dataset/task unfortunately work in the opposite direction, hurting reproducibilityCons: The paper only studies one task, on one data set, at one (very large) scale, and only compares to one baseline (standard transformer architecture). Although the paper deserves interest due to its potentially record setting model size and scale, due to the lack of variety in tasks, data and scale, it comes across as a singular engineering effort rather than a study into the benefits of conditional computation. The theme of conditional computation is interesting, but the MoE design isn t too much different from Shazeer et al., 2017. Beyond the 600b parameter scale, it seems the key original contribution is to fuse transformers with MOE, but I did have a hard time understanding what other contributions were significant and novel. If, on top of that, the authors studied a second task (other than multilingual translation), I think the paper would be very solid.
Reject. rating score: 3. rating score: 4. rating score: 8. <BRK>"It is clear that images in S are visually much harder. The point, as I understand it, is not to select the best model, but to find the most "controversial" image. Detailed comments  "First, segmentation benchmarks require pixel level dense annotation", I do not believe this is necessarily true, and there is little need to state this. There are also examples of benchmarks where the groundtruth consists of computer segmentations corrected by humans. "Second, it is much harder for segmentation data to be class balanced in the pixel level, making highly skewed class distributions notoriously common for this particular task", "Besides, the “universal” background class (often set to cover the distracting or uninteresting classes (Everingham et al., 2010)) adds additional complicacy to image segmentation (Mostajabi et al., 2015). To evaluate how this approach generalizes to the real world, an independent dataset would have to be used.<BRK>Summary:In this work, the authors seek to leverage external sources of data to improve the generalization of segmentation models. In particular, they seek to identify images which generate discordance among models, hypothesizing that they would be well suited to improve model performance. * The authors identify that scaling human annotation, in particular for image segmentation, can be cost prohibitive and propose a method to optimize this process. If successful, such a method could have significant implications for industries where the cost of annotation is high (e.g., healthcare where highly paid experts are required). * There are a few magic constants throughout the paper. Nits:* The comment on maximum test set size on page 1 is a bit strong. * There is a typo on page 2: "not be[en] spotted beforehand"* It would be nice to quantify the computational cost of constructing M in each iteration. * There is a typo on page 4:(ARC)  > (ACR)* The F in "failure" is erroneously capitalized in the first paragraph of section 3.2* You may wish to consider citing the field of computer assisted annotation as relevant work.<BRK>This work used a variety of existing segmentation algorithms to discover most "controversial" samples from massive online unlabeled images. The paper addressed an important and somewhat overlooked problem for segmentation and deep learning in the general. Leveraging those counterexamples to improve the segmentation models  generalization performance on unseen images seems to be novel in this field. This approach is also an instance of the basic active learning paradigm that iterates between spotting hard examples, labeling them, and tuning the model. Besides, this paper uses different deep networks trained as competing models. What if using learning based but non deep segmentation models to compete?
Reject. rating score: 3. rating score: 3. rating score: 4. rating score: 7. <BRK>: Ecological inference through distribution regression,” in Proceedings of the 21st ACM SIGKDD International Conference on Knowledge Discovery and Data Mining. ACM, 2015, pp. [R4] H. C. L. Law, D. Sejdinovic, E. Cameron, T. C. D. Lucas, S. Flaxman, K. Battle, and K. Fukumizu. In NeurIPS, pages 6084–6094, 2018. Accordingly, my opinion is that this paper is not ready for publication. Important references are missing. Also, ecological inference relates to other problems discussed in the machine learning community.<BRK>Here are some of them:    Sec 3: "This gives us a useful test case for examining not correlations in voting patterns between races." The paper evaluates the proposed model using real Maryland 2018 midterm election data and produces interesting insights#### Weaknesses  The paper is not easy to follow. In addition, there are various baselines/methods included in the "results" section but it s unclear what they are in details.<BRK>This paper is a solid attempt to combine these disparate fields. The datasets it uses for its experiments are interesting and should be of value to the ML community. On a more minor note, there were quite a few typos throughout. This is depicted in Table 1 but not discussed in the paper. Are they on the precinct level?<BRK>This paper proposes a deep learning framework for approximating ecological inference for estimating voting propensities based on demographic aggregates. It would be nice to see the performance of these methods on some synthetic data as well and at least one comparison to one of the current state of the art methods on an aggregate version of the data would be useful. Overall, this paper is interesting and presents an approximation that is likely to be useful in practice for real world problems and given the space constraints appears to present sufficient work to be publishable.
Reject. rating score: 3. rating score: 3. rating score: 4. rating score: 4. <BRK>Summary: The paper presents a method that attacks existing out of distribution (OOD) detection methods. Main motivation of the paper is that the size of the latent representation is much smaller than the input images which results mapping both OOD and in distribution images to the same place in the latent space and diminishing OOD detection performance. The paper contains experiments on multiple dataset to demonstrate that the proposed method obtains a latent representation similar to the representation of an in distribution image. I would not consider this as a contribution since there is no experimental evaluation showing the OOD detection performance of the idea. Also, since this contribution is mentioned in the abstract, I would expect seeing its description and the results in the main paper rather than the Appendix. I liked the motivating example in Figure 1, but the contribution can be given in a more clear way. The contribution of the noise is not very clear to me and I think showing results with and without the noise would be very useful to demonstrate how the noise helps to avoid local minima. Also, it would be interesting to show how this loss evolves during the optimization. 7   I think that experimental evaluations are not comprehensive enough, 7.1. [ref1] Erdil et al., Unsupervised out of distribution detection using kernel density estimation[ref2] Lee et al., A Simple Unified Framework for Detecting Out of Distribution Samples and Adversarial Attacks[ref3] Sastry et al., Detecting Out of Distribution Examples with Gram Matrices7.2. Please see the references above. I wonder why the datasets choice are different than the standard benchmarks?<BRK>Summary:The paper defines an out of distribution attack, a process which drives an out of distribution (OOD) input to have the same latent representation to an inlier. The paper also analyzes that an encoder is inevitably vulnerable to out of distribution attack when its latent dimensionality is smaller than the dimensionality of the input. Decision:RejectStrength:The paper addresses an important vulnerability of classifier based OOD detection. As classifier based method is one of the currently dominating approaches for OOD detection, investigating its weakness is a significant contribution to the research community. Weakness:The proposed attack algorithm is significantly similar to the previously known adversarial attack algorithms and therefore seems trivial. The main quantitative result, Table 1, is not very convincing. I suggest the authors  provide AUC scores computed before and after OOD attack, so that the difference clearly shows that the proposed attack causes a decrease in OOD detection performance. The method should include at least [1,2,3] to show the effectiveness of the proposed attack. The organization of the paper needs to be improved. In the last sentence of the abstract, "a simple theoretical solution" is mentioned but is only addressed in Appendix. If it is a contribution that is important enough to be mentioned in the abstract, it should be covered in depth in the main manuscript instead of Appendix. Minor comments:  The visibility of figures are poor. "Enhancing the reliability of out of distribution image detection in neural networks."<BRK>**UPDATE**I acknowledge that I have read the author responses as well as the other reviews. **Additional feedback and ideas for improvement**I think the OOD detection problem, both from an attack and defense perspective is relevant and of great interest to the community, which is why I encourage the authors to build upon and extend the current manuscript. Overall, I appreciate the clarifications and added experiments given by the authors. 25.Section 2: ‘Usually, *the* training set is only a subset’26. This could be insightful to further improve OOD robustness and see pros/cons of existing approaches. #####**Summary**This paper presents an algorithm for out of distribution (OOD) attacks on neural networks. Add missing related work [5]. Finally, a theoretical sketch for a solution of the problem is described. 3.‘[...], the above mentioned classification based OOD detection is theoretically almost ineffective [...]’; ‘[...], then it is highly possible that the entire latent space is crawling with the shadows of OOD samples.’; ‘[...], there are a huge number of “holes” in the space, [...]’ Rather avoid such vague formulations. **Pros**+ The paper presents a simple OOD attack algorithm and demonstrates empirically that OOD samples can be perturbed such that they map to the embedding of some arbitrary in distribution example. [5] A. Meinke and M. Hein. Add a label indicating the network. (see minor comments below)**Recommendation**I think the current paper is ok but not good enough (score: 4) due to (i) low novelty, (ii) a limited experimental evaluation, and (iii) solution claims that are left to be validated. In particular, the OOD attack has a greater degree of freedom since any arbitrary OOD input can be used as a starting point for perturbation (pure noise is also OOD, as remarked in the paper), i.e.there is no similarity constraint on the input as there is for adversarial attacks. It would be interesting to see how these approaches perform and compare, which could be insightful for improving OOD robustness. Currently, the main paper is only phenomenological, i.e.demonstrates that OOD attacks are an open issue, but the description of a possible defense at the end of the paper and in the appendix makes only a solution claim which is left to be validated.<BRK>Summary of paper: this work shows that adversarial perturbations can make any OOD image, map to the same latent code as an in distribution   creating an attack on confidence based or flow based ODD detection methods. Results are shown on a few datasets with some attempts at evaluation. Novelty: Unfortunately, I don t think there is much new here. Adversarial attacks are of course well known   I am not sure that attacks on intermediate latent codes present novelty either. do the perturbed images look realistic   if I understood Fig.12, they don t   but the caption there is not clear. Clarity   the paper is not particularly clearly written   although the idea is simple enough. Overall: ultimately, this is conceptually repeating the same thing as any other adversarial examples work, perturbations can make a network confident that any image has the label of another image   and this obviously would overcome confidence based OOD detection methods. I therefore do not see a strong contribution by this work. As there is also very limited methodological novelty, I do not think it should be accepted. ###############################################################################I understand the distinction the authors are trying to draw between adversarial examples for anomaly detection and fooling OOD to think that images are in distribution where in fact they are OOD. I still don t think that technically or conceptually, there is much difference. The authors presented many fresh results during the rebuttal (which might have been better presented just as a table in the manuscript, rather than on this thread). The experiments can form a part of a resubmission of this work, that will incorporate the extensive comments presented by the current reviews. "Adversarial deep learning against intrusion detection classifiers."
Reject. rating score: 5. rating score: 5. rating score: 6. rating score: 7. <BRK>I feel the ideas of the proposed SPACE algorithm is good, but I have some concerns for the current version. In Lemma 4.1, only a big O value for the update of $h^{k+1}_D$ is provided and no description on what the big O hides. I think the actual steps should be stated in the main text with discussion on the effect of the approximations. After reading the authors  response, I would like to thank the authors to clarify some of the my questions. The value of the key bound in Lemma 4.1 is unknown and it s not clear about the effect of the described empirical trick.<BRK>SummaryThe authors propose SPACE, an RL algorithm for learning a policy that maximizes reward while satisfying given constraints in a setting where a baseline policy is provided. They design a three step update rule for learning such a policy and provide a finite sample analysis of the resulting method in a simplified setting. Pros  The paper is very well written and easy to follow. The experimental evaluation is performed in complex domains. The results are significant and well described. Cons  The paper might lack some novelty. However, at the present time, I am not sure about the novelty wrt Yang et al.2020, so I am quite borderline.<BRK>Summary of review:A clear problem statement & technical approach for an important problem, supported by positive empirical results, and some theoretical analysis. Strengths: 	The paper’s overall approach is easy to understand, and decomposes in a few conceptually simple steps. I appreciated that the experiments considered tasks with both safety constraints and fairness constraints. The paper includes a good range of baseline comparisons. I wasn’t sure what to make of this result, what it was significant or useful. Key experimental details are missing.<BRK>update after rebuttal: the authors answered all my questions to my satisfaction. "Constrained upper confidence reinforcement learning." The authors provide both the an theoretical analysis and a number of experiments. The paper states that other approaches using a baseline require it to be safe. I do not think that is true, see the detailed comment. Safe exploration for optimization with Gaussian processes. Safe exploration in finite Markov decision processes with Gaussian processes.
Reject. rating score: 3. rating score: 3. rating score: 3. rating score: 4. <BRK>Here are my main concerns:1. The authors spend lots of content on the related work. Is it possible to conduct all of the baselines for all datasets? In summary, the paper show that a GCN network is a good prior for the Matrix completion problem. However,  the technical contributions of this paper is limited and the overall presentation can be further improved.<BRK>In section 1 Introduction:“In this work, we present a simplified …” sentence is too long. After “a classical end to end graph convolutional neural network and is/was inspired…”, is/was is missing before inspired. 2.The structure of the paper is not completely standard. The appendix is usually for detailed descriptions of methods. Should be merged with other explanations of this section.<BRK>The technical depth of this work may have large room to improve. The embeddings are constructed from graph Laplacians, and the mapping (regression model) is a particularly constructed neural network. Clarity and Soundness:   Most of the effort was invested onto constructing the graph CNN that is claimed tailored for non Euclidean matrices.<BRK>It seems to be a combination of prior work: Multi graph convolution combined with Dirichlet energy on row and column graph laplacian where the input rating matrix is corrupted with noise. Also, considering some of the work mentioned below, SOTA results is an overclaim. *translate" its network  so it would *feet* for Matrix completionb) State of the art on Matrix completion:While this work claims state of the art results, it is missing some recent work (Iclr 20) that have achieved better state of the art results than reported in this paper.
Accept (Oral). rating score: 9. rating score: 8. rating score: 8. <BRK>The paper presents a method for generating synthetic datasets from the large realworld datasets. The method is built on the idea that the gradients of the network being trained on the real images should be similar to gradients, which were obtained by the training on the synthetic images. The method is validated on MNIST, SVHN, FashionMNIST and CIFAR 10 on several different architectures: MLP, AlexNet, VGG like and ResNet architectures. The method is well motivated, shows good (sota) results and also often (for MNIST, SVHN, FashionMNIST)  produces human recognizable examples, although there is no term/regularization directly encouraging this. The rebuttal didn t raised any concerns and made the paper even stronger, thus I am keeping my score.<BRK>Summary: This paper tackles the challenging dataset condensation problem. The proposed method tackles the problem by gradient matching. Strength:+ Comparing to existing approaches, the proposed method is efficient and effective, achieving the state of the art performance+ The authors use the synthetic dataset for two other downstream tasks and achieve promising results+ The authors conduct extensive experiments to study and analyze the proposed method+ The synthetic dataset trained on one architecture can be also used to train any other networks with different architectures, which makes to method more applicable Weakness:  In Section 2.3 “Gradient matching loss”, the authors claim that the proposed distance metric is “a better distance”. I wonder if this method can be used for other loss functions as well. Can the authors comment on these? After reading other reviews  comments and the rebuttal, I think this paper is in a good shape now.<BRK>##########################################################################Summary: The paper proposes a novel dataset condensation technique that generates synthetic samples by matching model gradients with those obtained on the original input dataset. This technique is investigated empirically on several smaller datasets like MNIST, SVHN and CIFAR10. #########################################################################Post rebuttal. The paper is well written.
Accept (Poster). rating score: 7. rating score: 6. rating score: 6. rating score: 4. <BRK>The intuitive argument is supported by a good experimental analysis and a theoretical argument. Pros:  The method is trivial to implement and thus very general. I was pleased to see an application in RL, which is often shunned by most works in CL. Nevertheless, the CL community may be ignorant to the particular effectiveness of the technique in this setting, which is why this paper makes for a suitable publication.<BRK>The paper has claimed several times that one can improve stability and plasticity simultaneously. That s why it s usually referred to as the stability plasticity dilemma. arXiv preprint arXiv:2006.06958 (2020). Since it s been around for a short while the authors were probably not aware of this paper but it basically connects widness of the local minima to stability of continual training. It s worth elaborating on the differences from and similarities to their approach. The paper is generally well written. But, some notations are a little unclear and can be improved. It s unclear whether this "regularization" is the proposed CPR (using KL) or the normal EWC like regularization (like last term in equation 3)?<BRK>many citations are in the wrong format (the authors should check how to correctly use the cite, citep, citet (I am not sure))Final recommendation: The proposed method is very simple, but could be widely applied in CL losses and the paper could therefore have an impact. Other methods that also have this effect could be discussed in more detail. the simplicity of the method could mean that will be widely adopted in CL losses.<BRK>However, the simple method in this paper lacks sufficient explanation and theoretical justification. The experiments are also less convincing if without comparison to other types of continual learning methods. To close this gap, the authors need to prove this very specifically. Since the proposed regularization can easily work within these methods, it is more convincing to compare their performance before and after adding the regularization. Its analysis is in the model parameter space instead of the output space and is rigorous. Does this indicate that the stability is achieved with the price of significantly degraded loss?
Accept (Poster). rating score: 8. rating score: 7. rating score: 7. rating score: 5. <BRK>This paper presents a new pooling layer that is based on the Lifting Scheme from signal processing. It motivates this approach with the desire for reversible pooling functions for certain tasks. Figure 7 is not very clear. *I recommend to accept*.<BRK>This paper proposes a pooling method for CNNs using a Lifting scheme. The proposed LiftPool consists of LiftDownPool and LiftUpPool. The LiftDownPool decomposes a feature map into four sub bands (LL/LH/HL/HH). LiftPool is applied to each of the local pooling of each layer of CNNs. There is a work that uses Lifting wavelet for CNNs.<BRK>This paper proposes a simple downscaling / upscaling method LiftPool, inspired by Lifting Scheme from signal processing. This is specially important given the extra params and computes LiftPool brings. The results on DeeplabV3plus looks good, but it is more from transferability (pretrained on image classification using liftpool). Figure 3: it might be beneficial to show the original high res feature map together with baseline pooling results (e.g.max / avg pooled) to demonstrate information preserved by LiftPool.<BRK>This paper introduces a learnable pooling strategy to preserve details when down sampling feature maps. The proposed LiftPool method is derived from the classical lifting scheme of signal processing, which contains a LiftDownPool to decompose a feature map into various down sampled sub bands in the down sampling process, and a LiftUpPool to merge these sub bands in the up sampling process.
Reject. rating score: 4. rating score: 4. rating score: 5. rating score: 6. <BRK>**Summary**: This paper studies the problem of visual imitation learning: given a video of an expert demonstration, take actions to reproduce that same behavior. The proposed method learns a distance metric on videos and uses that distance metric as a reward function for RL. Experiments show that this method does recover reasonable behaviors across a range of simulated robotic tasks. Compared with prior methods, the main contribution of this work is that the distance metric is parametrized and trained as a siamese network. Instead, just use "GAIL without access to actions." At a high level the main difference is that this paper uses a siamese network to parametrize the discriminator, and employs a few types of data augmentation. If the experiments had shown that this architectural choice made a significant improvement in performance, and was useful inside a range of imitation learning frameworks (e.g., GAIL, AIRL, and Value Duce), then I think it d represent a significant contribution. **Experiments:*** I found it challenging to assess the experimental results without any quantitative comparisons with baselines (besides TCN). The videos for other domains seemed to have been rendered incorrectly. * "Additional works..., none of these ..."  > "While additional works ..., none of these ..." **Update after author response**: Thanks to the authors for answering my questions and for incorporating feedback into the paper. I think this is a really neat observation, and potentially quite important; the experiments definitely support this hypothesis. **Questions for discussion**:* Precisely, what are the differences between this method and GAIL?<BRK>### Paper summaryThis paper proposes using a recurrent siamese network to learn distance functions between observed behaviours and agent behaviours, and the use of this distance for reinforcement learning. Experiments on a set of simulated walking tasks show that this approach works reasonably well. ### Pros  The proposed approach performs well when compared against GAILfO and learns better distance functions than a TCN. The results look good, and I really like the zombie walk. ### Cons  The ideas in this work are very close to T Rex [Brown et al.19a](https://arxiv.org/abs/1904.06387) and D Rex [Brown et al.19b](https://arxiv.org/pdf/1907.03976.pdf) which uses a pairwise trajectory ranking formulation, which essentially learns the distance between trajectories, much like a siamese network. The proposed approach relies on a number of heuristics and connected neural components and seems to perform well in walking tasks, but makes no real theoretical contribution. ### General comments and queries  The paper seems to have been quite rushed and in need of some refinement when it comes to presentation. Eg.in the introduction it is claimed that *The fundamental problem of imitation learning is how to align a demonstration in space and time with the agent’s own state.<BRK>SUMMARY:The authors propose an extension of recent imitation from observation techniques called VIRL that explicitly incorporates Siamese network and LSTM encodings in order to attempt to better overcome some of the challenges of imitation learning from visual observations. They evaluate their algorithm in several simulation domains and find reasonable results. (S2) The paper proposes an interesting new technique that, in some ways, seems to perform very well in experiments. Learning a distance metric between behaviors would seem to be applicable for learning from _many_ demonstrations   in what way is the proposed approach specialized for single shot learning? (W2) The presentation of the experimental results is, at times, incomplete and confusing. RECOMMENDATION STATEMENT:While the paper considers and important problem and may indeed describe a very promising new approach, the current experiments section is in need of a major rework before the story is clear. MINOR COMMENTS:	(MC1) The authors need to state in the abstract that their experiments are done on _simulated_ agents. (MC2) The authors state in the introduction that "the fundamental problem of imitation learning is how to align a demonstration in space and time with the agent s own space." The figures need to be reformatted to address this.<BRK>Overall, I think that there are some rough edges to the paper which could be improved, but the contribution of the paper is enough to warrant publication. One problem is that the method is poorly explained in some places. ;  Where states and actions are discrete. The paper claims that the method can train an agent to do imitations from noisy visual data from single demonstrations, but this is not clearly shown by the experiments. On the more challenging domains, the videos show that the agent imitates the expert for a very short timespan before diverging. However, visual imitation is very difficult and the proposed method does perform better than the other baselines. Pros:   The method is well designed and is a natural extension on existing work (TCNs, Siamese nets, GAIfO). The results show that the approach works well on a number of different continuous control environments  The comparisons to other published methods, the baselines and ablations, are well chosen. The authors have not adequately explained whether the model is actually capable of 1 shot imitation.
Accept (Oral). rating score: 9. rating score: 8. rating score: 8. rating score: 7. <BRK>However,  it suffers from slow convergence and limited feature spatial resolution. This paper proposes deformable attention, which attends to a small set of sampling locations rather than all the locations in the original DETR. The paper is well written and obtains very impressive results.<BRK>The resulting model is very interesting in terms of convergence and complexity compared to the original DETR. Combined with eq.2, it will give a better understanding of the method. This paper proposes a simple strategy for attention modules that also reduce complexity.<BRK>Summary:This paper proposes Deformable DETR with multi scale deformable attention modules to solve the problems of DETR: slow convergence and limited feature spatial resolution.<BRK>### SummaryThis paper aims to improve a very recent detection model   DETR, which suffers from two issues: long training time and limited feature spatial resolution. Specifically, comparisons between this module and other "linear"/"efficient" implementations of attention should be performed. Personally, I think the discussion about FPN in main paper is distractive. The authors may want to move all of them into Appendix. Fig.2 shows that Deformable DETR keeps improving over time.